(Contribution||has||Results)
(Results||When predicting||multiple relations)
(multiple relations||in||one - pass)
(multiple relations||have||further 0.8 % improvement)
(further 0.8 % improvement||on||Micro - F1)
(multiple relations||have||0.9 % drop)
(0.9 % drop||on||Macro - F1)
(Results||Note||our method)
(our method||not designed for||domain adaptation)
(our method||still outperforms||methods)
(methods||with||domain adaptation)
(Results||works for||singlerelation per pass setting)
(singlerelation per pass setting||has||performance)
(performance||lags behind using||only indicators)
(only indicators||of||two target entities)
(Results||For||BERT SP with position embeddings)
(BERT SP with position embeddings||test with||two different settings)
(two different settings||so||results)
(results||are||same)
(BERT SP with position embeddings||on||final attention layer)
(BERT SP with position embeddings||train||model)
(model||in||single - relation setting)
(Results||For||BERT SP)
(BERT SP||with||entity indicators)
(entity indicators||on||inputs)
(BERT SP||observed||2 % gap)
(Results||has||Our full model)
(Our full model||with||structured fine - tuning)
(structured fine - tuning||of||attention layers)
(Our full model||brings||further improvement)
(further improvement||in||MRE one - pass setting)
(further improvement||of||about 5.5 %)
(further improvement||achieves||new state - of - the - art performance)
(new state - of - the - art performance||compared to||methods with domain adaptation)
(Results||has||Our Entity - Aware BERT SP)
(Our Entity - Aware BERT SP||gives||comparable results)
(comparable results||with||slightly lower Macro - F1)
(comparable results||with||slightly higher Micro - F1)
(comparable results||to||top - ranked system)
(top - ranked system||in||shared task)
(Results||has||first observation)
(first observation||is that||our model architecture)
(our model architecture||achieves||much better results)
(much better results||compared to||previous state - of - the - art methods)
(Results||has||BERT SP)
(BERT SP||successfully adapt||pre-trained BERT)
(pre-trained BERT||to||MRE task)
(BERT SP||achieves||comparable performance)
(Results||Among||all the BERT - based approaches)
(all the BERT - based approaches||finetuning||off - the - shelf BERT)
(off - the - shelf BERT||does not give||satisfying result)
(Results||compared to||top singlemodel result)
(top singlemodel result||which makes use of||additional word and entity embeddings)
(additional word and entity embeddings||pretrained on||in - domain data)
(additional word and entity embeddings||has||our methods)
(our methods||demonstrate||clear advantage)
(clear advantage||as||single model)
