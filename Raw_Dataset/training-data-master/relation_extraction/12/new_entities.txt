238	7	19	p	observe that
238	20	26	n	adding
238	27	85	n	either attention guided layers or densely connected layers
238	86	94	p	improves
238	99	110	n	performance
238	111	113	p	of
238	118	123	n	model
244	20	44	n	all the C - AGGCN models
244	45	49	p	with
244	50	68	n	varied values of K
244	81	91	p	outperform
244	96	132	n	state - of - the - art C - GCN model
240	8	19	p	notice that
240	24	44	n	feed - forward layer
240	48	60	p	effective in
240	61	70	n	our model
247	33	44	n	performance
247	45	47	p	of
247	48	73	n	C - AGGCN with full trees
247	74	85	p	outperforms
247	86	118	n	all C - AGGCNs with pruned trees
241	0	7	p	Without
241	12	32	n	feed - forward layer
241	39	45	n	result
241	46	54	p	drops to
241	58	74	n	F1 score of 67.8
252	13	38	n	C - AGGCN with full trees
252	39	50	p	outperforms
252	51	90	n	C - AGGCN with pruned trees and C - GCN
252	91	98	p	against
252	99	123	n	various sentence lengths
254	15	26	n	improvement
254	27	38	p	achieved by
254	39	66	n	C - AGGCN with pruned trees
254	67	73	n	decays
254	74	78	p	when
254	83	98	n	sentence length
254	99	108	n	increases
261	0	9	n	C - AGGCN
261	10	34	p	consistently outperforms
261	35	42	n	C - GCN
261	43	48	p	under
261	53	81	n	same amount of training data
257	5	18	p	suggests that
257	19	28	n	C - AGGCN
257	33	50	p	benefit more from
257	51	78	n	larger graphs ( full tree )
262	0	4	p	When
262	9	13	n	size
262	14	16	p	of
262	17	30	n	training data
262	31	40	n	increases
262	50	62	p	observe that
262	67	82	n	performance gap
262	83	90	p	becomes
262	91	103	n	more obvious
263	15	20	p	using
263	21	25	n	80 %
263	26	28	p	of
263	33	46	n	training data
263	53	68	n	C - AGGCN model
263	72	87	p	able to achieve
263	90	99	n	F 1 score
263	100	102	p	of
263	103	107	n	66.5
263	110	121	p	higher than
263	122	129	n	C - GCN
263	130	140	p	trained on
263	145	158	n	whole dataset
182	0	3	p	For
182	4	52	n	cross - sentence n- ary relation extraction task
183	6	32	n	feature - based classifier
183	33	41	p	based on
183	42	67	n	shortest dependency paths
183	68	75	p	between
183	76	92	n	all entity pairs
183	99	130	n	Graph - structured LSTM methods
183	133	142	p	including
183	143	153	n	Graph LSTM
183	156	197	n	bidirectional DAG LSTM ( Bidir DAG LSTM )
183	202	231	n	Graph State LSTM ( GS GLSTM )
184	104	140	n	Graph convolutional networks ( GCN )
184	76	80	p	with
184	146	158	n	pruned trees
49	25	65	n	https://github.com/Cartus / AGGCN_TACRED
174	3	9	p	choose
174	14	31	n	number of heads N
174	32	35	p	for
174	36	58	n	attention guided layer
174	59	63	p	from
174	64	81	n	{ 1 , 2 , 3 , 4 }
174	88	102	n	block number M
174	103	107	p	from
174	108	121	n	{ 1 , 2 , 3 }
174	128	152	n	number of sub - layers L
174	153	155	p	in
174	156	184	n	each densely connected layer
174	185	189	p	from
174	190	203	n	{ 2 , 3 , 4 }
176	0	14	n	Glo Ve vectors
176	19	26	p	used as
176	31	45	n	initialization
176	46	49	p	for
176	50	65	n	word embeddings
32	19	26	p	propose
32	31	93	n	novel Attention Guided Graph Convolutional Networks ( AGGCNs )
32	102	121	p	operate directly on
32	126	135	n	full tree
33	17	24	p	develop
33	27	52	n	" soft pruning " strategy
33	53	68	p	that transforms
33	73	97	n	original dependency tree
33	98	102	p	into
33	105	139	n	fully connected edgeweighted graph
34	6	13	n	weights
34	21	30	p	viewed as
34	35	58	n	strength of relatedness
34	59	66	p	between
34	67	72	n	nodes
34	81	98	p	can be learned in
34	102	124	n	end - to - end fashion
34	125	133	p	by using
34	134	160	n	self - attention mechanism
41	8	17	p	introduce
41	18	35	n	dense connections
41	38	40	p	to
41	45	54	n	GCN model
42	0	3	p	For
42	4	8	n	GCNs
42	11	19	n	L layers
42	44	54	p	to capture
42	55	79	n	neighborhood information
42	80	87	p	that is
42	88	99	n	L hops away
45	0	16	p	With the help of
45	17	34	n	dense connections
45	44	51	p	able to
45	52	57	n	train
45	62	73	n	AGGCN model
45	74	78	p	with
45	81	92	n	large depth
2	50	69	n	Relation Extraction
202	0	3	p	For
202	4	31	n	ternary relation extraction
202	59	74	n	our AGGCN model
202	75	83	p	achieves
202	84	94	n	accuracies
202	95	97	p	of
202	98	111	n	87.1 and 87.0
202	112	114	p	on
202	115	124	n	instances
202	125	131	p	within
202	132	158	n	single sentence ( Single )
202	166	189	n	all instances ( Cross )
202	213	223	p	outperform
202	224	241	n	all the baselines
205	4	30	n	binary relation extraction
205	65	70	n	AGGCN
205	71	95	p	consistently outperforms
205	96	104	n	GS GLSTM
205	109	112	n	GCN
203	24	38	n	AG - GCN model
203	39	48	p	surpasses
203	53	115	n	state - of - the - art Graphstructured LSTM model ( GS GLSTM )
203	116	118	p	by
203	119	137	n	6.8 and 3.8 points
203	138	141	p	for
203	146	171	n	Single and Cross settings
207	0	5	n	AGGCN
207	11	19	p	performs
207	20	26	n	better
207	27	31	p	than
207	32	36	n	GCNs
215	10	25	n	our AGGCN model
215	32	39	p	obtains
215	40	58	n	8.0 and 5.7 points
215	59	70	p	higher than
215	75	89	n	GS GLSTM model
215	90	93	p	for
215	94	122	n	ternary and binary relations
216	20	29	n	our AGGCN
216	30	38	p	achieves
216	41	61	n	better test accuracy
216	62	66	p	than
216	67	81	n	all GCN models
227	4	19	n	C - AGGCN model
227	20	28	p	achieves
227	32	40	n	F1 score
227	41	43	p	of
227	44	48	n	68.2
227	57	68	p	outperforms
227	73	100	n	state - ofart C - GCN model
227	101	103	p	by
227	104	114	n	1.8 points
229	4	19	n	performance gap
229	20	27	p	between
229	28	77	n	GCNs with pruned trees and AGGCNs with full trees
229	78	94	p	empirically show
229	104	115	n	AGGCN model
229	119	143	p	better at distinguishing
229	144	180	n	relevant from irrelevant information
229	181	193	p	for learning
229	196	223	n	better graph representation
204	0	11	p	Compared to
204	12	22	n	GCN models
204	25	34	n	our model
204	35	42	p	obtains
204	43	68	n	1.3 and 1.2 points higher
204	69	73	p	than
204	78	99	n	best performing model
204	100	104	p	with
204	105	124	n	pruned tree ( K=1 )
228	8	14	p	notice
228	20	39	n	AGGCN and C - AGGCN
228	40	47	p	achieve
228	48	82	n	better precision and recall scores
228	83	87	p	than
228	88	103	n	GCN and C - GCN
230	27	29	p	on
230	34	49	n	SemEval dataset
233	0	28	n	Our C - AGGCN model ( 85.7 )
233	29	53	p	consistently outperforms
233	58	80	n	C - GCN model ( 84.8 )
233	83	90	p	showing
233	95	116	n	good generalizability
