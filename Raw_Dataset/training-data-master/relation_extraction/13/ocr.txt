1906.03158v1 [cs.CL] 7 Jun 2019

arXiv

Matching the Blanks: Distributional Similarity for Relation Learning

Livio Baldini Soares

Nicholas FitzGerald

Jeffrey Ling* Tom Kwiatkowski

Google Research

{liviobs,nfitz, jeffreyling, tomkwiat }@google.com

Abstract

General purpose relation extractors, which can
model arbitrary relations, are a core aspiration
in information extraction. Efforts have been
made to build general purpose extractors that
represent relations with their surface forms, or
which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in
their ability to generalize. In this paper, we
build on extensions of Harris’ distributional
hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation
representations solely from entity-linked text.
We show that these representations significantly outperform previous work on exemplar
based relation extraction (FewRel) even without using any of that task’s training data. We
also show that models initialized with our task
agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED.

1 Introduction

Reading text to identify and extract relations between entities has been a long standing goal in
natural language processing (Cardie, 1997). Typically efforts in relation extraction fall into one of
three groups. In a first group, supervised (Kambhatla, 2004; GuoDong et al., 2005; Zeng et al.,
2014), or distantly supervised relation extractors
(Mintz et al., 2009) learn a mapping from text
to relations in a limited schema. Forming a second group, open information extraction removes
the limitations of a predefined schema by instead
representing relations using their surface forms
(Banko et al., 2007; Fader et al., 2011; Stanovsky
et al., 2018), which increases scope but also leads

“Work done as part of the Google AI residency.

to an associated lack of generality since many surface forms can express the same relation. Finally,
the universal schema (Riedel et al., 2013) embraces both the diversity of text, and the concise
nature of schematic relations, to build a joint representation that has been extended to arbitrary textual input (Toutanova et al., 2015), and arbitrary
entity pairs (Verga and McCallum, 2016). However, like distantly supervised relation extractors,
universal schema rely on large knowledge graphs
(typically Freebase (Bollacker et al., 2008)) that
can be aligned to text.

Building on Lin and Pantel (2001)’s extension
of Harris’ distributional hypothesis (Harris, 1954)
to relations, as well as recent advances in learning
word representations from observations of their
contexts (Mikolov et al., 2013; Peters et al., 2018;
Devlin et al., 2018), we propose a new method
of learning relation representations directly from
text. First, we study the ability of the Transformer
neural network architecture (Vaswani et al., 2017)
to encode relations between entity pairs, and we
identify a method of representation that outperforms previous work in supervised relation extraction. Then, we present a method of training this relation representation without any supervision from
a knowledge graph or human annotators by matching the blanks.

[BLANK], inspired by Cale’s earlier cover, recorded one
of the most acclaimed versions of “[BLANK]”

[BLANK]’s rendition of “[BLANK]” has been called
“one of the great songs” by Time, and is included on
Rolling Stone’s list of “The 500 Greatest Songs of All
Time”.

Figure 1: “Matching the blanks” example where both relation statements share the same two entities.

Following Riedel et al. (2013), we assume access to a corpus of text in which entities have been
linked to unique identifiers and we define a relation statement to be a block of text containing two
marked entities. From this, we create training data
that contains relation statements in which the entities have been replaced with a special [BLANK]
symbol, as illustrated in Figure 1. Our training
procedure takes in pairs of blank-containing relation statements, and has an objective that encourages relation representations to be similar if they
range over the same pairs of entities. After training, we employ learned relation representations
to the recently released FewRel task (Han et al.,
2018) in which specific relations, such as ‘original language of work’ are represented with a few
exemplars, such as The Crowd (Italian: La Folla)
is a 1951 Italian film. Han et al. (2018) presented
FewRel as a supervised dataset, intended to evaluate models’ ability to adapt to relations from new
domains at test time. We show that through training by matching the blanks, we can outperform
Han et al. (2018)’s top performance on FewRel,
without having seen any of the FewRel training
data. We also show that a model pre-trained by
matching the blanks and tuned on FewRel outperforms humans on the FewRel evaluation. Similarly, by training by matching the blanks and then
tuning on labeled data, we significantly improve
performance on the SemEval 2010 Task 8 (Hendrickx et al., 2009), KBP-37 (Zhang and Wang,
2015), and TACRED (Zhang et al., 2017) relation
extraction benchmarks.

2 Overview

Task definition In this paper, we focus on learning mappings from relation statements to relation
representations. Formally, let x = [x9...%p|
be a sequence of tokens, where xp = [CLS] and
Xn = [SEP] are special start and end markers. Let
Ss; = (i,j) and sg = (k,l) be pairs of integers
such thatO <i<j-—-1,j<k,k <l—1l,andl <
n. A relation statement is a triple r = (x, 81,82),
where the indices in s; and sg delimit entity mentions in x: the sequence |;...2;~1| mentions
an entity, and so does the sequence |x, ... xj_1].
Our goal is to learn a function h, = fg(r) that
maps the relation statement to a fixed-length vector h, € R¢ that represents the relation expressed
in x between the entities marked by s; and sg.

Contributions This paper contains two main
contributions. First, in Section 3.1 we investigate
different architectures for the relation encoder fo,
all built on top of the widely used Transformer se
quence model (Devlin et al., 2018; Vaswani et al.,
2017). We evaluate each of these architectures
by applying them to a suite of relation extraction
benchmarks with supervised training.

Our second, more significant, contribution—
presented in Section 4—is to show that fg can be
learned from widely available distant supervision
in the form of entity linked text.

3 Architectures for Relation Learning

The primary goal of this work is to develop models
that produce relation representations directly from
text. Given the strong performance of recent deep
transformers trained on variants of language modeling, we adopt Devlin et al. (2018)’s BERT model
as the basis for our work. In this section, we explore different methods of representing relations
with the Transformer model.

3.1 Relation Classification and Extraction
Tasks

We evaluate the different methods of representation on a suite of supervised relation extraction
benchmarks. The relation extractions tasks we use
can be broadly categorized into two types: fully
supervised relation extraction, and few-shot relation matching.

For the supervised tasks, the goal is to, given a
relation statement r, predict a relation type t € 7
where 7 is a fixed dictionary of relation types and
t = 0 typically denotes a lack of relation between
the entities in the relation statement. For this type
of task we evaluate on SemEval 2010 Task 8 (Hendrickx et al., 2009), KBP-37 (Zhang and Wang,
2015) and TACRED (Zhang et al., 2017). More
formally,

In the case of few-shot relation matching, a set
of candidate relation statements are ranked, and
matched, according to a query relation statement.
In this task, examples in the test and development
sets typically contain relation types not present in
the training set. For this type of task, we evaluate on the FewRel (Han et al., 2018) dataset.
Specifically, we are given K sets of N labeled
relation statements S, = {(ro,to)...(ra,tn)}
where t; € {1... A} is the corresponding relation
type. The goal is to predict the t, € {1... A} for
a query relation statement rg.
Similarity score

 
  
   

Oo 9000 jo@__@e@|
Norm Layer Norm Layer

     
   

Per class
representation

Relation Query Relation Candidate Relation
Statement Statement Statement

Figure 2: Illustration of losses used in our models. The left figure depicts a model suitable for supervised training, where the
model is expected to classify over a predefined dictionary of relation types. The figure on the right depicts a pairwise similarity
loss used for few-shot classification task.

 

 
 

i

| Deep Transformer (BERT)

 

Deep Transformer (BERT) Deep Transformer (BERT)

 

   
 

 
 

 
 

 

 

 

 

 

 

 

 

 

: ‘ \ ef HERE ED
[CLS] Entity1 ... ... Entity2.... [SEP] [CLS] Entity1 ... ... Entity2.... [SEP] embeddings [0 HAao 2A a a
[CLS] ... Entity1 ... ... Entity 2 ... [SEP]
(a) STANDARD — [CLS] (b) STANDARD — MENTION POOLING (c) POSITIONAL EMB. — MENTION POOL.

          

Deep Transformer (BERT) Deep Transformer (BERT) Deep Transformer (BERT)
[| [| a

[CLS] [E1] Entity 1 [/E1] ... ... [E2] Entity 2 [/E2] [SEP] [CLS] [E1] Entity 17 [/E1] ... ... [E2] Entity 2 [/E2] [SEP] [CLS] [E1] Entity 1 [/E1] ... ... [E2] Entity 2 [/E2] [SEP]

(d) ENTITY MARKERS —[CLS] __ (€) ENTITY MARKERS — MENTION POOL. (f) ENTITY MARKERS — ENTITY START

Figure 3: Variants of architectures for extracting relation representations from deep Transformers network. Figure (a) depicts
a model with STANDARD input and [CLS] output, Figure (b) depicts a model with STANDARD input and MENTION
POOLING output and Figure (c) depicts a model with POSITIONAL EMBEDDINGS input and MENTION POOLING output. Figures
(d), (e), and (f) use ENTITY MARKERS input while using [CLS], MENTION POOLING, and ENTITY START output, respectively.

SemEval 2010 KBP37 TACRED FewRel
Task 8 5-way-1-shot

# training annotated examples 8,000 (6,500 for dev) 15,916 68,120 44,800
19

 

100
DevFI_ Test Fl
Wang et al. (2016)*
Zhang and Wang (2015)* — 79.6 — 58.8 - - 
 

Bilan and Roth (2018)* — 84.8 — — — 68.2 —
Han et al. (2018) — —

STANDARD

 

 

[CLS] 812
ENTITY START $2.1 $9.2 70 68.3 70.1 70.1

 

 

 

Table 1: Results for supervised relation extraction tasks. Results on rows where the model name is marked with a * symbol
are reported as published, all other numbers have been computed by us. SemEval 2010 Task 8 does not establish a default split
for development; for this work we use a random slice of the training set with 1,500 examples.
3.2 Relation Representations from Deep
Transformers Model

In all experiments in this section, we start with
the BERT;arce model made available by Devlin
et al. (2018) and train towards task-specific losses.
Since BERT has not previously been applied to the
problem of relation representation, we aim to answer two primary modeling questions: (1) how
do we represent entities of interest in the input to
BERT, and (2) how do we extract a fixed length
representation of a relation from BERT’s output.
We present three options for both the input encoding, and the output relation representation. Six
combinations of these are illustrated in Figure 3.

3.2.1 Entity span identification

Recall, from Section 2, that the relation statement
r = (x,8S1,S2) contains the sequence of tokens
x and the entity span identifiers s; and s9. We
present three different options for getting information about the focus spans s; and sg into our
BERT encoder.

Standard input First we experiment with a
BERT model that does not have access to any explicit identification of the entity spans s; and so.
We refer to this choice as the STANDARD input.
This is an important reference point, since we believe that BERT has the ability to identify entities
in x, but with the STANDARD input there is no way
of knowing which two entities are in focus when
x contains more than two entity mentions.

Positional embeddings For each of the tokens
in its input, BERT also adds a segmentation embedding, primarily used to add sentence segmentation information to the model. To address the
STANDARD representation’s lack of explicit entity
identification, we introduce two new segmentation
embeddings, one that is added to all tokens in the
span s;, while the other is added to all tokens in
the span sg. This approach is analogous to previous work where positional embeddings have been
applied to relation extraction (Zhang et al., 2017;
Bilan and Roth, 2018).

Entity marker tokens Finally, we augment x
with four reserved word pieces to mark the begin and end of each entity mention in the relation
statement. We introduce the |F 1 start], [Elena],

[FE 2 start] and [F2-,4] and modify x to give

~

x =|x0 wee [El start] Ui. Lj] [Elena]
woe |E2 start] Up... XI] | Fenda] - Lea,

and we feed this token sequence into BERT instead
of x. We also update the entity indices $; = (7 +
1,7 +1) and Sz = (K+ 3,143) to account for the
inserted tokens. We refer to this representation of
the input as ENTITY MARKERS.

3.3. Fixed length relation representation

We now introduce three separate methods of extracting a fixed length relation representation h,
from the BERT encoder. The three variants rely on
extracting the last hidden layers of the transformer
network, which we define as H = |{ho,...h,]| for
n = |x| (or |x| if entity marker tokens are used).

[CLS] token Recall from Section 2 that each x
starts with a reserved [CLS] token. BERT’s output state that corresponds to this token is used by
Devlin et al. (2018) as a fixed length sentence representation. We adopt the [CLS] output, ho, as our
first relation representation.

Entity mention pooling We obtain h, by maxpooling the final hidden layers corresponding to
the word pieces in each entity mention, to get two
vectors he, = MAXPOOL((hj...h;_1]) and h,, =
MAXPOOL(|hy...hy_1]) representing the two entity mentions. We concatenate these two vectors
to get the single representation h, = (he, |h-,)
where (a|b) is the concatenation of a and b. We
refer to this architecture as MENTION POOLING.

Entity start state Finally, we propose simply
representing the relation between two entities with
the concatenation of the final hidden states corresponding their respective start tokens, when ENTITY MARKERS are used. Recalling that ENTITY
MARKERS inserts tokens in x, creating offsets in
S; and sg, our representation of the relation is
ry, = (h;{h;+.2). We refer to this output representation as ENTITY START output. Note that this can
only be applied to the ENTITY MARKERS input.
Figure 3 illustrates a few of the variants we evaluated in this section. In addition to defining the
model input and output architecture, we fix the
training loss used to train the models (which is
illustrated in Figure 2). In all models, the output representation from the Transformer network
is fed into a fully connected layer that either (1)
contains a linear activation, or (2) performs layer
normalization (Ba et al., 2016) on the representation. We treat the choice of post Transfomer layer
as a hyper-parameter and use the best performing
layer type for each task.

For the supervised tasks, we introduce a new
classification layer WW € R**4 where H is the
size of the relation representation and Kk is the
number of relation types. The classification loss
is the standard cross entropy of the softmax of
h,W with respect to the true relation type.

For the few-shot task, we use the dot product
between relation representation of the query statement and each of the candidate statements as a
similarity score. In this case, we also apply a cross
entropy loss of the softmax of similarity scores
with respect to the true class.

We perform task-specific fine-tuning of the
BERT model, for all variants, with the following
set of hyper-parameters:

e Transformer Architecture: 24 layers, 1024 hidden

size, 16 heads

e Weight Initialization: BERT LARGE

e Post Transformer Layer: Dense with linear activation

(KBP-37 and TACRED), or Layer Normalization layer
(SemEval 2010 and FewRel).

e Training Epochs: | to 10

e Learning Rate (supervised): 3e-5 with Adam

e Batch Size (supervised): 64

e Learning Rate (few shot): le-4 with SGD

e Batch Size (few shot): 256

Table 1 shows the results of model variants on
the three supervised relation extraction tasks and
the 5-way-1-shot variant of the few-shot relation
classification task. For all four tasks, the model
using the ENTITY MARKERS input representation
and ENTITY START output representation achieves
the best scores.

From the results, it is clear that adding positional information in the input is critical for the
model to learn useful relation representations. Unlike previous work that have benefited from positional embeddings (Zhang et al., 2017; Bilan and
Roth, 2018), the deep Transformers benefits the
most from seeing the new entity boundary word
pieces (ENTITY MARKERS). It is also worth noting that the best variant outperforms previous published models on all four tasks. For the remainder
of the paper, we will use this architecture when
further training and evaluating our models.

4 Learning by Matching the Blanks

So far, we have used human labeled training data
to train our relation statement encoder fg. Inspired

by open information extraction (Banko et al.,
2007; Angeli et al., 2015), which derives relations
directly from tagged text, we now introduce a new
method of training fg without a predefined ontology, or relation-labeled training data. Instead, we
declare that for any pair of relation statements r
and r’, the inner product fg(r)' fg(r’) should be
high if the two relation statements, r and r’, express semantically similar relations. And, this inner product should be low if the two relation statements express semantically different relations.

Unlike related work in distant supervision for
information extraction (Hoffmann et al., 2011;
Mintz et al., 2009), we do not use relation labels
at training time. Instead, we observe that there
is a high degree of redundancy in web text, and
each relation between an arbitrary pair of entities
is likely to be stated multiple times. Subsequently,
r = (X,81,S2) is more likely to encode the same
semantic relation as r’ = (x’,s},85) if s; refers
to the same entity as Si; and Ss» refers to the same
entity as s,. Starting with this observation, we introduce a new method of learning fg from entity
linked text. We introduce this method of learning by matching the blanks (MTB). In Section 5
we show that MTB learns relation representations
that can be used without any further tuning for relation extraction—even beating previous work that
trained on human labeled data.

4.1 Learning Setup

Let € be a predefined set of entities. And let
D = [(r°, ef, e9)... (x, et’, e5’)] be a corpus of
relation statements that have been labeled with two
entities e/ € E€ and e}, € E. Recall, from Section 2,
that r’? = (x’, s,s), where s! and s}, delimit entity mentions in x’. Each item in D is created by
pairing the relation statement r’ with the two entities e} and e}, corresponding to the spans si and
si, respectively.

We aim to learn a relation statement encoder fg
that we can use to determine whether or not two
relation statements encode the same relation. To
do this, we define the following binary classifier

1

p(l = Ir, r’) = ——______~
UTED TF exp Jol) fol)

to assign a probability to the case that r and r’ encode the same relation (J = 1), or not (J = 0).
We will then learn the parameterization of fg that
In 1976, e1 (then of Bell Labs) published eg, the first of his books on programming inspired by the Unix operating
system.
r The “e2” series spread the essence of “C/Unix thinking” with makeovers for Fortran and Pascal. e1’s Ratfor was
B eventually put in the public domain.

e, worked at Bell Labs alongside e3 creators Ken Thompson and Dennis Ritchie.

 

e1 = Brian Kernighan, e2 = Software Tools, e3 = Unix

Table 2: Example of “matching the blanks” automatically generated training data. Statement pairs r4 and rg form a positive
example since they share resolution of two entities. Statement pairs 74 and rc as well as rg and rc form strong negative pairs
since they share one entity in common but contain other non-matching entities.

minimizes the loss

L(D) = “De, S Ss" (1)

r,e1,€2)€D (r’,e4,,e5) ED

Jer ,e4 Pea,eh ‘log p(l = Lr, r')+

(1 a be1,€/, Je2,e4) " log(1 - p(l — Lr, r’))

where d¢.¢/ is the Kronecker delta that takes the
value 1 iff e = e’, and 0 otherwise.

4.2 Introducing Blanks

Readers may have noticed that the loss in Equation 1 can be minimized perfectly by the entity
linking system used to create D. And, since this
linking system does not have any notion of relations, it is not reasonable to assume that fg will
somehow magically build meaningful relation representations. To avoid simply relearning the entity
linking system, we introduce a modified corpus

D = [(r°,e7, €9)...(#,e7,€3'))

where each ¢’ = (X',s',s5) contains a relation statement in which one or both entity
mentions may have been replaced by a special
[BLANK] symbol. Specifically, x contains the
span defined by s; with probability a. Otherwise, the span has been replaced with a single
[BLANK] symbol. The same is true for sg. Only
a? of the relation statements in D explicitly name
both of the entities that participate in the relation.
As a result, minimizing £(D) requires fg to do
more than simply identifying named entities in r.
We hypothesize that training on D will result in a
fg that encodes the semantic relation between the
two possibly elided entity spans. Results in Section 5 support this hypothesis.

4.3. Matching the Blanks Training

To train a model with matching the blank task, we
construct a training setup similar to BERT, where
two losses are used concurrently: the masked language model loss and the matching the blanks

loss. For generating the training corpus, we
use English Wikipedia and extract text passages
from the HTML paragraph blocks, ignoring lists,
and tables. We use an off-the-shelf entity linking system! to annotate text spans with a unique
knowledge base identifier (e.g., Freebase ID or
Wikipedia URL). The span annotations include
not only proper names, but other referential entities such as common nouns and pronouns. From
this annotated corpus we extract relation statements where each statement contains at least two
grounded entities within a fixed sized window of
tokens’. To prevent a large bias towards relation statements that involve popular entities, we
limit the number of relation statements that contain the same entity by randomly sampling a constant number of relation statements that contain
any given entity.

We use these statements to train model parameters to minimize £(D) as described in the previous section. In practice, it is not possible to compare every pair of relation statements, as in Equation 1, and so we use a noise-contrastive estimation (Gutmann and Hyvarinen, 2012; Mnih and
Kavukcuoglu, 2013). In this estimation, we consider all positive pairs of relation statements that
contain the same entity, so there is no change to the
contribution of the first term in Equation 1—where
0¢1,e4 e9,e, = 1. The approximation does, however, change the contribution of the second term.

Instead of summing over all pairs of relation
statements that do not contain the same pair of entities, we sample a set of negatives that are either
randomly sampled uniformly from the set of all
relation statement pairs, or are sampled from the
set of relation statements that share just a single

'We use the public Google Cloud Natural Language
API to annotate our corpus extracting the “entity analysis” results — https://cloud.google.com/natural-language/
docs/basics#entity_analysis .

“We use a window of 40 tokens, which we observed provides some coverage of long range entity relations, while
avoiding a large number of co-occurring but unrelated entities.
5-way | 5-way | 10-way | 10-way
1-shot | 5-shot | 1-shot 5-shot

69.2 | 8479 | 5644 | 7555 _|

[____Semfval 2010 [KBP37 | TACRED |

  

(Human [9222 | — | 8588 | —

Table 3: Test results for FewRel few-shot relation classification task. Proto Net is the best published system from
Han et al. (2018). At the time of writing, our BERTEM+MTB
model outperforms the top model on the leaderboard (http:
//www.zhuhao.me/fewrel/) by over 10% on the 5-way-1-shot
and over 15% on the 10-way-1-shot configurations.

entity. We include the second set ‘hard’ negatives
to account for the fact that most randomly sampled relation statement pairs are very unlikely to
be even remotely topically related, and we would
like to ensure that the training procedure sees pairs
of relation statements that refer to similar, but different, relations. Finally, we probabilistically replace each entity’s mention with [BLANK] symbols, with a probability of a = 0.7, as described
in Section 3.2, to ensure that the model is not
confounded by the absence of [BLANK] symbols
in the evaluation tasks. In total, we generate
600 million relation statement pairs from English
Wikipedia, roughly split between 50% positive
and 50% strong negative pairs.

5 Experimental Evaluation

In this section, we evaluate the impact of training by matching the blanks. We start with the
best BERT based model from Section 3.3, which
we call BERTgm, and we compare this to a variant that is trained with the matching the blanks
task (BERTEM+MTB). We train the BERT gyw+MTB
model by initializing the Transformer weights to
the weights from BERTLArGcE and use the following parameters:

e Learning rate: 3e-5 with Adam

e Batch size: 2,048

e Number of steps: 1 million

e Relation representation: ENTITY MARKER

We report results on all of the tasks from
Section 3.1, using the same _task-specific
training methodology for both BERTgy and
BERTEM+MTB.

5.1 Few-shot Relation Matching

First, we investigate the ability of BERTEM+MTB
to solve the FewRel task without any task-specific
training data. Since FewRel is an exemplar-based
approach, we can just rank each candidate rela
Table 4: F1 scores of BERTgEm+MTB and BERT em based relation classifiers on the respective test sets. Details of the
SOTA systems are given in Table 1.

tion statement according to its representation’s inner product with the exemplars’ representations.
Figure 4 shows that the task agnostic BERTEM
and BERTgm+MTB models outperform the previous published state of the art on FewRel task even
when they have not seen any FewRel training data.
For BERTEM+MTB, the increase over Han et al.
(2018)’s supervised approach is very significant—
8.8% on the 5-way-1-shot task and 12.7% on the
10-way-1l-shot task. BERTEM+MTB also significantly outperforms BERTg»y in this unsupervised
setting, which is to be expected since there is no
relation-specific loss during BERTgm’s training.
To investigate the impact of supervision on
BERTgem and BERTEM+MTB, we introduce increasing amounts of FewRel’s training data. Figure 4 shows the increase in performance as we e1ther increase the number of training examples for
each relation type, or we increase the number of
relation types in the training data. When given access to all of the training data, BERTEy approaches
BERTEM+MTB’s performance. However, when we
keep all relation types during training, and vary the
number of types per example, BERT—EM+MTB only
needs 6% of the training data to match the performance of a BERTEM model trained on all of the
training data. We observe that maintaining a diversity of relation types, and reducing the number
of examples per type, is the most effective way to
reduce annotation effort for this task. The results
in Figure 4 show that MTB training could be used
to significantly reduce effort in implementing an
exemplar based relation extraction system.
Finally, we report BERTgEyMj+MTB’s performance
on all of FewRel’s fully supervised tasks in Table 3. We see that it outperforms the human upper
bound reported by Han et al. (2018), and it significantly outperforms all other submissions to the
FewRel leaderboard, published or unpublished.

5.2 Supervised Relation Extraction

Table 4 contains results for our classifiers tuned on
supervised relation extraction data. As was established in Section 3.2, our BERTgm based classifiers
x BERTem @ BERTeEmt+MTB

Accuracy

 

0 5 10 20 40 80 160 320 700
examples per relation type (log scale)

5 way 1 shot

examples pertypey 0 | 5 | 20] 80 [320] 700
- ProtNet. (CNN) | - [| - | —|— | — [716
88.9
90.1
10 way 1 shot
Fexamples pertype[| 0 | 5 | 20 | 80 [320]
Prot.Net. (CNN) — — — —
BERTEM
BERTEM+MTB

 

   
   
   
      
  

 

  
 

76.9 | 79.0 | 81.4 | 82.8
81.2 | 82.9 | 83.7 | 83.4

 

 

 

   

 

x BERTem @® BERTEm+MTB

85

80

75

70

Accuracy

65

 

 

60
0 20 40 60

number of relation types

ProtNet. CNNY| — | — | — | — [71.6]

30.4 | 84.04 86.8 [90.1

Prot.Net. (CNN) | — - — — |58.8
BERTEM 62.3 | 68.9 | 71.9 | 74.3 | 81.4
BERTeEM+MTB |71.5| 76.2 | 76.9 | 78.5 | 83.7

 

     

 

 

 

 

 

Figure 4: Comparison of classifiers tuned on FewRel. Results are for the development set while varying the amount of
annotated examples available for fine-tuning. On the left, we display accuracies while varying the number of examples per
relation type, while maintaining all 64 relations available for training. On the right, we display accuracy on the development
set of the two models while varying the total number of relation types available for tuning, while maintaining all 700 examples
per relation type. In both graphs, results for the 10-way-1-shot variant of the task are displayed.

100%

SemEval 2010Task8[
P_BERTem ‘| 286] 66.9 | 75.5 [803 | 82.1

KBP-37

SSSCS—~S

BERTEM+MTB 44.2 | 66.3 | 67.2 | 68.8 | 70.3

TACRED

 

 

SS SS—~SY
BERT [32.8] 59-6 | 65.6 | 9.0 | 7.1
BERT ew tMTB [43.4] 648 | 67.2 [69.9 | 70.6

 

Table 5: F1 scores on development sets for supervised relation extraction tasks while varying the amount of tuning data
available to our BERTEm and BERTEm+MTB models.

outperform previously published results for these
three tasks. The additional MTB based training
further increases F1 scores for all tasks.

We also analyzed the performance of our two
models while reducing the amount of supervised
task specific tuning data. The results displayed
in Table 5 show the development set performance when tuning on a random subset of the
task specific training data. For all tasks, we see
that MTB based training is even more effective
for low-resource cases, where there is a larger
gap in performance between our BERTgy and
BERTEM+MTB based classifiers. This further supports our argument that training by matching the
blanks can significantly reduce the amount of human input required to create relation extractors,

and populate a knowledge base.

6 Conclusion and Future Work

In this paper we study the problem of producing
useful relation representations directly from text.
We describe a novel training setup, which we call
matching the blanks, which relies solely on entity resolution annotations. When coupled with
a new architecture for fine-tuning relation representations in BERT, our models achieves state-ofthe-art results on three relation extraction tasks,
and outperforms human accuracy on few-shot relation matching. In addition, we show how the
new model is particularly effective in low-resource
regimes, and we argue that it could significantly
reduce the amount of human effort required to create relation extractors.

In future work, we plan to work on relation discovery by clustering relation statements
that have similar representations according to
BERTEmM+MTB. This would take us some of the
way toward our goal of truly general purpose relation identification and extraction. We will also
study representations of relations and entities that
can be used to store relation triples in a distributed
knowledge base. This is inspired by recent work in
knowledge base embedding (Bordes et al., 2013;
Nickel et al., 2016).
References

Gabor Angeli, Melvin Jose Johnson Premkumar, and
Christopher D Manning. 2015. Leveraging linguistic structure for open domain information extraction.
In Proceedings of the 53rd Annual Meeting of the
Association for Computational Linguistics and the
7th International Joint Conference on Natural Language Processing, volume 1, pages 344-354.

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint
arXiv: 1607.06450.

Michele Banko, Michael J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proceedings of the 20th International Joint Conference
on Artifical Intelligence, ISCAI’07, pages 2670-—
2676, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.

Ivan Bilan and Benjamin Roth. 2018. Position-aware
self-attention with relative positional encodings for
slot filling. CoRR, abs/1807.03052.

Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring
human knowledge. In Proceedings of the 2008
ACM SIGMOD International Conference on Management of Data, SIGMOD ’08, pages 1247-1250,
New York, NY, USA. ACM.

Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multirelational data. In C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger,
editors, Advances in Neural Information Processing
Systems 26, pages 2787-2795. Curran Associates,
Inc.

Claire Cardie. 1997. Empirical methods in information
extraction. Al Magazine, 18(4):65-80.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. BERT: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv: 1810.04805.

Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information extraction. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing, pages 1535-1545, Edinburgh, Scotland, UK.
Association for Computational Linguistics.

Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation extraction. In Proceedings of the 43rd annual meeting
on association for computational linguistics, pages
427-434. Association for Computational Linguistics.

Michael U Gutmann and Aapo Hyvarinen. 2012.
Noise-contrastive estimation of unnormalized statistical models, with applications to natural image
statistics. Journal of Machine Learning Research,
13(Feb):307-361.

Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,
Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A
large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4803—
4809.

Zellig S Harris. 1954. Distributional structure. Word,
10(2-3):146-162.

Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O Séaghdha, Sebastian
Pad6, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009. Semeval-2010 task 8:
Multi-way classification of semantic relations between pairs of nominals. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 94-99.
Association for Computational Linguistics.

Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledgebased weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, pages 541-550. Association for Computational Linguistics.

Nanda Kambhatla. 2004. Combining lexical, syntactic,
and semantic features with maximum entropy models for extracting relations. In Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions, page 22. Association for Computational
Linguistics.

Dekang Lin and Patrick Pantel. 2001. DIRT: Discovery
of Inference Rules from Text. In Proceedings of the
Seventh ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD’01),
pages 323-328, New York, NY, USA. ACM Press.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing
systems, pages 3111-3119.

Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 1003-1011, Suntec, Singapore. Association
for Computational Linguistics.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Advances in neural information processing systems, pages 2265-2273.

Maximilian Nickel, Lorenzo Rosasco, and Tomaso
Poggio. 2016. Holographic embeddings of know]edge graphs. In Proceedings of the Thirtieth AAAI
Conference on Artificial Intelligence, AAAI 16,
pages 1955-1961. AAAI Press.

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long Papers), volume 1,
pages 2227-2237.

Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M Marlin. 2013. Relation extraction with
matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
74-84.

Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer,
and Ido Dagan. 2018. Supervised open information
extraction. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume I (Long Papers), pages 885-—
895, New Orleans, Louisiana. Association for Computational Linguistics.

Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon.
2015. Representing text for joint embedding of text
and knowledge bases. In Proceedings of the 2015
Conference on Empirical Methods in Natural Language Processing, pages 1499-1509, Lisbon, Portugal. Association for Computational Linguistics.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing Systems, pages 5998-6008.

Patrick Verga and Andrew McCallum. 2016. Row-less
universal schema. In Proceedings of the 5th Workshop on Automated Knowledge Base Construction,
pages 63-68, San Diego, CA. Association for Computational Linguistics.

Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan
Liu. 2016. Relation classification via multi-level attention cnns. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics, pages 1298-1307. Association for Computational Linguistics.

Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In Proceedings of
COLING 2014, the 25th International Conference
on Computational Linguistics: Technical Papers,
pages 2335-2344. Dublin City University and Association for Computational Linguistics.

Dongxu Zhang and Dong Wang. 2015. Relation classification via recurrent neural network. CoRR,
abs/1508.01006.

Yuhao Zhang, Victor Zhong, Danqgi Chen, Gabor Angeli, and Christopher D Manning. 2017. Positionaware attention and supervised data improve slot filling. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,
pages 35-45.
