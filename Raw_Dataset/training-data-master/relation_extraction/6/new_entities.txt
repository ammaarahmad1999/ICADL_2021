99	16	29	p	trained using
99	34	48	n	Adam optimizer
99	49	53	p	with
99	54	78	n	categorical crossentropy
99	79	81	p	as
99	86	99	n	loss function
100	3	6	p	use
100	10	34	n	early stopping criterion
100	35	37	p	on
100	42	57	n	validation data
100	58	70	p	to determine
100	75	100	n	number of training epochs
101	4	17	n	learning rate
101	21	29	p	fixed to
101	30	34	n	0.01
101	151	159	n	training
101	163	175	p	performed in
101	176	183	n	batches
101	48	50	p	of
101	187	200	n	128 instances
102	3	8	p	apply
102	9	16	n	Dropout
102	17	19	p	on
102	24	41	n	penultimate layer
102	42	55	p	as well as on
102	60	76	n	embeddings layer
102	77	81	p	with
102	84	95	n	probability
102	96	98	p	of
102	99	102	n	0.5
103	3	9	p	choose
103	14	18	n	size
103	99	103	p	with
103	106	119	n	random search
103	120	122	p	on
103	127	141	n	validation set
103	19	21	p	of
103	26	59	n	layers ( RNN layer size o = 256 )
20	3	10	p	present
20	13	31	n	novel architecture
20	37	46	p	considers
20	47	62	n	other relations
20	63	65	p	in
20	70	78	n	sentence
20	79	81	p	as
20	84	91	n	context
20	92	106	p	for predicting
20	111	116	n	label
20	117	119	p	of
20	124	139	n	target relation
22	4	16	n	architecture
22	17	21	p	uses
22	25	45	n	LSTM - based encoder
22	46	62	p	to jointly learn
22	63	78	n	representations
22	79	82	p	for
22	83	96	n	all relations
22	97	99	p	in
22	102	117	n	single sentence
23	87	90	p	are
23	91	99	n	combined
23	4	21	p	representation of
23	26	41	n	target relation
23	46	64	p	representations of
23	69	86	n	context relations
23	100	107	p	to make
23	112	128	n	final prediction
2	36	70	n	Knowledge Base Relation Extraction
4	24	60	n	sentence - level relation extraction
12	17	36	n	relation extraction
13	32	62	n	sentential relation extraction
113	4	10	n	models
113	11	20	p	that take
113	25	32	n	context
113	33	37	p	into
113	38	45	n	account
113	46	53	p	perform
113	54	61	n	similar
113	62	64	p	to
113	69	78	n	baselines
113	79	81	p	at
113	86	109	n	smallest recall numbers
113	116	121	n	start
113	122	124	p	to
113	125	143	n	positively deviate
113	154	156	p	at
113	157	176	n	higher recall rates
114	20	36	n	ContextAtt model
114	37	45	p	performs
114	46	52	n	better
114	53	57	p	than
114	58	74	n	any other system
114	88	92	p	over
114	97	116	n	entire recall range
115	0	11	p	Compared to
115	16	43	n	competitive LSTM - baseline
115	86	102	n	ContextAtt model
115	103	111	p	achieves
115	114	128	n	24 % reduction
115	129	131	p	of
115	136	149	n	average error
118	0	5	p	shows
118	15	31	n	ContextAtt model
118	32	40	p	performs
118	41	45	n	best
118	46	50	p	over
118	51	69	n	all relation types
119	13	21	p	see that
119	26	36	n	ContextSum
119	37	69	p	does n't universally outperforms
119	74	89	n	LSTM - baseline
120	27	32	p	using
120	33	42	n	attention
120	46	64	p	crucial to extract
120	65	85	n	relevant information
120	86	90	p	from
120	95	112	n	context relations
121	38	45	p	observe
121	55	78	n	context - enabled model
121	79	91	p	demonstrates
121	96	112	n	most improvement
121	113	115	p	on
121	116	125	n	precision
121	153	163	p	useful for
121	164	182	n	taxonomy relations
