157	3	13	p	initialize
157	18	25	n	weights
157	26	28	p	of
157	29	43	n	all sub-layers
157	44	46	p	as
157	47	73	n	random orthogonal matrices
158	4	20	n	other parameters
158	42	53	p	by sampling
158	54	66	n	each element
158	67	71	p	from
158	74	95	n	Gaussian distribution
158	96	100	p	with
158	101	126	n	mean 0 and variance 1 ? d
159	4	19	n	embedding layer
159	20	26	p	can be
159	27	47	n	initialized randomly
159	57	84	n	pre-trained word embeddings
164	4	13	n	dimension
164	14	16	p	of
164	17	62	n	word embeddings and predicate mask embeddings
164	66	72	p	set to
164	73	76	n	100
164	85	108	n	number of hidden layers
164	112	118	p	set to
164	119	121	n	10
166	4	21	n	number of heads h
166	25	31	p	set to
166	32	33	n	8
168	0	14	n	Dropout layers
168	19	31	p	added before
168	32	52	n	residual connections
168	53	57	p	with
168	60	76	n	keep probability
168	77	79	p	of
168	80	83	n	0.8
169	0	7	n	Dropout
169	16	30	p	applied before
169	35	58	n	attention softmax layer
169	67	99	n	feed - froward ReLU hidden layer
169	110	128	n	keep probabilities
169	133	139	p	set to
169	140	143	n	0.9
171	0	31	n	Learning Parameter optimization
171	35	50	p	performed using
171	51	78	n	stochastic gradient descent
174	5	8	n	SGD
174	9	17	p	contains
174	20	30	n	mini-batch
174	31	47	p	of approximately
174	48	59	n	4096 tokens
174	60	63	p	for
174	68	88	n	CoNLL - 2005 dataset
174	93	104	n	8192 tokens
174	105	108	p	for
174	113	133	n	CoNLL - 2012 dataset
175	4	17	n	learning rate
175	21	35	p	initialized to
175	36	39	n	1.0
165	3	6	p	set
165	11	35	n	number of hidden units d
165	36	38	p	to
165	39	42	n	200
170	8	14	p	employ
170	15	40	n	label smoothing technique
170	41	45	p	with
170	48	63	n	smoothing value
170	64	66	p	of
170	67	70	n	0.1
170	71	77	p	during
170	78	86	n	training
172	3	8	p	adopt
172	9	17	n	Adadelta
172	44	46	p	as
172	51	60	n	optimizer
173	42	46	p	clip
173	51	55	n	norm
173	56	58	p	of
173	59	68	n	gradients
173	69	73	p	with
173	76	100	n	predefined threshold 1.0
173	0	8	p	To avoid
173	9	36	n	exploding gradients problem
176	0	14	p	After training
176	15	26	n	400 k steps
176	32	37	p	halve
176	42	55	n	learning rate
176	56	61	p	every
176	62	73	n	100 K steps
177	3	8	p	train
177	9	19	n	all models
177	20	23	p	for
177	24	35	n	600 K steps
178	0	3	p	For
178	4	14	n	DEEP - ATT
178	15	19	p	with
178	20	36	n	FFN sub - layers
178	43	63	n	whole training stage
178	64	69	p	takes
178	76	84	n	two days
178	85	87	p	to
178	88	94	n	finish
178	95	97	p	on
178	100	118	n	single Titan X GPU
178	121	129	p	which is
178	130	146	n	2.5 times faster
29	37	44	p	present
29	47	90	n	deep attentional neural network ( DEEPATT )
29	91	94	p	for
29	107	110	n	SRL
30	11	18	p	rely on
30	23	49	n	self - attention mechanism
30	56	70	p	directly draws
30	75	94	n	global dependencies
30	95	97	p	of
30	102	108	n	inputs
31	24	42	p	major advantage of
31	43	59	n	self - attention
31	71	79	p	conducts
31	80	98	n	direct connections
31	99	106	p	between
31	107	127	n	two arbitrary tokens
31	128	130	p	in
31	133	141	n	sentence
32	12	28	n	distant elements
32	29	46	p	can interact with
32	47	57	n	each other
32	58	60	p	by
32	61	74	n	shorter paths
32	108	114	p	allows
32	115	141	n	unimpeded information flow
32	142	149	p	through
32	154	161	n	network
33	22	30	p	provides
33	33	50	n	more flexible way
33	51	87	p	to select , represent and synthesize
33	92	103	n	information
33	104	106	p	of
33	111	117	n	inputs
33	125	141	p	complementary to
33	142	158	n	RNN based models
34	0	10	p	Along with
34	11	27	n	self - attention
34	30	40	n	DEEP - ATT
34	41	51	p	comes with
34	52	66	n	three variants
34	162	180	p	to further enhance
34	185	200	n	representations
34	73	77	p	uses
34	78	95	n	recurrent ( RNN )
34	98	119	n	convolutional ( CNN )
34	124	161	n	feed - forward ( FFN ) neural network
2	0	27	n	Deep Semantic Role Labeling
4	0	30	n	Semantic Role Labeling ( SRL )
7	67	70	n	SRL
12	0	22	n	Semantic Role Labeling
180	19	22	p	get
180	23	37	n	74.1 F 1 score
180	38	40	p	on
180	45	70	n	out - of - domain dataset
182	0	15	p	When ensembling
182	16	24	n	5 models
182	25	29	p	with
182	30	56	n	FFN nonlinear sub - layers
182	59	71	n	our approach
182	72	80	p	achieves
182	84	93	n	F 1 score
182	94	96	p	of
182	97	110	n	84.6 and 83.9
182	159	179	n	absolute improvement
182	180	182	p	of
182	183	194	n	1.4 and 0.5
182	195	199	p	over
182	204	235	n	previous state - of - the - art
183	6	13	n	results
183	48	52	p	that
183	57	80	n	self - attention layers
183	84	102	p	helpful to capture
183	103	125	n	structural information
183	130	156	n	long distance dependencies
