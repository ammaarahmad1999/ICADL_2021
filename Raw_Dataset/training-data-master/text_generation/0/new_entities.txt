192	21	44	n	random token generation
193	22	40	n	MLE trained LSTM G
194	17	35	n	scheduled sampling
195	22	61	n	Policy Gradient with BLEU ( PG - BLEU )
185	45	61	p	first initialize
185	66	76	n	parameters
185	77	82	p	of an
185	83	95	n	LSTM network
185	96	105	p	following
185	110	141	n	normal distribution N ( 0 , 1 )
185	142	144	p	as
185	149	155	n	oracle
185	156	166	p	describing
185	171	231	n	real data distribution G oracle ( x t |x 1 , . . . , x t?1 )
187	0	2	p	In
187	3	19	n	SeqGAN algorithm
187	26	38	n	training set
187	39	42	p	for
187	47	60	n	discriminator
187	61	76	p	is comprised by
187	81	99	n	generated examples
187	100	104	p	with
187	109	116	n	label 0
187	125	134	n	instances
187	135	139	p	from
187	140	141	n	S
187	142	146	p	with
187	151	156	n	label
196	7	25	n	scheduled sampling
196	32	48	p	training process
196	49	66	n	gradually changes
196	67	71	p	from
196	74	126	n	fully guided scheme feeding the true previous tokens
196	127	131	p	into
196	132	136	n	LSTM
196	139	146	p	towards
196	149	195	n	less guided scheme which mostly feeds the LSTM
196	196	200	p	with
196	205	221	n	generated tokens
189	91	93	p	in
189	94	124	n	our synthetic data experiments
189	131	142	n	kernel size
189	146	150	p	from
189	151	157	n	1 to T
189	166	172	n	number
189	173	175	p	of
189	176	192	n	each kernel size
189	196	203	p	between
189	204	214	n	100 to 200
190	0	31	n	Dropout ) and L2 regularization
190	41	49	p	to avoid
190	50	62	n	over-fitting
197	2	19	n	curriculum rate ?
197	28	38	p	to control
197	43	54	n	probability
197	55	57	p	of
197	58	83	n	replacing the true tokens
197	84	88	p	with
197	93	107	n	generated ones
32	66	74	p	consider
32	79	108	n	sequence generation procedure
32	109	111	p	as
32	114	148	n	sequential decision making process
33	4	20	n	generative model
33	24	34	p	treated as
33	38	76	n	agent of reinforcement learning ( RL )
33	83	88	n	state
33	21	23	p	is
33	96	119	n	generated tokens so far
33	128	134	n	action
33	89	91	p	is
33	142	168	n	next token to be generated
34	132	138	p	employ
34	141	154	n	discriminator
34	155	166	p	to evaluate
34	53	61	n	sequence
34	184	192	p	feedback
34	197	207	n	evaluation
34	208	216	p	to guide
34	221	229	n	learning
34	230	232	p	of
34	237	253	n	generative model
35	114	120	p	regard
35	64	80	n	generative model
35	142	146	p	as a
35	147	177	n	stochastic parametrized policy
36	0	2	p	In
36	7	22	n	policy gradient
36	28	34	p	employ
36	35	60	n	Monte Carlo ( MC ) search
36	61	75	p	to approximate
36	80	100	n	state - action value
37	3	17	p	directly train
37	22	49	n	policy ( generative model )
37	50	53	p	via
37	54	69	n	policy gradient
37	78	94	p	naturally avoids
37	99	125	n	differentiation difficulty
37	126	129	p	for
37	130	143	n	discrete data
37	144	148	p	in a
37	149	165	n	conventional GAN
4	195	224	n	generating real - valued data
5	50	89	n	generating sequences of discrete tokens
13	0	36	n	Generating sequential synthetic data
202	66	69	p	see
202	74	90	n	impact of SeqGAN
202	99	110	p	outperforms
202	111	140	n	other baselines significantly
203	2	23	n	significance T - test
203	24	26	p	on
203	31	60	n	NLL oracle score distribution
203	61	63	p	of
203	68	87	n	generated sequences
203	88	92	p	from
203	97	112	n	compared models
203	139	151	p	demonstrates
203	156	179	n	significant improvement
203	180	182	p	of
203	183	189	n	SeqGAN
203	190	198	p	over all
203	199	214	n	compared models
207	15	21	n	SeqGAN
207	22	33	p	outperforms
207	34	43	n	PG - BLEU
205	0	11	p	After about
205	12	31	n	150 training epochs
205	34	38	p	both
205	43	106	n	maximum likelihood estimation and the schedule sampling methods
205	107	118	p	converge to
205	121	153	n	relatively high NLL oracle score
205	164	170	n	SeqGAN
205	175	195	p	improve the limit of
205	200	209	n	generator
205	210	214	p	with
205	219	233	n	same structure
205	234	236	p	as
205	241	250	n	baselines
206	5	14	p	indicates
206	19	71	n	prospect of applying adversarial training strategies
206	72	74	p	to
206	75	110	n	discrete sequence generative models
206	111	126	p	to breakthrough
206	131	149	n	limitations of MLE
