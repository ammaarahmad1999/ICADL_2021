(Contribution||has||Hyperparameters)
(Hyperparameters||reduce||effects)
(effects||of||gradient exploding)
(gradient exploding||use||gradient clipping)
(gradient clipping||of||5.0)
(Hyperparameters||For each of||embeddings)
(embeddings||fine - tune||initial embeddings)
(embeddings||modifying them during||gradient updates)
(gradient updates||of||neural network model)
(neural network model||by||back - propagating gradients)
(Hyperparameters||fix||dropout rate)
(dropout rate||at||0.5)
(0.5||for||all dropout layers)
(Hyperparameters||apply||dropout)
(dropout||on||character embeddings)
(character embeddings||before inputting to||CNN)
(dropout||on||input and output vectors)
(input and output vectors||of||BLSTM)
(Hyperparameters||use||early stopping)
(early stopping||based on||performance)
(performance||on||validation sets)
(Hyperparameters||To mitigate||overfitting)
(overfitting||apply||dropout method)
(dropout method||to regularize||model)
(Hyperparameters||choose||initial learning rate)
(initial learning rate||updated on||each epoch)
(each epoch||of||training)
(initial learning rate||of||0.01)
(0.01||for||POS tagging)
(initial learning rate||of||0.015)
(0.015||for||NER)
(Hyperparameters||has||" best " parameters)
(" best " parameters||appear at||around 50 epochs)
(Hyperparameters||has||Parameter optimization)
(Parameter optimization||performed with||minibatch stochastic gradient descent ( SGD ))
(minibatch stochastic gradient descent ( SGD )||with||batch size)
(batch size||has||10)
(minibatch stochastic gradient descent ( SGD )||with||momentum)
(momentum||has||0.9)
