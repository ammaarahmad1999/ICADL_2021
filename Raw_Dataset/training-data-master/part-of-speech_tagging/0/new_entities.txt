132	3	8	p	train
132	13	61	n	model parameters and word / character embeddings
132	62	64	p	by
132	69	115	n	mini-batch stochastic gradient descent ( SGD )
132	116	120	p	with
132	121	131	n	batch size
132	132	134	n	10
132	137	145	n	momentum
132	146	149	n	0.9
132	152	173	n	initial learning rate
132	174	178	n	0.01
132	183	193	n	decay rate
132	194	198	n	0.05
133	8	11	p	use
133	14	31	n	gradient clipping
133	32	34	p	of
133	35	38	n	5.0
134	15	27	p	trained with
134	28	42	n	early stopping
134	45	53	p	based on
134	58	81	n	development performance
32	71	100	p	propose and carefully analyze
32	103	150	n	neural part - of - speech ( POS ) tagging model
32	151	164	p	that exploits
32	165	185	n	adversarial training
33	0	4	p	With
33	7	25	n	BiLSTM - CRF model
33	26	28	p	as
33	33	52	n	baseline POS tagger
33	58	63	p	apply
33	64	84	n	adversarial training
33	88	99	p	considering
33	100	113	n	perturbations
33	114	116	p	to
33	117	150	n	input word / character embeddings
2	20	46	n	Part - of - Speech Tagging
6	41	59	n	neural POS tagging
139	0	17	n	PTB - WSJ dataset
141	18	49	n	baseline ( BiLSTM - CRF ) model
141	71	79	p	performs
141	80	86	n	on par
141	87	91	p	with
141	92	128	n	other state - of - the - art systems
141	52	60	n	accuracy
141	61	68	n	97.54 %
142	0	10	p	Built upon
142	31	64	n	adversarial training ( AT ) model
142	65	72	p	reaches
142	73	81	n	accuracy
142	82	89	n	97.58 %
142	127	140	p	outperforming
142	141	159	n	recent POS taggers
