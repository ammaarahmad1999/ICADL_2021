58	26	64	n	https : //github.com/bplank/bilstm-aux
52	9	30	n	default learning rate
52	33	36	n	0.1
52	41	55	n	128 dimensions
52	56	59	p	for
52	60	75	n	word embeddings
52	78	81	n	100
52	82	85	p	for
52	86	115	n	character and byte embeddings
52	122	135	n	hidden states
52	140	154	n	Gaussian noise
52	155	159	p	with
52	163	166	n	0.2
53	3	11	n	training
53	12	14	p	is
53	15	25	n	stochastic
53	41	44	p	use
53	47	57	n	fixed seed
55	16	19	p	use
55	20	54	n	offthe - shelf polyglot embeddings
22	13	22	p	introduce
22	25	36	n	novel model
22	41	78	n	bi - LSTM trained with auxiliary loss
23	10	26	p	jointly predicts
23	31	68	n	POS and the log frequency of the word
2	0	39	n	Multilingual Part - of - Speech Tagging
5	104	115	n	POS tagging
79	33	41	p	compared
79	42	69	n	Tnt , HunPos and TreeTagger
79	74	79	p	found
79	80	83	n	Tnt
79	84	89	p	to be
79	90	109	n	consistently better
79	110	114	p	than
79	115	125	n	Treetagger
80	4	50	n	combined word + character representation model
80	51	53	p	is
80	58	77	n	best representation
80	80	93	n	outperforming
80	98	106	n	baseline
80	107	109	p	on
80	110	148	n	all except one language ( Indonesian )
81	21	28	p	reaches
81	33	52	n	biggest improvement
81	82	84	p	on
81	85	103	n	Hebrew and Slovene
81	55	79	n	more than + 2 % accuracy
85	4	24	n	over all best system
85	25	27	p	is
85	32	60	n	multi-task bi - LSTM FREQBIN
82	0	12	p	Initializing
82	17	47	n	word embeddings ( + POLYGLOT )
82	48	52	p	with
82	53	98	n	off - the - shelf languagespecific embeddings
82	107	115	p	improves
82	116	124	n	accuracy
