arXiv:1811.04210v2 [cs.CL] 2 Apr 2019

 

Densely Connected Attention Propagation
for Reading Comprehension

Yi Tay', Luu Anh Tuan’, Siu Cheung Hui? and Jian Su*
'-3Nanyang Technological University, Singapore
?.4Tnstitute for Infocomm Research, Singapore
ytay017@e.ntu.edu.sg!
at.luu@i2r.a-star.edu.sg”
asschui@ntu.edu.sg®
sujian@i2r.a-star.edu.sg*

Abstract

We propose DECAPROP (Densely Connected Attention Propagation), a new
densely connected neural architecture for reading comprehension (RC). There
are two distinct characteristics of our model. Firstly, our model densely connects
all pairwise layers of the network, modeling relationships between passage and
query across all hierarchical levels. Secondly, the dense connectors in our network
are learned via attention instead of standard residual skip-connectors. To this end,
we propose novel Bidirectional Attention Connectors (BAC) for efficiently forging
connections throughout the network. We conduct extensive experiments on four
challenging RC benchmarks. Our proposed approach achieves state-of-the-art
results on all four, outperforming existing baselines by up to 2.6% — 14.2% in
absolute F1 score.

1 Introduction

The dominant neural architectures for reading comprehension (RC) typically follow a standard

‘encode-interact-point’ design [Wang and Jiang||2016 2016 2016
2017c}|Kundu and Ng}|2018]. Following the embedding layer, a compositional encoder typically
encodes (@ (query) and P (passage) individually. Subsequently, an (bidirectional) attention layer

is then used to model interactions between P/Q. Finally, these attended representations are then
reasoned over to find (point to) the best answer span. While, there might be slight variants of this
architecture, this overall architectural design remains consistent across many RC models.

Intuitively, the design of RC models often possess some depth, 1.e., every stage of the network easily
comprises several layers. For example, the R-NET architecture adopts three
BiRNN layers as the encoder and two additional BiRNN layers at the interaction layer. BiDAF [Seo
uses two BiLSTM layers at the pointer layer, etc. As such, RC models are often relatively
deep, at the very least within the context of NLP.

Unfortunately, the depth of a model is not without implications. It is well-established fact that
increasing the depth may impair gradient flow and feature propagation, making networks harder

to train [He et al.| 2016} 2015} 2017]. This problem is prevalent

in computer vision, where mitigation strategies that rely on shortcut connections such as Residual

networks [He et al.||2016], GoogLeNet |Szegedy et al.|/2015] and DenseNets [Huang et al.|/2017|

were incepted. Naturally, many of the existing RC models already have some built-in designs to
workaround this issue by shortening the signal path in the network. Examples include attention flow

[Seo et al.|/2016], residual connections [Xiong et al.|/2017} 2018] or simply the usage

32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.
of highway encoders [Srivastava et al.|/2015]. As such, we hypothesize that explicitly improving

information flow can lead to further and considerable improvements in RC models.

A second observation is that the flow of P/Q representations across the network are often well-aligned
and ‘synchronous’, i.e., P is often only matched with Q at the same hierarchical stage (e.g., only
after they have passed through a fixed number of encoder layers). To this end, we hypothesize that
increasing the number of interaction interfaces, 1.e., matching in an asynchronous, cross-hierarchical
fashion, can also lead to an improvement in performance.

Based on the above mentioned intuitions, this paper proposes a new architecture with two distinct
characteristics. Firstly, our network is densely connected, connecting every layer of P with every
layer of Q. This not only facilitates information flow but also increases the number of interaction
interfaces between P/Q. Secondly, our network is densely connected by attention, making it vastly
different from any residual mitigation strategy in the literature. To the best of our knowledge, this is
the first work that explicitly considers attention as a form of skip-connector.

Notably, models such as BiDAF incorporates a form of attention propagation (flow). However, this is
inherently unsuitable for forging dense connections throughout the network since this would incur a
massive increase in the representation size in subsequent layers. To this end, we propose efficient
Bidirectional Attention Connectors (BAC) as a base building block to connect two sequences at
arbitrary layers. The key idea is to compress the attention outputs so that they can be small enough to
propagate, yet enabling a connection between two sequences. The propagated features are collectively
passed into prediction layers, which effectively connect shallow layers to deeper layers. Therefore,
this enables multiple bidirectional attention calls to be executed without much concern, allowing us
to efficiently connect multiple layers together.

Overall, we propose DECAPROP (Densely Connected Attention Propagation), a novel architecture
for reading comprehension. DECAPROP achieves a significant gain of 2.6% — 14.2% absolute
improvement in F1 score over the existing state-of-the-art on four challenging RC datasets, namely

NewsQA [Trischler et al.|/2016], Quasar-T |Dhingra et al.|!2017], SearchQA [Dunn et al.|/2017] and
NarrativeQA |Kocisky et al.||2017].

2 Bidirectional Attention Connectors (BAC)

This section introduces the Bidirectional Attention Connectors (BAC) module which is central to our
overall architecture. The BAC module can be thought of as a connector component that connects two
sequences/layers.

The key goals of this module are to (1) connect any two layers of P/Q in the network, returning a
residual feature that can be propagated! to deeper layers, (2) model cross-hierarchical interactions
between P/Q and (3) minimize any costs incurred to other network components such that this
component may be executed multiple times across all layers.

Let P € R’*4 and Q € R‘«*? be inputs to the BAC module. The initial steps in this module
remains identical to standard bi-attention in which an affinity matrix is constructed between P/Q. In
our bi-attention module, the affinity matrix 1s computed via:

Ei; = = F(p;)"F(q)) (1)

where F(.) is a standard dense layer with ReLU activations and d is the dimensionality of the vectors.

Note that this is the scaled dot-product attention from 12017]. Next, we learn an

alignment between P/Q as follows:

A =Softmax(E')P and B = Softmax(E)Q (2)

where A, B are the aligned representations of the query/passsage respectively. In many standard neural
QA models, it is common to pass an augmented” matching vector of this attentional representation to
subsequent layers. For this purpose, functions such as f = W([b; ; p;; 6; © pi, b; — p;|) + b have

‘Notably, signals still have to back-propagate through the BAC parameters. However, this still enjoys the
benefits when connecting far away layers and also by increasing the number of pathways.
°This refers to common element-wise operations such as the subtraction or multiplication.
been used [Wang and Jiang\|2016]. However, simple/naive augmentation would not suffice in our use

case. Even without augmentation, every call of bi-attention returns a new d dimensional vector for
each element in the sequence. If the network has / layers, then connecting?| all pairwise layers would
require /? connectors and therefore an output dimension of J? x d. This is not only computationally
undesirable but also require a large network at the end to reduce this vector. With augmentation, this
problem is aggravated. Hence, standard birectional attention is not suitable here.

To overcome this limitation, we utilize a parameterized function G(.) to compress the bi-attention
vectors down to scalar.

gi = [G([bi; pil); G(bi — pi); GH: © pi) (3)

where i € R? is the output (for each element in P) of the BAC module. This is done in an identical
fashion for a; and q; to form g/ for each element in Q. Intuitively g* where * = {p,q} are the
learned scalar attention that is propagated to upper layers. Since there are only three scalars, they will
not cause any problems even when executed for multiple times. As such, the connection remains
relatively lightweight. This compression layer can be considered as a defining trait of the BAC,
differentiating it from standard bi-attention.

Naturally, there are many potential candidates for the function G(.). One natural choice is the
standard dense layer (or multiple dense layers). However, dense layers are limited as they do not
compute dyadic pairwise interactions between features which inhibit its expressiveness. On the other
hand, factorization-based models are known to not only be expressive and efficient, but also able to
model low-rank structure well.

To this end, we adopt factorization machines (FM) [Rendle\|2010] as G(.). The FM layer is defined
as:

G(x) =wot > wv; r+ >) S- (Uj,U0;) Li Lj (4)
i=l

i=1 j=it1

where v € R2**, wo € R and w; € R¢. The output G(x) is a scalar. Intuitively, this layer tries to
learn pairwise interactions between every x; and x; using factorized (vector) parameters v. In the
context of our BAC module, the FM layer is trying to learn a low-rank structure from the ‘match’
vector (e.g., 6; — p;, b; © p; or [b;; p;]). Finally, we note that the BAC module takes inspiration from
the main body of our CAFE model for entailment classification. However, this
work demonstrates the usage and potential of the BAC as a residual connector.

Factorization

 

OOOO
©

(OOOO)
(O)
uoneBbedold

   

Connectors

OO0O0

Bidirectional Aligned
Attention Vectors

Figure 1: High level overview of our proposed Bidrectional Attention Connectors (BAC). BAC supports
connecting two sequence layers with attention and produces connectors that can be propagated to deeper layers
of the network.

3 Densely Connected Attention Propagation (DECAPROP)

In this section, we describe our proposed model in detail. Figure [2|depicts a high-level overview of
our proposed architecture.

 

*See encoder component of Figure [2] for more details.
Start
Pointer

Gated [CF iicre| Pointer
Attention Self- EW oe
Attention

End
Pointer

 

Passage

 

| J} | } \ }
Y

Input Encoder Densely Connected Attention Encoder Gated Attention + Dense Core Pointer Layer

Figure 2: Overview of our proposed model architecture

3.1 Contextualized Input Encoder

The inputs to our model are two sequences P and @ which represent passage and query respectively.
Given Q, the task of the RC model is to select a sequence of tokens in P as the answer. Following
many RC models, we enhance the input representations with (1) character embeddings (passed into
a BiRNN encoder), (2) a binary match feature which denotes if a word in the query appears in the
passage (and vice versa) and (3) a normalized frequency score denoting how many times a word
appears in the passage. The Char BiRNN of h, dimensions, along with two other binary features, is
concatenated with the word embeddings w; € R%, to form the final representation of d,, +h. + 2
dimensions.

3.2 Densely Connected Attention Encoder (DECAENC)

The DECAENC accepts the inputs P and Q from the input encoder. DECAENC is a multi-layered
encoder with k& layers. For each layer, we pass P/Q into a bidirectional RNN layer of h dimensions.
Next, we apply our attention connector (BAC) to H? /H®@ € R* where H represents the hidden state
outputs from the BiRNN encoder where the RNN cell can either be a GRU or LSTM encoder. Let d
be the input dimensions of P and Q, then this encoder goes through a process of d > h > h+3—-h
in which the BiRNN at layer / + 1 consumes the propagated features from layer /.

Intuitively, this layer models P/Q whenever they are at the same network hierarchical level. At this
point, we include ‘asynchronous’ (cross hierarchy) connections between P and Q. Let P’, Q* denote
the representations of P,Q at layer 7. We apply the Bidirectional Attention Connectors (BAC) as
follows:

Zi), Zi) = Fo(P',Q’) Vi,g=1,2---n (5)
where Fo represents the BAC component. This densely connects all representations of P and Q

across multiple layers. Z,’ € R?** represents the generated features for each ij combination of
P/Q. In total, we obtain 3n? compressed attention features for each word. Intuitively, these features
capture fine-grained relationships between P/Q at different stages of the network flow. The output of
the encoder is the concatenation of all the BiRNN hidden states H!, H?--- H* and Z* which is a
matrix of (nh + 3n7) x ¢ dimensions.

3.3. Densely Connected Core Architecture (DECACORE)

This section introduces the core architecture of our proposed model. This component corresponds to
the interaction segment of standard RC model architecture.

Gated Attention The outputs of the densely connected encoder are then passed into a standard
gated attention layer. This corresponds to the ‘interact’ component in many other popular RC models
that models Q/P interactions with attention. While there are typically many choices of implementing

this layer, we adopt the standard gated bi-attention layer following | Wang et al.\|2017c].

ti T
S= Ta F(P)' (F(Q) (6)
P = Softmax($)Q (7)
P’ = BiRNN(o(W,,([P; P]) +b,) © P) (8)

where @ is the sigmoid function and F'(.) are dense layers with ReLU activations. The output P’ is
the query-dependent passage representation.

Gated Self-Attention Next, we employ a self-attention layer, applying Equation (8) yet again on
P’, matching P’ against itself to form B, the output representation of the core layer. The key idea is
that self-attention models each word in the query-dependent passsage representation against all other
words, enabling each word to benefit from a wider global view of the context.

Dense Core At this point, we note that there are two intermediate representations of P, 1.e., one
after the gated bi-attention layer and one after the gated self-attention layer. We denote them as
U+,U? respectively. Unlike the Densely Connected Attention Encoder, we no longer have two
representations at each hierarchical level since they have already been ‘fused’. Hence, we apply a
one-sided BAC to all permutations of [U!, U7] and Q*, Vi = 1,2---k. Note that the one-sided BAC
only outputs values for the left sequence, ignoring the right sequence.

R' = FL(UI,Q*) Vk=1,2---n,Vj = 1,2 (9)

where R*) € R°** represents the connection output and FG is the one-sided BAC function. All
values of R®J, Vj = 1,2, Vk = 1,2---n are concatenated to form a matrix R’ of (2n x 6) x £,
which is then concatenated with U? to form M € R’&*‘4+12")_ This final representation is then
passed to the answer prediction layer.

3.4 Answer Pointer and Prediction Layer

Next, we pass / through a stacked BIRNN model with two layers and obtain two representations,
A | and #7. i respectively.

H! = BiRNN(M) and H! = BiRNN(H) (10)
The start and end pointers are then learned via:
p! = Softmax(w,H}) and p? = Softmax(w2H}) (11)

where w1,w2 € R?@ are parameters of this layer. To train the model, following prior work, we
minimize the sum of negative log probabilities of the start and end indices:

l N
L0) =~; D_loalpy:) + log(py2) (12)

where N is the number of samples, y}, y? are the true start and end indices. pz, is the k-th value of
the vector p. The test span is chosen by finding the maximum value of pj,, p7 where k < 1.

4 Experiments

This section describes our experiment setup and empirical results.

4.1 Datasets and Competitor Baselines

We conduct experiments on four challenging QA datasets which are described as follows:
NewsQA This challenging RC dataset |Trischler et al.}!2016] comprises 100k QA pairs. Passages

are relatively long at about 600 words on average. This dataset has also been extensively used
in benchmarking RC models. On this dataset, the key competitors are BiDAF [Seo et al.}|2016],

Match-LSTM [Wang and Jiang}|2016], FastQA/FastQA-Ext |Weissenborn et al.}/2017], R2-BiLSTM
| Weissenborn\|2017], AMANDA [Kundu and Ng}|2018}.

Quasar-T This dataset |Dhingra et al.||2017]| comprises 43k factoid-based QA pairs and is constructed using ClueWeb09 as its backbone corpus. The key competitors on this dataset are BiDAF
and the Reinforced Ranker-Reader (R°) |Wang et al. 2017a]. Several variations of the ranker-reader

model (e.g., SR, SR?), which use the Match-LSTM underneath, are also compared against.

SearchQA This dataset [|Dunn et al.|/2017] aims to emulate the search and retrieval process in

question answering applications. The challenge involves reasoning over multiple documents. In
this dataset, we concatenate all documents into a single passage context and perform RC over the
documents. The competitor baselines on this dataset are Attention Sum Reader (ASR) [Kadlec et al.

2016], Focused Hierarchical RNNs (FH-RNN) [Ke et al.}/2018], AMANDA | Kundu and Ng}/2018
BiDAF, AQA [Buck et al.||2017] and the Reinforced Ranker-Reader (R*) [Wang et al.|/2017a].

NarrativeQA = |Kocisky et al.} /2017] is a recent QA dataset that involves comprehension over

stories. We use the summaries settin Jt which is closer to a standard QA or reading comprehension
setting. We compare with the baselines in the original paper, namely Seq2Seq, Attention Sum Reader
and BiDAF. We also compare with the recent BiAttention + MRU model [Tay et al.}/2018b].

As compared to the popular SQuAD dataset [Rajpurkar et al.||2016], these datasets are either (1)

more challengin g>| involves more multi-sentence reasoning or (2) is concerned with searching
across multiple documents in an ‘open domain’ setting (SearchQA/Quasar-T). Hence, these datasets
accurately reflect real world applications to a greater extent. However, we regard the concatenated
documents as a single context for performing reading comprehension. The evaluation metrics are
the EM (exact match) and F1 score. Note that for all datasets, we compare all models solely on the
RC task. Therefore, for fair comparison we do not compare with algorithms that use a second-pass
answer re-ranker 2017]. Finally, to ensure that our model is not a failing case of
SQuAD, and as requested by reviewers, we also include development set scores of our model on
SQuAD.

   
     

 

4.2 Experimental Setup

Our model is implemented in Tensorflow [Abadi et al.|}2015]. The sequence lengths are capped
at 800/700/1500/1100 for NewsQA, SearchQA, Quasar-T and NarrativeQA respectively. We use

Adadelta with a = 0.5 for NewsQA, Adam with a = 0.001
for SearchQA, Quasar-T and NarrativeQA. The choice of the RNN encoder is tuned between
GRU and LSTM cells and the hidden size is tuned amongst {32, 50, 64,75}. We use the CUDNN
implementation of the RNN encoder. Batch size is tuned amongst {16, 32,64}. Dropout rate is tuned
amongst {0.1,0.2,0.3} and applied to all RNN and fully-connected layers. We apply variational
dropout 2016] in-between RNN layers. We initialize the word embeddings
with 300D GloVe embeddings [Pennington et al. and are fixed during training. The size of the
character embeddings is set to 8 and the character RNN is set to the same as the word-level RNN
encoders. The maximum characters per word is set to 16. The number of layers in DECAENC is set
to 3 and the number of factors in the factorization kernel is set to 64. We use a learning rate decay
factor of 2 and patience of 3 epochs whenever the EM (or ROUGE-L) score on the development set
does not increase.

5 Results

Overall, our results are optimistic and promising, with results indicating that DECAPROP achieves
state-of-the-art performancg” jon all four datasets.

“Notably, a new SOTA was set by Hu et al. 2018] after the NIPS submission deadline.
°This is claimed by authors in most of the dataset papers.
°As of NIPS 2018 submission deadline.
Dev Test

 

Model EM FI EM FI 2; a |
a a D T
Match-LSTM 34.4 49.6 34.9 50.0 Le
EM FI EM FI
BARB 36.1 49.6 34.1 48.2 Sao. E REDE
. GA 25.6 25.6 264 26.4
BiDAF N/A N/A 37.1 52.3 .
BiDAF 25.7 28.9 25.9 28.5
Neural BoW 25.8 37.6 24.1 36.6
SR N/A N/A 31.5 38.5
FastQA 43.7 564 41.9 55.7 2
SR N/A N/A 31.9 38.8
FastQAExt 43.7 56.1 42.8 56.1 3
; R N/A N/A 34.2 40.9
R2-BiLSTM N/A N/A 43.7 56.7 —DEcAPROP 39.7 48.1386 469
AMANDA 48.4. 63.3. 48.4.—«63.7 _DECAPROP _39.7_48.1_38.6 46.9 _
—DECAPROP 52.5. 65.7 53.1 663 feng 2: Performance comparison on Quasar-T
Table 1: Performance comparison on NewsQA aLASCl
dataset.
Dev Test
——— ET a a
TF-IDF max 13.0 N/A 12.7. N/A =A aT ee Se SS ea
BiDAF 31.7 37.9 286 34.6
ASR 43.9 242 41.3 22.8
FH-RNN 496 567 468 534 AQA 40.5 474 38.7 45.6
7 R° N/A N/A 49.0 55.3

AMANDA 48.6 57.7 468 56.6
DECAPROP 645 71.9 62.2 70.8
Table 3: Evaluation on original setting, Unigram
Accuracy and N-gram F1 scores on SearchQA

DECAPROP 58.8 65.5 568 63.6
Table 4: Evaluation on Exact Match and F1 Metrics
on SearchQA dataset.

dataset.
Test / Validation
BLEU-1 BLEU-4 METEOR ROUGE-L

Seq2Seq 15.89 / 16.10 1.26 / 1.40 4.08 / 4.22 13.15 / 13.29

Attention Sum Reader 23.20 / 23.54 6.39 / 5.90 7.77 / 8.02 22.26 / 23.28

BiDAF 33.72/33.45  15.53/15.69 15.38/15.68  36.30/ 36.74

BiAttention + MRU - / 36.55 -/19.79 -/ 17.87 -/41.44

DECAPROP 42.00/ 44.35 23.42/27.61 23.42/21.80  40.07/ 44.69

 

Table 5: Evaluation on NarrativeQA (Story Summaries).

   

R-NET (Our re-implementation 71.9 79.6
DECAPROP (This paper) 72.9 81.4
QANet [Yu et al.}|2018 73.6 82.7

Table 6: Single model dev scores (published scores) of some representative models on SQUAD.

NewsQA Table[I] reports the results on NewsQA. On this dataset, DECAPROP outperforms the
existing state-of-the-art, i.e., the recent AMANDA model by (+4.7% EM / +2.6% F1). Notably,
AMANDA is a strong neural baseline that also incorporates gated self-attention layers, along with
question-aware pointer layers. Moreover, our proposed model also outperforms well-established
baselines such as Match-LSTM (+18% EM / +16.3% F1) and BiDAF (+16% EM / +14% F1).

Quasar-T Table [2|reports the results on Quasar-T. Our model achieves state-of-the-art performance
on this dataset, outperforming the state-of-the-art R? (Reinforced Ranker Reader) by a considerable
margin of +4.4% EM / +6% F1. Performance gain over standard baselines such as BiDAF and GA
are even larger (> 15% F1).

SearchQA Table[3]and Table (4|report the results Jon SearchQA. On the original setting, our model
outperforms AMANDA by +15.4% EM and +14.2% in terms of FI score. On the overall setting, our
model outperforms both AQA (+18.1% EM / +18% F1) and Reinforced Reader Ranker (+7.8% EM /

’ The original SearchQA paper 2017], along with AMANDA [Kundu and Ng}/2018] report

results on Unigram Accuracy and N-gram F1. On the other hand, 2017] reports results on overall
EM/F1 metrics. We provide comparisons on both.
+8.3% F1). Both models are reinforcement learning based extensions of existing strong baselines
such as BiDAF and Match-LSTM.

NarrativeQA Table[5|reports the results on NarrativeQA. Our proposed model outperforms all
baseline systems (Seq2Seq, ASR, BiDAF) in the original paper. On average, there is a ~ +5%
improvement across all metrics.

SQuAD Table [6] reports dev scores’| of our model against several representative models on the
popular SQUAD benchmark. While our model does not achieve state-of-the-art performance, our
model can outperform the base R-NET (both our implementation as well as the published score). Our
model achieves reasonably competitive performance.

5.1 Ablation Study

We conduct an ablation study on the NewsQA development set (Table (7). More specifically, we
report the development scores of seven ablation baselines. In (1), we removed the entire DECAPROP
architecture, reverting it to an enhanced version of the original R-NET model?| In (2), we removed
DECACORE and passed U? to the answer layer instead of /. In (3), we removed the DECAENC
layer and used a 3-layered BiRNN instead. In (4), we kept the DECAENC but only compared layer
of the same hierarchy and omitted cross hierarchical comparisons. In (5), we removed the Gated
Bi-Attention and Gated Self-Attention layers. Removing these layers simply allow previous layers to
pass through. In (6-7), we varied n, the number of layers of DECAENC. Finally, in (8-9), we varied
the FM with linear and nonlinear feed-forward layers.

 

  
  

 

 

 

Ablation EM Fl

(1) Remove All (R-NET) 48.1 61.2 zoel leer

(2) w/o DECACORE 51.5 64.5 s

(3) w/o DECAENC 49.3 62.0 =

(4) w/o Cross Hierarchy 50.0 63.1 =|

(5) w/o Gated Attention 49.4 62.8 g

(6) Set DECAENC n = 2 50.5 63.4 $2)

(7) Set DECAENC n = 4 50.7 63.3 * 254

(8) DecaProp (Linear d->1) 50.9 63.0 204

(9) DecaProp (Nonlinear d->d->1) 48.9 60.0 po

Full Architecture (n = 3) 52.5. 65.7 Table 8 D " ‘ ' anit w “ >
Tle 7. A Mate ode AT INA denn, ~»>S-d Table 8: Developmen score (DECAPROP versus
jeple 7: Ablation study on NewsQA developmen R-NET) on NewsQA.

From (1), we observe a significant gap in performance between DECAPROP and R-NET. This
demonstrates the effectiveness of our proposed architecture. Overall, the key insight is that all model
components are crucial to DECAPROP. Notably, the DECAENC seems to contribute the most to the
overall performance. Finally, Figure[8|]shows the performance plot of the development EM metric
(NewsQA) over training. We observe that the superiority of DECAPROP over R-NET is consistent
and relatively stable. This is also observed across other datasets but not reported due to the lack of
space.

6 Related Work

In recent years, there has been an increase in the number of annotated RC datasets such as SQUAD

|Rajpurkar et al.|/2016], NewsQA [Trischler et al.|/2016], TriviaQA [Joshi et al.}/2017] and RACE

‘Early testing of our model was actually done on SQUAD. However, since taking part on the heavily contested
public leaderboard requires more computational resources than we could muster, we decided to focus on other
datasets. In lieu of reviewer requests, we include preliminary results of our model on SQuAD dev set.

For fairer comparison, we make several enhancements to the R-NET model as follows: (1) We replaced the
additive attention with scaled dot-product attention similar to ours. (2) We added shortcut connections after the
encoder layer. (3) We replaced the original Pointer networks with our BiRNN Pointer Layer. We found that
these enhancements consistently lead to improved performance. The original R-NET performs at ~ 2% lower
on NewsQA.
[Lai et al.|[2017]. 2017]. Spurred on by the avaliability of data, many neural aon have also been proposed
to tackle these challenges. These models include 3] BORET 2016], Mat STM [Wang [Wang]
ang] DOT8), DCN/DCNs (Xiong et al] DOTS] BOTT), CNET (Wang-et al 2017, DiQa
Chen et al.|/2017], RO Reader [Cui et al. , Reinforced Mnemonic Rawder ita eval: 1.}/2017],
ReasoNet [Shen et al. “eee |

a and Ng|/2018], R® Reinforced Reader Ranker
2 | Leo OANet !Yu et al.)|2018]. Many ot these models innovate at either (1) the

bidirectional attention layer (BiDAF, DCN), (2) invoking multi-hop reasoning (Mnemonic Reader,
ReasoNet), (3) reinforcement learning (R°, DCN+), (4) self-attention (AMANDA, R-NET, QANet)
and finally, (5) improvements at the encoder level (QANet). While not specifically targeted at reading

comprehension, a multitude of pretraining schemes [McCann et al.|[2017}/Peters et al.|[2018}/Radford|

let al.{/Devlin et al.|/2018] have recently proven to be very effective for language understanding tasks.

Our work is concerned with densely connected networks aimed at improving information flow |Huang

et al.|[2017] Srivastava et al.|[2015) Szegedy et al, 2015}. While most works are concemed with

computer vision tasks or general machine learning, there are several notable works in the NLP domain.
proposed Densely Connected BiLSTMs for standard text classification tasks.
2018a] proposed a co-stacking residual affinity mechanims that includes all pairwise layers
of a text matching model in the affinity matrix calculation. In the RC domain, DCN+
2017] used Residual Co-Attention encoders. QANet used residual self-attentive
convolution encoders. While the usage of highway/residual networks is not an uncommon sight in
NLP, the usage of bidirectional attention as a skip-connector is new. Moreover, our work introduces
new cross-hierarchical connections, which help to increase the number of interaction interfaces
between P/Q.

     
  

   
     

         
    

 

7 Conclusion

We proposed a new Densely Connected Attention Propagation (DECAPROP) mechanism. For
the first time, we explore the possibilities of using birectional attention as a skip-connector. We
proposed Bidirectional Attention Connectors (BAC) for efficient connection of any two arbitary
layers, producing connectors that can be propagated to deeper layers. This enables a shortened signal
path, aiding information flow across the network. Additionally, the modularity of the BAC allows it
to be easily equipped to other models and even other domains. Our proposed architecture achieves
state-of-the-art performance on four challenging QA datasets, outperforming strong and competitive
baselines such as Reinforced Reader Ranker (R°), AMANDA, BiDAF and R-NET.

8 Acknowledgements

This paper is partially supported by Baidu I°R Research Centre, a joint laboratory between Baidu and
A-Star I?R. The authors would like to thank the anonymous reviewers of NeuRIPS 2018 for their
valuable time and feedback!

References

Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath
Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning

on heterogeneous systems, 2015. URL |http://tensorflow.org/, Software available from
tensorflow.org.

Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Wojciech Gajewski, and Wei Wang. Ask the right questions: Active question reformulation with
reinforcement learning. arXiv preprint arXiv: 1705.07830, 2017.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer opendomain questions. arXiv preprint arXiv: 1704.00051, 2017.
Yiming Cui, Zhipeng Chen, Si Wei, Shijin Wang, Ting Liu, and Guoping Hu. Attention-over-attention
neural networks for reading comprehension. arXiv preprint arXiv: 1607.04423, 2016.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2018.

Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. Quasar: Datasets for question answering
by search and reading. arXiv preprint arXiv: 1707.03904, 2017.

Zixiang Ding, Rui Xia, Jianfei Yu, Xiang Li, and Jian Yang. Densely connected bidirectional Istm
with applications to sentence classification. arXiv preprint arXiv: 1802.00889, 2018.

Matthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, and Kyunghyun Cho.
Searchqa: A new qé&a dataset augmented with context from a search engine. arXiv preprint
arXiv: 1704.05179, 2017.

Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent
neural networks. In Advances in neural information processing systems, pages 1019-1027, 2016.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770-778, 2016.

Minghao Hu, Yuxing Peng, and Xipeng Qiu. Mnemonic reader for machine comprehension. arXiv
preprint arXiv: 1705.02798, 2017.

Minghao Hu, Yuxing Peng, Furu Wei, Zhen Huang, Dongsheng Li, Nan Yang, and Ming
Zhou. Attention-guided answer distillation for machine reading comprehension. arXiv preprint
arXiv: 1808.07644, 2018.

Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In 20/7 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 2261-2269, 2017. doi: 10.1109/CVPR.

2017.243. URL|https://doi.org/10.1109/CVPR.2017.243

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaga: A large scale distantly
supervised challenge dataset for reading comprehension. arXiv preprint arXiv: 1705.03551, 2017.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. Text understanding with the
attention sum reader network. arXiv preprint arXiv: 1603.01547, 2016.

Nan Rosemary Ke, Konrad Zolna, Alessandro Sordoni, Zhouhan Lin, Adam Trischler, Yoshua Bengio,
Joelle Pineau, Laurent Charlin, and Chris Pal. Focused hierarchical rnns for conditional sequence
processing. arXiv preprint arXiv: 1806.04342, 2018.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014.

Tomas Kocisky, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gabor Melis,
and Edward Grefenstette. The narrativeqa reading comprehension challenge. arXiv preprint
arXiv: 1712.07040, 2017.

Souvik Kundu and Hwee Tou Ng. A question-focused multi-factor attention network for question
answering. 2018.

Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading
comprehension dataset from examinations. arXiv preprint arXiv: 1704.04683, 2017.

Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:

Contextualized word vectors. In Advances in Neural Information Processing Systems, pages
6294-6305, 2017.

10
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special
Interest Group of the ACL, pages 1532-1543, 2014.

Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv: 1802.05365,
2018.

Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for
machine comprehension of text. arXiv preprint arXiv: 1606.05250, 2016.

Steffen Rendle. Factorization machines. In Data Mining (ICDM), 2010 IEEE 10th International
Conference on, pages 995-1000. IEEE, 2010.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hayjishirzi. Bidirectional attention
flow for machine comprehension. arXiv preprint arXiv: 1611.01603, 2016.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. Reasonet: Learning to stop reading in
machine comprehension. In Proceedings of the 23rd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 1047-1055. ACM, 2017.

Rupesh Kumar Srivastava, Klaus Greff, and Jiirgen Schmidhuber. Highway networks. CoRR,
abs/1505.00387, 2015.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. 2015.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. A compare-propagate architecture with alignment
factorization for natural language inference. arXiv preprint arXiv: 1801.00102, 2017.

Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. Co-stack residual affinity networks with multi-level
attention refinement for matching text sequences. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing, pages 4492-4502, 2018a.

Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. Multi-range reasoning for machine comprehension.
arXiv preprint arXiv: 1803.09074, 2018b.

Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and
Kaheer Suleman. Newsga: A machine comprehension dataset. arXiv preprint arXiv: 1611.09830,
2016.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, pages 6000-6010, 2017.

Shuohang Wang and Jing Jiang. Machine comprehension using match-Istm and answer pointer. arXiv
preprint arXiv: 1608.07905, 2016.

Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,
Gerald Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced reader-ranker for open-domain
question answering. arXiv preprint arXiv: 1709.00023, 2017a.

Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim
Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in
open-domain question answering. arXiv preprint arXiv: 1711.05116, 2017b.

Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. Gated self-matching networks
for reading comprehension and question answering. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 189-198,
2017c.

11
Dirk Weissenborn. Reading twice for natural language understanding. arXiv preprint
arXiv: 1706.02596, 2017.

Dirk Weissenborn, Georg Wiese, and Laura Seiffe. Making neural ga as simple as possible but not
simpler. arXiv preprint arXiv: 1703.04816, 2017.

Caiming Xiong, Victor Zhong, and Richard Socher. Dynamic coattention networks for question
answering. CoRR, abs/1611.01604, 2016.

Caiming Xiong, Victor Zhong, and Richard Socher. Dcn+: Mixed objective and deep residual
coattention for question answering. arXiv preprint arXiv: 1711.00106, 2017.

Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,
and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading
comprehension. arXiv preprint arXiv: 1804.09541, 2018.

Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv: 1212.5701,
2012.

12
