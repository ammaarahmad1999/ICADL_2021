190	5	45	n	char - level and word - level embeddings
190	46	64	p	contribute towards
190	69	89	n	model 's performance
195	0	13	n	C2Q attention
195	14	26	p	proves to be
195	27	35	n	critical
195	36	40	p	with
195	43	47	n	drop
195	48	50	p	of
195	51	70	n	more than 10 points
199	50	75	n	proposed static attention
199	76	87	p	outperforms
199	92	122	n	dynamically computed attention
199	123	125	p	by
199	126	144	n	more than 3 points
207	0	2	p	At
207	7	27	n	word embedding layer
207	30	41	n	query words
207	42	49	p	such as
207	50	70	n	When , Where and Who
207	75	94	p	not well aligned to
207	95	111	n	possible answers
207	112	114	p	in
207	119	126	n	context
174	0	27	n	Each paragraph and question
174	32	44	p	tokenized by
174	47	108	n	regular - expression - based word tokenizer ( PTB Tokenizer )
174	113	121	p	fed into
174	126	131	n	model
176	4	27	n	hidden state size ( d )
176	28	30	p	of
176	35	40	n	model
176	41	43	p	is
176	44	47	n	100
177	0	9	n	The model
177	10	19	p	has about
177	20	42	n	2.6 million parameters
179	2	16	n	dropout ) rate
179	17	19	p	of
179	20	23	n	0.2
179	27	35	p	used for
179	40	43	n	CNN
179	46	61	n	all LSTM layers
179	72	93	n	linear transformation
179	94	100	p	before
179	105	112	n	softmax
179	113	116	p	for
179	121	128	n	answers
182	4	20	n	training process
182	21	26	p	takes
182	27	43	n	roughly 20 hours
182	44	46	p	on
182	49	67	n	single Titan X GPU
175	3	6	p	use
175	7	21	n	100 1D filters
175	22	25	p	for
175	26	44	n	CNN char embedding
175	52	56	p	with
175	59	69	n	width of 5
178	11	47	n	AdaDelta ( Zeiler , 2012 ) optimizer
178	50	54	p	with
178	57	71	n	minibatch size
178	72	74	p	of
178	75	77	n	60
178	85	106	n	initial learning rate
178	107	109	p	of
178	110	113	n	0.5
178	116	119	p	for
178	120	129	n	12 epochs
180	0	6	p	During
180	7	15	n	training
180	22	37	n	moving averages
180	38	40	p	of
180	41	52	n	all weights
180	53	55	p	of
180	60	65	n	model
180	70	85	p	maintained with
180	90	112	n	exponential decay rate
180	113	115	p	of
180	116	121	n	0.999
181	0	2	p	At
181	3	12	n	test time
181	62	70	p	are used
181	19	34	n	moving averages
181	35	45	p	instead of
181	50	61	n	raw weights
17	19	28	p	introduce
17	33	81	n	Bi- Directional Attention Flow ( BIDAF ) network
17	86	123	n	hierarchical multi-stage architecture
17	124	136	p	for modeling
17	141	156	n	representations
17	157	159	p	of
17	164	181	n	context paragraph
17	182	184	p	at
17	185	216	n	different levels of granularity
18	0	5	n	BIDAF
18	6	14	p	includes
18	15	75	n	character - level , word - level , and contextual embeddings
18	82	86	p	uses
18	87	116	n	bi-directional attention flow
18	117	126	p	to obtain
18	129	165	n	query - aware context representation
21	14	23	n	attention
21	27	39	p	computed for
21	40	55	n	every time step
21	66	81	n	attended vector
21	82	84	p	at
21	85	99	n	each time step
21	159	169	p	allowed to
21	170	182	n	flow through
21	183	185	p	to
21	190	215	n	subsequent modeling layer
23	12	15	p	use
23	18	51	n	memory - less attention mechanism
26	3	9	p	forces
26	14	29	n	attention layer
26	30	41	p	to focus on
26	42	50	n	learning
26	55	64	n	attention
26	65	72	p	between
26	77	98	n	query and the context
26	105	112	p	enables
26	117	131	n	modeling layer
26	132	143	p	to focus on
26	144	152	n	learning
26	157	168	n	interaction
26	169	175	p	within
26	180	216	n	query - aware context representation
26	237	246	n	attention
27	8	14	p	allows
27	29	31	p	at
27	32	46	n	each time step
27	47	52	p	to be
27	53	63	n	unaffected
27	64	68	p	from
27	69	90	n	incorrect attendances
27	91	93	p	at
27	94	113	n	previous time steps
29	15	35	n	attention mechanisms
29	36	38	p	in
29	39	54	n	both directions
29	57	77	n	query - to - context
29	82	102	n	context - to - query
29	105	118	p	which provide
29	119	144	n	complimentary information
29	145	147	p	to
29	148	158	n	each other
2	36	57	n	MACHINE COMPREHENSION
4	0	28	n	Machine comprehension ( MC )
5	67	69	n	MC
10	46	71	n	question answering ( QA )
187	0	18	n	BIDAF ( ensemble )
187	19	27	p	achieves
187	31	39	n	EM score
187	40	42	p	of
187	43	47	n	73.3
187	55	64	n	F 1 score
187	65	67	p	of
187	68	72	n	81.1
