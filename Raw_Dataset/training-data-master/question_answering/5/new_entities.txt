160	22	29	p	include
160	46	48	n	GA
160	53	80	n	reading comprehension model
160	81	85	p	with
160	86	103	n	gated - attention
160	106	111	n	BiDAF
160	118	126	n	RC model
160	127	131	p	with
160	132	160	n	bidirectional attention flow
160	163	166	n	AQA
160	173	199	n	reinforced system learning
160	200	212	p	to aggregate
160	217	224	n	answers
160	225	237	p	generated by
160	242	262	n	re-written questions
160	265	268	n	R 3
160	275	291	n	reinforced model
160	292	305	p	making use of
160	308	314	n	ranker
160	315	328	p	for selecting
160	329	337	n	passages
160	338	346	p	to train
160	351	359	n	RC model
184	32	68	n	https://github.com/shuohangwang/mprc
167	9	12	p	use
167	15	36	n	pre-trained R 3 model
167	132	143	p	to generate
167	148	170	n	top 50 candidate spans
167	171	174	p	for
167	179	219	n	training , development and test datasets
167	229	241	p	use them for
167	242	257	n	further ranking
169	0	3	p	For
169	8	34	n	coverage - based re-ranker
169	40	43	p	use
169	44	48	n	Adam
169	49	60	p	to optimize
169	65	70	n	model
171	3	6	p	set
171	7	20	n	all the words
171	21	27	p	beyond
171	28	33	n	Glove
171	34	36	p	as
171	37	49	n	zero vectors
172	7	8	n	l
172	9	11	p	to
172	12	15	n	300
172	18	28	n	batch size
172	29	31	p	to
172	32	34	n	30
172	37	50	n	learning rate
172	51	53	p	to
172	54	59	n	0.002
173	3	7	p	tune
173	12	31	n	dropout probability
173	32	36	p	from
173	37	45	n	0 to 0.5
173	54	81	n	number of candidate answers
173	82	85	p	for
173	86	102	n	re-ranking ( K )
173	103	105	p	in
173	106	120	n	[ 3 , 5 , 10 ]
23	19	26	p	propose
23	29	35	n	method
23	36	46	p	to improve
23	47	63	n	open - domain QA
23	64	89	p	by explicitly aggregating
23	90	98	n	evidence
23	99	110	p	from across
23	111	128	n	multiple passages
35	3	12	p	formulate
35	23	43	n	evidence aggregation
35	44	46	p	as
35	50	75	n	answer re-ranking problem
37	39	42	p	for
37	43	64	n	each answer candidate
37	70	93	p	efficiently incorporate
37	94	112	n	global information
37	113	117	p	from
37	118	133	n	multiple pieces
37	23	25	p	of
37	137	153	n	textual evidence
37	154	161	p	without
37	162	186	n	significantly increasing
37	191	201	n	complexity
37	134	136	p	of
37	209	219	n	prediction
37	202	204	p	of
37	227	235	n	RC model
39	4	14	n	re-rankers
39	15	18	p	are
40	2	28	n	strength - based re-ranker
40	37	42	p	ranks
40	47	64	n	answer candidates
40	65	87	p	according to how often
40	94	102	n	evidence
40	103	112	p	occurs in
40	113	131	n	different passages
43	2	28	n	coverage - based re-ranker
43	37	49	p	aims to rank
43	53	69	n	answer candidate
43	70	76	n	higher
43	77	79	p	if
43	84	89	n	union
43	90	92	p	of
43	93	109	n	all its contexts
43	110	112	p	in
43	113	131	n	different passages
43	132	143	p	could cover
43	144	156	n	more aspects
43	157	168	p	included in
43	173	181	n	question
2	48	80	n	OPEN - DOMAIN QUESTION ANSWERING
16	0	37	n	Open-domain question answering ( QA )
18	15	31	n	open - domain QA
181	12	23	p	showed that
181	24	27	n	R 3
181	28	36	p	achieved
181	37	54	n	F1 56.0 , EM 50.9
181	55	57	p	on
181	58	69	n	Wiki domain
181	74	91	n	F1 68.5 , EM 63.0
181	92	94	p	on
181	95	105	n	Web domain
181	117	131	p	competitive to
181	136	159	n	state - of - the - arts
190	34	42	p	see that
190	47	61	n	full re-ranker
190	68	82	p	combination of
190	83	103	n	different re-rankers
190	106	131	p	significantly outperforms
190	136	161	n	previous best performance
190	162	164	p	by
190	167	179	n	large margin
190	182	195	p	especially on
190	196	206	n	Quasar - T
190	211	220	n	Search QA
192	26	56	n	our coverage - based re-ranker
192	57	65	p	achieves
192	66	95	n	consistently good performance
192	96	98	p	on
192	103	117	n	three datasets
191	11	20	n	our model
191	21	23	p	is
191	24	35	n	much better
191	36	40	p	than
191	45	62	n	human performance
191	63	65	p	on
191	70	87	n	Search QA dataset
