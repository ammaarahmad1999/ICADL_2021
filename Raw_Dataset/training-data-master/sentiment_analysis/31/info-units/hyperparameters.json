{
  "has" : {
    "Hyperparameters" : {
      "use" : {
        "rectifier" : {
          "as" : {
            "non-linear function f" : {
              "in" : "CNN g , CNN t"
            },
            "sigmoid" : {
              "in" : "CNN s"
            }
          }
        },
        "filter window sizes" : {
          "of" : "1 , 2 , 3 , 4 with 100 feature maps each"
        },
        "l 2 regularization term" : {
          "of" : "0.001"
        },
        "minibatch size" : {
          "of" : "25"
        },
        "from sentence" : "We use rectifier as non-linear function f in the CNN g , CNN t and sigmoid in the CNN s , filter window sizes of 1 , 2 , 3 , 4 with 100 feature maps each , l 2 regularization term of 0.001 and minibatch size of 25 ."
      },
      "has" : {
        "Parameterized filters and gates" : {
          "have" : {
            "same size and number" : {
              "as" : "normal filters"
            },
            "from sentence" : "Parameterized filters and gates have the same size and number as normal filters ."
          },
          "generated uniformly by" : {
            "CNN" : {
              "with" : {
                "window sizes" : {
                  "of" : "1 , 2 , 3 , 4"
                }
              }
            },
            "from sentence" : "They are generated uniformly by CNN with window sizes of 1 , 2 , 3 , 4 , eg. among 100 parameterized filters with size 3 , 25 of them are generated by aspect CNN with filter size 1 , 2 , 3 , 4 respectively ."
          }          
        },
        "word embeddings" : {
          "initialized with" : {
            "300 - dimensional Glove vectors" : {
              "fixed during" : "training"
            }
          },
          "from sentence" : "The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training ."
        },
        "dropout rate" : {
          "chosen as" : "0.3",
          "from sentence" : "The dropout rate is chosen as 0.3 ."
        },
        "Training" : {
          "done through" : {
            "mini-batch stochastic gradient descent" : {
              "with" : "Adam update rule"
            }
          },
          "from sentence" : "Training is done through mini-batch stochastic gradient descent with Adam update rule ."
        },
        "initial learning rate" : {
          "is" : "0.001",
          "from sentence" : "The initial learning rate is 0.001 ."
        },
        "training loss" : {
          "does not" : {
            "drop" : {
              "after" : {
                "every three epochs" : {
                  "decrease" : {
                    "learning rate" : {
                      "by" : "half"
                    }
                  }
                }
              }
            }
          },
          "from sentence" : "If the training loss does not drop after every three epochs , we decrease the learning rate by half ."
        }
      },
      "For" : {
        "out of vocabulary words" : {
          "initialize them" : "randomly",
          "from" : "uniform distribution U ( ? 0.01 , 0.01 )",
          "from sentence" : "For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) ."
        }
      },
      "apply" : {
        "dropout" : {
          "on" : {
            "final classification features" : {
              "of" : "PG - CNN"
            }
          },
          "from sentence" : "We apply dropout on the final classification features of PG - CNN ."
        }
      },
      "adopt" : {
        "early stopping" : {
          "based on" : {
            "validation loss" : {
              "on" : "development sets"
            }
          },
          "from sentence" : "We adopt early stopping based on the validation loss on development sets ."
        }
      }
    }
  }
}