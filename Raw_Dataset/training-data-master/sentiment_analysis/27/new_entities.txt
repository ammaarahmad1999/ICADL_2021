196	0	9	n	TD - LSTM
196	10	20	p	constructs
196	21	51	n	aspect-specific representation
196	52	54	p	by
196	59	71	n	left context
196	72	76	p	with
196	77	83	n	aspect
196	92	105	n	right context
196	106	110	p	with
196	111	117	n	aspect
196	125	132	p	employs
196	133	142	n	two LSTMs
197	4	22	n	last hidden states
197	23	25	p	of
197	30	39	n	two LSTMs
197	44	64	n	finally concatenated
197	65	79	p	for predicting
197	84	102	n	sentiment polarity
197	103	105	p	of
197	110	116	n	aspect
198	0	11	n	ATAE - LSTM
198	18	26	p	attaches
198	31	47	n	aspect embedding
198	48	50	p	to
198	51	70	n	each word embedding
198	71	81	p	to capture
198	82	112	n	aspect - dependent information
198	124	131	p	employs
198	132	151	n	attention mechanism
198	152	158	p	to get
198	163	186	n	sentence representation
198	187	190	p	for
198	191	211	n	final classification
199	0	7	n	Mem Net
199	8	12	p	uses
199	15	34	n	deep memory network
199	35	37	p	on
199	42	65	n	context word embeddings
199	66	69	p	for
199	70	93	n	sentence representation
199	94	104	p	to capture
199	109	118	n	relevance
199	119	126	p	between
199	127	159	n	each context word and the aspect
201	0	3	n	IAN
201	4	13	p	generates
201	18	33	n	representations
201	34	37	p	for
201	38	63	n	aspect terms and contexts
201	64	68	p	with
201	69	103	n	two attention - based LSTM network
203	0	3	n	RAM
203	11	18	p	employs
203	21	49	n	gated recurrent unit network
203	50	58	p	to model
203	61	89	n	multiple attention mechanism
203	96	104	p	captures
203	109	118	n	relevance
203	119	126	p	between
203	127	159	n	each context word and the aspect
205	0	4	n	PBAN
205	5	12	p	appends
205	17	35	n	position embedding
205	36	40	p	into
205	41	60	n	each word embedding
207	0	3	n	TSN
207	4	6	p	is
207	9	30	n	two - stage framework
207	31	34	p	for
207	35	68	n	aspect - level sentiment analysis
210	0	3	n	AEN
210	11	22	p	consists of
210	26	41	n	embedding layer
210	47	72	n	attentional encoder layer
210	78	111	n	aspect - specific attention layer
210	121	133	n	output layer
212	0	10	n	AEN - BERT
212	11	13	p	is
212	14	17	n	AEN
212	18	22	p	with
212	23	37	n	BERT embedding
184	130	143	p	to initialize
184	148	163	n	word embeddings
184	40	43	p	use
184	48	67	n	GloVe 3 word vector
184	76	127	n	pre-trained language model word representation BERT
185	4	13	n	dimension
185	14	16	p	of
185	17	33	n	each word vector
185	34	36	p	is
185	37	40	n	300
185	41	44	p	for
185	45	50	n	GloVe
185	55	58	n	768
185	59	62	p	for
185	63	67	n	BERT
186	4	10	n	number
186	11	13	p	of
186	14	31	n	LSTM hidden units
186	35	41	p	set to
186	42	45	n	300
186	56	72	n	output dimension
186	73	75	p	of
186	76	85	n	GCN layer
186	89	95	p	set to
186	96	99	n	600
187	4	17	n	weight matrix
187	18	20	p	of
187	21	45	n	last fully connect layer
187	49	72	p	randomly initialized by
187	75	106	n	normal distribution N ( 0 , 1 )
188	39	62	n	all the weight matrices
188	0	7	p	Besides
188	12	36	n	last fully connect layer
188	67	90	p	randomly initialized by
188	93	133	n	uniform distribution U ( ? 0.01 , 0.01 )
189	17	20	p	add
189	21	38	n	L2-regularization
189	39	41	p	to
189	46	70	n	last fully connect layer
189	71	75	p	with
189	78	84	n	weight
189	85	87	p	of
189	88	92	n	0.01
190	0	6	p	During
190	7	15	n	training
190	21	24	p	set
190	25	32	n	dropout
190	33	35	p	to
190	36	39	n	0.5
190	46	56	n	batch size
190	60	66	p	set to
190	67	69	n	32
190	78	87	n	optimizer
190	57	59	p	is
190	91	105	n	Adam Optimizer
190	106	110	p	with
190	113	126	n	learning rate
190	127	129	p	of
190	130	135	n	0.001
191	3	12	p	implement
191	17	31	n	proposed model
191	32	37	p	using
191	38	48	n	Tensorflow
39	19	26	p	propose
39	29	41	n	novel method
39	42	50	p	to model
39	51	117	n	Sentiment Dependencies with Graph Convolutional Networks ( SDGCN )
39	118	121	p	for
39	122	161	n	aspect - level sentiment classification
40	0	3	n	GCN
40	4	6	p	is
40	9	58	n	simple and effective convolutional neural network
40	59	71	p	operating on
40	72	78	n	graphs
40	87	96	p	can catch
40	97	124	n	inter-dependent information
40	125	129	p	from
40	130	150	n	rich relational data
42	17	23	n	aspect
42	27	37	p	treated as
42	40	44	n	node
42	54	58	n	edge
42	59	69	p	represents
42	74	103	n	sentiment dependency relation
42	104	106	p	of
42	107	116	n	two nodes
41	0	3	p	For
41	4	14	n	every node
41	15	17	p	in
41	18	23	n	graph
41	26	29	n	GCN
41	30	37	p	encodes
41	38	58	n	relevant information
41	59	64	p	about
41	69	82	n	neighborhoods
41	83	85	p	as
41	88	121	n	new feature representation vector
43	10	16	p	learns
43	21	43	n	sentiment dependencies
43	44	46	p	of
43	47	54	n	aspects
43	55	58	p	via
43	64	79	n	graph structure
44	36	53	p	first to consider
44	58	80	n	sentiment dependencies
44	81	88	p	between
44	89	96	n	aspects
44	97	99	p	in
44	100	112	n	one sentence
44	113	116	p	for
44	117	161	n	aspect - level sentiment classification task
45	84	91	p	applies
45	92	125	n	bidirectional attention mechanism
45	126	130	p	with
45	131	148	n	position encoding
45	149	155	p	before
45	156	159	n	GCN
2	70	109	n	Aspect - level Sentiment Classification
14	31	49	n	sentiment analysis
218	0	5	p	Among
218	6	35	n	all the GloVe - based methods
218	42	60	n	TD - LSTM approach
218	61	69	p	performs
218	70	75	n	worst
220	0	12	p	After taking
220	17	27	n	importance
220	28	30	p	of
220	35	41	n	aspect
220	42	46	p	into
220	47	54	n	account
220	55	59	p	with
220	60	79	n	attention mechanism
220	87	94	p	achieve
220	97	115	n	stable improvement
220	116	128	p	comparing to
220	133	142	n	TD - LSTM
221	0	3	n	RAM
221	4	12	p	achieves
221	15	33	n	better performance
221	34	38	p	than
221	39	75	n	other basic attention - based models
222	0	4	n	PBAN
222	5	13	p	achieves
222	16	35	n	similar performance
222	36	38	p	as
222	39	42	n	RAM
222	43	55	p	by employing
222	58	76	n	position embedding
223	25	36	p	better than
223	37	40	n	RAM
223	41	43	p	on
223	44	62	n	Restaurant dataset
223	69	79	p	worse than
223	80	83	n	RAN
223	84	86	p	on
223	87	101	n	Laptop dataset
225	0	3	n	AEN
225	7	27	p	slightly better than
225	28	31	n	TSN
225	38	54	p	still worse than
225	55	67	n	RAM and PBAN
229	15	53	n	two models ( SDGCN - A and SDGCN - G )
229	54	58	p	with
229	59	79	n	position information
229	80	84	p	gain
229	87	110	n	significant improvement
229	111	122	p	compared to
229	127	137	n	two models
229	138	145	p	without
229	146	166	n	position information
224	0	13	p	Compared with
224	14	26	n	RAM and PBAN
224	33	53	n	over all performance
224	54	56	p	of
224	57	60	n	TSN
224	64	80	n	not perform well
224	81	88	p	on both
224	89	126	n	Restaurant dataset and Laptop dataset
227	0	9	p	Comparing
227	14	21	n	results
227	22	24	p	of
227	25	74	n	SDGCN - A w/o position and SDGCN - G w/o position
227	77	100	n	SDGCN - A and SDGCN - G
227	121	128	p	observe
227	138	141	n	GCN
227	142	152	p	built with
227	153	170	n	global - relation
227	174	194	p	slightly higher than
227	195	225	n	built with adjacent - relation
227	226	233	p	in both
227	234	265	n	accuracy and Macro - F1 measure
231	0	13	p	Benefits from
231	18	23	n	power
231	24	26	p	of
231	27	43	n	pre-trained BERT
231	46	65	n	BERT - based models
231	71	76	p	shown
231	77	93	n	huge superiority
231	94	98	p	over
231	99	119	n	GloVe - based models
232	14	27	p	compared with
232	28	38	n	AEN - BERT
232	41	43	p	on
232	48	66	n	Restaurant dataset
232	69	81	n	SDGCN - BERT
232	82	90	p	achieves
232	91	109	n	absolute increases
232	110	112	p	of
232	113	130	n	1.09 % and 1.86 %
232	131	133	p	in
232	134	165	n	accuracy and Macro - F1 measure
232	185	190	p	gains
232	191	209	n	absolute increases
232	210	212	p	of
232	213	230	n	1.42 % and 2.03 %
232	231	233	p	in
232	234	265	n	accuracy and Macro - F1 measure
232	279	281	p	on
232	286	300	n	Laptop dataset
