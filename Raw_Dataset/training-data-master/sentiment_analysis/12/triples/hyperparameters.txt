(Contribution||has||Hyperparameters)
(Hyperparameters||When implementing||our approach)
(our approach||empirically set||maximum iteration number K)
(maximum iteration number K||as||5)
(maximum iteration number K||as||0.1)
(0.1||on||LAPTOP data set)
(maximum iteration number K||as||0.5)
(0.5||on||REST data set)
(maximum iteration number K||as||0.1)
(0.1||on||TWITTER data set)
(Hyperparameters||For||out - of - vocabulary words)
(out - of - vocabulary words||randomly sampled||embeddings)
(embeddings||from||uniform distribution)
(Hyperparameters||used||pre-trained Glo Ve vectors)
(pre-trained Glo Ve vectors||to initialize||word embeddings)
(word embeddings||with||vector dimension)
(vector dimension||has||300)
(Hyperparameters||has||All hyper - parameters)
(All hyper - parameters||tuned on||20 % randomly held - out training data)
(Hyperparameters||has||Adam ( Kingma and Ba , 2015 ))
(Adam ( Kingma and Ba , 2015 )||adopted||optimizer)
(optimizer||with||learning rate)
(learning rate||has||0.001)
(Hyperparameters||To alleviate||overfitting)
(overfitting||employed||dropout strategy ( Hinton et al. , 2012 ))
(dropout strategy ( Hinton et al. , 2012 )||on||input word embeddings)
(input word embeddings||of||LSTM)
