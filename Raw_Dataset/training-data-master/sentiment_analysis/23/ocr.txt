arX1v:1504.01106v5 [cs.CL] 2 Jun 2015

Discriminative Neural Sentence Modeling
by Tree-Based Convolution

Lili Mous Hao Peng* Ge Li! Yan Xu, Lu Zhang, Zhi Jin
{moull12, lige, xuyan14, zhanglu, zhijin} @sei.pku.edu.cn
penghao.pku @ gmail.com
Software Institute, Peking University, 100871, P. R. China

Abstract

This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling. Our models leverage either constituency trees or
dependency trees of sentences. The treebased convolution process extracts sentences’ structural features, and these features are aggregated by max pooling.
Such architecture allows short propagation paths between the output layer and
underlying feature detectors, which enables effective structural feature learning
and extraction. We evaluate our models
on two tasks: sentiment analysis and question classification. In both experiments,
TBCNN outperforms previous state-ofthe-art results, including existing neural
networks and dedicated feature/rule engineering. We also make efforts to visualize
the tree-based convolution process, shedding light on how our models work.

1 Introduction

Discriminative sentence modeling aims to capture
sentence meanings, and classify sentences according to certain criteria (e.g., sentiment). It is related
to various tasks of interest, and has attracted much

attention in the NLP community
Zhao et al. 2015).
Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated

ones (Silva et al., 2011)—can play an important
role in modeling sentences. Kernel machines, e.g.,

SVM, are exploited in [Moschitti (2006) and
ichartz et al. (2010) by specifying a certain mea
sure of similarity between sentences, without explicit feature representation.

“ These authors contribute equally to this paper.
' Corresponding author.

Recent advances of neural networks bring new
techniques in understanding natural languages,
and have exhibited considerable potential.
and[Mikolov etal. 2073) propose unsupervised approaches to learn word embeddings,
mapping discrete words to real-valued vectors in
a meaning space. extend such approaches to learn sentences’ and paragraphs’ representations. Compared with human
engineering, neural networks serve as a way of au
tomatic feature learning (Bengio et al., 2013).

Two widely used neural sentence models are
convolutional neural networks (CNNs) and recursive neural networks (RNNs). CNNs can extract
words’ neighboring features effectively with short
propagation paths, but they do not capture inherent sentence structures (e.g., parsing trees). RNNs
encode, to some extent, structural information by
recursive semantic composition along a parsing
tree. However, they may have difficulties in learning deep dependencies because of long propagation paths (Erhan et al., 2009). (CNNs/RNNs and
a variant, recurrent networks, will be reviewed in
Section|2])

A curious question is whether we can combine the advantages of CNNs and RNNs, Le.,
whether we can exploit sentence structures (like
RNNs) effectively with short propagation paths
(like CNNs).

In this paper, we propose a novel neural architecture for discriminative sentence modeling,
called the Tree-Based Convolutional Neural Network (TBCNN). Our models can leverage different sentence parsing trees, e.g., constituency trees
and dependency trees. The model variants are denoted as c-TBCNN and d-TBCNN, respectively.
The idea of tree-based convolution is to apply a set
of subtree feature detectors, sliding over the entire
parsing tree of a sentence; then pooling aggregates
these extracted feature vectors by taking the maximum value in each dimension. One merit of such
Output layer

- A Representing hidden
layers as vectors

recursively
W, Ww.

[eee]

Pooling layer CL)
ZINN

Extracted

Max pooling
by heuristics

=||=||-Go

   

   

features by Ss = SS layer layer
Ne SN va
— ; Parsing tree Extracted features
Embeddings eee 7 Word embeddings ofa sentence by tree-based convolution
(a) CNN (b) RNN (c) TBCNN

Figure 1: A comparison of information flow in the convolutional neural network (CNN), the recursive
neural network (RNN), and the tree-based convolutional neural network (TBCNN).

architecture is that all features, along the tree, have
short propagation paths to the output layer, and
hence structural information can be learned effectively.

TBCNNs are evaluated on two tasks, sentiment
analysis and question classification; our models
have outperformed previous state-of-the-art results in both experiments. To understand how
TBCNNs work, we also visualize the network by
plotting the convolution process. We make our
code and results available on our project website||

2 Background and Related Work

In this section, we present the background and related work regarding two prevailing neural architectures for discriminative sentence modeling.

2.1 Convolutional Neural Networks

Convolutional neural networks (CNNs), early
used for image processing (LeCun et al., 1995),
turn out to be effective with natural languages
as well. Figure depicts a classic convolution process on a sentence
2008). A set of fixed-width-window feature detectors slide over the sentence, and output the extracted features. Let t be the window size, and
1,°°: ,a@- € IR”* be n-dimensional word embeddings. The output of convolution, evaluated at
the current position, is

y = f (W- [a1;--- ; a] +6)

where y € IR”< (n, is the number of feature detectors). W € Re) and b € R"- are parameters; f is the activation function. Semicolons represent column vector concatenation. After convolution, the extracted features are pooled to a fixedsize vector for classification.

' https://sites.google.com/site/tbcnnsentence/ (This site is
properly anonymized, and complies with the double-blind review requirement.)

Convolution can extract neighboring information effectively. However, the features are
““local”—words that are not in a same convolution window do not interact with each other, even
though they may be semantically related.
build deep convolutional networks so that local features can mix at high-level
layers. Similar deep CNNs include
and |Hu et al. (2014). All these models are “flat,”

by which we mean no structural information is
used explicitly.

2.2 Recursive Neural Networks

Recursive neural networks (RNNs), proposed in
Socher et al. (2011b), utilize sentence parsing
trees. In the original version, RNN is built upon
a binarized constituency tree. Leaf nodes correspond to words in a sentence, represented by n,dimensional embeddings. Non-leaf nodes are sentence constituents, coded by child nodes recursively. Let node p be the parent of c, and co, vector representations denoted as p, c;, and cg. The
parent’s representation is composited by

p= f(W - |e1; 2] + b) (1)
where W and 6 are parameters. This process is
done recursively along the tree; the root vector is
then used for supervised classification (Figure[Ip).

Dependency parsing and the combinatory categorical grammar can also be exploited as RNNs’

skeletons (Hermann and Blunsom, 2013)
al., 2014). |Irsoy and Cardie (2014) build deep

RNNs to enhance information interaction. Improvements for semantic compositionality include
matrix-vector interaction (Socher et al., 2012),
tensor interaction (Socher. et al., 2013). They are
more suitable for capturing logical information in
sentences, such as negation and exclamation.

One potential problem of RNNs is that the long
propagation paths—through which leaf nodes are
connected to the output layer—may lead to information loss. Thus, RNNs bury illuminating information under a complicated neural architecture.
Further, during back-propagation over a long path,
gradients tend to vanish (or blow up), which makes

training difficult (Erhan et al., 2009). Long short

term memory (LSTM), first proposed for model
ing time-series data
/1997), is integrated to RNNs to alleviate this probiem
et al., 2015).

Recurrent networks. A variant class of RNNs
is the recurrent neural network
(Shang et al., 2015), whose architecture is

a rightmost tree. In such models, meaningful tree
Structures are also lost, similar to CNNs.

3 Tree-based Convolution

This section introduces the proposed tree-based
convolutional neural networks (TBCNNs). Figure
depicts the convolution process on a tree.

First, a sentence is converted to a parsing tree,
either a constituency or dependency tree. The
corresponding model variants are denoted as cTBCNN and d-TBCNN. Each node in the tree is
represented as a distributed, real-valued vector.

Then, we design a set of fixed-depth subtree feature detectors, called the tree-based convolution
window. The window slides over the entire tree
to extract structural information of the sentence,
illustrated by a dashed triangle in Figure (ip. Formally, let us assume we have ¢ nodes in the convolution window, %1,--- , Z;, each represented as
an n--dimensional vector. Let n, be the number
of feature detectors. The output of the tree-based
convolution window, evaluated at the current subtree, is given by the following generic equation.

t

yas (Somrn +0) (2)
i=1

where W; € R”«*"< is the weight parameter asso
ciated with node x;; b € IR”< 1s the bias term.

Extracted features are thereafter packed into
one or more fixed-size vectors by max pooling,
that is, the maximum value in each dimension 1s
taken. Finally, we add a fully connected hidden
layer, and a softmax output layer.

From the designed architecture (Figure (Ib), we
see that our TBCNN models allow short propagation paths between the output layer and any position in the tree. Therefore structural feature learning becomes effective.

Several main technical points in tree-based convolution include: (1) How can we represent hidden nodes as vectors in constituency trees? (2)
How can we determine weights, W;, for dependency trees, where nodes may have different numbers of children? (3) How can we pool varying
sized and shaped features to fixed-size vectors?

In the rest of this section, we explain model
variants in detail. Particularly, Subsections|3. I]and
3.2) address the first and second problems; Subsection [3.3] deals with the third problem by introducing several pooling heuristics. Subsection 3.4]
presents our training objective.

3.1 c-TBCNN

Figure illustrates an example of the constituency tree, where leaf nodes are words in the
sentence, and non-leaf nodes represent a grammatical constituent, e.g., a noun phrase. Sentences
are parsed by the Stanford parser{-| further, constituency trees are binarized for simplicity.

One problem of constituency trees is that nonleaf nodes do not have such vector representations
as word embeddings. Our strategy is to pretrain
the constituency tree with an RNN by Equation|I]
(Socher et al., 2011b). After pretraining, vector
representations of nodes are fixed.

We now consider the tree-based convolution
process in c-TBCNN with a two-layer-subtree
convolution window, which operates on a parent
node p and its direct children c; and c,., their vector representations denoted as p, cj, and c,;. The
convolution equation, specific for c-TBCNN, is

y =f (Wi-p+WO-4 + Wc, + 6)

where wi, Ww, and we? are weights associated with the parent and its child nodes. Superscript (c) indicates that the weights are for cTBCNN. For leaf nodes, which do not have children, we set c; and c, to be O.

Tree-based convolution windows can be extended to arbitrary depths straightforwardly. The
complexity is exponential to the depth of the
window, but linear to the number of nodes.
Hence, tree-based convolution, compared with
“flat” CNNs, does not add to computational cost,
provided the same amount of information to process at a time. In our experiments, we use convolution windows of depth 2.

* ‘http://nlp.stanford.edu/software/lex-parser.shtml
Constituency tree c-TBCNN

Extracted features by

(b) fs,
“wea.
- ON
"ON
oe \

7
oo “loved

(cece)

a ¥,
x og ‘ ;

ZA et ‘
77 nsubj

N
/ \ fs %
LL NN it _‘\ SS
7 \ / \
Z \ Z \

—------ Extracted features by
d-TBCNN

Dependency tree

Figure 2: Tree-based convolution in (a) c-TBCNN, and (b) d-TBCNN. The parsing trees correspond to
the sentence “I loved it.” The dashed triangles illustrate a shared-weight convolution window sliding over
the tree. For clarity, only two positions are drawn in c-TBCNN. Notice that dotted arrows are not part of
neural connections; they merely indicate the topologies of tree structures. Specially, an edge a —> b in
the dependency tree refers to a being governed by 6 with dependency type r.

3.2 d-TBCNN

Dependency trees are another representation of
sentence structures. The nature of dependency
representation leads to d-TBCNN’s major difference from traditional convolution: there exist nodes with different numbers of child nodes.
This causes trouble if we associate weight parameters according to positions in the window, which
is standard for traditional convolution, e.g.,
lobert and Weston (2008) or c-TBCNN.

To overcome the problem, we extend the notion of convolution by assigning weights according to dependency types (e.g, nsub 74) rather than
positions. We believe this strategy makes much

sense because dependency types (de Marneffe et
al., 2006) reflect the relationship between a gov
erning word and its child words. To be concrete,
the generic convolution formula (Equation [2) for
d-TBCNN becomes

_ d - (d) d
y=f (w Pt 2 Wiley @ + BI )

where wi is the weight parameter for the parent p (governing word); wi)

TCE
child c;, who has orammatical relationship r[c;|
to its parent, p. Superscript (d) indicates the parameters are for d-TBCNN. Note that we keep 15
most frequently occurred dependency types; others appearing rarely in the corpus are mapped to
one shared weight matrix.

Both c-TBCNN and d-TBCNN have their own
advantages: d-TBCNN exploits structural features
more efficiently because of the compact expressiveness of dependency trees; c-TBCNN may be
more effective in integrating global features due
to the underneath pretrained RNN.

is the weight for

3.3. Pooling Heuristics

As different sentences may have different lengths
and tree structures, the extracted features by tree
Ab

(a) Global pooling (b) 3-slot pooling for c-TBCNN
k pooling slots (A = 2)
Each slot chooses the CJ

s *
. XN XN
maximum value i \ Y \
7 N 4 N
4 N 7 N

in a dimension
/ NX 4 AS
[eeee) ... (eeee| (cece) ... (cece)
Extracted features by tree-based convolution in the order of words
(c) k-slot pooling for d- TBCNN

CD

 

 

 

 

Figure 3: Pooling heuristics. (a) Global pooling.
(b) 3-slot pooling for c-TBCNN. (c) k-slot pooling
for d-TBCNN.

based convolution also have topologies varying in
size and shape. Dynamic pooling
is a common technique for dealing with
this problem. We propose several heuristics for
pooling along a tree structure. Our generic design criteria for pooling include: (1) Nodes that
are pooled to one slot should be “neighboring”
from some viewpoint. (2) Each slot should have
similar numbers of nodes, in expectation, that are
pooled to it. Thus, (approximately) equal amount
of information is aggregated along different parts
of the tree. Following the above intuition, we propose pooling heuristics as follows.

e Global pooling. All features are pooled to
one vector, shown in Figure |3h. We take
the maximum value in each dimension. This
simple heuristic is applicable to any structure,
including c-TBCNN and d-TBCNN.

e 3-slot pooling for c-TBCNN. To preserve
more information over different parts of constituency trees, we propose 3-slot pooling
(Figure [3p). If a tree has maximum depth
d, we pool nodes of less than a - d layers to a TOP slot (a is set to 0.6); lower
 

 

Task Data samples Label
; Offers that rare combination of entertainment and education. ++
Sentiment — a. i

; An idealistic love story that brings out the latent 15-year-old romantic in everyone. +
Analysis ' na .

Its mysteries are transparently obvious, and it’s too slowly paced to be a thriller. =

Question What is the temperature at the center of the earth? number

Classification What state did the Battle of Bighorn take place in? location

Table 1: Data samples in sentiment analysis and question classification. In the first task, “+-+” refers to
strongly positive; “+” and “—” refer to positive and negative, respectively.

nodes are pooled to slots LOWER_LEFT or
LOWER _ RIGHT according to their relative
position with respect to the root node.

For a constituency tree, it is not completely
obvious how to pool features to more than
3 slots and comply with the aforementioned
criteria at the same time. Therefore, we regard 3-slot pooling for c-TBCNN is a “hard
mechanism” temporarily. Further improvement can be addressed in future work.

e k-slot pooling for d-TBCNN. Different from
constituency trees, nodes in dependency trees
are one-one corresponding to words in a sentence. Thus, a total order on features (after convolution) can be defined according
to their corresponding word orders. For kslot pooling, we can adopt an “equal allocation” strategy, shown in Figure (3p. Let
2 be the position of a word in a sentence

(@ = 1,2,---,n). Its extracted feature vector is pooled to the j-th slot, if
(j 1) nm cic . 1
— — a ——

We assess the efficacy of pooling quantitatively
in Section [4.3.1] As we shall see by the experimental results, complicated pooling methods do
preserve more information along tree structures to
some extent, but the effect is not large. TBCNNs
are not very sensitive to pooling methods.

3.4 Training Objective

After pooling, information is packed into one or
more fixed-size vectors (slots). We add a hidden
layer, and then a softmax layer to predict the probability of each target label in a classification task.
The error function of a sample is the standard cross
entropy loss, i.e., J = — S>;_, t; log y;, where t is
the ground truth (one-hot represented), y the output by softmax, and c the number of classes. To
regularize our model, we apply both £2 penalty and

dropout (Srivastava et al., 2014). Training details
are further presented in Section|4. I]and[4.2]

4 Experimental Results

In this section, we evaluate our models with two
tasks, sentiment analysis and question classification. We also conduct quantitative and qualitative
model analysis in Subsection|4.3]

4.1 Sentiment Analysis

4.1.1 The Task and Dataset

Sentiment analysis is a widely studied task for
discriminative sentence modeling. The Stanford
sentiment treebanK?| consists of more than 10,000
movie reviews. Two settings are considered for
sentiment prediction: (1) fine-grained classification with 5 labels (strongly positive,
positive, neutral, negative, and
strongly negative), and (2) coarse-gained
polarity classification with 2 labels (positive
versus negative). Some examples are shown in
Table[I] We use the standard split for training, validating, and testing, containing 8544/1101/2210
sentences for 5-class prediction. Binary classification does not contain the neutral class.

In the dataset, phrases (sub-sentences) are also
tagged with sentiment labels. RNNs deal with
them naturally during the recursive process. We
regard sub-sentences as individual samples during

training, like [Kalchbrenner et al. (2014) and
and Mikolov (2014). The training set therefore has

more than 150,000 entries in total. For validating
and testing, only whole sentences (root labels) are
considered in our experiments.

Both c-TBCNN and d-TBCNN use the Stanford
parser for data preprocessing.

4.1.2 Training Details

This subsection describes training details for dTBCNN, where hyperparameters are chosen by
validation. |. c-TBCNN is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters. c-TBCNN’s settings can be found on our
(anonymized) website.

*http://nlp.stanford.edu/sentiment/
Group

Method Reported in

Baseline SVM 40.7 79.4 Socher. et al. (2013)
Naive Bayes 7 : a . Socher. et al. (2013)

1-layer convolution
Deep CNN
Non-static
Multichannel

CNNs

Kalchbrenner et al. (2014)

Kalchbrenner et al. (2014)
Kim (2014)
iim (2014)

 

 

Basic
Matrix-vector
Tensor

RNNs Tree LSTM (variant 1)

Tree LSTM (variant 2)
Tree LSTM (variant 3)
Deep RNN

Socher. et al. (2013)
Socher. et al. (20T3}
Socher. et al. (2013)
Zhu et al. (2015)

Tai et al. (2015)

Le and Zuidema (2015)

Irsoy and Cardie (2014)

 

LSTM
bi-LSTM

Recurrent

 

ai et al. (2015)
al et al. (2ZUI5)

 

Word vector avg.
Paragraph vector
c-TBCNN
d-TBCNN

Vector

TBCNNs 51.4

 

Socher. et al. (2013)
Le and Mikolov (2014)

Our implementation

87.91 Our implementation

 

Table 2: Accuracy of sentiment prediction (in percentage). For 2-class prediction, “}” remarks indicate
that the network is transferred directly from that of 5-class.

In our d-TBCNN model, the number of units
is 300 for convolution and 200 for the last hidden layer. Word embeddings are 300 dimensional,

pretrained ourselves using word2vec (Mikolov

et al., 2013) on the English Wikipedia corpus. 2slot pooling is applied for d-TBCNN. (c-TBCNN

uses 3-slot pooling.)

To train our model, we compute gradient by
back-propagation and apply stochastic gradient
descent with mini-batch 200. We use ReLU
as the activation function .

For regularization, we add ¢ penalty for
weights with a coefficient of 10~°. Dropout (Sriis further applied to both
weights and embeddings. All hidden layers are
dropped out by 50%, and embeddings 40%.

4.1.3 Performance

Table |2} compares our models to state-of-the-art
results in the task of sentiment analysis. For 5class prediction, d-TBCNN yields 51.4% accuracy, outperforming the previous state-of-the-art
result, achieved by the RNN based on long-short
term memory (Tai et al., 2015). c-TBCNN is
slightly worse. It achieves 50.4% accuracy, ranking third in the state-of-the-art list Gncluding our
d-TBCNN model).

Regarding 2-class prediction, we adopted a sim
ple strategy in|Irsoy and Cardie (2014) where the

* Richard Socher, who first applies neural networks to this
task, thinks direct transfer is fine for binary classification. We
followed this strategy for simplicity as it is non-trivial to deal
with the neutral sub-sentences in the training set if we train a
separate model. Our website reviews some related work and

5-class network is “transferred” directly for binary
classification, with estimated target probabilities
(by 5-way softmax) reinterpreted for 2 classes.
(The neutral class is discarded as in other studies.) This strategy enables us to take a glance at the
stability of our TBCNN models, but places itself
in a difficult position. Nonetheless, our d-TBCNN
model achieves 87.9% accuracy, ranking third in
the list.

In a more controlled comparison—with shallow architectures and the basic interaction (linearly transformed and non-linearly squashed)—
TBCNNs, of both variants, consistently outperform RNNs to a large extent (50.4—-51.4% versus 43.2%); they also consistently outperform “flat” CNNs by more than
10%. Such results show that structures are important when modeling sentences; tree-based convolution can capture these structural information
more effectively than RNNs.

We also observe d-TBCNN achieves higher performance than c-TBCNN. This suggests that compact tree expressiveness 1s more important than integrating global information in this task.

4.2 Question Classification

We further evaluate TBCNN models on a question classification task] The dataset contains
5452 annotated sentences plus 500 test samples in TREC 10. We also use the stan
dard split, like |Silva et al. (2QO11). Target la
provides more discussions.

> Ihttp://cogcomp.cs.illinois.edu/Data/QA/QC/
Method

SVM
10k features + 60 rules
CNN-non-static

Acc. (%) Reported in

95.0 |Silva et al. (2011)

    
  

  

   
  

 
    
   

CNN-mutlichannel Kim (2014)

RNN 90.2 Zhao etal (2013)
Deep-CNN 93.0 |Kalchbrenner et al. (2014)
Ada-CNN 92.4
c-TBCNN 94.8 Our imp Snantion

d-TBCNN Our implementation

Table 3: Accuracy of 6-way question classification.

bels contain 6 classes, namely abbreviation,
entity, description, human, location,
and numeric. Some examples are also shown in
Table[l]

We chose this task to evaluate our models because the number of training samples is rather
small, so that we can know TBCNNs’ performance when applied to datasets of different sizes.
To alleviate the problem of data sparseness, we set
the dimensions of convolutional layer and the last
hidden layer to 30 and 25, respectively. We do
not back-propagate gradient to embeddings in this
task. Dropout rate for embeddings is 30%; hidden
layers are dropped out by 5%.

Table |3| compares our models to various other
methods. The first entry presents the previous
state-of-the-art result, achieved by traditional feature/rule engineering (Silva et al., 2011). Their
method utilizes more than 10k features and 60
hand-coded rules. On the contrary, our TBCNN
models do not use a single human-engineered feature or rule. Despite this, c-TBCNN achieves
similar accuracy compared with feature engineering; d-TBCNN pushes the state-of-the-art result to
96%. To the best of our knowledge, this is the first
time that neural networks beat dedicated human
engineering in this question classification task.

The result also shows that both c-TBCNN and
d-TBCNN reduce the error rate to a large extent,
compared with other neural architectures in this
task.

4.3. Model Analysis

In this part, we analyze our models quantitatively
and qualitatively in several aspects, shedding some
light on the mechanism of TBCNNs.

4.3.1 The Effect of Pooling

The extracted features by tree-based convolution
have topologies varying in size and shape. We propose in Section [3.3] several heuristics for pooling.
This subsection aims to provide a fair comparison
among these pooling methods.

Model Pooling method 5-class accuracy (%)
Global 4848 £0.54
¢-TBCNN 3-slot 48.69 + 0.40
Global 49.39 £0.24
¢-TBCNN 2-slot 49.94 + 0.63

Table 4: Accuracies of different pooling methods,
averaged over 5 random initializations. We chose
sensible hyperparameters manually in advance to
make a fair comparison. This leads to performance
degradation (1-2%) vis-a-vis Table[2]

 

 

 

 

 

29 1014 1519 30-24 25~29 30-34 535
Setence length

Figure 4: Accuracies versus sentence lengths.

One reasonable protocol for comparison is to
tune all hyperparameters for each setting and compare the highest accuracy. This methodology,
however, is too time-consuming, and depends
largely on the quality of hyperparameter tuning.
An alternative is to predefine a set of sensible hyperparameters and report the accuracy under the
same setting. In this experiment, we chose the
latter protocol, where hidden layers are all 300dimensional; no £2 penalty is added. Each configuration was run five times with different random
initializations. We summarize the mean and standard deviation in Table[4]

As the results imply, complicated pooling 1s better than global pooling to some degree for both
model variants. But the effect is not strong; our
models are not that sensitive to pooling methods,
which mainly serve as a necessity for dealing with
varying-structure data. In our experiments, we apply 3-slot pooling for c-TBCNN and 2-slot pooling for d-TBCNN.

Comparing with other studies in the literature,
we also notice that pooling 1s very effective and efficient in information gathering.
report 200 epochs for training a deep RNN,
which achieves 49.8% accuracy in the 5-class sentiment classification. Our TBCNNs are typically
trained within 25 epochs.

4.3.2 The Effect of Sentence Lengths

We analyze how sentence lengths affect our models. Sentences are split into 7 groups by length,
with granularity 5. A few too long or too short
sentences are grouped together for smoothing; the
numbers of sentences in each group vary from 126
  
   
      
 
 

visual (.19)

The — stunning dreamlike even
(0) (07) ~— (.02) (0)

those
(.03)

impress (.26)
viewers (.05)
have (.06)

patience (.01)

who
(.10 for (.01)
little
(.06) pretension (.09)
Euro-film
(.04)

Figure 5: Visualizing how features (after convolution) are related to the sentiment of a sentence. The
sample corresponds a sentence in the dataset, “The stunning dreamlike visual will impress even those
viewers who have little patience for Euro-film pretension.’ The numbers in brackets denote the fraction
of a node’s features that are gathered by the max pooling layer (also indicated by colors).

to 457. Figure|4|presents accuracies versus lengths
in TBCNNs. For comparison, we also reimplemented RNN, achieving 42.7% overall accuracy,
slightly worse than 43.2% reported in
al. (2011b). Thus, we think our reimplementation
is fair and that the comparison is sensible.

We observe that c-TBCNN and d-TBCNN yield
very similar behaviors. They consistently outperform the RNN in all scenarios. We also notice the
gap, between TBCNNs and RNN, increases when
sentences contain more than 20 words. This result confirms our theoretical analysis in Section
(2|for long sentences, the propagation paths in
RNNs are deep, causing RNNs’ difficulty in information processing. By contrast, our models explore structural information more effectively with
tree-based convolution. As information from any
part of the tree can propagate to the output layer
with short paths, TBCNNs are more capable for
sentence modeling, especially for long sentences.

4.3.3 Visualization

Visualization is important to understanding the
mechanism of neural networks. For TBCNNs, we
would like to see how the extracted features (after convolution) are further processed by the max
pooling layer, and ultimately related to the supervised task.

To show this, we trace back where the max
pooling layer’s features come from. For each dimension, the pooling layer chooses the maximum
value from the nodes that are pooled to it. Thus,
we can count the fraction in which a node’s features are gathered by pooling. Intuitively, if a
node’s features are more related to the task, the
fraction tends to be larger, and vice versa.

Figure (5]illustrates an example processed by dTBCNN in the task of sentiment analysis\9 Here,

° We only have space to present one example in the paper.

we applied global pooling because information
tracing is more sensible with one pooling slot.
As shown in the figure, tree-based convolution
can effectively extract information relevant to the
task of interest. The 2-layer windows corresponding to “visual will impress viewers,’ “the stunning
dreamlike visual,’ say, are discriminative to the
sentence’s sentiment. Hence, large fractions (0.24
and 0.19) of their features, after convolution, are
gathered by pooling. On the other hand, words
like the, will, even are known as stop words
1989). They are mostly noninformative for sentiment; hence, no (or minimal) features are gathered. Such results are consistent with human intuition.

We further observe that tree-based convolution
does integrate information of different words in
the window. For example, the word stunning appears in two windows: (a) the window “stunning”
itself, and (b) the window of “the stunning dreamlike visual,’ with root node visual, stunning acting
as a child. We see that Window 6 is more relevant to the ultimate sentiment than Window a,
with fractions 0.19 versus 0.07, even though the
root visual itself is neutral in sentiment. In fact,
Window a has a larger fraction than the sum of its
children’s (the windows of “the,” “stunning,” and
“dreamlike’’).

5 Conclusion

In this paper, we proposed a novel neural discriminative sentence model based on sentence parsing
structures. Our model can be built upon either
constituency trees (denoted as c-TBCNN) or dependency trees (d-TBCNN).

This example was not chosen deliberately. Similar traits can
be found through out the entire gallery, available on our website. Also, we only present d-TBCNN, noticing that dependency trees are intrinsically more suitable for visualization
since we know the “meaning” of every node.
Both variants have achieved high performance
in sentiment analysis and question classification.
d-TBCNN is slightly better than c-TBCNN in our
experiments, and has outperformed previous stateof-the-art results in both tasks. The results show
that tree-based convolution can capture sentences’
structural information effectively, which is useful
for sentence modeling.

References

[Allan et al.2003] J. Allan, C. Wade, and A. Bolivar.
2003. Retrieval and novelty detection at the sentence level. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and
Development in Informaion Retrieval.

[Bengio et al.1994] Y. Bengio, P. Simard, and P. Frasconi. 1994. Learning long-term dependencies with
gradient descent is difficult. JEEE Transactions on
Neural Networks, 5(2):157—166.

[Bengio et al.2003] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic
language model. Journal of Machine Learning Research, 3:1137-1155.

[Bengio et al.2013] Y. Bengio, A. Courville, and P. Vincent. 2013. Representation learning: A review and
new perspectives. JEEE Transactions on Pattern
Analysis and Machine Intelligence, 35(8):17981828.

[Collobert and Weston2008] R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine learning.

[Cui et al.2006] H. Cui, V. Mittal, and M. Datar. 2006.
Comparative experiments on sentiment classification for online product reviews. In Proceedings 21st
AAAI Conference on Artificial Intelligence.

[de Marneffe et al.2006] M. de Marneffe, B. MacCartney, and C. Manning. 2006. Generating typed dependency parses from phrase structure parses. In
Proceedings of Language Resource and Evaluation
Conference.

[Erhan et al.2009] D. Erhan, P. Manzagol, Y. Bengio,
S. Bengio, and P. Vincent. 2009. The difficulty of
training deep architectures and the effect of unsupervised pre-training. In Proceedings of International
Conference on Artificial Intelligence and Statistics.

[Fox1989] C. Fox. 1989. A stop list for general text. In
ACM SIGIR Forum, volume 24, pages 19-21.

[Hermann and Blunsom2013] K. Hermann and P. Blunsom. 2013. The role of syntax in vector space models of compositional semantics. In Proceedings of
the 5Ist Annual Meeting of the Association for Computational Linguistics.

[Hochreiter and Schmidhuber1997] S. Hochreiter and
J. Schmidhuber. 1997. Long short-term memory.
Neural computation, 9(8):1735—1780.

[Hu et al.2014] B. Hu, D. Lu, H. Li, and Q. Chen.
2014. Convolutional neural network architectures
for matching natural language sentences. In Advances in Neural Information Processing Systems.

[Irsoy and Cardie2014] O. Irsoy and C. Cardie. 2014.
Deep recursive neural networks for compositionality in language. In Advances in Neural Information
Processing Systems.

[Iyyer et al.2014] M. _ Iyyer, J. Boyd-Graber,
L. Claudino, R. Socher, and Hal D. III. 2014.
A neural network for factoid question answering
over paragraphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing.

[Kalchbrenner et al.2014] N. Kalchbrenner, E. GrefenStette, and P Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings
of the 52nd Annual Meeting of the Association for
Computational Linguistics.

[Kim2014] Y. Kim. 2014. Convolutional neural networks for sentence classification.

[Le and Mikolov2014] Q. Le and T. Mikolov. 2014.
Distributed representations of sentences and documents. In Proceedings of the 31st International
Conference on Machine Learning.

[Le and Zuidema2015] P. Le and WW. Zuidema.
2015. Compositional distributional semantics
with long short term memory. arXiv preprint
arXiv: 1503.02510.

[LeCun et al.1995] Y. LeCun, L. Jackel, L. Bottou,
A. Brunot, C. Cortes, J. Denker, H. Drucker,
I. Guyon, U. Muller, and E. Sackinger. 1995. Comparison of learning algorithms for handwritten digit
recognition. In Proceedings of International Conference on Artificial Neural Networks.

[Mikolov et al.2013] T. Mikolov, I. Sutskever, K. Chen,
G. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems.

[Moschitti2006] A. Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of European Conference of
Machine Learning, pages 318-329. Springer.

[Nair and Hinton2010] V. Nair and G. Hinton. 2010.
Rectified linear units improve restricted Boltzmann
machines. In Proceedings of the 27th International
Conference on Machine Learning, pages 807-8 14.

[Nakagawa et al.2010] T. Nakagawa, K. Inui, and
S. Kurohashi. 2010. Dependency tree-based sentiment classification using CRFs with hidden variables. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics.
Association for Computational Linguistics.

[Reichartz et al.2010] F. Reichartz, H. Korte, and
G. Paass. 2010. Semantic relation extraction with
kernels over typed dependency trees. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.

[Shang et al.2015] L. Shang, D. Lu, and H. Li. 2015.
Neural responding machine for short-text conversation. arXiv preprint arXiv: 1503.02364.

[Silva et al.2011] J. Silva, L. Coheur, A. Mendes, and
A. Wichert. 2011. From symbolic to sub-symbolic
information in question classification. Artificial Intelligence Review, 35(2):137-154.

[Socher et al.201la] R. Socher, E. Huang, J. Pennin,
C. Manning, and A. Ng. 2011a. Dynamic pooling
and unfolding recursive autoencoders for paraphrase
detection. In Advances in Neural Information Processing Systems.

[Socher et al.2011b] R. Socher, J. Pennington,
E. Huang, A. Ng, and C. Manning. 2011b.
Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.

[Socher et al.2012] R. Socher, B. Huval, C. Manning,
and A. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learning.

[Socher. et al.2013] R. Socher., A. Perelygin, J. Wu,
J. Chuang, C. Manning, A. Ng, and C. Potts. 2013.
Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of
Conference on Empirical Methods in Natural Language Processing.

[Srivastava et al.2014] N. Srivastava, G. Hinton,
A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
2014. Dropout: A simple way to prevent neural
networks from overfitting. Journal of Machine
Learning Research, pages 1929-1958.

[Su and Markert2008] F. Su and K. Markert. 2008.
From words to senses: a case study of subjectivity
recognition. In Proceedings of the 22nd International Conference on Computational Linguistics.

[Tai et al.2015] K. Tai, R. Socher, and D. Manning.
2015. Improved semantic representations from treestructured long short-term memory networks. arXiv
preprint arXiv: 1503.00075, to appear in Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics.

[Zhao et al.2015] H. Zhao, Z. Lu, and P. Poupart. 2015.
Self-adaptive hierarchical sentence model. arXiv
preprint arXiv: 1504.05070, to appear in Proceedints of Intenational Joint Conference in Artificial Intelligence.

[Zhu et al.2015] X. Zhu, P. Sobhani, and Y. Guo. 2015.
Long short-term memory over tree structures. arXiv
preprint arXiv: 1503.04881, to appear in Proceedings of International Conference on Machine learning.
