ICON: Interactive Conversational Memory Network
for Multimodal Emotion Detection

Devamanyu Hazarika', Soujanya Poria’ , Rada Mihalcea®,
Erik Cambria? and Roger Zimmermann!
' School of Computing, National University of Singapore
2 School of Computer Science and Engineering, NTU, Singapore
3 Computer Science & Engineering, University of Michigan, Ann Arbor
{hazarika|rogerz}@comp.nus.edu.sg, {sporia|cambria}@ntu.edu.sg,
mihalcea@umich.edu

Abstract

Emotion recognition in conversations 1s crucial for building empathetic machines. Current work in this domain do not explicitly consider the inter-personal influences that thrive in
the emotional dynamics of dialogues. To this
end, we propose Interactive COnversational
memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos
and hierarchically models the se/f- and interspeaker emotional influences into global memories. Such memories generate contextual
summaries which aid in predicting the emotional orientation of utterance-videos. Our
model outperforms state-of-the-art networks
on multiple classification and regression tasks
in two benchmark datasets.

1 Introduction

Emotions play an important role in our daily life.
A long-standing goal of AI has been to create affective agents that can detect and comprehend emotions. Research in affective computing has mainly
focused on understanding affect (emotions and sentiment) in monologues. However, with increasing
interactions of humans with machines, researchers
now aim at building agents that can seamlessly analyze affective content in conversations. This can
help in creating empathetic dialogue systems, thus
improving the overall human-computer interaction
experience (Young et al., 2018).

Analyzing emotional dynamics in conversations,
however, poses complex challenges. This is due
to the presence of intricate dependencies between
the affective states of speakers participating in the
dialogue. In this paper, we address the problem of
emotion recognition in conversational videos. We
specifically focus on dyadic conversations where
two entities participate in a dialogue.

We propose Interactive COnversational memory Network (ICON), a multimodal network for
identifying emotions in utterance-videos. Here, utterances are units of speech bounded by breaths
or pauses of the speaker. Emotional dynamics in
conversations consist of two important properties:
self and inter-personal dependencies (Motris and
Keltner, 2000). Self-dependencies, also known as
emotional inertia, deal with the aspect of emotional
influence that speakers have on themselves during
conversations (Kuppens et al., 2010). On the other
hand, inter-personal dependencies relate to the emotional influences that the counterparts induce into
a speaker. Conversely, during the course of a dialogue, speakers also tend to mirror their counterparts to build rapport (Navarretta et al., 2016).

Figure 1 demonstrates a sample conversation
from the dataset involving both self and interpersonal dependencies. While most conversational frameworks only focus on self dependencies,
ICON leverages both such dependencies to generate affective summaries of conversations. First,
it extracts multimodal features from all utterancevideos. Next, given a test utterance to be classified,
ICON considers the preceding utterances of both
speakers falling within a context-window and models their self-emotional influences using local gated
recurrent units (GRUs).

Furthermore, to incorporate inter-speaker influences, a global representation is generated using
a GRU that intakes output of the local GRUs. For
each instance in the context-window, the output of
this global GRU is stored as a memory cell. These
memories are then subjected to multiple read/write
cycles that include attention mechanism for generating contextual summaries of the conversational
history. At each iteration, the representation of
the test utterance is improved with this summary
representation and finally used for prediction.

2594

Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2594—2604
Brussels, Belgium, October 31 - November 4, 2018. ©2018 Association for Computational Linguistics
 

 

 

 

 

 

 

 

Person A Person B
f >< | don’t think | can do this
< anymore. [ frustrated ]
_
1 Well | guess you aren’t trying hard
enough. [ neutral ]
a2 Its been three years. | have tried
“ everything. [ frustrated ]
~~

 

——.— —. -—. - -. -— — — - - - - - -- — - - teen

' Maybe you’re not smart enough.
\ [ neutral ]

 

| am smart enough. | am really good at

what I do. I just don’t know how to make
someone else see that. [anger]

 

Figure 1: An abridged dialogue from the IEMOCAP dataset.
P,, 1s frustrated over her long term unemployment and seeks
encouragement (ui, U3). Py, however, is pre-occupied and
replies sarcastically (u4). This enrages P, to appropriate
an angry response (ue). In this dialogue, emotional inertia
is evident in P, who does not deviate from his nonchalant
behavior. P,, however, gets emotionally influenced by her
counterpart. This influence is content-based, not label-based.

The contributions of this paper are as follows:

e We propose ICON, a novel model for emotion recognition that incorporates self and interspeaker influences in a dialogue. Memory networks are used to model contextual summaries
for prediction.

We introduce a multimodal approach that provides comprehensive features from modalities
such as language, visual, and audio in utterancevideos.

e ICON can be considered as a generic framework
for conversational modeling that can be extended
to multi-party conversations.

e Experiments on two benchmark datasets show
that ICON significantly outperforms existing
models on multiple discrete and continuous emotional categories.

The remainder of the paper is organized as follows: Section 2 presents related works; Section 3
formalizes the problem statement and Section 4 describes our proposed approach; Section 5 provides
details on experimental setup; Section 6 reports
the results and related analysis; finally, Section 7
concludes the paper.

2 Related Works

Emotion recognition is an interdisciplinary field of
research with contributions from psychology, cognitive science, machine learning, natural language
processing, and others (Picard, 2010).

Initial research in this area primarily involved
visual and audio processing (Ekman, 1993; Datcu
and Rothkrantz, 2008). The role of text in emotional analysis became evident with later research
such as Alm et al. (2005); Strapparava and Mihalcea (2010). Current research in this domain
is mainly performed from a multimodal learning
perspective (Poria et al., 2017a; BaltruSaitis et al.,
2018). Numerous previous approaches have relied
on fusion techniques that leverage multiple modalities for affect recognition (Soleymani et al., 2012;
Zadeh et al., 2017; Chen et al., 2017; Tzirakis et al.,
2017; Zadeh et al., 2018b).

Understanding conversations is crucial for machines to replicate human language and discourse.
Emotions play an important role in shaping such
social interactions (Ruusuvuori, 2013). Richards
et al. (2003) attribute emotional dynamics to be an
interactive phenomena, rather than being withinperson. We utilize this trait in the design of our
model that accommodates inter-personal dynamics. Being a temporal event, context also plays
an important role in conversational analysis. Poria et al. (2017b) use contextual information from
neighboring utterances of the same speaker to predict emotions. However, there is no provision to
model interactive influences. Work by Yang et al.,
2011; Xiaolan et al., 2013 stresses the study of patterns for emotion transitions. In contrast, we posit
the use of utterance content to model context with
multimodal features.

In the literature, memory networks have been
successfully applied in many areas, including question-answering (Weston et al., 2014;
Sukhbaatar et al., 2015; Kumar et al., 2016), machine translation (Bahdanau et al., 2014), speech
recognition (Graves et al., 2014), and others. In
emotional analysis, Zadeh et al. (2018a) propose a
memory-based sequential learning for multi-view
signals. Although we utilize memory networks,
our work is different as we use memories to encode whole utterances. Also, each memory cell in
our network is processed using GRUs to capture
temporal dependencies. This technique deviates
from the traditional use of embedding matrices to
encode information into memory cells.

ICON builds on our previous research (Hazarika
et al., 2018) that used separate memory networks
for both interlocutors participating in a dyadic conversation. In contrast, ICON adopts an interactive scheme that actively models inter-speaker emotional dynamics with fewer trainable parameters.

2595
3 Problem Setting

Let us define a conversation U to be a set of asynchronous exchange of utterances between two persons P, and P, over time. With 7’ utterances,
U = {uy, ug,..., ur} is a totally ordered set which
can be arranged as a sequence (w1,..., wr) based
on temporal occurrence. Here, each utterance
u; 18 spoken by either P, or Py. Furthermore,
for each A € {a,b}, U) denotes person P,’s individual utterances in U, i.e., Uy = {uj | uj €
U and u; spokenby Py, Vi € [1,/U|]}. This provides two sets of utterances for both the respective
speakers, such that U = U, UUs.

Our aim is to identify the emotions of utterances
in conversational videos. At each time step ¢ ¢
[1,7'] of video U, our model is provided with the
utterance spoken at that time, i.e. w;, and tasked
to predict its emotion. Moreover, we also utilize
the previous utterances within U spoken by both
persons. Considering a context-window of size K,
the preceding utterances of P, and P, (starting with
the most recent) within this context-window can be
represented by H, and Ap, respectively. Formally,
for each \ € {a,b}, H) is created as,

Ay ={u;|ie[t-K,t-1] andu;¢ Uy} (1)

and \H,|\+|Hy|< K (2)

Table 1 provides a sample conversation with a
context-window of size Kt = 5.

U { uf, UZ, U3, UZ, US, UE f
Ua, Up { U1, U2, Ud, Us }, { UZ, UG}
test utterance Us

Ha, Hy { U2, U4, U5 }, { UZ, U6 t

 

Table 1: Sample conversation U with test utterance uz.
Context-window Kk = 5. Here, u; = i*” utterance by Py.

4 Methodology

ICON has been designed as a generic framework
for affective modeling of conversations. Its computations can be categorized as a sequence of four successive modules: Multimodal Feature Extraction,
Self-Influence Module, Dynamic Global-Influence
Module, and Multi-hop Memory. Figure 2 illustrates the overall model.

4.1 Multimodal Feature Extraction

ICON adopts a multimodal framework and performs feature extraction from three modalities, 1.e.,
language (transcripts), audio and visual.

These features are extracted for each utterance
in the conversation and their concatenated vectors serve as the utterance representations. The
motivation of this setup derives from previous
works that demonstrate the effectiveness of multimodal features in creating rich feature representations (D’mello and Kory, 2015). These features
provide complementary information from heterogeneous sources which helps to accumulate comprehensive features. Its need is particularly pronounced in videos as they are often plagued with
noisy signals and missing-information within individual modalities (e.g., facial occlusion, loud background music, imperfect transcriptions).

4.1.1 Textual Features

We employ a convolutional neural network (CNN)
to extract textual features from the transcript of
each utterance. CNNSs are capable of learning
abstract semantic representations of a sentence
based on its words and n-grams (Kalchbrenner
et al., 2014). For our purpose, we utilize a simple
CNN with a single convolutional layer followed by
max-pooling (Kim, 2014). The input to this network consists of pre-trained word embeddings extracted from the 300-dimensional FastText embeddings (Bojanowski et al., 2016). The convolution
layer consists of three filters with sizes f;, f7, f?
with foue feature maps each. We perform 1D convolutions using these filters followed by max-pooling
on its output. The pooled features are finally projected onto a dense layer with dimension d; and its
activations are used as the textual representation
ty, « R®.

4.1.2 Audio Features

Audio plays a significant role in determining the
emotional states of a speaker (De Silva and Ng,
2000; Song et al., 2004). To extract audio features,
we first format the audio of each utterance-video as
a 16-bit PCM WAV file and use the open-sourced
software openSMILE (Eyben et al., 2010). This
tool provides high dimensional vectors for audio
files that summarizes important statistical descriptors such as loudness, pitch, Mel-spectra, MFCC,
etc. Specifically, we use the S13_ComParE! extractor which provides 6373 features for each utterance. The features are then normalized using
Min-Max scaling followed by L2-based feature selection. This selection provides low-dimensional
audio features a, € R@ of dimensions dz.

1
http: //audeering.com/technology/opensmile

2596
4.1.3 Visual Features

Visual indicators such as facial expressions are key
to understand emotions. In our work, we use a deep
3D-CNN to model spatiotemporal features of each
utterance video (Tran et al., 2015). 3D-CNN helps
to understand emotional concepts such as smiling
or frowning that are often spread across multiple
frames of a video with no predefined spatial location. The input to this network is a video with
dimensions (c,h, w, f), where c is the number of
channels, h,w are the height and width of each
frame, with a total of f frames per video.

The network contains three blocks of convolution where each block contains two convolutional
layers followed by max-pooling. For the convolution, 3D filters are employed having dimensions
( fout, fin, ths tus fa): where, F [out /in|/h/w/d] represents the number of feature maps, input channels,
height, width, and depth of the filter, respectively.
After a non-linear reLU activation (LeCun et al.,
2015), max-pooling is performed using a sliding
window of dimensions (my, My, Mp). For an input utterance video, the final features of the third
convolutional block is mapped onto a dense layer
of dimension d, whose activations are used as the
visual features v,, € R®.

4.1.4 Fusion

We generate the final representation of an utterance
u by concatenating all three multimodal features:

u = tanh((W![tu;au;vu]) +b’) — ()

Concatenation is one of the most common fusion
methods (Shwartz et al., 2016). Its simplicity also
allows us to emphasize the contribution of the remaining components of ICON.

4.2 SIM: Self-Influence Module

Given a test utterance wu; to be classified, this module independently processes the histories of both
speakers. SIM consists of two GRUs, GRU; and
GRU;, for Ha and Hy, respectively. For each
A € {a,b}, GRU attempts to model the emotional
inertia of speaker P, which represents the emotional dependency of a speaker with their own previous states. In particular, for each historical utterance u;<; € Hy, an internal memory state nl’ )
is computed by GRUY conditioned on utterance

u; and previous memory state nv “This can be
abbreviated as po = GRUY (ui, Red ).

Gated Recurrent Unit: GRUs are gated recurrent cells introduced by Cho et al. (2014). At time
step 7, GRU computes hidden state s; « Reem by
calculating two gates, r; (reset gate) and z; (update gate) with j*” input x j and previous state $;_1.
The computations are:

Zi = o(V*a; + W*sj-4 + b*)
rj = ao(V'a; a W" 85-1 a 5 b’)
U; = tanh(V" x; + W"(s5-1 ® r;) + b')

8; =(1-2;) @ vu; +2; @ 85-1

In this work, input 7; = u; and s; = nv ) SIM
computes both sequences H> ¢ Réem*|Hal and
Hy « Réem*|Hol ysing the respective GRUs,

Hy = [AY IES = GRUS(Hy) , X€ {a,b} 4)

4.3. DGIM: Dynamic Global Influence
Module

Emotions are not only regarded as internalpsychological phenomena but also interpreted and
processed communicatively through social interactions (Fiehler, 2002). Conversations exemplify
such a scenario where inter-personal emotional influence persists. Theories in cognitive science also
suggest the existence of emotional contagion that
causes humans to mirror their counterpart’s gesture, posture and emotional state (Chartrand and
Bargh, 1999; Navarretta et al., 2016). Additionally,
these interactions occur dynamically through the
discourse of a dialogue.

While modeling the contextual history, we incorporate such properties using a dynamic influence
module. This module maintains a global representation of the conversation and updates it recurrently
at each time step of the A -length conversation history. For any k € [1, K’], the global state is updated
using a GRU operation on the previous state s;_;
and current speaker P)’s SIM memory nv ) for the
corresponding spoken utterance U(_K44-1), Le,
hy! = GRUS(ua_K+k-1)):

Formally, DGIM consists of a GRU network,
GRU, where the k*” global state s, is computed
as:

_ GRUS(AY?, 8,1), if U(t-K+k-1) € Ha

" GRUS(h\”, Sr-1), tf UGe_K+k-1) € Ho

(5)

2597
Prediction

Multi-hop Memory 0: (0@C60e@

ee
EEE EEE EEE EEE EEE EEE EEE EINER ENED

eee

   

 

Attention block S.
U7 :

i ids
Prin (OSOSSO|— P—- mm |

 
   

I

Person A : Test utterance
SIM eee
Multimodal
features
‘Person B

Contextual History

Figure 2: Illustration of ICON. Input conversation is as presented in Table 1.

4.4 Multi-hop Memory

The overall operation of the GRU produces a sequence of memories M = [s1,...,8sxK] ¢ R&**.
These memories incorporate dynamic influences
from each of the / utterances spoken in the history. They serve as a contextual memory bank
from which selective person-specific information
can be incorporated into test utterance u; to get
discriminative features. To achieve this, a series
of R memory read/write cycles are performed that
are coupled with soft attention for refinement of vw;
into a context-aware representation.

The need for multiple hops is inspired by recent
works on memory networks (Kumar et al., 2016;
Weston et al., 2014), which suggests the importance of multiple read/write iterations for performing transitive inference. Multiple hops also help
in improving the focus of attention heads which
might miss essential memories in a single hop. At
the r’? hop, the computations are as follows:

e Memory Read: An attention mechanism is used
to read the memories from r“” memory bank

M“) (Weston et al., 2014). First, each memory

mi ¢ M‘") is matched with test utterance ut”?

(initially, wu’? = w and M@ = M).

This matching generates an attention vector
pe), « R* whose k*” normalized score represents the relevance of k*” memory cell with
respect to the test utterance. Inner product is

used for the matching as follows:
Pirtn = softmar((M™)ul) (6)

Where, softmaxr(x;) = e”'/>,e"%. These
scores are then used to find a weighted representation of the memories as

(ry yk )
m” — » (Datin) (mx) — MO py (7)
k=1

This vector denotes the summary of the context
that is person-specific and based on the test utterance. Finally, the representation of the test
utterance is updated by consolidating itself with
the weighted memory m as:

uth) = tanh(m? + us”) (8)

Memory Write: After the read operation at each
hop, memories are updated for the next hop. For
this purpose, a GRU network, GRU"”, takes the
r*” memory cells M (") as input and reprocesses
this sequence to generate memories M+), ice.,
M(r*)) = GRU™(M). Across all hops, this
write operation can be viewed as that of a stacked
recurrent neural network (RNN) where each level
(or hop) improves the representational output of
the RNN. The parameters of GRU™’ are shared
across all hops.

Final Prediction: We use the (R + 1)*” test utterance vector ult) and get the final prediction

vector through its affine transformation,
o = softmar(W°ul*) 46°) 9)

For classification, dimensions of vector o is the
number of classes C, i.e., o € R and categorical
cross-entropy loss is used as the cost measure for
training. For regression, o is a scalar (without
softmax normalization) whose scores are used to
calculate the mean squared error cost metric.

2598
No. of No. of Avg. histor
Dataset Fold = 2
Utterances Videos length

 

train/val 5810 120 36.54
TEMOCAP | test 1623 31 39.00
train/val | 4368 | 63 43.61
SEMAINE | “test 1430 32 45.61

 

*val = validation set.

Table 2: Summary of datasets. Note: Avg. history length represents the expected number of historical utterances available
for any utterance in the dataset.

5 Experiments

5.1 Datasets

We perform experiments on two benchmark
datasets in dialogue-based emotion detection:
IEMOCAP? (Busso et al., 2008) and SEMAINE? (McKeown et al., 2012).

IEMOCAP isa database consisting of videos of
dyadic conversations between pairs of 10 speakers.
Grouped into five sessions, each pair is assigned
with diverse scenarios for dialogues. Videos are
segmented into utterances with annotations of finegrained emotion categories. We consider six such
categories for the classification task: anger, happiness, sadness, neutral, excitement, and frustration.
The training set is curated using the first 8 speakers
from session 1-4 while session 5 is used for testing.

SEMAINE _ is a video database of human-agent
interactions. Here, users interact with characters
whose responses are based on users’ emotional
state. Specifically, we utilize the AVEC 2012’s fully
continuous sub-challenge (Schuller et al., 2012)
that requires predictions of four continuous affective dimensions: arousal, expectancy, power, and
valence. The gold annotations are available for every 0.2 seconds in each video (Nicolle et al., 2012).
However, to align with our problem statement, we
approximate the utterance-level annotation as the
mean of the continuous values within the spoken
utterance. The sub-challenge provides standard
training and testing splits which has been summarized in Table 2.

5.2. Training Details

20% of the training set is used as validation set
for hyper-parameter tuning. We use the Adam optimizer (Kingma and Ba, 2014) for training the
parameters starting with an initial learning rate
of 0.001. Termination of the training-phase is
decided by early-stopping with a patience of 10

2
3

 

http://sail.usc.edu/iemocap/

 

 

 

http://sspnet.eu/avec2012/

 

Table 3: Hyper-parameter values for the best model.

epochs. The network is subjected to regularization
in the form of Dropout (Srivastava et al., 2014)
and Gradient-clipping for a norm of 40. Finally,
the best hyper-parameters are decided using a gridsearch. Their values are summarized in Table 3.
For multimodal feature extraction, we explore
different designs for the employed CNNs. For text,
we find the single layer CNN to perform at par
with deeper variants. For visual features, however,
a deeper CNN provides better representations. We
also find that contextually conditioned features perform better than context-less features. Thus, in
our experiments, we extract video-level contextual
features for utterances from each modality using
the network proposed by Poria et al. 2017b. These
modified features are then used to form the multimodal utterance representations using equation 3.

5.3. Baselines

We compare our proposed model with multiple
state-of-the-art networks in multimodal utterancelevel emotion detection.

memnet (Sukhbaatar et al., 2015) is an end-toend memory network. For comparison, we modify our network to adopt their embedding-based
memory-encoding in the multi-hop stage.

cLSTM* (Poria et al., 2017b) classifies utterances
using neighboring utterances (of same speaker)
as context. LSTM 1s used for this purpose.

TFN? (Zadeh et al., 2017) models intra- and intermodality dynamics by explicitly aggregating uni, bi- and trimodal interactions. Unlike cLSTM,
contextual utterances are not considered.

MFN (Zadeh et al., 2018a) performs multi-view
learning by using Delta-memory Attention Network, a fusion mechanism to learn cross-view
interactions. Similar to TFN, the modeling is
performed within utterances.

CMN (Hazarika et al., 2018) models separate

contexts for both speaker and listener to an ut
terance. These contexts are stored as memories

and combined with test utterance using attention
mechanism.

4

 

http://github.com/senticnet/
contextual-sentiment-—analysis

 

 

 

 

http://github.com/A2Zadeh/TensorFusionNetwork

2599
 

IEMOCAP: Emotion Categories

 

Wlodels Happy Sad Neutral Angry Excited Frustrated Avg.
acc. Fl acc. Fl acc. Fl acc. Fl acc. Fl acc. Fl acc. Fl

memnet 65.2 62.3 . 59.9 59.5
cLSTM | 25. 5 35. 6 70. 0 66. 3 | 58.8 61.1 ; 59.8 59.0
TFN 23.2 33.7 69.1 64.2 | 63.1 62.4 . 58.8 58.5
MFN 24.0 34.1 72.3' 66.8 | 64.3 62.1 . . 60.1 59.9
CMN 25.7 32.6 67.6 64.6 | 69.9 67.9 . . 61.9 61.4
ICON 64.0' 63.5

 

 

 

Table 4: Performance of ICON on the IEMOCAP dataset. ' represents statistical significance over state-of-the-art scores under

the paired-t test (p < 0.05).

SEMAINE
Models DV DA
MAE r MAE r

     
 
 
 
 
  
 

memnet
cLSTM
TEN

MEN
CMN

18 24 | .19 31

Table 5: Performance on the SEMAINE dataset. Note: MAE
= Mean Absolute Error, r= Pearson’s correlation coefficient,
DV = Valence, DA = Activation/Arousal, DP = Power, DE =
Anticipation/Expectation.

6 Results

Tables 4 and 5 present the results on the IEMOCAP and SEMAINE testing sets, respectively. In
Table 4, we evaluate the mean classification performance using Weighted Accuracy (acc.) and
Fl-Score (F'/) on the discrete emotion categories.
ICON performs better than the compared models
with significant performance increase in emotions
(~2.1% acc.). For each emotion, ICON outperforms all the compared models except for happiness emotion. However, its performance is still at
par with cLSTM without a significant gap. Also,
ICON manages to correctly identify the relatively
similar excitement emotion by a large margin.

In Table 5, evaluations of the four continuous
labels from SEMAINE are performed using Mean

Accuracy
65 65

63
62
i=

) R.: Number of * nope

 

a
‘iat

 

a

     

). K : Context-window size

Figure 3: Trends in the performance of ICON on IEMOCAP
dataset with varying R (hops) and K (Context-window size).

 

 

. IEMOCAP SEMAINE
Modality .
Emotions DV DA DP DE
acc. Fl r r r r

T 58.3. 57.9 | .237) .297) 260 ~~ .225
A 50.7 50.9 | .021 082 .250 = .035
Vv 41.2 39.8 | 001 .068 .251 .001
A+V 52.0 51.2 | .031 122 .283 =©.050
T+A 63.8 63.2 | .237  .310 .272 = .242
T+V 61.4 61.2 | .238 .293 .268 = .239
T+A+V 64.0 63.5 | .243 312) .279 = .244

 

Table 6: Comparison of the performance of ICON on both
TIEMOCAP and SEMAINE considering different modality
combinations. Note: T=Text, A=Audio, V=Video

Absolute Error (WAE) and Pearson’s Correlation
Coefficient (7). In all the labels, ICON attains improved performance over its counterparts, suggesting the efficacy of its context-modeling scheme.

Hyperparameters: We plot the performance
trends of ICON on the IEMOCAP dataset concerning the two main hyperparameters, R (number of
hops) and K (context-window size). For R, the
performance initially improves showing the importance of multiple hops in the memories. However, with a further increase, the hopping recurrence deepens and causes the vanishing gradient
problem. This leads to decrease in performance.
The best performance is obtained at R = 3. For
K, similar trends are observed where performance
improvement is seen by increasing the number of
historical utterances. The best results are obtained
for K = 40 which also aligns with the average number of historical utterances in the dataset (Table 2).
Further increase in context does not provide relevant information and rather leads to performance
degradation due to model confusion.

Multimodality: We investigate the importance
of multimodal features for our task. Table 6
presents the results for different combinations of
modes used by ICON on IEMOCAP. As seen, the
trimodal network provides the best performance
which is preceded by the bimodal variants. Among

2600
 

; IEMOCAP SEMAINE
ICON variants DY DA BE DE
history | DGIM | hop } acc. Fl if r r if
l.| - 10 14 .10 01
2.| self 17 .23 15 .13
3.| dual 19 .24 19 .20
4.) self 19 .23 .22 .20
5.| dual 2l 25 26 22
6.| self 20 .28 .21 .22
7.| dual 24 .31 .27 .24

 

 

 

Table 7: Ablation study for components of ICON.

unimodals, language modality performs the best,
reaffirming its significance in multimodal systems.
Interestingly, the audio and visual modality, on
their own, do not provide good performance, but
when used with text, complementary data is shared
to improve overall performance.

6.1 Ablation Study

To check the importance of the modules present
in ICON, we perform an ablation study where we
remove constituent components and evaluate the
model’s performance. Table 7 provides the results
on this study. In the first variant, none of the histories and the associated context-modeling is used.
This provides the worst relative performance.

Self vs Dual History: We evaluate the scenarios
where only self-history of the speaker is considered
(variants 2, 4, and 6). Compared to the dual-history
variants (variants 3, 5, and 7), these models provide
lesser performance. Reasons involve the provision
of partial information from the conversational histories. Similar trends can be seen for the cLSTM
model in Table 4 which works in the same regime.

DGIM vs no-DGIM: Variants 4 and 5 do not
contain the DGIM. In variant 5, separate memory
banks are created for both histories (M, = Hj
and M, = H;,). Memory hops are also separately
performed without parameter sharing. Absence of

70 Ms long history
MEE medium history 63.3
60) ME short history

50
40

30

Occurrence (%)

22.1

20
| | 14.5
10 a

ICON’s attention on conversational history

 

Figure 4: Distribution of top-attention by ICON on correctly
classified instances in the testing set.

DGIM prevents the storage of dynamic influences
between speakers at each historical time step and
leads to performance deterioration.

Multi-hop vs No-hop: Variants 2 and 3 represent cases where multi-hop is omitted, 1.e., R = 1.
Performance for them are poorer than variants having multi-hop mechanism (variants 4-7). Also, removal of multi-hop leads to worse performance
than the removal of DGIM. This suggests that
multi-hop is more crucial than the latter. However,
best performance is achieved by variant 6 which
contains all the proposed modules in its pipeline.

6.2 Dependency on distant history

For all the test utterances of IEMOCAP correctly
classified by ICON, we analyze the global memories receiving the highest attention. First, we divide
the conversational history (context-length Kk = 40)
into three regions: long, short, and medium. Figure 4 provides a summary of how much the model
attends each of these regions. The short region (labeled green) covering 10 utterances, corresponds
to conversational history just preceding the test
utterance. Utterances which occur more than 30
time steps behind the current test utterance are considered part of the long region (labeled red). Remaining utterances in between fall on the medium
region (labeled blue).

The distribution of top-valued attention scores
across the histories reveal interesting insights.
Most of the correctly classified instances focus on
the immediate or short history. In other words, 63%
of the time, at least one of the top-5 attention value
belongs to a memory in the short-history range.
A significant share is also present for distant history (22%). This result indicates the presence of
long-term emotional dependencies and the need to
consider histories far away from the current test
utterance.

Do you want
Pa: my jacket? [hap]
Pe: Its after eleven.

8 Lets just go home. [ang]
Py: Are you kidding?
* We just got here! [fru]
There is no point

Pg: . "

in coming here [ang]

Figure 5: As a conversation develops, different speakers
induce different affective bias which reflects in the memory
selection for generation of the summaries.

 

Conversation
>

 

oO

 

 

2601
 

We will have your
problem solved.
[neutral]

It won’t listen They have never Tell them this
to me. [angry] worked for me. [angry] _ is ridiculous!
[angry]

    
 

  

Test utterance

Hop 3 U15

Hop2 |_s  D
Hopt || OT El

Tk Uta

 

Fine, fine. First off, my
bill’s wrong. [frustrateq]

a) Self-Emotional Influence

 

You let him kiss

£2 &

And what of it? You wouldn’t have Nice point of view |
[neutral] heard about it. [neutral] _ must say. [angry]

 

you. [angry]

Hop3 | TR ve unerance Ab
. le a low

   

  

Hop 1
Up rrr ttsstsses U17 | am bored of this
conversation. [frustrated]

b) Inter-speaker Influence

Figure 6: Case studies for emotional influence. 20 memories in the history which are nearest to test utterance, i.e. k € [21,40]

are visualized from the trained ICON.

6.3. Dynamic Modeling of Global Memories:

ICON holds the capability to model dynamic interactions between speakers. The memories by its
DGIM (§4.3) are used to create summaries conditioned on the test utterance. Consequently, these
summaries contain characteristics that are specific
to the affective state of the current speaker (of the
test utterance).

Figure 5 presents a sample slice of conversation from the dataset. As seen, summary selection for Person A varies from Person B. Such differences arise due to person-specific characteristics and unique affective interpretations of the conversation. Apart from the inter-speaker variance,
the emotional state of a speaker also varies across
turns.

6.4 Case Studies

To understand ICON’s behavior while processing
the global memories through multi-hop, we manually explore the utterances in the testing set of
IEMOCAP. Figure 6 presents two cases which provide traces of self and inter-personal emotional
influences and were correctly classified by ICON.
Both the figures show the trend where multiple
hops gradually improve the focus of attention mechanism on relevant memories.

In Figure 6a, person P;, registers a complaint to
an operator P,. Throughout the dialogue, P, maintains an angry demeanor while P, remains calm
and neutral (u 4). While classifying utterance w15,
ICON focuses more on the histories uttered by P,
(ug, ug, and w41). This demonstrates ICON’s ability to model self-emotional influences. It should be
noted that emotion of P, here also depends on the
utterances of P, but compared to self-utterances,
this dependency is much less. Figure 6b presents
another scenario where a couple argue over an alleged affair.

A man (P,) is angry over this fact and questions
his partner (P,,) asking for details. The woman
tries to behave unperturbed by providing neutral
responses (t12, U1g¢) but is eventually affected by
P,’s continuous anger and expresses a frustrated
response (u1g). These characteristics are captured
by the attention mechanism applied on the global
memories (generated by DGIM), which finds contextual information from histories that are relevant
to the test utterance u;g. This example displays
the role of inter-speaker influences and how ICON
processes such dependencies.

7 Conclusion

In this paper, we presented ICON, a multimodal
framework for emotion detection in conversations.
ICON capitalizes on modeling contextual information that incorporates self and inter-speaker influences. We accomplish this by using an RNNbased memory network with multi-hop attention
modeling. Experiments show that ICON outperforms state-of-the-art models on multiple benchmark datasets. Extensive evaluations and case studies demonstrate the effectiveness of our proposed
model. Additionally, the ability to visualize the
attentions brings a sense of interpretability to the
model, as it allows us to investigate which utterances in the conversational history provide important emotional cues for the current emotional state
of the speaker.

In the future, we plan to test ICON on other
relevant dialogue-based applications and also use
it for empathetic dialogue generation.

Acknowledgments

This research has been supported in part by Singapore’s Ministry of Education (MOE) Academic Research Fund Tier 1, grant number T1 251RES1713.

2602
References

Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.
2005. Emotions from text: machine learning for
text-based emotion prediction. In Proceedings of
the conference on human language technology and
empirical methods in natural language processing,
pages 579-586. Association for Computational Linguistics.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv: 1409.0473.

Tadas BaltruSaitis, Chaitanya Ahuja, and LouisPhilippe Morency. 2018. Multimodal machine learning: A survey and taxonomy. JEEE Transactions on
Pattern Analysis and Machine Intelligence.

Piotr Bojanowski, Edouard Grave, Armand Joulin,
and Tomas Mikolov. 2016. Enriching word vectors with subword information. arXiv preprint
arXiv: 1607.04606.

Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe
Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S
Narayanan. 2008. Iemocap: Interactive emotional
dyadic motion capture database. Language resources and evaluation, 42(4):335.

Tanya L Chartrand and John A Bargh. 1999. The
chameleon effect: the perception—behavior link and
social interaction. Journal of personality and social
psychology, 76(6):893.

Minghai Chen, Sen Wang, Paul Pu Liang, Tadas BaltruSaitis, Amir Zadeh, and Louis-Philippe Morency.
2017. Multimodal sentiment analysis with wordlevel fusion and reinforcement learning. In Proceedings of the 19th ACM International Conference on
Multimodal Interaction, pages 163—171. ACM.

Kyunghyun Cho, Bart Van Merriénboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv: 1406.1078.

Dragos Datcu and L Rothkrantz. 2008. Semantic audiovisual data fusion for automatic emotion recognition.
Euromedia’2008.

Liyanage C De Silva and Pei Chi Ng. 2000. Bimodal
emotion recognition. In Automatic Face and Gesture Recognition, 2000. Proceedings. Fourth IEEE
International Conference on, pages 332-335. IEEE.

Sidney K D’mello and Jacqueline Kory. 2015. A review and meta-analysis of multimodal affect detection systems. ACM Computing Surveys (CSUR),
47(3):43.

Paul Ekman. 1993. Facial expression and emotion.
American psychologist, 48(4):384.

Florian Eyben, Martin Wollmer, and Bjorn Schuller.
2010. Opensmile: the munich versatile and fast
open-source audio feature extractor. In Proceedings
of the 18th ACM international conference on Multimedia, pages 1459-1462. ACM.

Reinhard Fiehler. 2002. How to do emotions with
words: Emotionality in conversations. The verbal
communication of emotions, pages 79-106.

Alex Graves, Greg Wayne, and Ivo Danihelka.
2014. Neural turing machines. arXiv preprint
arXiv: 1410.5401.

Devamanyu Hazarika, Soujanya Poria, Amir Zadeh,
Erik Cambria, Louis-Philippe Morency, and Roger
Zimmermann. 2018. Conversational memory network for emotion recognition in dyadic dialogue
videos. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume I (Long Papers), volume 1, pages
2122-2132.

Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In ACL 20/4, volume 1, pages
655-665.

Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In EMNLP 201/14, pages
1746-1751.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv: 1412.6980.

Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer,
James Bradbury, Ishaan Gulrajani, Victor Zhong,
Romain Paulus, and Richard Socher. 2016. Ask me
anything: Dynamic memory networks for natural
language processing. In /nternational Conference
on Machine Learning, pages 1378-1387.

Peter Kuppens, Nicholas B Allen, and Lisa B Sheeber.
2010. Emotional inertia and psychological maladjustment. Psychological Science, 21(7):984—991.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
2015. Deep learning. nature, 521(7553):436.

Gary McKeown, Michel Valstar, Roddy Cowie, Maja
Pantic, and Marc Schroder. 2012. The semaine
database: Annotated multimodal records of emotionally colored conversations between a person and a
limited agent. [EEE Transactions on Affective Computing, 3(1):5-17.

Michael W Morris and Dacher Keltner. 2000. How
emotions work: The social functions of emotional
expression in negotiations. Research in organizational behavior, 22:1—50.

Costanza Navarretta, K Choukri, T Declerck, S Goggi,
M Grobelnik, and B Maegaard. 2016. Mirroring
facial expressions and emotions in dyadic conversations. In LREC.

2603
Jérémie Nicolle, Vincent Rapp, Kévin Bailly, Lionel
Prevost, and Mohamed Chetouani. 2012. Robust
continuous prediction of human emotions using multiscale dynamic cues. In Proceedings of the 14th
ACM international conference on Multimodal interaction, pages 501-508. ACM.

Rosalind W Picard. 2010. Affective computing: from
laughter to ieee. JEEE Transactions on Affective
Computing, 1(1):11-17.

Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir
Hussain. 2017a. A review of affective computing:
From unimodal analysis to multimodal fusion. Information Fusion, 37:98—-125.

Soujanya Poria, Erik Cambria, Devamanyu Hazarika,
Navonil Majumder, Amir Zadeh, and Louis-Philippe
Morency. 2017b. Context-dependent sentiment analysis in user-generated videos. In ACL 2017, volume I, pages 873-883.

Jane M Richards, Emily A Butler, and James J Gross.
2003. Emotion regulation in romantic relationships: The cognitive consequences of concealing
feelings. Journal of Social and Personal Relationships, 20(5):599-620.

Johanna Ruusuvuori. 2013. Emotion, affect and conversation. The handbook of conversation analysis,
pages 330-349.

Bjorn Schuller, Michel Valster, Florian Eyben, Roddy
Cowie, and Maja Pantic. 2012. Avec 2012: the continuous audio/visual emotion challenge. In Proceedings of the 14th ACM international conference on
Multimodal interaction, pages 449-456. ACM.

Vered Shwartz, Yoav Goldberg, and Ido Dagan.
2016. Improving hypernymy detection with an integrated path-based and distributional method. arXiv
preprint arXiv: 1603.06076.

Mohammad Soleymani, Maja Pantic, and Thierry Pun.
2012. Multimodal emotion recognition in response
to videos. [EEE transactions on affective computing,
3(2):211-223.

Mingli Song, Jiajun Bu, Chun Chen, and Nan Li. 2004.
Audio-visual based emotion recognition-a new approach. In CVPR 2004, volume 2, pages HII. IEEE.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. The Journal of Machine Learning
Research, 15(1):1929-1958.

Carlo Strapparava and Rada Mihalcea. 2010. Annotating and identifying emotions in text. In Jntelligent
Information Access, pages 21—38. Springer.

Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.
2015. End-to-end memory networks. In NIPS 2015,
pages 2440-2448.

Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. 2015. Learning spatiotemporal features with 3d convolutional networks.
In ICCV 2015, pages 4489-4497.

Panagiotis Tzirakis, George Trigeorgis, Mihalis A
Nicolaou, Bjorn W Schuller, and Stefanos Zafeiriou.
2017. End-to-end multimodal emotion recognition using deep neural networks. [EEE Journal of
Selected Topics in Signal Processing, 11(8):13011309.

Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint
arXiv: 1410.3916.

Peng Xiaolan, Xie Lun, Liu Xin, and Wang Zhiliang. 2013. Emotional state transition model based
on stimulus and personality characteristics. China
Communications, 10(6):146—155.

Liang Yang, Hong-fei LIN, and Wei GUO. 2011. Textbased emotion transformation analysis. Computer
Engineering & Science, 9:026.

Tom Young, Erik Cambria, Iti Chaturvedi, Hao Zhou,
Subham Biswas, and Minlie Huang. 2018. Augmenting end-to-end dialogue systems with commonsense knowledge. In AAAI, pages 4970-4977.

Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2017. Tensor
fusion network for multimodal sentiment analysis.
In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages
1103-1114.

Amir Zadeh, Paul Pu Liang, Navonil Mazumder,
Soujanya Poria, Erik Cambria, and Louis-Philippe
Morency. 2018a. Memory fusion network for multiview sequential learning. In AAAI, pages 5634—
5641.

Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, and Louis-Philippe Morency.
2018b. Multi-attention recurrent network for human
communication comprehension. In AAAI, pages
5642-5649.

2604
