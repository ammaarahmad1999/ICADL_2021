179	0	8	n	Majority
179	9	11	p	is
179	16	30	n	basic baseline
179	39	46	p	chooses
179	51	77	n	largest sentiment polarity
179	78	80	p	in
179	85	97	n	training set
179	98	105	p	to each
179	106	114	n	instance
179	115	117	p	in
179	122	130	n	test set
180	0	6	n	MemNet
180	7	13	p	applys
180	14	34	n	multi-hop attentions
180	35	37	p	on
180	42	57	n	word embeddings
180	60	66	p	learns
180	71	88	n	attention weights
180	89	91	p	on
180	92	112	n	context word vectors
180	113	128	p	with respect to
180	133	154	n	averaged query vector
181	0	3	n	IAN
181	4	24	p	interactively learns
181	29	56	n	coarse - grained attentions
181	57	64	p	between
181	69	87	n	context and aspect
181	94	105	p	concatenate
181	110	117	n	vectors
181	118	121	p	for
181	122	132	n	prediction
182	0	15	n	BILSTM - ATT -G
182	41	47	p	models
182	48	70	n	left and right context
182	71	75	p	with
182	76	103	n	two attention - based LSTMs
182	108	116	p	utilizes
182	117	122	n	gates
182	123	133	p	to control
182	138	148	n	importance
182	149	151	p	of
182	152	164	n	left context
182	167	180	n	right context
182	189	204	n	entire sentence
182	205	208	p	for
182	209	219	n	prediction
183	0	3	n	RAM
183	4	10	p	learns
183	11	31	n	multi-hop attentions
183	32	34	p	on
183	39	52	n	hidden states
183	53	55	p	of
183	56	83	n	bidirectional LSTM networks
183	84	87	p	for
183	88	101	n	context words
183	108	123	p	proposes to use
183	124	135	n	GRU network
183	136	142	p	to get
183	147	164	n	aggregated vector
183	165	169	p	from
183	174	184	n	attentions
186	0	8	n	MGAN - C
186	14	21	p	employs
186	26	53	n	coarse - grained attentions
186	54	57	p	for
186	58	68	n	prediction
187	0	8	n	MGAN - F
187	14	22	p	utilizes
187	27	61	n	proposed fine - grained attentions
187	62	65	p	for
187	66	76	n	prediction
188	0	9	n	MGAN - CF
188	10	21	p	adopts both
188	26	72	n	coarse - grained and fine - grained attentions
189	0	4	n	MGAN
189	5	7	p	is
189	12	58	n	complete multi-grained attention network model
171	21	36	n	word embeddings
171	37	45	p	for both
171	46	70	n	context and aspect words
171	75	89	p	initialized by
171	90	95	n	Glove
172	4	13	n	dimension
172	14	16	p	of
172	17	35	n	word embedding d v
172	40	53	n	hidden stated
173	0	6	p	set to
173	7	10	n	300
174	4	26	n	weight matrix and bias
174	27	30	p	are
174	31	42	n	initialized
174	43	45	p	by
174	46	54	n	sampling
174	55	59	p	from
174	62	100	n	uniform distribution U ( 0.01 , 0.01 )
175	4	15	n	coefficient
175	18	20	p	of
175	21	39	n	L 2 regularization
175	45	47	p	is
175	48	54	n	10 ? 5
176	29	41	n	dropout rate
176	46	52	p	set to
176	53	56	n	0.5
29	19	26	p	propose
29	29	61	n	multi -grained attention network
30	28	62	n	fine - grained attention mechanism
30	133	148	p	to characterize
30	153	178	n	word - level interactions
30	179	186	p	between
30	187	211	n	aspect and context words
30	218	225	p	relieve
30	230	246	n	information loss
30	247	258	p	occurred in
30	259	295	n	coarse - grained attention mechanism
31	17	24	p	utilize
31	29	66	n	bidirectional coarsegrained attention
31	121	138	p	combine them with
31	139	168	n	finegrained attention vectors
31	169	179	p	to compose
31	184	214	n	multigrained attention network
31	215	218	p	for
31	223	258	n	final sentiment polarity prediction
32	28	42	p	to make use of
32	47	94	n	valuable aspect - level interaction information
32	100	106	p	design
32	110	131	n	aspect alignment loss
32	19	21	p	in
32	139	157	n	objective function
32	158	168	p	to enhance
32	173	183	n	difference
32	184	186	p	of
32	191	208	n	attention weights
32	209	216	p	towards
32	221	228	n	aspects
32	229	239	p	which have
32	244	256	n	same context
32	261	291	n	different sentiment polarities
2	38	77	n	Aspect - Level Sentiment Classification
4	70	107	n	aspect level sentiment classification
193	6	14	n	Majority
193	15	23	p	performs
193	24	29	n	worst
195	11	15	n	MGAN
195	16	27	p	outperforms
195	28	54	n	Majority and Feature + SVM
196	6	17	n	ATAE - LSTM
196	18	20	p	is
196	21	27	n	better
196	28	32	p	than
196	33	37	n	LSTM
197	0	9	n	TD - LSTM
197	10	18	p	performs
197	19	34	n	slightly better
197	35	39	p	than
197	40	51	n	ATAE - LSTM
198	19	24	n	worse
198	25	29	p	than
198	30	45	n	our method MGAN
199	6	9	n	IAN
199	10	18	p	achieves
199	19	42	n	slightly better results
199	43	47	p	with
199	52	81	n	previous LSTM - based methods
200	0	10	n	Our method
200	11	32	p	consistently performs
200	33	39	n	better
200	40	44	p	than
200	45	48	n	IAN
202	0	16	n	BILSTM - ATT - G
202	17	23	p	models
202	24	54	n	left context and right context
202	55	60	p	using
202	61	84	n	attention - based LSTMs
202	93	101	p	achieves
202	102	120	n	better performance
202	121	125	p	than
202	126	132	n	MemNet
203	0	3	n	RAM
203	4	12	p	performs
203	13	19	n	better
203	20	24	p	than
203	25	40	n	other baselines
206	0	17	n	Our proposed MGAN
206	18	39	p	consistently performs
206	40	46	n	better
206	47	51	p	than
206	52	58	n	MemNet
206	61	77	n	BILSTM - ATT - G
206	82	85	n	RAM
