242	3	10	p	observe
242	16	63	n	+ Message passing - a and + Message passing - d
242	64	77	p	contribute to
242	82	99	n	performance gains
242	117	129	p	demonstrates
242	134	147	n	effectiveness
242	148	150	p	of
242	155	189	n	proposed message passing mechanism
243	28	68	n	adding documentlevel tasks ( + DS / DD )
243	69	73	p	with
243	74	91	n	parameter sharing
243	97	116	p	marginally improves
243	121	132	n	performance
243	133	135	p	of
243	136	142	n	IMN ?d
245	12	30	n	Message passing -d
245	34	52	p	still helpful with
245	53	83	n	considerable performance gains
157	3	8	p	adopt
157	13	42	n	multi - layer - CNN structure
157	43	47	p	from
157	55	75	n	CNN - based encoders
161	15	49	n	released domainspecific embeddings
161	50	53	p	for
161	54	83	n	restaurant and laptop domains
161	84	88	p	with
161	89	103	n	100 dimensions
161	116	126	p	trained on
161	129	159	n	large domain - specific corpus
161	160	165	p	using
161	166	175	n	fast Text
160	0	3	p	For
160	4	33	n	word embedding initialization
160	39	50	p	concatenate
160	53	87	n	general - purpose embedding matrix
160	94	128	n	domain - specific embedding matrix
162	4	32	n	general - purpose embeddings
162	33	36	p	are
162	37	62	n	pre-trained Glove vectors
162	63	67	p	with
162	68	82	n	300 dimensions
176	0	28	n	Learning rate and batch size
176	33	39	p	set to
176	40	59	n	conventional values
176	60	87	p	without specific tuning for
176	88	96	n	our task
171	3	7	p	tune
171	12	42	n	maximum number of iterations T
171	43	45	p	in
171	50	75	n	message passing mechanism
171	76	87	p	by training
171	88	94	n	IMN ?d
171	95	98	p	via
171	99	115	n	cross validation
172	6	12	p	set to
172	13	14	n	2
175	3	6	p	use
175	7	21	n	Adam optimizer
175	22	26	p	with
175	27	40	n	learning rate
175	41	47	p	set to
175	48	54	n	10 ? 4
175	64	67	p	set
175	68	78	n	batch size
175	79	81	p	to
175	82	84	n	32
177	0	2	p	At
177	3	17	n	training phase
177	23	38	p	randomly sample
177	39	43	n	20 %
177	44	46	p	of
177	51	64	n	training data
177	65	69	p	from
177	74	96	n	aspect - level dataset
177	97	99	p	as
177	104	119	n	development set
177	129	132	p	use
177	137	151	n	remaining 80 %
177	152	155	p	for
177	156	164	n	training
20	18	25	p	propose
20	29	75	n	interactive multitask learning network ( IMN )
21	14	17	n	IMN
21	18	24	p	allows
21	25	34	n	AE and AS
21	35	62	p	to be trained together with
21	63	93	n	related document - level tasks
21	96	106	p	exploiting
21	111	120	n	knowledge
21	121	125	p	from
21	126	157	n	larger document - level corpora
27	25	72	n	fined - grained tokenlevel classification tasks
27	73	100	p	to be trained together with
27	101	138	n	document - level classification tasks
26	170	187	p	explicitly models
26	192	204	n	interactions
26	205	212	p	between
26	213	218	n	tasks
26	72	79	p	through
26	231	256	n	message passing mechanism
26	259	267	p	allowing
26	268	283	n	different tasks
26	284	303	p	to better influence
26	304	314	n	each other
24	4	15	n	information
24	24	37	p	combined with
24	42	70	n	shared latent representation
24	75	92	p	made available to
24	93	102	n	all tasks
24	103	106	p	for
24	107	125	n	further processing
22	4	14	p	introduces
22	17	48	n	novel message passing mechanism
22	54	60	p	allows
22	61	85	n	informative interactions
22	86	93	p	between
22	94	99	n	tasks
23	18	23	p	sends
23	24	42	n	useful information
23	43	47	p	from
23	48	63	n	different tasks
23	64	71	p	back to
23	74	102	n	shared latent representation
28	3	15	p	incorporated
28	16	57	n	two document - level classification tasks
28	60	91	n	sentiment classification ( DS )
28	96	124	n	domain classification ( DD )
28	127	153	p	to be jointly trained with
28	154	163	n	AE and AS
28	166	174	p	allowing
28	179	199	n	aspect - level tasks
28	200	215	p	to benefit from
28	216	244	n	document - level information
2	64	97	n	Aspect - Based Sentiment Analysis
11	0	42	n	Aspect - based sentiment analysis ( ABSA )
12	69	98	n	aspect term extraction ( AE )
12	193	239	n	aspect - level sentiment classification ( AS )
220	10	22	p	observe that
220	23	29	n	IMN ?d
220	33	40	p	able to
220	41	65	n	significantly outperform
220	66	71	p	other
220	72	81	n	baselines
220	82	84	p	on
220	85	87	n	F1
221	0	3	n	IMN
221	12	18	p	boosts
221	23	34	n	performance
221	39	50	p	outperforms
221	55	62	n	best F1
221	75	79	p	from
221	84	93	n	baselines
221	94	96	p	by
221	97	125	n	2.29 % , 1.77 % , and 2.61 %
221	126	128	p	on
221	129	145	n	D1 , D2 , and D3
229	0	9	n	IMN wo DE
229	10	18	p	performs
229	19	34	n	only marginally
229	35	40	p	below
229	41	44	n	IMN
231	0	6	n	IMN ?d
231	10	31	p	more affected without
231	32	60	n	domain - specific embeddings
231	78	89	p	outperforms
231	90	109	n	all other baselines
231	110	116	p	except
231	117	132	n	DECNN - d Trans
232	0	14	n	DECNN - dTrans
232	15	17	p	is
232	20	40	n	very strong baseline
232	47	55	p	exploits
232	56	76	n	additional knowledge
232	77	81	p	from
232	82	96	n	larger corpora
232	97	100	p	for
232	101	111	n	both tasks
233	0	12	n	IMN ?d wo DE
233	16	32	p	competitive with
233	33	47	n	DECNN - dTrans
222	15	18	p	for
222	19	43	n	AE ( F1 - a and F1 - o )
222	46	52	n	IMN ?d
222	53	61	p	performs
222	66	70	n	best
222	71	73	p	in
222	74	84	n	most cases
223	0	3	p	For
223	4	29	n	AS ( acc - s and F1 - s )
223	32	35	n	IMN
223	36	47	p	outperforms
223	48	61	n	other methods
223	62	64	p	by
223	65	78	n	large margins
