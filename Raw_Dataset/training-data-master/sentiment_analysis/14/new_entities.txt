87	0	20	n	Pre-training Emo2Vec
88	0	43	n	Emo2 Vec embedding matrix and the CNN model
88	48	65	p	pre-trained using
88	66	80	n	hashtag corpus
89	0	23	n	Parameters of T and CNN
89	24	27	p	are
89	28	48	n	randomly initialized
89	53	57	n	Adam
89	61	69	p	used for
89	70	82	n	optimization
91	0	3	p	For
91	8	18	n	best model
91	24	27	p	use
91	32	42	n	batch size
91	43	45	p	of
91	46	48	n	16
91	51	65	n	embedding size
91	66	68	p	of
91	69	72	n	100
91	75	87	n	1024 filters
91	92	104	n	filter sizes
91	105	108	p	are
91	109	123	n	1 , 3 ,5 and 7
94	0	21	n	Multi - task training
95	3	7	p	tune
95	8	22	n	our parameters
95	23	25	p	of
95	26	39	n	learning rate
95	42	59	n	L2 regularization
96	3	13	p	early stop
96	14	23	n	our model
96	24	28	p	when
96	33	54	n	averaged dev accuracy
96	55	59	p	stop
96	60	70	n	increasing
97	4	14	n	best model
97	15	19	p	uses
97	20	33	n	learning rate
97	34	36	p	of
97	37	42	n	0.001
97	45	62	n	L2 regularization
97	63	65	p	of
97	66	69	n	1.0
97	72	82	n	batch size
97	83	85	p	of
97	86	88	n	32
98	3	7	p	save
98	12	22	n	best model
98	27	31	p	take
98	36	51	n	embedding layer
98	52	54	p	as
98	55	70	n	Emo2Vec vectors
14	10	22	p	demonstrates
14	27	40	n	effectiveness
14	41	57	p	of incorporating
14	58	74	n	sentiment labels
14	75	77	p	in
14	80	101	n	wordlevel information
14	102	105	p	for
14	106	131	n	sentiment - related tasks
14	132	143	p	compared to
14	144	165	n	other word embeddings
24	7	14	p	propose
24	15	22	n	Emo2Vec
24	31	34	p	are
24	35	63	n	word - level representations
24	64	75	p	that encode
24	76	95	n	emotional semantics
24	96	100	p	into
24	101	138	n	fixed - sized , real - valued vectors
25	7	23	p	propose to learn
25	24	31	n	Emo2Vec
25	32	36	p	with
25	39	68	n	multi-task learning framework
25	69	81	p	by including
25	82	119	n	six different emotion - related tasks
2	11	54	n	Learning Generalized Emotion Representation
115	30	38	n	Emo2 Vec
115	39	56	p	works better than
115	14	27	n	CNN embedding
115	71	73	p	on
115	74	90	n	14 / 18 datasets
115	93	99	p	giving
115	100	135	n	2.6 % absolute accuracy improvement
115	136	139	p	for
115	144	158	n	sentiment task
115	163	197	n	1.6 % absolute f1score improvement
115	198	200	p	on
115	205	216	n	other tasks
117	30	35	p	works
117	36	47	n	much better
117	48	50	p	on
117	51	63	n	all datasets
117	64	70	p	except
117	71	86	n	SS - T datasets
117	173	175	p	on
117	176	201	n	sentiment and other tasks
117	95	100	p	gives
117	101	127	n	3.3 % accuracy improvement
117	132	159	n	4.7 % f 1 score improvement
124	18	32	p	not trained by
124	33	60	n	predicting contextual words
124	15	17	p	is
124	69	73	n	weak
124	74	86	p	on capturing
124	87	117	n	synthetic and semantic meaning
132	0	16	n	GloVe + Emo2 Vec
132	17	25	p	achieves
132	26	45	n	better performances
132	46	48	p	on
132	49	61	n	SOTA results
132	62	64	p	on
132	65	115	n	three datasets ( SE0714 , stress and tube tablet )
132	120	137	n	comparable result
132	138	140	p	to
132	141	145	n	SOTA
132	146	148	p	on
132	149	156	n	dataset
116	3	8	p	shows
116	9	28	n	multi-task training
116	29	44	p	helps to create
116	45	92	n	better generalized word emotion representations
116	93	108	p	than just using
116	111	122	n	single task
121	16	21	p	gives
121	22	39	n	1.3 % improvement
121	40	42	p	in
121	43	51	n	accuracy
121	52	55	p	for
121	60	74	n	sentiment task
121	79	96	n	1.1 % improvement
121	97	99	p	of
121	100	111	n	f 1 - score
121	112	114	p	on
121	119	130	n	other tasks
128	33	45	p	solely using
128	48	65	n	simple classifier
128	66	70	p	with
128	71	95	n	good word representation
128	96	107	p	can achieve
128	108	125	n	promising results
131	0	13	p	Compared with
131	14	48	n	GloVe+ DeepMoji , GloVe + Emo2 Vec
131	49	57	p	achieves
131	58	80	n	same or better results
131	81	83	p	on
131	84	100	n	11 / 14 datasets
131	109	125	p	on average gives
131	126	143	n	1.0 % improvement
136	7	16	p	to detect
136	21	42	n	corresponding emotion
136	45	79	p	more attention needs to be paid to
136	80	85	n	words
