247	4	8	n	RNTN
247	17	42	n	highest reversal accuracy
247	45	67	p	showing its ability to
247	68	117	n	structurally learn negation of positive sentences
255	0	5	p	shows
255	8	20	n	typical case
255	21	29	p	in which
255	30	39	n	sentiment
255	44	48	p	made
255	49	62	n	more positive
255	63	75	p	by switching
255	80	90	n	main class
255	91	95	p	from
255	96	115	n	negative to neutral
259	17	25	p	conclude
259	35	39	n	RNTN
259	43	64	p	best able to identify
259	69	88	n	effect of negations
259	89	93	p	upon
259	99	140	n	positive and negative sentiment sentences
209	3	13	p	compare to
209	14	35	n	commonly used methods
209	36	44	p	that use
209	45	66	n	bag of words features
209	67	71	p	with
209	72	92	n	Naive Bayes and SVMs
209	106	117	n	Naive Bayes
209	118	122	p	with
209	123	145	n	bag of bigram features
211	21	26	n	model
211	32	40	p	averages
211	41	60	n	neural word vectors
211	65	72	p	ignores
211	73	94	n	word order ( VecAvg )
20	4	31	n	Stanford Sentiment Treebank
20	32	34	p	is
20	39	51	n	first corpus
20	52	56	p	with
20	57	82	n	fully labeled parse trees
20	88	98	p	allows for
20	101	118	n	complete analysis
20	119	121	p	of
20	126	147	n	compositional effects
20	148	150	p	of
20	151	172	n	sentiment in language
21	4	10	n	corpus
21	14	22	p	based on
21	27	34	n	dataset
21	53	64	p	consists of
21	65	88	n	11,855 single sentences
21	89	103	p	extracted from
21	104	117	n	movie reviews
22	7	13	n	parsed
22	14	18	p	with
22	23	38	n	Stanford parser
22	54	62	p	total of
22	63	85	n	215,154 unique phrases
22	86	90	p	from
22	97	108	n	parse trees
22	116	128	p	annotated by
22	129	143	n	3 human judges
23	5	16	n	new dataset
23	27	37	p	to analyze
23	42	53	n	intricacies
23	54	56	p	of
23	57	66	n	sentiment
23	71	81	p	to capture
23	82	110	n	complex linguistic phenomena
25	4	24	n	granularity and size
25	46	52	p	enable
25	57	66	n	community
25	67	75	p	to train
25	76	96	n	compositional models
25	106	114	p	based on
25	115	168	n	supervised and structured machine learning techniques
202	0	19	n	Optimal performance
202	20	23	p	for
202	24	34	n	all models
202	39	50	p	achieved at
202	51	68	n	word vector sizes
202	69	76	p	between
202	77	97	n	25 and 35 dimensions
202	102	113	n	batch sizes
202	114	121	p	between
202	122	131	n	20 and 30
206	4	8	n	RNTN
206	15	30	p	usually achieve
206	35	51	n	best performance
206	52	54	p	on
206	59	66	n	dev set
206	67	85	p	after training for
206	86	97	n	3 - 5 hours
212	4	13	n	sentences
212	14	16	p	in
212	21	29	n	treebank
212	30	45	p	were split into
212	48	102	n	train ( 8544 ) , dev ( 1101 ) and test splits ( 2210 )
208	3	6	p	use
208	7	15	n	f = tanh
208	16	18	p	in
208	19	34	n	all experiments
27	92	98	p	called
27	103	143	n	Recursive Neural Tensor Network ( RNTN )
28	0	32	n	Recursive Neural Tensor Networks
28	33	46	p	take as input
28	47	54	n	phrases
28	55	57	p	of
28	58	68	n	any length
29	5	14	p	represent
29	17	23	n	phrase
29	24	31	p	through
29	32	61	n	word vectors and a parse tree
29	71	78	p	compute
29	79	86	n	vectors
29	87	90	p	for
29	91	103	n	higher nodes
29	104	106	p	in
29	111	115	n	tree
29	116	121	p	using
29	126	166	n	same tensor - based composition function
2	59	77	n	Sentiment Treebank
5	102	153	n	richer supervised training and evaluation resources
217	0	6	p	showed
217	14	41	n	fine grained classification
217	42	46	p	into
217	47	56	n	5 classes
217	57	59	p	is
217	62	86	n	reasonable approximation
217	87	97	p	to capture
217	98	124	n	most of the data variation
219	4	8	n	RNTN
219	9	13	p	gets
219	18	37	n	highest performance
219	40	51	p	followed by
219	56	72	n	MV - RNN and RNN
220	4	20	n	recursive models
220	21	25	p	work
220	26	35	n	very well
220	36	38	p	on
220	39	54	n	shorter phrases
220	57	62	p	where
220	63	87	n	negation and composition
220	88	91	p	are
220	92	101	n	important
220	110	135	n	bag of features baselines
220	136	143	p	perform
220	144	148	n	well
220	154	158	p	with
220	159	175	n	longer sentences
229	4	18	p	combination of
229	23	58	n	new sentiment treebank and the RNTN
229	59	65	p	pushes
229	70	86	n	state of the art
229	87	89	p	on
229	90	103	n	short phrases
229	104	109	p	up to
229	110	116	n	85.4 %
