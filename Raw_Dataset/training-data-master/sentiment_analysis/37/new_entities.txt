162	0	4	n	LSTM
163	16	24	n	sentence
163	28	34	p	fed to
163	41	77	n	short - term memory ( LSTM ) network
163	78	90	p	to propagate
163	91	98	n	context
163	99	104	p	among
163	109	126	n	constituent words
166	0	8	n	TD- LSTM
167	12	20	n	sequence
167	21	23	p	of
167	24	108	n	words preceding ( left context ) and succeeding ( right context ) target aspect term
167	113	119	p	fed to
167	120	139	n	two different LSTMs
172	52	63	n	ATAE - LSTM
172	67	79	p	identical to
172	80	89	n	AE - LSTM
172	92	98	p	except
172	103	107	n	LSTM
172	111	119	p	fed with
172	124	137	n	concatenation
172	138	140	p	of
172	141	193	n	aspect - term representation and word representation
33	52	74	p	independently generate
33	75	114	n	aspect - aware sentence representations
33	115	118	p	for
33	119	134	n	all the aspects
33	135	140	p	using
33	141	169	n	gated recurrent unit ( GRU )
33	174	193	n	attention mechanism
34	10	16	p	employ
34	17	32	n	memory networks
34	33	52	p	to repeatedly match
34	57	85	n	target aspect representation
34	86	90	p	with
34	95	108	n	other aspects
34	109	120	p	to generate
34	121	149	n	more accurate representation
34	150	152	p	of
34	157	170	n	target aspect
35	5	27	n	refined representation
35	31	37	p	fed to
35	40	58	n	softmax classifier
35	59	62	p	for
35	63	83	n	final classification
2	62	95	n	Aspect - Based Sentiment Analysis
4	0	18	n	Sentiment analysis
13	0	42	n	Aspect - based sentiment analysis ( ABSA )
26	15	19	n	ABSA
196	6	13	p	evident
196	36	50	n	our IARM model
196	51	62	p	outperforms
196	63	86	n	all the baseline models
196	89	98	p	including
196	103	119	n	state of the art
196	122	132	p	in both of
196	137	144	n	domains
197	3	11	p	obtained
197	12	30	n	bigger improvement
197	31	33	p	in
197	34	47	n	laptop domain
197	50	52	p	of
197	53	58	n	1.7 %
197	61	72	p	compared to
197	73	90	n	restaurant domain
197	93	95	p	of
197	96	101	n	1.4 %
