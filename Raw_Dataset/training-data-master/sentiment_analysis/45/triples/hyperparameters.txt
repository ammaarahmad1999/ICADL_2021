(Contribution||has||Hyperparameters)
(Hyperparameters||set||number of epochs)
(number of epochs||to||4)
(Hyperparameters||use||pre-trained uncased BERT - base model)
(pre-trained uncased BERT - base model||for||fine - tuning)
(Hyperparameters||has||dropout probability)
(dropout probability||at||0.1)
(Hyperparameters||has||initial learning rate)
(initial learning rate||is||2 e - 5)
(Hyperparameters||has||number of self - attention heads)
(number of self - attention heads||is||12)
(Hyperparameters||has||total number of parameters)
(total number of parameters||for||pretrained model)
(total number of parameters||is||110M)
(Hyperparameters||has||batch size)
(batch size||is||24)
(Hyperparameters||has||hidden layer size)
(hidden layer size||is||768)
(Hyperparameters||has||number of Transformer blocks)
(number of Transformer blocks||is||12)
