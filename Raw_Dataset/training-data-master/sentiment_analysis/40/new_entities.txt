149	0	15	n	Average Context
150	23	25	p	of
151	4	13	n	first one
151	16	21	p	named
151	22	28	n	AC - S
151	31	39	p	averages
151	44	56	n	word vectors
151	57	63	p	before
151	68	74	n	target
151	96	101	p	after
151	106	112	n	target
151	83	95	n	word vectors
152	4	14	n	second one
152	17	22	p	named
152	23	25	n	AC
152	28	36	p	averages
152	61	73	n	full context
153	0	3	n	SVM
153	63	65	p	on
153	66	122	n	surface features , lexicon features and parsing features
154	0	8	n	Rec - NN
154	14	26	p	firstly uses
154	27	32	n	rules
154	33	45	p	to transform
154	50	65	n	dependency tree
154	70	73	p	put
154	78	92	n	opinion target
154	93	95	p	at
154	100	104	n	root
154	116	124	p	performs
154	125	145	n	semantic composition
154	146	150	p	with
154	151	164	n	Recursive NNs
154	165	168	p	for
154	169	189	n	sentiment prediction
155	0	8	n	TD- LSTM
155	14	18	p	uses
155	21	53	n	forward LSTM and a backward LSTM
155	54	65	p	to abstract
155	70	81	n	information
155	82	98	p	before and after
155	103	109	n	target
158	0	13	n	TD - LSTM - A
158	19	28	p	developed
158	29	38	n	TD - LSTM
158	50	54	p	have
158	55	68	n	one attention
158	69	71	p	on
158	76	83	n	outputs
160	0	6	n	MemNet
160	12	19	p	applies
160	20	29	n	attention
160	30	44	n	multiple times
160	45	47	p	on
160	52	67	n	word embeddings
160	78	102	n	last attention 's output
160	106	112	p	fed to
160	113	120	n	softmax
160	121	124	p	for
160	125	135	n	prediction
30	19	26	p	propose
30	29	44	n	novel framework
30	45	53	p	to solve
30	64	72	n	problems
30	73	75	p	in
30	76	101	n	target sentiment analysis
31	19	28	n	framework
31	29	41	p	first adopts
31	44	72	n	bidirectional LSTM ( BLSTM )
31	73	83	p	to produce
31	88	146	n	memory ( i.e. the states of time steps generated by LSTM )
31	147	151	p	from
31	156	161	n	input
32	4	17	n	memory slices
32	27	48	p	weighted according to
32	55	73	n	relative positions
32	74	76	p	to
32	81	87	n	target
35	0	13	n	Our framework
35	14	24	p	introduces
35	27	36	n	novel way
35	37	48	p	of applying
35	49	79	n	multiple - attention mechanism
35	80	93	p	to synthesize
35	94	112	n	important features
35	113	115	p	in
35	116	145	n	difficult sentence structures
33	16	19	p	pay
33	20	39	n	multiple attentions
33	40	42	p	on
33	47	73	n	position - weighted memory
33	78	97	p	nonlinearly combine
33	102	119	n	attention results
33	120	124	p	with
33	127	156	n	recurrent network , i.e. GRUs
34	13	18	p	apply
34	19	26	n	softmax
34	27	29	p	on
34	34	40	n	output
34	41	43	p	of
34	48	59	n	GRU network
34	60	70	p	to predict
34	75	84	n	sentiment
34	85	87	p	on
34	92	98	n	target
2	42	67	n	Aspect Sentiment Analysis
167	29	36	n	our RAM
167	37	61	p	consistently outperforms
167	66	82	n	compared methods
168	0	13	n	AC and AC - S
168	14	21	p	perform
168	22	28	n	poorly
169	0	8	n	Rec - NN
169	12	23	p	better than
169	24	33	n	TD - LSTM
169	38	52	p	not as good as
169	53	63	n	our method
172	0	9	n	TD - LSTM
172	10	18	p	performs
172	19	35	n	less competitive
172	36	40	p	than
172	41	51	n	our method
173	0	13	n	TD - LSTM - A
173	19	27	p	performs
173	28	33	n	worse
173	34	38	p	than
173	39	49	n	our method
175	0	6	n	MemNet
175	7	13	p	adopts
175	14	33	n	multiple attentions
175	34	53	p	in order to improve
175	58	75	n	attention results
