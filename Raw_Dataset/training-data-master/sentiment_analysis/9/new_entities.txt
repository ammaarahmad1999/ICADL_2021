192	0	11	n	ATAE - LSTM
192	12	14	p	is
192	17	47	n	classical LSTM - based network
192	48	51	p	for
192	56	64	n	APC task
192	73	80	p	applies
192	85	104	n	attention mechanism
192	105	116	p	to focus on
192	121	136	n	important words
192	137	139	p	in
192	144	151	n	context
195	0	7	n	ATSM -S
196	0	2	p	is
196	5	19	n	baseline model
196	20	22	p	of
196	27	42	n	ATSM variations
196	43	46	p	for
196	47	84	n	Chinese language - oriented ABSA task
198	0	4	n	GANN
198	5	7	p	is
198	8	34	n	novel neural network model
198	35	38	p	for
198	39	47	n	APC task
198	48	62	p	aimed to solve
198	67	108	n	shortcomings of traditional RNNs and CNNs
201	0	3	n	AEN
201	6	8	p	is
201	12	39	n	attentional encoder network
201	40	48	p	based on
201	53	74	n	pretrained BERT model
201	83	96	p	aims to solve
201	101	131	n	aspect polarity classification
202	0	4	n	BERT
202	7	9	p	is
202	12	32	n	BERT - adapted model
202	33	36	p	for
202	37	78	n	Review Reading Comprehension ( RRC ) task
203	0	11	n	BERT - BASE
203	12	14	p	is
203	19	46	n	basic pretrained BERT model
204	3	14	p	adapt it to
204	15	39	n	ABSA multi-task learning
204	48	54	p	equips
204	59	71	n	same ability
204	72	96	p	to automatically extract
204	97	109	n	aspect terms
204	114	122	p	classify
204	123	139	n	aspects polarity
204	140	142	p	as
204	143	160	n	LCF - ATEPC model
207	0	10	n	BERT - ADA
209	0	2	p	is
209	5	40	n	domain - adapted BERT - based model
209	41	53	p	proposed for
209	58	66	n	APC task
209	75	84	p	finetuned
209	89	106	n	BERT - BASE model
209	107	109	p	on
209	110	131	n	task - related corpus
211	0	11	n	LCF - ATEPC
211	14	16	p	is
211	21	47	n	multi -task learning model
211	48	51	p	for
211	56	73	n	ATE and APC tasks
212	0	9	n	LCF - ATE
212	10	13	p	are
212	18	53	n	variations of the LCF - ATEPC model
212	65	77	p	optimize for
212	82	90	n	ATE task
213	0	9	n	LCF - APC
213	10	13	p	are
213	18	43	n	variations of LCF - ATEPC
213	56	68	p	optimize for
213	73	81	n	APC task
213	82	88	p	during
213	89	105	n	training process
48	42	81	n	https://github.com/yangheng95/LCF-ATEPC
32	140	148	p	proposes
32	151	176	n	multi-task learning model
32	177	180	p	for
32	181	214	n	aspect - based sentiment analysis
34	4	23	n	LCF - ATEPC 3 model
34	47	49	p	is
34	52	102	n	novel multilingual and multi-task - oriented model
36	22	30	p	based on
36	31	67	n	multi-head self - attention ( MHSA )
36	72	82	p	integrates
36	87	136	n	pre-trained and the local context focus mechanism
36	139	145	p	namely
36	146	157	n	LCF - ATEPC
37	3	14	p	training on
37	17	76	n	small amount of annotated data of aspect and their polarity
37	96	106	p	adapted to
37	109	130	n	large - scale dataset
37	133	157	p	automatically extracting
37	162	169	n	aspects
37	174	184	p	predicting
37	189	209	n	sentiment polarities
2	70	127	n	Aspect Polarity Classification and Aspect Term Extraction
8	0	42	n	Aspect - based sentiment analysis ( ABSA )
8	136	166	n	aspect term extraction ( ATE )
8	171	209	n	aspect polarity classification ( APC )
9	52	82	n	aspect term polarity inferring
9	115	137	n	aspect term extraction
13	147	177	n	aspect polarity classification
16	0	33	n	Aspect - based sentiment analysis
23	4	7	n	APC
24	62	65	n	ATE
238	4	13	n	CDM layer
238	14	29	p	works better on
238	30	45	n	twitter dataset
244	78	110	n	joint model based on BERT - BASE
244	111	119	p	achieved
244	120	139	n	hopeful performance
244	140	142	p	on
244	143	161	n	all three datasets
244	166	180	p	even surpassed
244	181	222	n	other proposed BERT based improved models
244	223	225	p	on
244	226	239	n	some datasets
248	0	14	n	ATEPC - Fusion
248	15	17	p	is
248	20	40	n	supplementary scheme
248	41	43	p	of
248	44	57	n	LCF mechanism
248	67	73	p	adopts
248	76	93	n	moderate approach
248	94	105	p	to generate
248	106	128	n	local context features
249	25	29	p	show
249	39	50	n	performance
249	59	70	p	better than
249	75	103	n	existing BERT - based models
246	0	13	p	Compared with
246	18	35	n	BERT - BASE model
246	38	48	n	BERT - SPC
246	49	71	p	significantly improves
246	76	98	n	accuracy and F 1 score
246	99	101	p	of
246	102	132	n	aspect polarity classification
247	14	17	p	for
247	22	32	n	first time
247	35	45	n	BERT - SPC
247	46	59	p	has increased
247	64	73	n	F 1 score
247	74	76	p	of
247	77	88	n	ATE subtask
247	89	91	p	on
247	92	106	n	three datasets
247	107	112	p	up to
247	113	117	n	99 %
