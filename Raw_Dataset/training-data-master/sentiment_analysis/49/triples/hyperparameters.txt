(Contribution||has||Hyperparameters)
(Hyperparameters||employ||ReLu activation function)
(ReLu activation function||in||dense layers)
(Hyperparameters||employ||softmax activation)
(softmax activation||in||final classification layer)
(Hyperparameters||Utilizing||dense layer)
(dense layer||project||input features)
(input features||of||all the three modalities)
(input features||to||same dimensions)
(Hyperparameters||use||Bi-directional GRUs)
(Bi-directional GRUs||having||300 neurons)
(300 neurons||followed by||dense layer)
(dense layer||consisting of||100 neurons)
(Hyperparameters||for||Bi - GRU layers)
(Bi - GRU layers||use||dropout)
(dropout||=||0.4 ( MOSI ))
(dropout||=||0.3 ( MOSEI ))
(Hyperparameters||For||training the network)
(training the network||set||batch size)
(batch size||=||32)
(training the network||use||Adam optimizer)
(Adam optimizer||with||cross - entropy loss function)
(training the network||train for||50 epochs)
(Hyperparameters||as a measure of||regularization)
(regularization||set||dropout)
(dropout||=||0.5 ( MOSI ))
(dropout||=||0.3 ( MOSEI ))
