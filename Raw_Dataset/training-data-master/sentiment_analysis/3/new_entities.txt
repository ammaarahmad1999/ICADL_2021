274	17	43	n	both context and knowledge
274	48	60	p	essential to
274	65	83	n	strong performance
274	84	86	p	of
274	87	90	n	KET
274	91	93	p	on
274	94	106	n	all datasets
275	10	18	p	removing
275	19	26	n	context
275	33	47	n	greater impact
275	48	50	p	on
275	51	69	n	long conversations
275	70	74	p	than
275	75	94	n	short conversations
199	0	6	n	c LSTM
199	11	32	n	contextual LSTM model
200	3	39	n	utterance - level bidirectional LSTM
200	48	57	p	to encode
200	58	72	n	each utterance
201	2	37	n	context - level unidirectional LSTM
201	46	55	p	to encode
201	60	67	n	context
204	0	9	n	CNN+cLSTM
204	15	18	n	CNN
204	27	37	p	to extract
204	38	56	n	utterance features
205	3	9	n	c LSTM
205	18	34	p	applied to learn
205	35	58	n	context representations
206	0	9	n	BERT BASE
208	3	8	p	treat
208	9	23	n	each utterance
208	24	28	p	with
208	33	40	n	context
208	41	43	p	as
208	46	61	n	single document
209	3	8	p	limit
209	13	28	n	document length
209	29	31	p	to
209	36	51	n	last 100 tokens
209	52	60	p	to allow
209	61	78	n	larger batch size
211	0	11	n	DialogueRNN
211	18	43	n	stateof - the - art model
211	44	47	p	for
211	48	65	n	emotion detection
211	66	68	p	in
211	69	90	n	textual conversations
212	3	14	p	models both
212	15	47	n	context and speakers information
216	0	18	n	KET SingleSelfAttn
217	3	10	p	replace
217	15	44	n	hierarchical self - attention
217	45	47	p	by
217	50	79	n	single self - attention layer
217	80	88	p	to learn
217	89	112	n	context representations
218	0	21	n	Contextual utterances
218	22	25	p	are
218	26	47	n	concatenated together
218	48	56	p	prior to
218	61	90	n	single self - attention layer
219	0	11	n	KET StdAttn
220	3	10	p	replace
220	15	61	n	dynamic contextaware affective graph attention
220	62	64	p	by
220	69	93	n	standard graph attention
222	3	15	p	preprocessed
222	16	28	n	all datasets
222	29	31	p	by
222	32	46	n	lower - casing
222	51	63	n	tokenization
222	64	69	p	using
222	70	75	n	Spacy
224	3	6	p	use
224	11	24	n	released code
224	25	28	p	for
224	29	54	n	BERT BASE and DialogueRNN
227	7	23	n	Glo Ve embedding
227	24	27	p	for
227	28	42	n	initialization
227	43	45	p	in
227	50	83	n	word and concept embedding layers
225	0	3	p	For
225	4	16	n	each dataset
225	19	29	n	all models
225	30	33	p	are
225	34	46	n	fine - tuned
225	47	55	p	based on
225	62	73	n	performance
225	74	76	p	on
225	81	95	n	validation set
226	4	13	n	our model
226	14	16	p	in
226	17	29	n	all datasets
226	35	38	p	use
226	39	81	n	Adam optimization ( Kingma and Ba , 2014 )
226	82	86	p	with
226	89	99	n	batch size
226	100	102	p	of
226	103	105	n	64
226	110	123	n	learning rate
226	124	126	p	of
226	127	133	n	0.0001
228	8	21	n	class weights
228	22	24	p	in
228	25	45	n	cross - entropy loss
228	46	49	p	for
228	50	62	n	each dataset
228	68	79	p	set them as
228	84	89	n	ratio
228	90	92	p	of
228	97	115	n	class distribution
228	116	118	p	in
228	123	137	n	validation set
228	138	140	p	to
228	145	163	n	class distribution
228	164	166	p	in
228	171	183	n	training set
29	17	24	p	propose
29	27	67	n	Knowledge - Enriched Transformer ( KET )
29	83	94	p	incorporate
29	95	146	n	contextual information and external knowledge bases
33	27	66	n	hierarchical self - attention mechanism
33	67	75	p	allowing
33	76	79	n	KET
33	80	88	p	to model
33	93	132	n	hierarchical structure of conversations
31	56	67	n	Transformer
31	4	48	n	self - attention and cross-attention modules
31	68	75	n	capture
31	80	126	n	intra-sentence and inter-sentence correlations
32	4	16	n	shorter path
32	17	19	p	of
32	20	36	n	information flow
32	90	96	n	allows
32	97	100	n	KET
32	101	109	p	to model
32	110	149	n	contextual information more efficiently
34	10	19	p	separates
34	20	40	n	context and response
34	41	45	p	into
34	50	69	n	encoder and decoder
35	14	21	p	exploit
35	22	43	n	commonsense knowledge
35	49	57	p	leverage
35	58	82	n	external knowledge bases
35	83	96	p	to facilitate
35	101	127	n	understanding of each word
35	128	130	p	in
35	135	145	n	utterances
35	146	161	p	by referring to
35	162	188	n	related knowledge entities
36	4	21	n	referring process
36	22	24	p	is
36	25	32	n	dynamic
36	37	53	p	balances between
36	54	83	n	relatedness and affectiveness
36	84	86	p	of
36	91	119	n	retrieved knowledge entities
36	120	125	p	using
36	128	179	n	context - aware affective graph attention mechanism
2	37	79	n	Emotion Detection in Textual Conversations
5	12	55	n	detecting emotions in textual conversations
14	32	113	n	detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations
237	0	6	n	c LSTM
237	7	15	p	performs
237	16	31	n	reasonably well
237	32	34	p	on
237	35	84	n	short conversations ( i.e. , EC and DailyDialog )
237	95	100	n	worst
237	101	103	p	on
237	104	161	n	long conversations ( i.e. , MELD , EmoryNLP and IEMOCAP )
240	0	9	n	BERT BASE
240	10	18	p	achieves
240	19	47	n	very competitive performance
240	48	50	p	on
240	51	63	n	all datasets
240	64	70	p	except
240	71	73	n	EC
240	74	80	p	due to
240	85	114	n	strong representational power
240	115	118	p	via
240	119	151	n	bi-directional context modelling
240	152	157	p	using
240	162	173	n	Transformer
243	16	27	n	DialogueRNN
243	28	48	p	performs better than
243	49	58	n	our model
243	59	61	p	on
243	62	69	n	IEMOCAP
246	4	16	n	KET variants
246	17	51	n	KET SingleSelfAttn and KET StdAttn
246	52	59	p	perform
246	60	70	n	comparably
246	71	75	p	with
246	80	94	n	best baselines
246	95	97	p	on
246	98	110	n	all datasets
246	111	117	p	except
246	118	125	n	IEMOCAP
247	32	48	n	noticeably worse
247	49	53	p	than
247	54	57	n	KET
247	58	60	p	on
247	61	73	n	all datasets
247	74	80	p	except
247	81	83	n	EC
248	68	76	n	on a par
248	77	81	p	with
248	86	95	n	KET model
248	96	98	p	on
248	99	101	n	EC
239	14	18	p	when
239	23	45	n	utterance - level LSTM
239	46	48	p	in
239	49	55	n	c LSTM
239	59	70	p	replaced by
239	71	79	n	features
239	80	92	p	extracted by
239	93	96	n	CNN
239	129	134	n	model
239	135	143	p	performs
239	144	164	n	significantly better
239	165	169	p	than
239	116	122	n	c LSTM
239	177	179	p	on
239	180	198	n	long conversations
245	13	27	p	indicates that
245	28	37	n	our model
245	38	40	p	is
245	41	47	n	robust
245	48	54	p	across
245	55	63	n	datasets
245	64	68	p	with
245	69	121	n	varying training sizes , context lengths and domains
