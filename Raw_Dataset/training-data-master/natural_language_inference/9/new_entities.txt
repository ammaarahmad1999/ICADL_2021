229	58	62	p	When
229	67	83	n	number of layers
229	84	86	p	is
229	87	95	n	only one
229	102	107	n	model
229	108	113	p	lacks
229	114	134	n	reasoning capability
230	0	14	p	In the case of
230	15	26	n	1 k dataset
230	29	33	p	when
230	44	59	n	too many layers
230	77	95	p	correctly training
230	100	105	n	model
230	106	113	p	becomes
230	114	136	n	increasingly difficult
231	15	27	n	10 k dataset
231	30	77	n	many layers ( 6 ) and hidden dimensions ( 200 )
231	78	83	p	helps
231	84	93	n	reasoning
232	6	12	p	Adding
232	17	27	n	reset gate
232	28	33	p	helps
233	6	15	p	Including
233	16	28	n	vector gates
233	29	34	n	hurts
233	35	37	p	in
233	38	50	n	1 k datasets
234	20	32	n	vector gates
234	33	35	p	in
234	36	70	n	bAbI story - based QA 10 k dataset
234	71	80	p	sometimes
234	81	85	n	help
236	10	27	p	hypothesized that
236	30	49	n	larger hidden state
236	53	65	p	required for
236	66	75	n	real data
235	6	16	p	Increasing
235	21	30	n	dimension
235	31	33	p	of
235	38	50	n	hidden state
235	51	53	p	to
235	54	57	n	100
235	58	60	p	in
235	65	91	n	dialog 's Task 6 ( DSTC2 )
235	92	97	p	helps
217	6	13	p	include
217	14	18	n	LSTM
217	21	59	n	End - to - end Memory Networks ( N2N )
217	62	95	n	Dynamic Memory Networks ( DMN + )
217	98	148	n	Gated End - to - end Memory Networks ( GMe m N2N )
217	155	193	n	Differentiable Neural Computer ( DNC )
204	3	11	p	withhold
204	12	16	n	10 %
204	17	19	p	of
204	24	32	n	training
204	33	36	p	for
204	37	48	n	development
205	3	6	p	use
205	11	28	n	hidden state size
205	29	31	p	of
205	32	34	n	50
205	35	37	p	by
205	38	45	n	deafult
206	0	11	n	Batch sizes
206	12	14	p	of
206	15	17	n	32
206	18	21	p	for
206	22	46	n	bAbI story - based QA 1k
206	49	61	n	bAb I dialog
206	66	78	n	DSTC2 dialog
206	85	88	n	128
206	89	92	p	for
206	93	105	n	bAbI QA 10 k
207	4	11	n	weights
207	12	14	p	in
207	19	43	n	input and output modules
207	48	64	p	initialized with
207	65	74	n	zero mean
207	83	112	n	standard deviation of 1 / ? d
209	0	11	n	Forget bias
209	12	14	p	of
209	15	18	n	2.5
209	22	30	p	used for
209	31	43	n	update gates
210	0	15	n	L2 weight decay
210	16	18	p	of
210	19	47	n	0.001 ( 0.0005 for QA 10 k )
210	51	59	p	used for
210	60	71	n	all weights
211	4	17	n	loss function
211	18	20	p	is
211	25	38	n	cross entropy
211	39	46	p	between
211	47	73	n	v and the one - hot vector
211	74	76	p	of
211	81	92	n	true answer
212	4	8	n	loss
212	12	24	p	minimized by
212	25	52	n	stochastic gradient descent
212	53	56	p	for
212	57	77	n	maximally 500 epochs
212	84	92	n	training
212	9	11	p	is
212	96	109	n	early stopped
212	110	112	p	if
212	117	121	n	loss
212	122	124	p	on
212	129	145	n	development data
212	151	163	n	not decrease
212	164	167	p	for
212	168	177	n	50 epochs
213	4	17	n	learning rate
213	21	34	p	controlled by
213	35	42	n	AdaGrad
213	43	47	p	with
213	52	73	n	initial learning rate
213	74	76	p	of
213	77	100	n	0.5 ( 0.1 for QA 10 k )
214	63	69	p	repeat
214	70	93	n	each training procedure
214	94	124	n	10 times ( 50 times for 10 k )
214	125	129	p	with
214	134	159	n	new random initialization
214	160	162	p	of
214	167	174	n	weights
24	21	56	n	Query - Reduction Network 1 ( QRN )
24	59	61	p	is
24	64	85	n	single recurrent unit
24	86	100	p	that addresses
24	105	135	n	long - term dependency problem
24	136	138	p	of
24	139	162	n	most RNN - based models
24	163	177	p	by simplifying
24	182	198	n	recurrent update
24	207	213	p	taking
24	218	227	n	advantage
24	228	230	p	of
24	231	248	n	RNN 's capability
24	249	257	p	to model
24	258	273	n	sequential data
25	4	13	p	considers
25	18	35	n	context sentences
25	36	38	p	as
25	41	49	n	sequence
25	50	52	p	of
25	53	78	n	state - changing triggers
25	158	166	p	observes
25	180	192	n	through time
25	85	95	p	transforms
25	112	126	n	original query
25	127	129	p	to
25	132	151	n	more informed query
33	48	62	p	better encodes
33	63	83	n	locality information
33	172	185	n	query updates
33	190	199	p	performed
33	200	207	n	locally
2	76	94	n	QUESTION ANSWERING
4	40	93	n	question answering when reasoning over multiple facts
220	0	2	p	In
220	3	11	n	1 k data
220	14	62	n	QRN 's ' 2 r' ( 2 layers + reset gate + d = 50 )
220	63	74	p	outperforms
220	75	91	n	all other models
220	92	94	p	by
220	97	121	n	large margin ( 2.8 + % )
221	3	15	n	10 k dataset
221	22	38	n	average accuracy
221	39	41	p	of
221	42	100	n	QRN 's ' 6r200 ' ( 6 layers + reset gate + d = 200 ) model
221	101	112	p	outperforms
221	113	132	n	all previous models
221	133	135	p	by
221	138	162	n	large margin ( 2.5 + % )
221	165	174	p	achieving
221	177	197	n	nearly perfect score
221	198	200	p	of
221	201	207	n	99.7 %
225	0	3	n	QRN
225	4	15	p	outperforms
225	16	29	n	previous work
225	30	32	p	by
225	35	59	n	large margin ( 2.0 + % )
