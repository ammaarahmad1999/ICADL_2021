163	3	8	p	learn
163	13	47	n	word vectors and paragraph vectors
163	48	53	p	using
163	54	79	n	75,000 training documents
163	82	96	n	25,000 labeled
163	101	127	n	50,000 unlabeled instances
164	4	21	n	paragraph vectors
164	22	25	p	for
164	30	54	n	25,000 labeled instances
164	64	75	p	fed through
164	78	92	n	neural network
164	93	97	p	with
164	98	114	n	one hidden layer
164	115	119	p	with
164	120	128	n	50 units
164	135	154	n	logistic classifier
164	164	174	p	to predict
164	179	188	n	sentiment
169	60	79	n	optimal window size
169	80	82	p	is
169	83	91	n	10 words
170	4	10	n	vector
170	11	23	p	presented to
170	28	38	n	classifier
170	44	60	p	concatenation of
170	84	93	n	PV - DBOW
170	107	114	n	PV - DM
171	19	49	n	learned vector representations
171	50	54	p	have
171	55	69	n	400 dimensions
172	17	47	n	learned vector representations
172	48	52	p	have
172	53	67	n	400 dimensions
172	68	71	p	for
172	77	96	n	words and documents
174	0	18	n	Special characters
174	19	26	p	such as
174	27	32	n	, .!?
175	4	14	p	treated as
175	17	28	n	normal word
173	0	10	p	To predict
173	15	27	n	10 - th word
173	33	44	p	concatenate
173	49	66	n	paragraph vectors
173	71	83	n	word vectors
23	19	26	p	propose
23	27	43	n	Paragraph Vector
23	49	71	n	unsupervised framework
23	72	83	p	that learns
23	84	129	n	continuous distributed vector representations
23	130	133	p	for
23	134	149	n	pieces of texts
25	9	25	n	Paragraph Vector
25	67	84	p	can be applied to
25	85	118	n	variable - length pieces of texts
26	19	40	n	vector representation
26	44	57	p	trained to be
26	58	64	n	useful
26	65	79	p	for predicting
26	80	85	n	words
26	86	88	p	in
26	91	100	n	paragraph
28	0	39	n	Both word vectors and paragraph vectors
28	44	54	p	trained by
28	59	86	n	stochastic gradient descent
28	91	106	n	backpropagation
29	6	23	n	paragraph vectors
29	24	27	p	are
29	28	34	n	unique
29	35	40	p	among
29	41	51	n	paragraphs
29	58	70	n	word vectors
29	71	74	p	are
29	75	81	n	shared
27	20	31	p	concatenate
27	36	52	n	paragraph vector
27	53	57	p	with
27	58	78	n	several word vectors
27	79	83	p	from
27	86	95	n	paragraph
27	100	107	p	predict
27	112	126	n	following word
27	127	129	p	in
27	134	147	n	given context
30	0	2	p	At
30	3	18	n	prediction time
30	25	42	n	paragraph vectors
30	47	58	p	inferred by
30	59	65	n	fixing
30	70	82	n	word vectors
30	87	95	p	training
30	100	120	n	new paragraph vector
30	121	126	p	until
30	127	138	n	convergence
2	0	54	n	Distributed Representations of Sentences and Documents
179	24	27	p	for
179	28	42	n	long documents
179	45	68	n	bag - of - words models
179	69	76	p	perform
179	77	87	n	quite well
179	98	118	n	difficult to improve
179	129	134	p	using
179	135	147	n	word vectors
181	4	29	n	combination of two models
181	30	36	p	yields
181	40	51	n	improvement
181	52	71	n	approximately 1.5 %
181	72	83	p	in terms of
181	84	95	n	error rates
184	4	20	n	method described
184	35	37	p	is
184	42	55	n	only approach
184	61	65	p	goes
184	66	98	n	significantly beyond the barrier
184	99	101	p	of
184	102	117	n	10 % error rate
185	3	11	p	achieves
185	12	18	n	7.42 %
185	19	27	p	which is
185	28	62	n	another 1.3 % absolute improvement
185	68	93	n	15 % relative improvement
185	96	100	p	over
185	105	125	n	best previous result
