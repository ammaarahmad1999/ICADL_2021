118	0	8	p	To train
118	9	18	n	our model
118	24	28	p	used
118	29	56	n	stochastic gradient descent
118	57	61	p	with
118	66	80	n	ADAM optimizer
118	116	137	n	initial learning rate
118	138	140	p	of
118	141	146	n	0.001
119	3	6	p	set
119	11	21	n	batch size
119	22	24	p	to
119	25	27	n	32
119	35	40	p	decay
119	45	58	n	learning rate
119	59	61	p	by
119	62	65	n	0.8
119	66	68	p	if
119	73	81	n	accuracy
119	82	84	p	on
119	89	103	n	validation set
119	109	121	n	not increase
119	122	127	p	after
119	130	142	n	half - epoch
119	145	149	p	i.e.
119	150	162	n	2000 batches
119	165	168	p	for
119	169	172	n	CBT
119	179	191	n	5000 batches
119	192	195	p	for
119	198	201	n	CNN
120	3	13	p	initialize
120	14	25	n	all weights
120	26	28	p	of
120	29	38	n	our model
120	39	41	p	by
120	42	50	n	sampling
120	51	55	p	from
120	60	94	n	normal distribution N ( 0 , 0.05 )
121	16	37	n	GRU recurrent weights
121	38	41	p	are
121	42	53	n	initialized
121	54	59	p	to be
121	60	70	n	orthogonal
121	75	81	n	biases
121	82	85	p	are
121	86	97	n	initialized
121	98	100	p	to
121	101	105	n	zero
125	0	9	n	Our model
125	13	27	p	implemented in
125	28	34	n	Theano
125	37	42	p	using
125	47	60	n	Keras library
122	9	21	p	to stabilize
122	26	34	n	learning
122	40	44	p	clip
122	49	58	n	gradients
122	59	61	p	if
122	68	72	n	norm
122	76	88	p	greater than
122	89	90	n	5
124	14	21	p	setting
124	22	46	n	embedding regularization
124	47	49	p	to
124	50	56	n	0.0001
124	95	101	p	worked
124	102	110	n	robustly
124	111	117	p	across
124	122	130	n	datasets
19	79	86	p	propose
19	89	135	n	novel neural attention - based inference model
19	145	155	p	to perform
19	156	191	n	machine reading comprehension tasks
20	10	21	p	first reads
20	26	48	n	document and the query
20	49	54	p	using
20	57	81	n	recurrent neural network
21	10	17	p	deploys
21	21	48	n	iterative inference process
21	49	59	p	to uncover
21	64	81	n	inferential links
21	93	100	p	between
21	105	123	n	missing query word
21	130	135	n	query
21	146	154	n	document
22	11	19	p	involves
22	22	59	n	novel alternating attention mechanism
22	65	78	p	first attends
22	82	92	n	some parts
22	93	95	p	of
22	100	105	n	query
22	108	118	p	then finds
22	125	146	n	corresponding matches
22	147	159	p	by attending
22	167	175	n	document
23	4	10	n	result
23	41	54	p	fed back into
23	59	86	n	iterative inference process
23	87	94	p	to seed
23	99	115	n	next search step
25	0	5	p	After
25	8	34	n	fixed number of iterations
25	41	46	n	model
25	47	51	p	uses
25	54	61	n	summary
25	62	64	p	of
25	69	86	n	inference process
25	87	97	p	to predict
25	102	108	n	answer
2	43	58	n	Machine Reading
4	59	80	n	machine comprehension
