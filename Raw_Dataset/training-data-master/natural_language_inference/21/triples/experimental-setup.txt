(Contribution||has||Experimental setup)
(Experimental setup||as||optimization objective)
(optimization objective||use||cross-entropy loss)
(cross-entropy loss||has||minimize)
(minimize||with||batch size)
(batch size||of||64)
(minimize||by||Adadelta)
(cross-entropy loss||plus||L2 regularization penalty)
(Experimental setup||use||Dropout)
(Dropout||with||keep probability)
(keep probability||has||0.75)
(0.75||for||language inference)
(keep probability||has||0.8)
(0.8||for||sentiment analysis)
(Experimental setup||implemented with||TensorFlow 2)
(Experimental setup||has||Initial learning rate)
(Initial learning rate||set to||0.5)
(Experimental setup||has||Hidden units number)
(Hidden units number||has||d h)
(d h||set to||300)
(Experimental setup||has||biases)
(biases||initialized with||0)
(Experimental setup||has||Activation functions)
(Activation functions||are||ELU ( exponential linear unit ))
(Experimental setup||has||weight matrices)
(weight matrices||initialized by||Glorot Initialization)
(Experimental setup||has||L2 regularization decay factors)
(L2 regularization decay factors||are||5 10 ?5 and 10 ? 4)
(5 10 ?5 and 10 ? 4||for||language inference and sentiment analysis)
(Experimental setup||has||Out - of - Vocabulary words)
(Out - of - Vocabulary words||in||training set)
(Out - of - Vocabulary words||has||randomly initialized)
(randomly initialized||by||uniform distribution)
(uniform distribution||between||( ? 0.05 , 0.05 ))
(Experimental setup||initialize||word embedding)
(word embedding||in||x)
(word embedding||by||300D Glo Ve 6B pre-trained vectors)
(Experimental setup||run on||Nvidia GTX 1080 Ti graphic card)
