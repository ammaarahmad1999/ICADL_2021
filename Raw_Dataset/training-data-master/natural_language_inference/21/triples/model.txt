(Contribution||has||Model)
(Model||compute||feature - wise attention)
(feature - wise attention||since||each element)
(each element||represented by||vector)
(each element||in||sequence)
(Model||apply||positional masks)
(positional masks||easily encode||prior structure knowledge)
(prior structure knowledge||such as||temporal order)
(prior structure knowledge||such as||dependency parsing)
(positional masks||to||attention distribution)
(Model||build||light - weight and RNN / CNN - free neural network)
(light - weight and RNN / CNN - free neural network||name||Directional Self - Attention Network ( DiSAN ))
(light - weight and RNN / CNN - free neural network||for||sentence encoding)
(Model||In||DiSAN)
(DiSAN||has||multi-dimensional attention)
(multi-dimensional attention||computes||vector representation)
(vector representation||passed into||classification / regression module)
(classification / regression module||to compute||final prediction for a particular task)
(vector representation||of||entire sequence)
(DiSAN||has||input sequence)
(input sequence||processed by||directional ( forward and backward ) self - attentions)
(directional ( forward and backward ) self - attentions||produce||context - aware representations)
(context - aware representations||for||all tokens)
(directional ( forward and backward ) self - attentions||to model||context dependency)
(Model||propose||novel attention mechanism)
(novel attention mechanism||differs from||previous ones)
(previous ones||in||multi-dimensional)
(previous ones||in||directional)
