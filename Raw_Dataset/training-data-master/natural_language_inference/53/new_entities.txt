15	19	24	p	study
15	29	33	n	task
15	34	36	p	of
15	37	45	n	learning
15	46	84	n	universal representations of sentences
15	96	118	n	sentence encoder model
15	127	137	p	trained on
15	140	152	n	large corpus
15	157	184	p	subsequently transferred to
15	185	196	n	other tasks
18	10	21	p	investigate
18	30	49	n	supervised learning
18	50	56	p	can be
18	57	66	n	leveraged
22	27	33	n	impact
22	34	36	p	of
22	41	71	n	sentence encoding architecture
22	72	74	p	on
22	75	107	n	representational transferability
22	114	121	p	compare
22	122	189	n	convolutional , recurrent and even simpler word composition schemes
88	0	3	p	For
88	12	18	n	models
88	19	29	p	trained on
88	30	34	n	SNLI
88	40	43	p	use
88	44	47	n	SGD
88	48	52	p	with
88	55	68	n	learning rate
88	69	71	p	of
88	72	75	n	0.1
88	82	94	n	weight decay
88	95	97	p	of
88	98	102	n	0.99
91	8	18	n	classifier
91	24	27	p	use
91	30	54	n	multi - layer perceptron
91	55	59	p	with
91	60	76	n	1 hidden - layer
91	77	79	p	of
91	80	96	n	512 hidden units
89	0	2	p	At
89	3	13	n	each epoch
89	19	25	p	divide
89	30	43	n	learning rate
89	44	46	p	by
89	47	48	n	5
89	49	51	p	if
89	56	68	n	dev accuracy
89	69	78	n	decreases
90	3	6	p	use
90	7	18	n	minibatches
90	19	26	p	of size
90	27	29	n	64
90	34	42	n	training
90	43	45	p	is
90	46	53	n	stopped
90	54	58	p	when
90	63	76	n	learning rate
90	77	87	p	goes under
90	92	110	n	threshold of 10 ?5
92	7	31	n	opensource GloVe vectors
92	32	42	p	trained on
92	43	60	n	Common Crawl 840B
92	61	65	p	with
92	66	80	n	300 dimensions
92	81	83	p	as
92	84	105	n	fixed word embeddings
2	0	57	n	Supervised Learning of Universal Sentence Representations
141	0	19	n	Architecture impact
144	4	17	n	BiLSTM - 4096
144	18	22	p	with
144	27	50	n	max - pooling operation
144	51	59	p	performs
144	60	64	n	best
144	65	67	p	on
144	73	96	n	SNLI and transfer tasks
145	0	10	p	Looking at
145	15	39	n	micro and macro averages
145	57	65	p	performs
145	66	86	n	significantly better
145	87	91	p	than
145	96	108	n	other models
145	109	113	n	LSTM
145	116	119	n	GRU
145	122	134	n	BiGRU - last
145	137	150	n	BiLSTM - Mean
145	153	168	n	inner-attention
145	177	199	n	hierarchical - ConvNet
150	24	40	n	transfer quality
150	49	61	p	sensitive to
150	66	88	n	optimization algorithm
150	91	109	p	when training with
150	110	114	n	Adam
150	115	125	p	instead of
150	126	129	n	SGD
150	135	143	p	observed
150	153	165	n	BiLSTM - max
150	166	175	p	converged
150	176	182	n	faster
150	183	185	p	on
150	186	190	n	SNLI
150	224	232	p	obtained
150	233	246	n	worse results
150	247	249	p	on
150	254	268	n	transfer tasks
152	0	14	n	Embedding size
153	124	149	n	increased embedding sizes
153	150	157	p	lead to
153	158	179	n	increased performance
153	180	183	p	for
153	184	201	n	almost all models
170	0	27	n	Comparison with SkipThought
172	0	4	p	With
172	5	55	n	much less data ( 570 k compared to 64M sentences )
172	60	64	p	with
172	65	91	n	high - quality supervision
172	92	96	p	from
172	101	113	n	SNLI dataset
172	123	130	p	able to
172	131	154	n	consistently outperform
172	159	166	n	results
172	167	178	p	obtained by
172	179	198	n	SkipThought vectors
174	0	16	n	Our BiLSTM - max
174	17	27	p	trained on
174	28	32	n	SNLI
174	33	41	p	performs
174	42	53	n	much better
174	54	58	p	than
174	59	87	n	released SkipThought vectors
174	88	90	p	on
174	91	93	n	MR
174	96	98	n	CR
174	101	105	n	MPQA
174	108	111	n	SST
174	114	129	n	MRPC - accuracy
174	132	140	n	SICK - R
174	143	151	n	SICK - E
174	156	159	n	STS
175	38	46	p	performs
175	47	53	n	better
175	54	58	p	than
175	59	70	n	SkipThought
175	76	78	p	on
175	79	81	n	MR
175	84	86	n	CR
175	91	95	n	MPQA
175	0	10	p	Except for
175	15	27	n	SUBJ dataset
176	19	29	p	looking at
176	34	47	n	STS14 results
176	8	15	p	observe
176	57	71	n	cosine metrics
176	72	74	p	in
176	75	94	n	our embedding space
176	95	97	p	is
176	103	132	n	more semantically informative
176	133	137	p	than
176	141	168	n	SkipThought embedding space
178	0	32	n	NLI as a supervised training set
179	13	21	p	indicate
179	27	36	n	our model
179	37	47	p	trained on
179	48	52	n	SNLI
179	53	60	p	obtains
179	61	89	n	much better over all results
179	90	94	p	than
179	95	101	n	models
179	102	112	p	trained on
179	113	135	n	other supervised tasks
179	136	143	p	such as
179	144	148	n	COCO
179	151	173	n	dictionary definitions
179	176	179	n	NMT
179	182	186	n	PPDB
179	191	194	n	SST
183	0	31	n	Domain adaptation on SICK tasks
184	0	30	n	Our transfer learning approach
184	31	38	p	obtains
184	39	53	n	better results
184	54	58	p	than
184	59	90	n	previous state - of - the - art
186	8	34	n	significantly outperformed
186	35	72	n	previous transfer learning approaches
186	73	75	p	on
186	76	84	n	SICK - E
186	110	119	p	that used
186	124	134	n	parameters
186	135	137	p	of
186	141	151	n	LSTM model
186	152	162	p	trained on
186	163	167	n	SNLI
186	168	185	p	to fine - tune on
186	186	190	n	SICK
185	3	9	p	obtain
185	12	25	n	pearson score
185	26	28	p	of
185	29	34	n	0.885
185	35	37	p	on
185	38	46	n	SICK - R
185	84	104	n	86.3 % test accuracy
185	105	107	p	on
185	108	116	n	SICK - E
185	47	52	p	while
185	123	158	n	previous best handengineered models
185	53	61	p	obtained
185	168	174	n	84.5 %
188	0	33	n	Image - caption retrieval results
191	5	17	p	trained with
191	18	61	n	ResNet features and 30 k more training data
191	68	87	n	SkipThought vectors
191	88	95	p	perform
191	96	116	n	significantly better
191	117	121	p	than
191	126	142	n	original setting
191	145	155	p	going from
191	156	168	n	33.8 to 37.9
191	169	172	p	for
191	173	194	n	caption retrieval R@1
191	201	205	p	from
191	206	218	n	25.9 to 30.6
191	219	221	p	on
191	222	241	n	image retrieval R@1
192	0	12	n	Our approach
192	13	19	p	pushes
192	24	31	n	results
192	32	44	n	even further
192	47	51	p	from
192	52	56	n	37.9
192	57	59	p	to
192	60	64	n	42.4
192	65	67	p	on
192	68	86	n	cap-tion retrieval
192	93	97	n	30.6
192	98	100	p	to
192	101	105	n	33.2
192	106	108	p	on
192	109	124	n	image retrieval
195	0	14	n	MultiGenre NLI
199	3	10	p	observe
199	13	30	n	significant boost
199	31	33	p	in
199	34	54	n	performance over all
199	55	66	p	compared to
199	71	76	n	model
199	77	92	p	trained only on
199	93	97	n	SLNI
200	0	9	n	Our model
200	15	22	p	reaches
200	23	42	n	AdaSent performance
200	43	45	p	on
200	46	48	n	CR
