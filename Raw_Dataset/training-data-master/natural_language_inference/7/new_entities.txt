131	4	45	n	passage - aligned question representation
131	46	48	p	is
131	49	56	n	crucial
157	11	18	p	observe
157	19	39	n	general improvements
157	40	50	p	when using
157	51	57	n	labels
157	58	62	p	that
157	63	76	n	closely align
157	77	81	p	with
157	86	90	n	task
162	47	81	n	interactions between the endpoints
162	82	87	p	using
162	92	106	n	spanlevel FFNN
163	0	5	n	RASOR
163	6	17	p	outperforms
163	22	47	n	endpoint prediction model
163	48	50	p	by
163	51	54	n	1.1
163	55	57	p	in
163	58	69	n	exact match
106	3	12	p	represent
106	25	30	n	words
106	31	33	p	in
106	38	59	n	question and document
106	60	65	p	using
106	66	98	n	300 dimensional GloVe embeddings
106	99	109	p	trained on
106	112	118	n	corpus
106	18	20	p	of
106	122	134	n	840 bn words
106	119	121	p	of
107	17	22	p	cover
107	23	34	n	200 k words
107	39	74	n	all out of vocabulary ( OOV ) words
107	79	93	p	projected onto
107	94	97	n	one
107	101	141	n	1 m randomly initialized 300d embeddings
108	3	9	p	couple
108	14	36	n	input and forget gates
108	37	39	p	in
108	40	49	n	our LSTMs
108	77	80	p	use
108	83	102	n	single dropout mask
108	103	111	p	to apply
108	112	119	n	dropout
108	120	126	p	across
108	127	148	n	all LSTM time - steps
109	0	13	n	Hidden layers
109	14	16	p	in
109	21	49	n	feed forward neural networks
109	50	53	p	use
109	54	76	n	rectified linear units
110	0	17	n	Answer candidates
110	22	32	p	limited to
110	33	38	n	spans
110	39	43	p	with
110	44	60	n	at most 30 words
112	4	14	n	best model
112	15	19	p	uses
112	20	35	n	50d LSTM states
112	38	57	n	two - layer BiLSTMs
112	58	61	p	for
112	66	78	n	span encoder
112	87	132	n	passage - independent question representation
112	135	142	n	dropout
112	143	145	p	of
112	146	149	n	0.1
112	169	188	n	learning rate decay
112	189	191	p	of
112	192	212	n	5 % every 10 k steps
113	4	10	n	models
113	15	32	p	implemented using
113	33	43	n	TensorFlow
113	50	60	p	trained on
113	65	83	n	SQUAD training set
113	84	89	p	using
113	94	108	n	ADAM optimizer
113	109	113	p	with
113	116	131	n	mini-batch size
113	132	134	p	of
113	135	136	n	4
113	141	154	p	trained using
113	155	187	n	10 asynchronous training threads
113	188	190	p	on
113	193	207	n	single machine
111	0	9	p	To choose
111	14	39	n	final model configuration
111	45	48	p	ran
111	49	62	n	grid searches
111	63	67	p	over
111	74	88	n	dimensionality
111	89	91	p	of
111	96	114	n	LSTM hidden states
111	121	136	n	width and depth
111	137	139	p	of
111	144	172	n	feed forward neural networks
111	175	182	n	dropout
111	183	186	p	for
111	191	196	n	LSTMs
111	203	209	n	number
111	210	212	p	of
111	213	232	n	stacked LSTM layers
111	243	280	n	decay multiplier [ 0.9 , 0.95 , 1.0 ]
111	295	303	p	multiply
111	308	338	n	learning rate every 10 k steps
24	22	29	p	present
24	32	57	n	novel neural architecture
24	58	64	p	called
24	65	70	n	RASOR
24	76	82	p	builds
24	83	118	n	fixed - length span representations
24	121	128	p	reusing
24	129	151	n	recurrent computations
24	152	155	p	for
24	156	176	n	shared substructures
25	3	14	p	demonstrate
25	20	40	n	directly classifying
25	41	48	p	each of
25	53	68	n	competing spans
25	75	83	n	training
25	84	88	p	with
25	89	109	n	global normalization
25	110	114	p	over
25	115	133	n	all possible spans
25	136	144	p	leads to
25	147	167	n	significant increase
25	168	170	p	in
25	171	182	n	performance
2	44	73	n	EXTRACTIVE QUESTION ANSWERING
7	33	50	n	answer extraction
121	83	88	n	RASOR
121	89	97	p	achieves
121	101	116	n	error reduction
121	57	59	p	of
121	120	134	n	more than 50 %
121	161	172	p	in terms of
121	173	191	n	exact match and F1
121	194	205	p	relative to
121	210	239	n	human performance upper bound
125	51	56	p	model
125	24	50	n	efficiently and explicitly
125	61	97	n	quadratic number of possible answers
125	106	114	p	leads to
125	117	137	n	14 % error reduction
125	138	142	p	over
125	147	181	n	best performing Match - LSTM model
