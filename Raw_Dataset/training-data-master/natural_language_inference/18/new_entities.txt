141	12	14	p	on
141	15	33	n	Document Modelling
145	25	33	p	indicate
145	39	43	n	NVDM
145	44	52	p	achieves
145	57	73	n	best performance
145	74	76	p	on
145	77	90	n	both datasets
146	0	3	p	For
146	8	19	n	experiments
146	20	22	p	on
146	23	40	n	RCV1 - v2 dataset
146	47	51	n	NVDM
146	52	56	p	with
146	57	72	n	latent variable
146	73	75	p	of
146	76	88	n	50 dimension
146	89	97	p	performs
146	98	109	n	even better
146	110	114	p	than
146	119	124	n	fDARN
146	125	129	p	with
146	130	143	n	200 dimension
155	16	19	p	for
155	20	45	n	Answer Sentence Selection
173	4	30	n	word embeddings ( K = 50 )
173	31	34	p	are
173	35	43	n	obtained
173	44	54	p	by running
173	59	72	n	word2vec tool
173	73	75	p	on
173	80	102	n	English Wikipedia dump
173	111	127	n	AQUAINT 5 corpus
174	3	6	p	use
174	7	12	n	LSTMs
174	13	17	p	with
174	18	26	n	3 layers
174	31	46	n	50 hidden units
174	53	58	p	apply
174	59	71	n	40 % dropout
174	72	77	p	after
174	82	97	n	embedding layer
175	0	23	p	For the construction of
175	28	45	n	inference network
175	51	54	p	use
175	58	61	n	MLP
175	73	77	p	with
175	78	86	n	2 layers
175	91	101	n	tanh units
175	102	104	p	of
175	105	117	n	50 dimension
175	188	201	p	for modelling
175	206	226	n	joint representation
175	127	130	n	MLP
175	142	146	p	with
175	147	155	n	2 layers
175	160	170	n	tanh units
175	171	173	p	of
175	174	187	n	150 dimension
176	0	6	p	During
176	7	15	n	training
176	19	28	p	carry out
176	29	50	n	stochastic estimation
176	51	60	p	by taking
176	61	71	n	one sample
176	72	85	p	for computing
176	90	99	n	gradients
176	108	110	p	in
176	111	121	n	prediction
176	125	128	p	use
176	129	139	n	20 samples
176	140	152	p	to calculate
176	157	168	n	expectation
176	169	171	p	of
176	176	187	n	lower bound
187	4	14	n	LSTM + Att
187	15	23	p	performs
187	24	39	n	slightly better
187	40	44	p	than
187	49	67	n	vanilla LSTM model
187	74	82	n	our NASM
187	83	91	p	improves
187	96	103	n	results
188	132	146	n	our best model
188	74	94	p	after combining with
188	97	129	n	co-occurrence word count feature
188	152	163	p	outperforms
188	164	187	n	all the previous models
188	190	199	p	including
188	205	232	n	neural network based models
188	237	248	n	classifiers
188	249	253	p	with
188	256	259	n	set
188	260	262	p	of
188	263	286	n	hand - crafted features
189	12	14	p	on
189	19	36	n	Wik - iQA dataset
189	39	56	n	all of our models
189	57	67	p	outperform
189	72	102	n	previous distributional models
189	103	105	p	by
189	108	120	n	large margin
190	3	12	p	including
190	15	33	n	word count feature
190	36	46	n	our models
190	47	62	n	improve further
190	67	74	p	achieve
190	79	101	n	state - of - the - art
22	11	21	p	introduces
22	24	52	n	neural variational framework
22	53	56	p	for
22	57	74	n	generative models
22	75	77	p	of
22	78	82	n	text
22	85	96	p	inspired by
22	101	124	n	variational autoencoder
23	25	30	p	build
23	34	51	n	inference network
23	54	68	p	implemented by
23	71	90	n	deep neural network
23	91	105	p	conditioned on
23	106	110	n	text
23	113	127	p	to approximate
23	132	157	n	intractable distributions
23	158	162	p	over
23	167	183	n	latent variables
24	87	115	n	neural variational inference
24	116	131	p	learns to model
24	136	157	n	posterior probability
31	2	17	n	primary feature
31	18	20	p	of
31	21	25	n	NVDM
31	26	28	p	is
31	34	43	n	each word
31	47	70	p	generated directly from
31	73	113	n	dense continuous document representation
31	114	124	p	instead of
31	129	163	n	more common binary semantic vector
33	4	8	n	NASM
33	11	13	p	is
33	16	44	n	supervised conditional model
33	51	57	p	imbues
33	58	63	n	LSTMs
33	64	68	p	with
33	71	108	n	latent stochastic attention mechanism
33	109	117	p	to model
33	122	131	n	semantics
33	132	134	p	of
33	135	158	n	question - answer pairs
34	4	19	n	attention model
34	23	34	p	designed to
34	35	40	n	focus
34	41	43	p	on
34	48	55	n	phrases
34	56	58	p	of
34	62	68	n	answer
34	78	99	p	strongly connected to
34	104	122	n	question semantics
34	130	141	p	modelled by
34	144	163	n	latent distribution
36	0	18	n	Bayesian inference
36	19	27	p	provides
36	30	47	n	natural safeguard
36	48	55	p	against
36	56	67	n	overfitting
27	3	8	p	using
27	13	39	n	reparameteris ation method
27	46	63	n	inference network
27	67	82	p	trained through
27	83	101	n	back - propagating
27	102	137	n	unbiased and low variance gradients
27	138	144	p	w.r.t.
27	149	165	n	latent variables
28	27	34	p	propose
28	37	79	n	Neural Variational Document Model ( NVDM )
28	80	83	p	for
28	84	102	n	document modelling
28	109	147	n	Neural Answer Selection Model ( NASM )
28	148	151	p	for
28	152	170	n	question answering
2	0	28	n	Neural Variational Inference
