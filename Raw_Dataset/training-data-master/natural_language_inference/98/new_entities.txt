102	6	12	p	remove
102	17	34	n	gated - attention
102	41	51	p	accuracies
102	52	56	n	drop
102	57	59	p	to
102	60	77	n	72.8 % and 73.6 %
103	13	40	n	charactercomposition vector
103	47	57	p	accuracies
103	58	62	n	drop
103	63	65	p	to
103	66	83	n	72.9 % and 73.5 %
104	13	35	n	word - level embedding
104	42	52	p	accuracies
104	53	57	n	drop
104	58	60	p	to
104	61	78	n	65.6 % and 66.0 %
82	55	94	n	https : //github.com/lukecq1231/enc_nli
84	3	6	p	use
84	11	40	n	Adam ( Kingma and Ba , 2014 )
84	41	44	p	for
84	45	57	n	optimization
87	7	45	n	pretrained GloVe - 840B - 300D vectors
87	46	48	p	as
87	49	76	n	our word - level embeddings
87	81	84	n	fix
87	102	108	p	during
87	113	129	n	training process
85	0	14	n	Stacked BiLSTM
85	19	27	n	3 layers
85	34	51	n	all hidden states
85	52	54	p	of
85	55	70	n	BiLSTMs and MLP
85	71	75	p	have
85	76	90	n	600 dimensions
86	103	117	n	100 dimensions
86	4	23	n	character embedding
86	28	41	n	15 dimensions
86	48	66	n	CNN filters length
86	67	69	p	is
86	70	83	n	[ 1 , 3 , 5 ]
88	0	35	n	Out - of - vocabulary ( OOV ) words
88	40	60	n	initialized randomly
88	61	65	p	with
88	66	82	n	Gaussian samples
29	20	28	p	proposed
29	29	64	n	natural language inference networks
29	75	86	p	composed of
29	120	134	n	word embedding
29	137	153	n	sequence encoder
29	156	173	n	composition layer
29	184	203	n	toplayer classifier
31	0	14	n	Word Embedding
33	3	14	p	concatenate
33	15	25	n	embeddings
33	26	36	p	learned at
33	37	57	n	two different levels
33	103	124	n	character composition
33	129	160	n	holistic word - level embedding
33	58	70	p	to represent
33	71	80	n	each word
33	81	83	p	in
33	88	96	n	sentence
40	0	16	n	Sequence Encoder
41	67	81	n	sentence pairs
41	86	94	p	fed into
41	95	112	n	sentence encoders
41	113	122	p	to obtain
41	123	153	n	hidden vectors ( h p and h h )
42	3	6	p	use
42	7	48	n	stacked bidirectional LSTMs ( BiL - STM )
42	49	51	p	as
42	56	64	n	encoders
53	0	17	n	Composition Layer
54	118	125	p	compose
54	130	144	n	hidden vectors
54	145	156	p	obtained by
54	161	199	n	sequence encoder layer ( h p and h h )
54	0	12	p	To transform
54	13	22	n	sentences
54	23	27	p	into
54	28	65	n	fixed - length vector representations
55	3	10	p	propose
55	11	43	n	intra-sentence gated - attention
55	44	53	p	to obtain
55	56	77	n	fixed - length vector
67	0	22	n	Top - layer Classifier
68	0	19	n	Our inference model
68	20	25	p	feeds
68	30	47	n	resulting vectors
68	63	65	p	to
68	70	86	n	final classifier
68	87	99	p	to determine
68	104	135	n	over all inference relationship
2	75	101	n	Natural Language Inference
14	85	119	n	natural language inference ( NLI )
16	15	18	n	NLI
95	26	52	n	our implementation of ESIM
95	61	69	p	achieves
95	73	81	n	accuracy
95	82	84	p	of
95	85	91	n	76.8 %
95	92	94	p	in
95	99	119	n	in - domain test set
95	126	132	n	75.8 %
95	133	135	p	in
95	140	163	n	cross - domain test set
95	172	180	p	presents
95	185	215	n	state - of - the - art results
96	90	97	p	achieve
96	98	108	p	accuracies
96	109	111	p	of
96	112	129	n	73.5 % and 73.6 %
96	0	14	p	After removing
96	19	45	n	cross - sentence attention
96	50	56	p	adding
96	57	84	n	our gated - attention model
96	226	231	p	among
96	236	249	n	single models
96	138	143	p	ranks
96	144	149	n	first
96	150	152	p	in
96	157	180	n	cross - domain test set
96	191	197	n	second
96	198	200	p	in
96	205	225	n	in - domain test set
97	0	4	p	When
97	5	15	n	ensembling
97	16	26	p	our models
97	32	38	n	obtain
97	39	49	p	accuracies
97	50	67	p	74.9 % and 74.9 %
97	76	81	p	ranks
97	82	90	p	first in
97	91	105	n	both test sets
