146	0	27	n	CNN and Daily Mail Datasets
149	0	10	n	Vocab Size
149	44	48	p	keep
149	53	86	n	most frequent | V | = 101 k words
149	143	145	p	in
149	150	161	n	CNN dataset
149	168	187	n	| V | = 151 k words
149	244	246	p	in
149	251	269	n	Daily Mail dataset
150	0	15	n	Embedding Layer
151	3	9	p	choose
151	10	43	n	300 - dimensional word embeddings
151	50	53	p	use
151	58	108	n	300 - dimensional pretrained Glove word embeddings
151	109	112	p	for
151	113	127	n	initialization
152	8	13	p	apply
152	14	21	n	dropout
152	22	26	p	with
152	27	42	n	probability 0.2
152	43	45	p	to
152	50	65	n	embedding layer
174	4	18	n	absolute value
174	19	21	p	of
174	22	30	n	gradient
174	31	33	p	on
174	34	48	n	each parameter
174	49	51	p	is
174	52	59	n	clipped
174	60	66	p	within
174	67	72	n	0.001
175	4	14	n	batch size
175	15	17	p	is
175	18	20	n	64
175	21	24	p	for
175	25	57	n	both CNN and Daily Mail datasets
173	3	6	p	use
173	7	21	n	ADAM optimizer
173	22	25	p	for
173	26	48	n	parameter optimization
173	49	53	p	with
173	57	78	n	initial learning rate
173	79	81	p	of
173	82	116	n	0.0005 , ? 1 = 0.9 and ? 2 = 0.999
179	11	21	p	trained on
179	22	38	n	GTX TitanX 12 GB
197	0	14	p	Comparing with
197	19	28	n	AS Reader
197	31	39	n	ReasoNet
197	40	45	p	shows
197	50	72	n	signi cant improvement
197	73	85	p	by capturing
197	97	106	n	reasoning
197	107	109	p	in
197	114	123	n	paragraph
198	0	52	n	Iterative Attention Reader , EpiReader and GA Reader
198	53	56	p	are
198	61	94	n	three multi-turn reasoning models
198	95	99	p	with
198	100	119	n	xed reasoning steps
199	14	25	n	outperforms
199	0	8	n	ReasoNet
199	38	52	p	by integrating
199	53	69	n	termination gate
199	70	72	p	in
199	77	82	n	model
199	89	95	p	allows
199	96	120	n	di erent reasoning steps
199	121	124	p	for
199	125	144	n	di erent test cases
201	0	8	n	ReasoNet
201	9	16	p	obtains
201	17	35	n	comparable results
201	36	40	p	with
201	41	51	n	AoA Reader
201	52	54	p	on
201	55	67	n	CNN test set
210	0	13	n	SQuAD Dataset
217	0	10	n	Vocab Size
218	3	6	p	use
218	11	32	n	python NLTK tokenizer
218	35	48	p	to preprocess
218	49	71	n	passages and questions
218	78	90	p	obtain about
218	91	101	n	100K words
218	102	104	p	in
218	109	119	n	vocabulary
219	0	15	n	Embedding Layer
219	21	24	p	use
219	29	71	n	100 - dimensional pretrained Glove vectors
219	72	74	p	as
219	75	90	n	word embeddings
247	4	32	n	maximum reasoning step T max
247	36	42	p	set to
247	43	45	n	10
248	3	6	p	use
248	7	25	n	AdaDelta optimizer
248	26	29	p	for
248	30	52	n	parameter optimization
248	53	57	p	with
248	61	82	n	initial learning rate
248	83	85	p	of
248	86	89	n	0.5
254	8	19	p	demonstrate
254	25	33	n	ReasoNet
254	34	45	n	outperforms
254	46	79	n	all existing published approaches
255	9	16	p	compare
255	17	25	n	ReasoNet
255	48	55	p	exceeds
255	31	36	n	BiDAF
255	62	69	p	both in
255	70	107	n	single model and ensemble model cases
257	35	43	n	ReasoNet
257	115	120	p	holds
257	125	140	n	second position
257	141	143	p	in
257	144	172	n	all the competing approaches
257	173	175	p	in
257	180	197	n	SQuAD leaderboard
258	0	18	n	Graph Reachability
283	0	15	n	Embedding Layer
284	3	6	p	use
284	9	43	n	100 - dimensional embedding vector
284	44	47	p	for
284	48	59	n	each symbol
284	60	62	p	in
284	67	94	n	query and graph description
300	4	32	n	maximum reasoning step T max
300	36	42	p	set to
300	43	52	n	15 and 25
300	53	56	p	for
300	61	96	n	small graph and large graph dataset
301	3	6	p	use
301	7	25	n	AdaDelta optimizer
301	26	29	p	for
301	30	52	n	parameter optimization
301	53	57	p	with
301	61	82	n	initial learning rate
301	83	85	p	of
301	86	89	n	0.5
301	96	106	n	batch size
301	107	109	p	of
301	110	112	n	32
307	0	16	n	Deep LSTM Reader
307	17	25	p	achieves
307	26	54	n	90.92 % and 71.55 % accuracy
307	55	57	p	in
307	62	91	n	small and large graph dataset
33	26	33	p	propose
33	36	69	n	novel neural network architecture
33	70	76	p	called
33	77	107	n	Reasoning Network ( ReasoNet )
35	0	4	p	With
35	7	15	n	question
35	16	18	p	in
35	19	23	n	mind
35	26	35	n	ReasoNets
35	36	40	p	read
35	43	51	n	document
35	52	62	n	repeatedly
35	75	86	p	focusing on
35	87	101	n	di erent parts
35	102	104	p	of
35	109	117	n	document
35	118	123	p	until
35	126	143	n	satisfying answer
35	144	146	p	is
35	147	162	n	found or formed
37	89	98	p	introduce
37	101	118	n	termination state
37	119	121	p	in
37	126	135	n	inference
42	43	52	p	proposing
42	55	86	n	reinforcement learning approach
42	95	103	p	utilizes
42	107	143	n	instance - dependent reward baseline
42	146	167	p	to successfully train
42	168	177	n	ReasoNets
2	39	60	n	Machine Comprehension
