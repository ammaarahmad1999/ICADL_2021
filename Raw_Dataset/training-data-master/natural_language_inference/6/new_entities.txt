167	19	26	p	achieve
167	27	43	n	good convergence
167	44	48	p	with
167	49	62	n	deeper models
168	10	19	p	seen that
168	20	29	n	all tasks
168	30	42	p	benefit from
168	43	56	n	deeper models
171	0	18	n	Multitask learning
173	18	31	n	NLI objective
173	32	40	p	leads to
173	43	61	n	better performance
173	62	64	p	on
173	69	89	n	English NLI test set
173	101	109	p	comes at
173	114	118	n	cost
173	119	121	p	of
173	124	166	n	worse cross - lingual transfer performance
173	9	11	p	in
173	170	186	n	XNLI and Tatoeba
9	91	134	n	https://github.com / facebookresearch/LASER
95	0	26	n	XNLI : cross - lingual NLI
106	2	21	n	Our proposed method
106	22	29	p	obtains
106	34	46	n	best results
106	47	49	p	in
106	50	86	n	zero - shot cross - lingual transfer
106	87	90	p	for
106	91	116	n	all languages but Spanish
107	15	31	n	transfer results
107	32	35	p	are
107	36	58	n	strong and homogeneous
107	59	65	p	across
107	66	79	n	all languages
110	21	45	n	zero - short performance
110	46	48	p	is
110	49	70	n	( at most ) 5 % lower
110	71	75	p	than
110	87	94	n	English
110	97	106	p	including
110	107	124	n	distant languages
110	125	129	p	like
110	130	161	n	Arabic , Chinese and Vietnamese
110	176	183	p	achieve
110	184	207	n	remarkable good results
110	84	86	p	on
110	211	235	n	low - resource languages
110	236	240	p	like
110	241	248	n	Swahili
113	21	31	p	outperform
113	32	45	n	all baselines
113	64	66	p	by
113	69	87	n	substantial margin
125	0	38	n	MLDoc : cross - lingual classification
131	14	24	n	our system
131	25	32	p	obtains
131	37	59	n	best published results
131	60	63	p	for
131	64	93	n	5 of the 7 transfer languages
133	0	20	n	BUCC : bitext mining
145	14	24	n	our system
145	25	36	p	establishes
145	39	65	n	new state - of - the - art
145	66	69	p	for
145	70	88	n	all language pairs
145	89	110	p	with the exception of
145	111	133	n	English - Chinese test
146	8	18	p	outperform
146	19	47	n	Artetxe and Schwenk ( 2018 )
148	0	27	n	Tatoeba : similarity search
156	69	91	n	similarity error rates
156	92	101	n	below 5 %
156	102	105	p	are
156	26	30	p	with
157	20	23	p	for
157	24	36	n	37 languages
157	55	67	n	48 languages
157	76	86	n	error rate
157	87	97	n	below 10 %
157	102	104	n	55
157	68	72	p	with
157	110	124	n	less than 20 %
158	15	27	n	15 languages
158	28	44	p	with error rates
158	45	55	n	above 50 %
21	22	35	p	interested in
21	36	83	n	universal language agnostic sentence embeddings
21	86	93	p	that is
21	96	118	n	vector representations
21	119	121	p	of
21	122	131	n	sentences
21	132	140	p	that are
21	141	148	n	general
21	149	164	p	with respect to
21	165	179	n	two dimensions
21	186	200	n	input language
21	209	217	n	NLP task
23	17	22	p	train
23	25	39	n	single encoder
23	40	49	p	to handle
23	50	68	n	multiple languages
2	0	42	n	Massively Multilingual Sentence Embeddings
4	38	81	n	joint multilingual sentence representations
