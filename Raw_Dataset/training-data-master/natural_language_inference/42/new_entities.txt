229	14	22	p	confirms
229	27	40	n	effectiveness
229	41	43	p	of
229	44	60	n	char- embeddings
229	70	78	p	addition
229	79	88	p	increased
229	93	109	n	F1 and EM scores
229	112	114	p	by
229	115	130	n	2.7 % and 3.1 %
230	19	23	p	when
230	28	51	n	convolutional attention
230	56	67	p	replaced by
230	72	100	n	standard attention mechanism
230	119	130	n	performance
230	131	138	n	dropped
230	139	141	p	by
230	142	147	n	2.4 %
230	110	112	p	in
230	151	153	n	F1
230	158	163	n	2.5 %
230	148	150	p	in
230	167	169	n	EM
232	26	34	p	indicate
232	44	59	n	reduction layer
232	63	83	p	capable of producing
232	84	111	n	useful word representations
232	112	116	p	when
232	117	128	n	compressing
232	133	143	n	embeddings
233	17	25	p	replaced
233	42	68	n	standard feedforward layer
233	69	73	p	with
233	78	98	n	same reduction ratio
233	113	117	n	drop
233	118	120	p	of
233	121	136	n	2.1 % and 2.5 %
233	137	139	p	in
233	144	160	n	F1 and EM scores
217	8	15	p	trained
217	20	31	n	FABIR model
217	32	38	p	during
217	39	48	n	54 epochs
217	49	53	p	with
217	56	66	n	batch size
217	67	69	p	of
217	70	72	n	75
217	73	75	p	in
217	78	96	n	GPU NVidia Titan X
217	97	101	p	with
217	102	107	n	12 GB
217	108	110	p	of
217	111	114	n	RAM
218	3	12	p	developed
218	13	22	n	our model
218	23	25	p	in
218	26	36	n	Tensorflow
219	3	16	p	pre-processed
219	21	26	n	texts
219	27	31	p	with
219	36	50	n	NLTK Tokenizer
221	0	3	p	For
221	4	18	n	regularization
221	24	31	p	applied
221	32	62	n	residual and attention dropout
221	63	65	p	of
221	66	69	n	0.9
221	70	72	p	in
221	73	90	n	processing layers
221	98	101	n	0.8
221	102	104	p	in
221	109	124	n	reduction layer
222	0	2	p	In
222	7	42	n	character - level embedding process
222	47	54	n	dropout
222	55	57	p	of
222	58	62	n	0.75
222	67	79	p	added before
222	84	95	n	convolution
223	17	24	n	dropout
223	25	27	p	of
223	28	31	n	0.8
223	36	48	p	added before
223	49	73	n	each convolutional layer
223	74	76	p	in
223	81	96	n	answer selector
224	3	6	p	set
224	7	34	n	processing layers dimension
224	35	42	n	d model
224	43	45	p	to
224	46	49	n	100
224	56	71	n	number of heads
224	72	79	n	n heads
224	80	82	p	in
224	83	106	n	each attention sublayer
224	107	109	p	to
224	110	111	n	4
224	118	144	n	feed - forward hidden size
224	145	147	p	to
224	148	151	n	200
224	152	154	p	in
224	155	172	n	processing layers
224	177	180	n	400
224	181	183	p	in
224	188	203	n	reduction layer
20	176	181	p	named
20	182	237	n	Fully Attention - Based Information Retriever ( FABIR )
21	18	27	p	to verify
21	28	48	n	how much performance
21	56	76	p	get exclusively from
21	81	100	n	attention mechanism
21	103	128	p	without combining it with
21	129	153	n	several other techniques
24	0	23	n	Convolutional attention
24	28	53	n	novel attention mechanism
24	59	66	p	encodes
24	67	97	n	many - to - many relationships
24	98	105	p	between
24	106	111	n	words
24	114	122	p	enabling
24	123	156	n	richer contextual representations
25	0	15	n	Reduction layer
25	20	36	n	new layer design
25	42	46	p	fits
25	51	59	n	pipeline
25	60	71	p	proposed by
25	72	87	n	Vaswani et al .
27	0	31	n	Column - wise cross - attention
27	37	43	p	modify
27	48	72	n	crossattention operation
27	80	87	p	propose
27	90	103	n	new technique
27	112	128	p	better suited to
27	129	149	n	question - answering
2	26	47	n	Information Retriever
9	0	27	n	Question - answering ( QA )
12	49	65	n	open - domain QA
13	77	79	n	QA
245	0	9	p	Regarding
245	10	27	n	EM and F 1 scores
245	30	45	n	FABIR and BiDAF
245	46	52	p	showed
245	53	73	n	similar performances
255	19	26	p	analyze
255	31	42	n	performance
255	43	45	p	of
255	46	61	n	FABIR and BiDAF
256	0	5	p	shows
256	11	26	n	shorter answers
256	27	30	p	are
256	31	37	n	easier
256	38	41	p	for
256	42	53	n	both models
262	33	49	n	best performance
262	50	54	p	with
262	55	73	n	" when " questions
264	35	64	n	" how long " and " how many "
264	70	76	p	proved
264	77	83	n	easier
264	84	86	p	to
264	87	94	n	respond
264	105	112	p	possess
264	117	130	n	same property
264	131	140	p	of having
264	143	159	n	smaller universe
264	160	162	p	of
264	163	179	n	possible answers
265	23	52	n	" how " and " why " questions
265	53	64	p	resulted in
265	65	100	n	considerably lower F1 and EM scores
270	0	9	n	Questions
270	16	22	p	expect
270	25	44	n	" yes " or a " no "
270	45	47	p	as
270	51	57	n	answer
270	58	61	p	are
270	62	76	n	also difficult
272	19	35	n	shorter passages
272	36	42	p	showed
272	47	64	n	worst performance
272	65	68	p	for
272	69	80	n	both models
