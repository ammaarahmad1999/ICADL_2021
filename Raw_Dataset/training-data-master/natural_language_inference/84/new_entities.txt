99	0	18	n	QUESTION ANSWERING
129	40	46	p	adding
129	47	71	n	any external information
129	72	82	p	results in
129	85	108	n	significant improvement
129	109	113	p	over
129	118	132	n	baseline model
129	141	158	n	3.7 - 10.5 points
133	25	33	n	spelling
133	40	50	n	helps more
133	51	55	p	than
133	14	20	p	adding
133	65	75	n	dictionary
133	84	103	n	3 points difference
130	9	25	n	dictionary alone
130	36	48	n	mean pooling
130	56	64	p	performs
130	65	74	n	similarly
130	75	77	p	to
130	78	82	n	LSTM
135	4	37	n	model with GLoVe embeddings ( G )
135	38	40	p	is
135	41	52	n	still ahead
135	53	57	p	with
135	60	76	n	1.1 point margin
134	25	29	p	uses
134	37	39	n	SD
134	48	67	n	1.1 point advantage
134	68	72	p	over
134	14	19	n	model
134	88	92	p	uses
134	93	110	n	just the spelling
155	0	21	n	ENTAILMENT PREDICTION
172	64	72	n	spelling
172	73	76	p	was
172	77	90	n	not as useful
172	91	93	p	on
172	94	111	n	SNLI and MultiNLI
178	27	54	n	dictionary - enabled models
178	55	79	n	significantly outperform
178	80	95	n	baseline models
178	96	99	p	for
178	100	109	n	sentences
178	110	120	p	containing
178	121	131	n	rare words
173	27	32	p	using
173	33	56	n	fixed random embeddings
173	57	60	p	for
173	61	70	n	OOV words
173	109	146	n	did not bring a significant advantage
173	147	151	p	over
173	156	164	n	baseline
179	0	18	n	LANGUAGE MODELLING
199	37	42	p	using
199	43	63	n	external information
199	64	74	p	to compute
199	75	85	n	embeddings
199	86	88	p	of
199	89	102	n	unknown words
199	103	108	n	helps
199	109	111	p	in
199	112	121	n	all cases
201	3	12	p	note that
201	13	30	n	lemma + lowercase
201	31	39	p	performs
201	40	45	n	worse
201	46	50	p	than
201	51	60	n	any model
201	61	65	p	with
201	70	80	n	dictionary
202	0	6	p	Adding
202	7	15	n	spelling
202	16	39	n	consistently helps more
202	40	44	p	than
202	45	51	p	adding
202	52	74	n	dictionary definitions
204	0	5	p	Using
204	11	34	n	dictionary and spelling
204	38	66	n	consistently slightly better
204	67	71	p	than
204	78	91	n	just spelling
205	6	23	n	Glo Ve embeddings
205	24	34	p	results in
205	39	54	n	best perplexity
20	17	24	p	propose
20	27	37	n	new method
20	38	51	p	for computing
20	52	77	n	embeddings " on the fly "
20	86	103	p	jointly addresses
20	108	132	n	large vocabulary problem
20	141	156	n	paucity of data
20	157	169	p	for learning
20	170	185	n	representations
20	186	188	p	in
20	193	230	n	long tail of the Zipfian distribution
21	181	186	p	train
21	189	196	n	network
21	197	207	p	to predict
21	108	123	n	representations
21	78	80	p	of
21	132	137	n	words
21	237	245	p	based on
21	246	260	n	auxiliary data
25	0	33	n	Several sources of auxiliary data
25	41	45	p	used
25	46	60	n	simultaneously
25	61	63	p	as
25	64	69	n	input
25	70	72	p	to
25	75	89	n	neural network
25	100	107	p	compute
25	110	133	n	combined representation
26	34	42	p	used for
26	43	70	n	out - of - vocabulary words
26	76	89	p	combined with
26	90	122	n	withinvocabulary word embeddings
26	123	142	p	directly trained on
26	147	163	n	task of interest
26	167	182	p	pretrained from
26	186	206	n	external data source
27	18	41	n	auxiliary data encoders
27	46	66	p	trained jointly with
27	71	80	n	objective
27	83	91	p	ensuring
27	96	108	n	preservation
27	109	111	p	of
27	112	130	n	semantic alignment
27	131	135	p	with
27	136	151	n	representations
27	152	154	p	of
27	155	180	n	within - vocabulary words
12	0	39	n	Learning representations for rare words
2	12	46	n	COMPUTE WORD EMBEDDINGS ON THE FLY
