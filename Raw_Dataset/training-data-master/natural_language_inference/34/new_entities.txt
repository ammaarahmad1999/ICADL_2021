230	21	23	p	of
230	24	39	n	QA performances
230	40	42	p	in
230	47	62	n	development set
230	63	65	p	of
230	66	75	n	Hotpot QA
231	22	30	p	see that
231	31	59	n	each of our model components
231	64	71	p	provide
231	72	87	n	from 1 % to 2 %
231	88	101	n	relative gain
231	102	106	p	over
231	111	125	n	QA performance
232	15	20	p	using
232	23	45	n	1 - layer fusion block
232	46	54	p	leads to
232	58	82	n	obvious performance loss
232	91	98	p	implies
232	103	115	n	significance
232	116	118	p	of
232	119	129	n	performing
232	130	149	n	multi-hop reasoning
232	150	152	p	in
232	153	162	n	Hotpot QA
233	14	39	n	dataset abla-tion results
233	40	44	p	show
233	50	59	n	our model
233	63	81	n	not very sensitive
233	82	84	p	to
233	89	105	n	noisy paragraphs
233	106	120	p	comparing with
233	125	139	n	baseline model
233	146	157	p	can achieve
233	160	190	n	more than 5 % performance gain
233	191	193	p	in
233	200	220	n	gold paragraphs only
233	229	250	n	supporting facts only
209	0	2	p	In
209	3	28	n	paragraph selection stage
209	34	37	p	use
209	42	75	n	uncased version of BERT Tokenizer
209	76	87	p	to tokenize
209	88	114	n	all passages and questions
212	3	27	n	graph construction stage
212	33	36	p	use
212	39	59	n	pretrained NER model
212	60	64	p	from
212	65	90	n	Stanford CoreNLP Toolkits
212	93	103	p	to extract
212	104	118	n	named entities
215	7	21	n	encoding stage
215	32	35	p	use
215	38	60	n	pre-trained BERT model
215	61	63	p	as
215	68	75	n	encoder
215	83	86	n	d 1
215	87	89	p	is
215	90	93	n	768
210	4	20	n	encoding vectors
210	21	23	p	of
210	24	38	n	sentence pairs
210	43	57	p	generated from
210	60	82	n	pre-trained BERT model
213	4	30	n	maximum number of entities
213	31	33	p	in
213	36	41	n	graph
213	45	51	p	set to
213	55	57	n	40
214	0	16	n	Each entity node
214	17	19	p	in
214	24	37	n	entity graphs
214	45	59	n	average degree
214	60	62	p	of
214	63	67	n	3.52
216	8	35	n	hidden state dimensions d 2
216	40	46	p	set to
216	47	50	n	300
211	3	6	p	set
211	9	33	n	relatively low threshold
211	34	40	p	during
211	41	50	n	selection
211	118	120	p	on
211	121	137	n	supporting facts
211	51	58	p	to keep
211	61	81	n	high recall ( 97 % )
211	88	117	n	reasonable precision ( 69 % )
217	11	23	n	dropout rate
217	24	27	p	for
217	28	44	n	all hidden units
217	45	47	p	of
217	48	80	n	LSTM and dynamic graph attention
217	81	83	p	to
217	84	95	n	0.3 and 0.5
218	0	3	p	For
218	4	16	n	optimization
218	22	25	p	use
218	26	40	n	Adam Optimizer
218	41	45	p	with
218	49	70	n	initial learning rate
218	71	73	p	of
218	74	80	n	1 e ?4
50	19	26	p	propose
50	27	67	n	Dynamically Fused Graph Network ( DFGN )
50	72	84	n	novel method
56	45	59	n	fusion process
56	60	62	p	in
56	63	67	n	DFGN
56	68	76	p	to solve
56	81	106	n	unrestricted QA challenge
51	26	30	n	DFGN
51	31	41	p	constructs
51	44	64	n	dynamic entity graph
51	65	73	p	based on
51	74	89	n	entity mentions
51	90	92	p	in
51	97	116	n	query and documents
52	5	12	n	process
52	13	24	p	iterates in
52	25	40	n	multiple rounds
52	41	51	p	to achieve
52	52	70	n	multihop reasoning
58	4	18	n	fusion process
58	19	21	p	is
58	22	43	n	iteratively performed
58	44	46	p	at
58	47	55	n	each hop
58	56	63	p	through
58	68	96	n	document tokens and entities
58	107	129	n	final resulting answer
58	138	151	p	obtained from
58	152	167	n	document tokens
59	19	21	p	of
59	22	46	n	doc2 graph and graph2doc
59	47	57	p	along with
59	62	82	n	dynamic entity graph
59	83	98	n	jointly improve
59	103	114	n	interaction
59	115	122	p	between
59	127	138	n	information
59	139	141	p	of
59	142	151	n	documents
59	160	172	n	entity graph
59	175	185	p	leading to
59	188	211	n	less noisy entity graph
59	221	242	n	more accurate answers
53	0	2	p	In
53	3	13	n	each round
53	16	20	n	DFGN
53	21	42	n	generates and reasons
53	43	45	p	on
53	48	61	n	dynamic graph
53	70	89	n	irrelevant entities
53	90	93	p	are
53	94	104	n	masked out
53	105	110	p	while
53	116	133	n	reasoning sources
53	134	137	p	are
53	138	147	n	preserved
53	150	153	p	via
53	156	178	n	mask prediction module
57	12	21	p	aggregate
57	22	33	n	information
57	34	38	p	from
57	39	48	n	documents
57	49	51	p	to
57	56	68	n	entity graph
57	71	81	n	doc2 graph
57	95	104	p	propagate
57	109	120	n	information
57	121	123	p	of
57	128	140	n	entity graph
57	141	148	p	back to
57	149	173	n	document representations
2	36	55	n	Multi-hop Reasoning
4	0	40	n	Text - based question answering ( TBQA )
13	0	25	n	Question answering ( QA )
14	0	2	n	QA
222	113	115	p	in
222	120	136	n	private test set
222	93	95	p	of
222	140	149	n	Hotpot QA
223	22	30	p	see that
223	31	40	n	our model
223	41	49	p	achieves
223	54	72	n	second best result
223	73	75	p	on
223	80	91	n	leaderboard
223	103	112	n	March 1st
224	41	58	n	joint performance
224	59	61	p	of
224	62	71	n	our model
224	72	75	p	are
224	76	87	n	competitive
224	88	95	p	against
224	96	137	n	state - of - the - art unpublished models
227	12	16	p	show
227	22	31	n	our model
227	32	40	p	achieves
227	43	53	n	1.5 % gain
227	54	56	p	in
227	61	77	n	joint F1 - score
227	78	82	p	with
227	87	99	n	entity graph
227	100	110	p	built from
227	113	137	n	better entity recognizer
