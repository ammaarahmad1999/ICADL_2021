150	3	7	p	used
150	8	34	n	word embeddings ( d = 50 )
150	45	59	p	computed using
150	60	105	n	Collobert and Weston 's neural language model
152	4	23	n	other model weights
152	29	50	n	randomly intitialised
152	51	56	p	using
152	59	99	n	Gaussian distribution ( = 0 , ? = 0.01 )
153	0	19	n	All hyperparameters
153	25	38	p	optimised via
153	39	50	n	grid search
153	51	53	p	on
153	58	91	n	MAP score on the development data
159	0	8	n	L - BFGS
159	18	26	p	to train
159	31	61	n	logistic regression classifier
159	64	68	p	with
159	69	83	n	L2 regulariser
159	84	86	p	of
159	87	91	n	0.01
154	3	6	p	use
154	11	28	n	AdaGrad algorithm
154	29	32	p	for
154	33	41	n	training
36	19	23	p	show
36	31	68	n	neural network - based sentence model
36	69	86	p	can be applied to
36	91	124	n	task of answer sentence selection
37	3	12	p	construct
37	13	47	n	two distributional sentence models
37	50	55	p	first
37	58	80	n	bag - of - words model
37	87	93	p	second
37	98	110	n	bigram model
37	111	119	p	based on
37	122	150	n	convolutional neural network
38	60	65	p	train
38	68	84	n	supervised model
38	85	93	p	to learn
38	96	113	n	semantic matching
38	114	121	p	between
38	122	147	n	question and answer pairs
38	0	17	p	Assuming a set of
38	18	54	n	pre-trained semantic word embeddings
40	8	15	p	present
40	19	35	n	enhanced version
40	58	66	p	combines
40	71	77	n	signal
40	36	38	p	of
40	85	115	n	distributed matching algorithm
40	116	120	p	with
40	121	154	n	two simple word matching features
2	18	43	n	Answer Sentence Selection
161	21	33	n	bigram model
161	34	42	p	performs
161	43	49	n	better
161	50	54	p	than
161	59	72	n	unigram model
161	81	92	p	addition of
161	97	131	n	IDF - weighted word count features
161	132	153	n	significantly improve
161	154	165	n	performance
161	166	169	p	for
161	170	181	n	both models
161	182	184	p	by
161	185	196	n	10 % - 15 %
169	24	54	n	best models ( bigram + count )
169	55	65	p	outperform
169	66	79	n	all baselines
