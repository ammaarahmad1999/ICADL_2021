1406.3676v3 [cs.CL] 4 Sep 2014

ar X1V

Question Answering with Subgraph Embeddings

Antoine Bordes, Jason Weston, and Sumit Chopra

Facebook, 770 Broadway, New York, NY, USA
{abordes, jase, spchopra}@fb.com

Abstract. This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features. Our model learns low-dimensional embeddings of words
and knowledge base constituents; these representations are used to score
natural language questions against candidate answers. ‘Training our system using pairs of questions and structured representations of their answers, and pairs of question paraphrases, yields competitive results on a
recent benchmark of the literature.

1 Introduction

Teaching machines how to automatically answer questions asked in natural language on any topic or in any domain has always been a long standing goal in
Artificial Intelligence. With the rise of large scale structured knowledge bases
(KBs), this problem, known as open-domain question answering (or open QA),
boils down to being able to query efficiently such databases with natural language. These KBs, such as FREEBASE |3! encompass huge ever growing amounts
of information and ease open QA by organizing a great variety of answers in a
structured format. However, the scale and the difficulty for machines to interpret
natural language still makes this task a challenging problem.

The state-of-the-art techniques in open QA can be classified into two main
classes, namely, information retrieval based and semantic parsing based. Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid
query and then use fine-grained detection heuristics to identify the exact answer
[8]12/14]. On the other hand, semantic parsing methods focus on the correct
interpretation of the meaning of a question by a semantic parsing system. A
correct interpretation converts a question into the exact database query that
returns the correct answer. Interestingly, recent works have shown that
such systems can be efficiently trained under indirect and imperfect supervision
and hence scale to large-scale regimes, while bypassing most of the annotation
costs.

Yet, even if both kinds of system have shown the ability to handle largescale KBs, they still require experts to hand-craft lexicons, grammars, and KB
schema to be effective. This non-negligible human intervention might not be
generic enough to conveniently scale up to new databases with other schema,
broader vocabularies or languages other than English. In contrast, {6] proposed
2 Antoine Bordes, Jason Weston, and Sumit Chopra

a framework for open QA requiring almost no human annotation. Despite being
an interesting approach, this method is outperformed by other competing methods. introduced an embedding model, which learns low-dimensional vector
representations of words and symbols (such as KBs constituents) and can be
trained with even less supervision than the system of [6] while being able to
achieve better prediction performance. However, this approach is only compared
with [6] which operates in a simplified setting and has not been applied in more
realistic conditions nor evaluated against the best performing methods.

In this paper, we improve the model of |5| by providing the ability to answer
more complicated questions. sThe main contributions of the paper are: (1) a
more sophisticated inference procedure that is both efficient and can consider
longer paths ({5] considered only answers directly connected to the question
in the graph); and (2) a richer representation of the answers which encodes
the question-answer path and surrounding subgraph of the KB. Our approach
is competitive with the current state-of-the-art on the recent benchmark WEBQUESTIONS without using any lexicon, rules or additional system for partof-speech tagging, syntactic or dependency parsing during training as most other
systems do.

2 Task Definition

Our main motivation is to provide a system for open QA able to be trained as
long as it has access to: (1) a training set of questions paired with answers and
(2) a KB providing a structure among answers. We suppose that all potential
answers are entities in the KB and that questions are sequences of words that
include one identified KB entity. When this entity is not given, plain string
matching is used to perform entity resolution. Smarter methods could be used
but this is not our focus.

We use WEBQUESTIONS |1| as our evaluation bemchmark. Since it contains
few training samples, it is impossible to learn on it alone, and this section describes the various data sources that were used for training. These are similar
to those used in [2].

WebQuestions This dataset is built using FREEBASE as the KB and contains
5,810 question-answer pairs. It was created by crawling questions through the
Google Suggest API, and then obtaining answers using Amazon Mechanical
Turk. We used the original split (3,778 examples for training and 2,032 for
testing), and isolated 1k questions from the training set for validation. WEBQUESTIONS is built on FREEBASE since all answers are defined as FREEBASE
entities. In each question, we identified one FREEBASE entity using string matching between words of the question and entity names in FREEBASE. When the
same string matches multiple entities, only the entity appearing in most triples,
i.e. the most popular in FREEBASE, was kept. Example questions (answers) in
the dataset include “Where did Edgar Allan Poe died?” (baltimore) or “What
degrees did Barack Obama get?” (bachelor_of_arts, juris_doctor).
Question Answering with Subgraph Embeddings 3

 

 

 

WEBQUESTIONS — Train. ex. 2,778
— Valid. ex. 1,000
— 'Test. ex. 2,032
FREEBASE — Train. ex. 14,790,259
CLUEWEB — Train. ex. 2,169,033
WIKIANSWERS — Train. quest.| 2,423,185
— Parap. clust. 349,957
Dictionary — Words 1,526,768
— Entities 2,154,345
— Rel. types 7,210

Table 1. Statistics of data sets used in the paper.

Freebase FREEBASE |3] is a huge and freely available database of general facts;
data is organized as triplets (subject, type1.type2.predicate, object), where
two entities subject and object (identified by mids) are connected by the relation type type1.type2.predicate. We used a subset, created by only keeping
triples where one of the entities was appearing in either the WEBQUESTIONS
training /validation set or in CLUEWEB extractions. We also removed all entities
appearing less than 5 times and finally obtained a FREEBASE set containing 14M
triples made of 2.2M entities and 7k relation types|!] Since the format of triples
does not correspond to any structure one could find in language, we decided to
transform them into automatically generated questions. Hence, all triples were
converted into questions “What is the predicate of the type2 subject?” (using
the mid of the subject) with the answer being object. An example is “What is
the nationality of the person barack_obama’?” (united states). More examples
and details are given in a longer version of this paper [4].

ClueWeb Extractions FREEBASE data allows to train our model on 14M questions but these have a fixed lexicon and vocabulary, which is not realistic. Following [i], we also created questions using CLUEWEB extractions provided by
[10]. Using string matching, we ended up with 2M extractions structured as
(subject, “text string”, object) with both subject and object linked to FREEBASE. We also converted these triples into questions by using simple patterns
and FREEBASE types. An example of generated question is “Where barack_obama
was allegedly bear in?” (hawaii).

Paraphrases The automatically generated questions that are useful to connect
FREEBASE triples and natural language, do not provide a satisfactory modeling
of natural language because of their semi-automatic wording and rigid syntax.
To overcome this issue, we follow {6] and supplement our training data with an
indirect supervision signal made of pairs of question paraphrases collected from
the WIKIANSWERS website. On WIKIANSWERS, users can tag pairs of questions

' WEBQUESTIONS contains ~2k entities, hence restricting FREEBASE to 2.2M entities
does not ease the task for us.
4 Antoine Bordes, Jason Weston, and Sumit Chopra

 

what is the judicial capital of the in state sikkim ? — gangtok

(sikkim, location.in_state. judicial_capital, gangtok)

who influenced the influence node yves_saint_laurent ? — helmut_newton
(yves_saint_laurent, influence.influence node.influenced, helmut_newton)

FREEBASE who is born in the location brighouse ? — edward_barber

generated questions |(brighouse, location.location.people_born here, edward _barber)

and associated triples|who is the producer of the recording rhapsody_in_b_minor,_op._79, _no._1 ? — glenn_gould
(rhapsody_in_b minor, op._79, no._1, music.recording. producer, glenn gould)

what are the symptoms of the disease sepsis ? — skin_discoloration

(sepsis, medicine.disease.symptoms, skin_discoloration)

 

 

 

 

 

 

what is cher’s son’s name ? — elijah_blue_allman

(cher, people.person. children, elijah_blue_allman)

what are dollars called in spain ? — peseta

(spain, location.country.currency formerly_used, peseta)

WEBQUESTIONS what is henry_clay known for ? — lawyer

training questions (henry_clay, people.person. profession, lawyer)

and associated paths |who is the president of the european_union 2011 ? — jerzy_buzek
(european_union, government .governmental_jurisdiction.governing officials
government .government_position_held.office_holder, jerzy_buzek)

what 6 states border south_dakota ? — iowa

(south_dakota, location. location.contains location.location.partially_containedby, iowa)

 

 

 

 

 

 

what does acetazolamide be an inhibitor of ? — carbonic_anhydrase

(acetazolamide, medicine.drug_ingredient.active moiety_of_drug, carbonic_anhydrase)
which place is a district in andhra_pradesh ? — adilabad

(andhra_pradesh, location.location.contains, adilabad)

CLUEWEB what is a regional airline based in dublin ? — aer_arann

generated questions |(dublin, location.location.nearby_airports aviation.airport.focus_city_for, aer_arann)
and associated paths |what mean fire in sanskrit ? — agni

(sanskrit, unknown_relation, agni)

where does american_legion proceed to ? — san_francisco

(american_legion, 22-rdf-syntax-ns#type type.type.instance, san francisco)

 

 

 

 

 

 

what are two reason to get a 404 ?

what is error 404 ?

you receive a 404 - unable to find error message ?
how do you correct error 404 ?

WIKIANSWERS what is the term for a teacher of islamic law ?
clusters of what is the islamic religious teacher called ?
quest. paraphrases what is the name of the religious book islam use ?
who is chief of islamic religious authority ?

what country is bueno aire in ?

what countrie is buenos aires in ?

what country is buenas aire in ?

what country is bueno are in ?

Table 2. Examples of questions, answer paths and paraphrases used in this paper.

 

 

 

 

 

 

as rephrasings of each other: [6] harvested a set of 2M distinct questions from
WIKIANSWERS, which were grouped into 350k paraphrase clusters.

3 Embedding Questions and Answers

Inspired by [5], our model works by learning low-dimensional vector embeddings
of words appearing in questions and of entities and relation types of FREEBASE,
so that representations of questions and of their corresponding answers are close
to each other in the joint embedding space. Let g denote a question and a a
candidate answer. Learning embeddings is achieved by learning a scoring function S(q,a), so that S generates a high score if a is the correct answer to the
question qg, and a low score otherwise. Note that both g and a are represented
as a combination of the embeddings of their individual words and/or symbols;
hence, learning S' essentially involves learning these embeddings. In our model,
Question Answering with Subgraph Embeddings 5

the form of the scoring function is:

S(q,a) = f(g)" g(a). (1)

Let W be a matrix of R**%, where k is the dimension of the embedding space
which is fixed a-priori, and N is the dictionary of embeddings to be learned. Let
Nw denote the total number of words and Ns the total number of entities and
relation types. With N = Nw + Ng, the 7-th column of W is the embedding of
the i-th element (word, entity or relation type) in the dictionary. The function
f(.), which maps the questions into the embedding space R* is defined as f(q) =
W ¢(q), where $(q) € NX, is a sparse vector indicating the number of times each
word appears in the question q (usually 0 or 1). Likewise the function g(.) which
maps the answer into the same embedding space R* as the questions, is given by
g(a) = Ww(a). Here w(a) € N*’ is a sparse vector representation of the answer
a, which we now detail.

 

How the candidate answer
Score S(q,a) fits the question

 

 

 

Si anitelelaute ol

  
    

 
     
    
 
 
     
    

Embedding of the Embedding of the

    
     

 

question f(q) = = subgraph g(a)
Embedding matrix W Embedding matrix W
Binary encoding of [(LLIL -. Li CL I iT I ] Binary encoding of

 

 

 

 

 

the question M/(q) the subgraph wW(a)

 

 

aerate

“Who did looney marry in 1987?” Rs
Detection of Freebase
entity in the question

Fig. 1. Illustration of the subgraph embedding model scoring a candidate answer: (i)
locate entity in the question; (ii) compute path from entity to answer; (iii) represent
answer as path plus all connected entities to the answer (the subgraph); (iv) embed
both the question and the answer subgraph separately using the learnt embedding
vectors, and score the match via their dot product.

Subgraph of a candidate
answer a (here K. Preston)

3.1 Representing Candidate Answers

We now describe possible feature representations for a single candidate answer.
(When there are multiple correct answers, we average these representations, see
Section [3.4]) We consider three different types of representation, corresponding
to different subgraphs of FREEBASE around it.
6 Antoine Bordes, Jason Weston, and Sumit Chopra

(i) Single Entity. The answer is represented as a single entity from FREEBASE:
w(a) is a 1-of-Ng coded vector with 1 corresponding to the entity of the
answer, and 0 elsewhere.

(ii) Path Representation. The answer is represented as a path from the entity

mentioned in the question to the answer entity. In our experiments, we

considered 1- or 2-hops paths (i.e. with either 1 or 2 edges to traverse):

(barack_obama, people.person.place_of birth, honolulu) is a 1-hop path and

(barack_obama, people.person.place_of_birth, location. location.containedby,

hawaii) a 2-hops path. This results in a ~(a) which is a 3-of-Ng or 4-of-Ng

coded vector, expressing the start and end entities of the path and the relation types (but not entities) in-between.

Subgraph Representation. We encode both the path representation from (ii),

and the entire subgraph of entities connected to the candidate answer entity.

That is, for each entity connected to the answer we include both the relation

type and the entity itself in the representation ~(a). In order to represent

the answer path differently to the surrounding subgraph (so the model can
differentiate them), we double the dictionary size for entities, and use one
embedding representation if they are in the path and another if they are
in the subgraph. Thus we now learn a parameter matrix R**" where N =

Nw +2Ng (Ng is the total number of entities and relation types). If there

are C’ connected entities with D relation types to the candidate answer, its

representation isa 3+ C+D or 4+ C+ D-of-Ng coded vector, depending
on the path length.

Scecee

(iii

Our hypothesis is that including more information about the answer in its
representation will lead to improved results. While it is possible that all required
information could be encoded in the k dimensional embedding of the single entity
(i), it is unclear what dimension & should be to make this possible. For example
the embedding of a country entity encoding all of its citizens seems unrealistic.
Similarly, only having access to the path ignores all the other information we
have about the answer entity, unless it is encoded in the embeddings of either
the entity of the question, the answer or the relations linking them, which might
be quite complicated as well. We thus adopt the subgraph approach. Figure [1]
illustrates our model.

3.2 Training and Loss Function

As in [13], we train our model using a margin-based ranking loss function. Let
D = {(q,a;) : i = 1,...,|D|} be the training set of questions q; paired with
their correct answer a;. The loss function we minimize is

|D|

Ss” S- max{0,m — S(q,a;) + S(q, a)}, (2)

i=1 GE A(a;)

where m is the margin (fixed to 0.1). Minimizing Eq. learns the embedding
matrix W so that the score of a question paired with a correct answer is greater
Question Answering with Subgraph Embeddings 7

than with any incorrect answer a by at least m. @ is sampled from a set of
incorrect candidates A. This is achieved by sampling 50% of the time from the
set of entities connected to the entity of the question (i.e. other candidate paths),
and by replacing the answer entity by a random one otherwise. Optimization is
accomplished using stochastic gradient descent, multi-threaded with Hogwild!
, with the constraint that the columns w; of W remain within the unit-ball,
1.€., Vis ||w;||o = 1.

3.3 Multitask Training of Embeddings

Since a large number of questions in our training datasets are synthetically
generated, they do not adequately cover the range of syntax used in natural
language. Hence, we also multi-task the training of our model with the task of
paraphrase prediction. We do so by alternating the training of S with that of
a scoring function Sprp(q1,q2) = f(a)' f(q2), which uses the same embedding
matrix W and makes the embeddings of a pair of questions (q1,q2) similar to
each other if they are paraphrases (i.e. if they belong to the same paraphrase
cluster), and make them different otherwise. Training S,,, is similar to that of S
except that negative samples are obtained by sampling a question from another
paraphrase cluster.

We also multitask the training of the embeddings with the mapping of the
mids of FREEBASE entities to the actual words of their names, so that the model
learns that the embedding of the mid of an entity should be similar to the embedding of the word(s) that compose its name(s).

3.4 Inference

Once W is trained, at test time, for a given question g the model predicts the
answer with:

@ = argmaxyc 4(q)5(4, 0’) (3)

where A(q) is the candidate answer set. This candidate set could be the whole
KB but this has both speed and potentially precision issues. Instead, we create
a candidate set A(q) for each question.

We recall that each question contains one identified FREEBASE entity. A(q) is
first populated with all triples from FREEBASE involving this entity. This allows
to answer simple factual questions whose answers are directly connected to them
(i.e. l-hop paths). This strategy is denoted C4.

Since a system able to answer only such questions would be limited, we
supplement A(q) with examples situated in the KB graph at 2-hops from the
entity of the question. We do not add all such quadruplets since this would lead
to very large candidate sets. Instead, we consider the following general approach:
given that we are predicting a path, we can predict its elements in turn using
a beam search, and hence avoid scoring all candidates. Specifically, our model
first ranks relation types using Eq. (1p, i.e. selects which relation types are the
most likely to be expressed in g. We keep the top 10 types (10 was selected on
8 Antoine Bordes, Jason Weston, and Sumit Chopra

Method P@l; Fl
(%) \(Berant)

Baselines

(Berant et al., 2013)

(Bordes et al., 2014)

(Yao and Van Durme, 2014)
(Berant and Liang, 2014)
Our approach

 

Subgraph & A(q) = C2
Ensemble with (Berant & Liang, 14)
Variants

 

Without multiple predictions

Subgraph & A(q) = All 2-hops

Subgraph & A(q) = Ci

Path & A(q) = C2

Single Entity & A(q) =Ci
Table 3. Results on the WEBQUESTIONS test set.

 

 

 

the validation set) and only add 2-hops candidates to A(q) when these relations
appear in their path. Scores of 1-hop triples are weighted by 1.5 since they have
one less element than 2-hops quadruplets. This strategy, denoted C2, is used by
default.

A prediction a’ can commonly actually be a set of candidate answers, not just
one answer, for example for questions like “Who are David Beckham’s children?”.
This is achieved by considering a prediction to be all the entities that lie on the
same l-hop or 2-hops path from the entity found in the question. Hence, all
answers to the above question are connected to david_beckham via the same path
(david_beckham, people.person.children, *). The feature representation of the
prediction is then the average over each candidate entity’s features (see Section
Bi) i.e. Wau(a’) = ial Daal sa! w(a;) where ai, are the individual entities in the
overall prediction a’. In the results, we compare to a baseline method that can
only predict single candidates, which understandly performs poorly.

4 Experiments

We compare our system in terms of F1 score as computed by the official evaluation script] (F1 (Berant)) but also with a slightly different F1 definition, termed
F1 (Yao) which was used in (the difference being the way that questions with
no answers are dealt with), and precision @ 1 (p@1) of the first candidate entity
(even when there are a set of correct answers), comparing to recently published
systems|"| The upper part of Table (3] indicates that our approach outperforms
(14), (1) and [5), and performs similarly as [2].

? Available from www-nlp.stanford.edu/software/sempre/
3 Results of baselines except [5] have been extracted from the original papers. For our
experiments, all hyperparameters have been selected on the WEBQUESTIONS validaQuestion Answering with Subgraph Embeddings 9

The lower part of Table[3|compares various versions of our model. Our default
approach uses the Subgraph representation for answers and C» as the candidate
answers set. Replacing C2 by Ci induces a large drop in performance because
many questions do not have answers thatare directly connected to their inluded
entity (not in C,). However, using all 2-hops connections as a candidate set is
also detrimental, because the larger number of candidates confuses (and slows
a lot) our ranking based inference. Our results also verify our hypothesis of
Section [3.1] that a richer representation for answers (using the local subgraph)
can store more pertinent information. Finally, we demonstrate that we greatly
improve upon the model of {5}, which actually corresponds to a setting with the
Path representation and C as candidate set.

We also considered an ensemble of our approach and that of [2]. As we only
had access to their test predictions we used the following combination method.
Our approach gives a score S(q, a) for the answer it predicts. We chose a threshold such that our approach predicts 50% of the time (when S(q,a) is above its
value), and the other 50% of the time we use the prediction of |2} instead. We
aimed for a 50/50 ratio because both methods perform similarly. The ensemble improves the state-of-the-art, and indicates that our models are significantly
different in their design.

5 Conclusion

This paper presented an embedding model that learns to perform open QA
using training data made of questions paired with their answers and of a KB to
provide a structure among answers, and can achieve promising performance on
the competitive benchmark WEBQUESTIONS.

References

1. J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing, October 2013.

2. J. Berant and P. Liang. Semantic parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the ACL, 2014.

3. K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of
the 2008 ACM SIGMOD international conference on Management of data. ACM,
2008.

4. A. Bordes, S. Chopra, and J. Weston. Question answering with subgraph embeddings. CoRR, abs/1406.3676, 2014.

5. A. Bordes, J. Weston, and N. Usunier. Open question answering with weakly
supervised embedding models. In Proceedings of ECML-PKDD’14. Springer, 2014.

tion set: k was chosen among {64, 128, 256}, the learning rate on a log. scale between
10~* and 107! and we used at most 100 paths in the subgraph representation.
10

10.

11.

12.

13.

14.

Antoine Bordes, Jason Weston, and Sumit Chopra

A. Fader, L. Zettlemoyer, and O. Etzioni. Paraphrase-driven learning for open
question answering. In Proceedings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 1608-1618, Sofia, Bulgaria, 2013.

. A. Fader, L. Zettlemoyer, and O. Etzioni. Open question answering over curated

and extracted knowledge bases. In Proceedings of KDD’14. ACM, 2014.

O. Kolomiyets and M.-F. Moens. A survey on question answering technology from
an information retrieval perspective. Information Sciences, 181(24):5412-5434,
2011.

T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer. Scaling semantic parsers
with on-the-fly ontology matching. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, October 2013.

T. Lin, O. Etzioni, et al. Entity linking at web scale. In Proceedings of the Joint
Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge
Extraction, pages 84-88. Association for Computational Linguistics, 2012.

B. Recht, C. Ré, S. J. Wright, and F. Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing
Systems (NIPS 24)., 2011.

C. Unger, L. Buhmann, J. Lehmann, A.-C. Ngonga Ngomo, D. Gerber, and P. Cimiano. Template-based question answering over rdf data. In Proceedings of the 21st
international conference on World Wide Web, 2012.

J. Weston, S. Bengio, and N. Usunier. Large scale image annotation: learning to
rank with joint word-image embeddings. Machine learning, 81(1), 2010.

X. Yao and B. Van Durme. Information extraction over structured data: Question
answering with freebase. In Proceedings of the 52nd Annual Meeting of the ACL,
2014.
