72	39	75	n	https://github.com/Helsinki-NLP/HBMP
71	4	16	n	architecture
71	21	38	p	implemented using
71	39	46	n	PyTorch
80	4	23	n	sentence embeddings
80	24	28	p	have
80	29	40	n	hidden size
80	41	43	p	of
80	44	47	n	600
80	48	51	p	for
80	52	66	n	both direction
80	161	200	n	3 - layer multilayer perceptron ( MLP )
80	201	205	p	have
80	210	214	n	size
80	215	217	p	of
80	218	232	n	600 dimensions
73	25	29	p	used
73	32	71	n	gradient descent optimization algorithm
73	72	80	p	based on
73	85	101	n	Adam update rule
73	113	131	p	pre-implemented in
73	132	139	n	PyTorch
74	10	23	n	learning rate
74	24	26	p	of
74	27	33	n	5e - 4
75	22	34	p	decreased by
75	39	45	n	factor
75	46	48	p	of
75	49	52	n	0.2
75	53	58	p	after
75	59	69	n	each epoch
75	70	72	p	if
75	77	82	n	model
75	87	90	p	not
75	91	98	n	improve
76	10	20	n	batch size
76	21	23	p	of
76	24	26	n	64
79	3	6	p	use
79	7	41	n	pre-trained Glo Ve word embeddings
79	42	44	p	of
79	45	49	n	size
79	50	64	n	300 dimensions
81	9	16	n	dropout
81	17	19	p	of
81	20	23	n	0.1
81	24	31	p	between
81	36	46	n	MLP layers
82	16	29	p	trained using
82	30	55	n	one NVIDIA Tesla P100 GPU
20	83	90	p	opt for
20	95	121	n	sentence encoding approach
21	58	64	p	extend
21	32	54	n	InferSent architecture
21	84	88	p	with
21	91	114	n	hierarchylike structure
21	25	27	p	of
21	118	154	n	bidirectional LSTM ( BiLSTM ) layers
21	155	159	p	with
21	160	171	n	max pooling
2	0	19	n	Sentence Embeddings
4	0	32	n	Sentence - level representations
124	3	22	n	clearly outperforms
124	39	69	n	non-hierarchical BiLSTM models
124	101	106	p	fares
124	107	111	n	well
124	112	128	p	in comparison to
124	129	165	n	other state of the art architectures
124	79	81	p	in
124	173	199	n	sentence encoding category
125	32	40	p	close to
125	45	69	n	current state of the art
125	70	72	p	on
125	73	77	n	SNLI
125	99	108	p	strong on
125	120	152	n	matched and mismatched test sets
125	153	155	p	of
125	156	164	n	MultiNLI
126	10	12	p	on
126	13	20	n	SciTail
126	26	33	p	achieve
126	38	58	n	new state of the art
126	59	63	p	with
126	67	75	p	accuracy
126	76	78	p	of
126	79	85	n	86.0 %
130	0	3	p	For
130	8	20	n	SNLI dataset
130	23	32	n	our model
130	33	41	p	provides
130	46	59	n	test accuracy
130	60	62	p	of
130	63	69	n	86.6 %
130	70	75	p	after
130	76	84	n	4 epochs
130	85	87	p	of
130	88	96	n	training
133	8	50	n	MultiNLI matched test set ( MultiNLI - m )
133	51	60	n	our model
133	61	69	p	achieves
133	72	85	n	test accuracy
133	86	88	p	of
133	89	95	n	73.7 %
133	96	101	p	after
133	102	110	n	3 epochs
133	111	113	p	of
133	114	122	n	training
133	125	133	p	which is
133	134	152	n	0.8 % points lower
133	153	157	p	than
133	162	185	n	state of the art 74.5 %
134	8	45	n	mismatched test set ( MultiNLI - mm )
134	46	55	n	our model
134	56	64	p	achieves
134	67	80	n	test accuracy
134	81	83	p	of
134	84	90	n	73.0 %
134	91	96	p	after
134	97	105	n	3 epochs
134	106	108	p	of
134	109	117	n	training
134	120	128	p	which is
134	129	147	n	0.6 % points lower
134	148	152	p	than
134	157	180	n	state of the art 73.6 %
138	0	2	p	On
138	7	22	n	SciTail dataset
138	26	34	p	compared
138	35	44	n	our model
138	50	57	p	against
138	58	95	n	non-sentence embedding - based models
139	3	9	p	obtain
139	12	17	n	score
139	18	20	p	of
139	21	27	n	86.0 %
139	28	33	p	after
139	34	42	n	4 epochs
139	43	45	p	of
139	46	54	n	training
139	63	65	p	is
139	68	101	n	2.7 % points absolute improvement
139	102	104	p	on
139	109	144	n	previous published state of the art
140	0	9	n	Our model
140	15	26	p	outperforms
140	27	40	n	In - fer Sent
140	41	46	p	which
140	47	55	p	achieves
140	59	67	p	accuracy
140	71	77	n	85.1 %
142	4	11	n	results
142	12	23	p	achieved by
142	28	42	n	proposed model
142	43	46	p	are
142	47	67	n	significantly higher
142	68	72	p	than
142	77	105	n	previously published results
