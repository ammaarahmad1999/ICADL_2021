arXiv:1612.04211v1 [cs.CL] 13 Dec 2016

Multi-Perspective Context Matching for Machine Comprehension

Zhiguo Wang, Haitao Mi, Wael Hamza and Radu Florian
IBM T-.J. Watson Research Center

1101 Kitchawan Rd, Yorktown Heights, NY 10598
{zhigwang, hmi, whamza, raduf}@us.ibm.com

Abstract

Previous machine comprehension (MC)
datasets are either too small to train endto-end deep learning models, or not difficult enough to evaluate the ability of current MC techniques. The newly released
SQuAD dataset alleviates these limitations, and gives us a chance to develop
more realistic MC models. Based on this
dataset, we propose a Multi-Perspective
Context Matching (MPCM) model, which
is an end-to-end system that directly predicts the answer beginning and ending
points in a passage. Our model first adjusts each word-embedding vector in the
passage by multiplying a relevancy weight
computed against the question. Then, we
encode the question and weighted passage
by using bi-directional LSTMs. For each
point in the passage, our model matches
the context of this point against the encoded question from multiple perspectives
and produces a matching vector. Given
those matched vectors, we employ another
bi-directional LSTM to aggregate all the
information and predict the beginning and
ending points. Experimental result on the
test set of SQUAD shows that our model
achieves a competitive result on the leaderboard.

1 Introduction

Machine Comprehension (MC) is a compelling
yet challenging task in both natural language processing and artificial intelligent research. Its task
is to enable machine to understand a given passage
and then answer questions related to the passage.

In recent years, several benchmark datasets
have been developed to measure and accelerate the

progress of MC technologies. RCTest (Richardson
et al., 2013) is one of the representative datasets.

It consists of 500 fictional stories and 4 multiple
choice questions per story (2,000 questions in total). A variety of MC methods were proposed
based on this dataset. However, the limited size
of this dataset prevents researchers from building
end-to-end deep neural network models, and the
state-of-the-art performances are still dominated
by the methods highly relying on hand-crafted fea2015) or employing additional knowledge
et al., 2016a). To deal with the scarcity of large
scale supervised data, pro
posed to create millions of Cloze style MC examples automatically from news articles on the CNN
and Daily Mail websites. They observed that each
news article has a number of bullet points, which
summarise aspects of the information in the article. Therefore, they constructed a corpus of (passage, question, answer) triples by replacing one
entity in these bullet points at a time with a placeholder. Then, the MC task is converted into filling the placeholder in the question with an entity
within the corresponding passage. Based on this
large-scale corpus, several end-to-end deep neural
network models are proposed successfully

mann et al., 2015;|/Kadlec et al., 2016;
2016). However, |Chen et al. (2016) did a careful

hand-analysis of this dataset, and concluded that
this dataset is not difficult enough to evaluate the
ability of current MC techniques.

To address the weakness of the previous MC

datasets, |Rajpurkar et al. (2016) developed the

Stanford Question Answering dataset (SQuAD).
Comparing with other datasets, SQuAD is more
realistic and challenging for several reasons: (1) it
is almost two orders of magnitude larger than previous manually labeled datasets; (2) all the questions are human-written, instead of the automatically generated Cloze style questions; (3) the answer can be an arbitrary span within the passage,
rather than a limited set of multiple choices or entities; (4) different forms of reasoning is required
for answering these questions.

In this work, we focus on the SQuAD dataset
and propose an end-to-end deep neural network
model for machine comprehension. Our basic assumption is that a span in a passage is more likely
to be the correct answer if the context of this span
is very similar to the question. Based on this assumption, we design a Multi-Perspective Context
Matching (MPCM) model to identify the answer
span by matching the context of each point in the
passage with the question from multiple perspectives. Instead of enumerating all the possible spans
explicitly and ranking them, our model identifies
the answer span by predicting the beginning and
ending points individually with globally normalized probability distributions across the whole passage. Ablation studies show that all components
in our MPCM model are crucial. Experimental result on the test set of SQUAD shows that our model
achieves a competitive result on the leaderboard.

In following parts, we start with a brief definition of the MC task (Section (2). followed by the
details of our MPCM model (Section 3). Then we
evaluate our model on the SQuAD dataset (Sec
tion|4).

2 Task Definition

Generally, a MC instance involves a question, a
passage containing the answer, and the correct answer span within the passage. To do well on this
task, a MC model need to comprehend the question, reason among the passage, and then identify
the answer span. Table [1] demonstrates three examples from SQuAD. Formally, we can represent
the SQuAD dataset as a set of tuples (Q, P, A),
where Q = (q1, «++; Gi, --- 1) is the question with
a length M, P = (pi,...,p;,...,;pn) is the passage with a length N, and A = (ap, de) is the answer span, dp and ae are the beginning and ending
points and 1 < ay < ae < N. The MC task can
be represented as estimating the conditional probability Pr (A|Q, P) based on the training set, and

Question #1: Who is Welsh medium education available to ?
Passage: ...... Welsh medium education is available to
all age groups through nurseries , schools , colleges ......

Question #2: What type of musical instruments did the Yuan
bring to China ?

Passage: Western musical instruments were introduced to
enrich Chinese performing arts ......

Question #3: What is the name of the Pulitzer Prize novelist
who was also a university alumni?

Passage: ...... , Pulitzer Prize winning novelist Philip Roth ,
ponent and American writer and satirist Kurt Vonnegut are notable alumni .

Table 1: Examples from SQuAD, where only the
relevant content of the original passage is retained,
and the blue underlined spans are the correct answers.

predicting answers for testing instances by

A* = arg max Pr(A|Q, P), (1)
AE A(P)

where A(P) is a set of answer candidates from P.
As the size of A(P) is in the order of O(.V?), we
make a simple independent assumption of predicting the beginning and endding points, and simplify
the model as

A* = argmax Pr(ap|Q, P) Pr(ae|Q,P), (2)

1<ap<ae<N

where Pr(ap|Q, P) (or Pr(ae|Q, P)) is the probability of the ap-th (or ae-th) position (point) of P
to be the beginning (or ending) point of the answer
span.

3 Multi-Perspective Context Matching
Model

In this section, we propose a Multi-Perspective
Context Matching (MPCM) model to estimate probability distributions Pr(ap|Q,P) and
Pr(ae|Q,P). Figure |1} shows the architecture of our MPCM model. The predictions of
Pr(ap|Q, P) and Pr(a.|Q, P) only differentiate at
the last prediction layer. And all other layers below the prediction layer are shared.

Given a pair of question @ and passage P, the
MPCM model estimates probability distributions
through the following six layers.

Word Representation Layer. The goal of this
layer is to represent each word in the question and
passage with a d-dimensional vector. We construct the d-dimensional vector with two components: word embeddings and character-composed
Pr(ae|Q, P)

softmax
sae ae 4
ware xr I
ee ee !
-- of 1

Pr(ap|Q, P)

Prediction Layer

  

Aggregation Layer

Multi-Perspective Context Matching Layer

Context Representation | “ |}- - ]

Layer Ze af ff Lt. 4. a
Filter Layer | I ieaeate 1 venees i
Pw

Po D2 vee Dj vee

Word Representation | I | —

Layer Pa PS ssi, PY ocean

Passage

 

92. Vi... IM

Relevancy matrix

Question

Figure 1: Architecture for Multi-Perspective Context Matching Model.

embeddings. The word embedding is a fixed vector for each individual word, which is pre-trained

with GloVe (Pennington et al., 2014) or word2vec
(Mikolov et al., 2013). The character-composed

embedding is calculated by feeding each character
(also represented as a vector) within a word into
a Long Short-Term Memory Network (LSTM)
(Hochreiter and Schmidhuber, 1997). The output

of this layer is word vector sequences for question
Q : gy, ---:Qjs], and passage P : [p,,...,py].

Filter Layer. In most cases, only a small piece
of the passage is needed to answer the question
(see examples in Table|1). Therefore, we define
the filter layer to filter out redundant information
from the passage. First, we calculate a relevancy
degree r; for each word p, in passage P. Inspired
from [Wang et al. (2016b), we compute the relevancy degree r;,; between each word pair g; € Q
and p; € P by calculating the cosine similarity
rij = eo. and get the relevancy degree by
rj; = Maxjey ri. Second, we filter each word
vector by p; = 1; -p,, and pass p’; to the next
layer. The main idea is that if a word in the pasSage is more relevant to the question, more information of the word should be considered in the
subsequent steps.

Context Representation Layer. The purpose
of this layer is to incorporate contextual informa
tion into the representation of each time step in
the passage and the question. We utilize a bidirectional LSTM (BiLSTM) to encode contextual
embeddings for each question word.

|

— —
h? =LSTM(h“_,,4q;) i=1,..,M A
— ~——

h? =LSTM(h 4,,,q;) i=M.,...,1

Meanwhile, we apply the same BiLSTM to the
passage:

—> ——> —
h® =LSTM(h?_ ,D’;) jg=1,...,N
J g-lr 9 (4)
< <———_ +—
h’ =LSTM(h ‘1, pj) Z=N,...,1

Multi-Perspective Context Matching Layer.
This is the core layer within our MPCM model.
The goal of this layer is to compare each contextual embedding of the passage with the question
with multi-perspectives. We define those multiperspective matching functions in following two
directions:

First, dimensional weighted matchings with

m = fm(v1, v2; W) (5)

where v1 and v2 are two d-dimensional vectors,
W e€ ¥'*74 is a trainable parameter, | is the number of perspectives, and the returned value ™ is
a [-dimensional vector m = |mj,..., Mx, ..., My].
Each element m, € m is a matching value from
the k-th perspective, and it is calculated by the cosine similarity between two weighted vectors

Mk = cosine(W, OV}, Wi. O v2) (6)

where © is the elementwise multiplication, and W;,
is the k-th row of W, which controls the k-th perspective and assigns different weights to different
dimensions of the d-dimensional space.

Second, on the orthogonal direction of fm, we
define three matching strategies to compare each
contextual embedding of the passage with the
question:

(1) Full-Matching: each forward (or backward)
contextual embedding of the passage is compared
with the forward (or backward) representation of
the entire question.

mi =

raf _

Im(h?, no; W?)

Im(h?,ht;W2) (7)

(2) Maxpooling-Matching: each forward (or
backward) contextual embedding of the passage is
compared with every forward (or backward) contextual embeddings of the question, and only the
maximum value is retained.

>

max __ TP qd. 3
m' = cman im hg, ha W )
mrt — max

Im(h?, h?;w4) (8)
i€(1...M) J

(3) Meanpooling-Matching: This is similar to
the Maxpooling-Matching, but we replace the
max operation with the mean operation.

M
mean 1 7 5
mee" = me his W")
i=1
minean _ O)

Thus, the matching vector for each position of the passage is the  concatenation of all the matching vectors m; =

mie mit". mre mre minean miner),

For the examples in Table|1] the forward FullMatching vector is extremely useful for question
#1, because we only need to match the left context to the entire question. Similarly, the backward
Full-Matching vector is very helpful for question
#2. However, for question #3, we have to utilize the Maxpooling-Matching and MeanpoolingMatching strategies, because both the left and
right contexts need to partially match the question.

Aggregation Layer. This layer is employed to
aggregate the matching vectors, so that each time
step of the passages can interactive with its surrounding positions. We incorporate the matching
vectors with a BiLSTM, and generate the aggregation vector for each time step.

Prediction Layer. We predict the probability distributions of Pr(a,p|Q, P) and Pr(a.|Q, P)
separately with two different feed-forward neural networks (shown in Figure |I| solid-lines for
Pr(ap|Q, P), dotted-lines for Pr(ae|Q, P)). We
feed the aggregation vector of each time step into
the feed-forward neural network individually, calculate a value for each time step, then normalize
the values across the entire passage with softmax
operation.

4 Experiments

4.1 Experiment Settings

We evaluate our model with the SQUAD dataset.
This dataset includes 87,599 training instances,
10,570 validation instances, and a large hidden test
set|'| We process the corpus with the tokenizer
from Stanford CorNLP (Manning et al., 2014).
To evaluate the experimental results, we employ
two metrics: Exact Match (EM) and F1 score
jpurkar et al., 2016).

To initialize the word embeddings in the word
representation layer, we use the 300-dimensional
GloVe word vectors pre-trained from the 840B
Common Crawl corpus (Pennington et al., 2014).
For the out-of-vocabulary (OOV) words, we initialize the word embeddings randomly. We set
the hidden size as 100 for all the LSTM layers,
and set the number of perspectives / of our multiperspective matching function (Equation (5)) as
50. We apply dropout to every layers in Figure
and set the dropout ratio as 0.2. To train the
model, we minimize the cross entropy of the be
'To evaluate on the hidden test set, we have
to submit the executable system to the leaderboard
(https://rajpurkar. github.io/SQuAD-explorer/)
Models EM Fl

Logistic Regression 40.4 51.0
Match-LSTM (Sequence) 54.5. 67.7

> Match-LSTM (Boundary) 60.5 70.7
“eh Dynamic Chunk Reader 62.5 71.0
~~ Match-LSTM with Bi-Ptr 64.7 73.7
MPCM (Ours) 65.5 75.1
Dynamic Coattention 66.2 75.9
BiDAF 68.0 77.3
r-net 69.5 77.9
Fine-Grained Gating 62.5 73.3

2 Match-LSTM (Boundary) 67.9 77.0
|= MPCM (Ours) 68.2 77.2
2 Dynamic Coattention 71.6 80.4
1 BiDAF 73.3 81.1
r-net 74.5 82.0

Table 2: Results on the SQUAD test set. All the
results here reflect the SQUAD leaderboard as of
Dec. 9, 2016.

ginning and end points, and use the ADAM optimizer to update parameters. We set the learning rate as 0.0001. For decoding, we enforce the end point is equal or greater
than the beginning point.

4.2 Results on the Test Set

Table [2] summarizes the performance of our models and other competing models. Our single
MPCM model achieves the EM of 65.5, and the F1
score of 75.1. We also build an ensemble MPCM
model by simply averaging the probability distributions of 5 models, where all the models have
the same architecture but initialized with different
seeds. With the help of the simple ensemble strategy, our MPCM model improves about 3% in term
of EM, and 2% in term of Fl score. Comparing the
performance of other models, our MPCM models
achieve competitive results in both single and ensemble scenarios.

4.3 Influence of the Multi-Perspective
Matching Function

In this sub-section, we study the influence of our
multi-perspective matching function in Eq.(5). We
built a baseline model vanilla-cosine by replacing
Eq.(5) with the vanilla cosine similarity function.
We also varied the number of perspectives / among
{1, 10, 30, 50}, and kept the other options un
l EM Fl
vanilla-cosine 58.1 69.7
1 60.7 71.7
10 64.1 74.6
30 64.7 74.6
50 66.1 75.8

Table 3: Influence of the multi-perspective matching function in Eq.(5) .

Models EM Fil

w/o character 62.8 73.0
w/o Filter Layer 64.0 74.0
w/o Full-Matching 64.3 74.8
w/o Maxpooling-Matching 63.1 73.7
w/o Meanpooling-Matching 64.1 74.9
w/o Aggregation Layer 61.0 72.3
MPCM (single) 66.1 75.8
MPCM (ensemble) 69.4 78.6

Table 4: Layer ablation on the dev set.

changed. Table |3|shows the performance on the
dev set. We can see that, even if we only utilize
one perspective, our multi-perspective matching
function works better than the vanilla-cosine baseline. When increasing the number of perspectives,
the performance improves significantly. Therefore, our multi-perspective matching function is
really effective for matching vectors.

4.4 Layer Ablation

In this sub-section, we evaluate the effectiveness
of various layers in our MPCM model. We built
several layer ablation models by removing one
layer at a time. For the Multi-Perspective Context
Matching Layer, we cannot remove it entirely. Instead, we built three models (w/o Full-Matching,
w/o Maxpooling-Matching, w/o MeanpoolingMatching) by eliminating each matching strategy
individually. Table |4| shows the performance of
all ablation models and our full MPCM model on
the dev set. We can see that removing any components from the MPCM model decreases the performance significantly. Among all the layers, the Aggregation Layer is the most crucial layer. Among
all the matching strategies, Maxpooling-Matching
has the biggest effect.
—eEM =F 1
80

70
60
50

40
1 2 3 4 5 6 7 >=8

Answer Length

Figure 2: Performance for different answer length.

what year (771) |liiiiimiaaaaaaaam
invwiicd (OO) a
in what (243)
when (696) |g
howl 6) i
what kind (33) E==-EE
how many (543) =a =
who (1059) _ iii =
how much (138) }
what arc(19) T=
itt) I 2= 7:
vhich 454
what type (193) _ |e
where (433) = ~ ~ what does (220) |i
what wa 622) ii
what can (44) xe
what do (122) TE
ies) wii
hve?) i ===
whith (4) ==
wi) ti ===

OF] SEM 0 20 40 60 80 100

 

 

Figure 3:
types.

Performance for different question

4.5 Result Analysis

To better understand the behavior of our MPCM
model, we conduct some analysis of the result on
the dev set.

Figure [2] shows the performance changes based
on the answer length. We can see that the
performance drops when the answer length increases, and the EM drops faster than the F1 score.
The phenomenon reveals that longer answers are
harder to find, and it is easier to find the approximate answer region than identify the precise
boundaries.

Figure |3} shows the performances of different
types of questions. The numbers inside the brackets are the frequency of that question type on the
dev set. We can see that the performances for

99 66 29 66

“when”, “what year’, “in what’, and “in which”
questions are much higher than the others. The
possible reason is that the temporal expressions
are easier to detect for “when” and “what year”
questions, and there is an explicit boundary word
“in” for “in what’ and “in which” questions.
Our model works poorly for the “how did” question. Because “how did” questions usually require
longer answers, and the answers could be any type
of phrases.

Figure |4] visualizes the probability distributions
produced by our MPCM model for an example
question from the dev set, where the upper subfigure is the probabilities for the beginning point
and the lower one is the probabilities for the ending point. We can see that our model assigns most
mass of the probability to the correct beginning
and ending points.

To conduct the error analysis, we randomly select 50 incorrect questions from the dev set. We
found that predictions for 16% questions are acceptable (even though they are not in the correct answer list) and 22% overlap with the correct
answer. 14% of the questions require reasoning
across multiple sentences, and most of the remaining questions require external knowledge or complex reasoning.

5 Related Work

Many deep learning based models were proposed
since the release of the SQuAD dataset. Based on
the method of identifying the answer spans, most
of the models can be roughly categorized into the
following two classes:

Chunking and Ranking. In this kind of methods, a list of candidate chunks (answers) are extracted firstly. Then, models are trained to rank the
correct chunk to the top of the list. Rajpurkar et al.|
proposed to collect the candidate chunks
from all constituents of parse trees, and designed
some hand-crafted features to rank the chunks
with logistic regression model (“Logistic Regression” in Table|2). However, over 20% of the questions do not have any correct answers within the
candidate list. To increase the recall,
(2016) extracted candidate chunks based on some
part-of-speech patterns, which made over 90% of
the questions answerable. Then, they employed an
attention-based RNN model to rank all the chunks

(“Dynamic Chunk Reader” in Table[2).
(2016) enumerated all possible chunks (up to 30 

 

 

 

 

 

 

Figure 4: Probability distributions for the question “What did Luther consider Christ ’s life ?’’, where the
correct answer is “an illustration of the Ten Commandments”, the upper sub-figure is for the beginning

point and the lower one is for the ending point.

grams) within the passage, learned a fixed length
representations for each chunk with a multi-layer
BiLSTM model, and scored each chunk based on
the fixed length representations.

Boundary Identification. Instead of extracting
a list of candidate answers, this kind of methods
learns to identify the answer span directly. Generally, some kinds of question-aware representations are learnt for each time step of the passage,
then the beginning and ending points are predict
based on the representations.
proposed a match-LSTM model to match
the passage with the question, then the Pointer
Network was utilized to select a list of positions from the passage as the
final answer (“Match-LSTM (Sequence)”’ in Table (2p. However, the returned positions are not
guaranteed to be consecutive. They further modified the Pointer Network to only predict the beginning or ending points (“Match-LSTM (Boundary)” and “Match-LSTM with Bi-Ptr” in Table
(2). introduced the Dynamic
Coattention Network (“Dynamic Coattention” in
Table |2). Their model first captured the interactions between the question and the passage with a
co-attentive encoder, then a dynamic pointing decoder was used for predicting the beginning and

ending points. |Seo et al. (2016) proposed a similar model with |Xiong et al. (2016). This model

employed a bi-directional attention flow mechanism to achieve a question-aware context representations for the passage, then the beginning and
ending points were predict based on the representations. Our model also belongs to this category.
However, different from all the previous models,
our model generates the question-aware representations by explicitly matching contextual embeddings of the passage with the question from multiple perspectives, and no lexical or word vector information is passed to the boundary identification
layer.

6 Conclusion

In this work, we proposed the Multi-Perspective
Context Matching (MPCM) model for machine
comprehension task. Our model identifies the
answer span by matching each time-step of the
passage with the question from multiple perspectives, and predicts the beginning and ending points
based on globally normalizing probability distributions. Ablation studies show that all aspects of
matching inside the MPCM model are crucial. Experimental result on the test set of SQUAD shows
that our model achieves a competitive result on the
leaderboard.

References

Danqi Chen, Jason Bolton, and Christopher D Manning. 2016. A thorough examination of the
cnn/daily mail reading comprehension task. arXiv
preprint arXiv: 1606.02858.

Karl Moritz Hermann, Tomas Kocisky, Edward
Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems, pages 16931701.

Sepp Hochreiter and Jiirgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8): 1735-1780.

Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and
Jan Kleindienst. 2016. Text understanding with
the attention sum reader network. arXiv preprint
arXiv: 1603.01547.

Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv: 1412.6980.

Kenton Lee, Tom Kwiatkowski, Ankur Parikh, and Dipanjan Das. 2016. Learning recurrent span representations for extractive question answering. arXiv
preprint arXiv: 1611.01436.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations,
pages 55-60.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing
systems, pages 3111-3119.

Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. In EMNLP, volume 14, pages 1532—
43.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv: 1606.05250.

Matthew Richardson, Christopher JC Burges, and Erin
Renshaw. 2013. Mctest: A challenge dataset for
the open-domain machine comprehension of text. In
EMNLP, volume 3, page 4.

Mrinmaya Sachan, Avinava Dubey, Eric P Xing, and
Matthew Richardson. 2015. Learning answerentailing structures for machine comprehension. In
Proceedings of ACL, pages 239-249.

Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2016. Bidirectional attention
flow for machine comprehension. arXiv preprint
arXiv: 1611.01603.

Yelong Shen, Po-Sen Huang, Jianfeng Gao, and
Weizhu Chen. 2016. Reasonet: Learning to stop
reading in machine comprehension. arXiv preprint
arXiv: 1609.05284.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Advances in Neural Information Processing Systems, pages 2692-2700.

Shuohang Wang and Jing Jiang. 2016. Machine comprehension using match-Ilstm and answer pointer.
arXiv preprint arXiv: 1608.07905.

Hai Wang and Mohit Bansal Kevin Gimpel David
McAllester. 2015. Machine comprehension with
syntax, frames, and semantics. Volume 2: Short Papers, page 700.

Bingning Wang, Shangmin Guo, Kang Liu, Shizhu He,
and Jun Zhao. 2016a. Employing external rich
knowledge for machine comprehension. In Proceedings of IJCAI.

Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah.
2016b. Sentence similarity learning by lexical decomposition and composition. In Proceddings of
Coling 2016.

Caiming Xiong, Victor Zhong, and Richard Socher.
2016. Dynamic coattention networks for question
answering. arXiv preprint arXiv: 1611.01604.

Yang Yu, Wei Zhang, Kazi Hasan, Mo Yu, Bing Xiang,
and Bowen Zhou. 2016. End-to-end answer chunk
extraction and ranking for reading comprehension.
arXiv preprint arXiv: 1610.09996.
