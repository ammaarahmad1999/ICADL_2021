26	23	25	p	in
26	30	37	n	context
26	38	40	p	of
26	43	104	n	simple one - to - many multi -task learning ( MTL ) framework
26	107	114	p	wherein
26	117	150	n	single recurrent sentence encoder
26	154	167	p	shared across
26	168	182	n	multiple tasks
29	15	22	p	aims at
29	23	31	n	learning
29	32	83	n	fixed - length distributed sentence representations
31	43	50	p	combine
31	55	63	n	benefits
31	25	27	p	of
31	67	120	n	diverse sentence - representation learning objectives
31	121	125	p	into
31	128	155	n	single multi-task framework
2	45	109	n	LEARNING GENERAL PURPOSE DISTRIBUTED SEN - TENCE REPRESENTATIONS
4	86	129	n	distributed vector representations of words
6	36	82	n	learning representations of sequences of words
18	39	90	n	learning general - purpose sentence representations
136	24	30	p	adding
136	31	41	n	more tasks
136	42	50	p	improves
136	55	75	n	transfer performance
136	76	78	p	of
136	79	88	n	our model
137	0	10	p	Increasing
137	15	23	n	capacity
137	24	44	n	our sentence encoder
137	45	49	p	with
137	50	75	n	more hidden units ( + L )
137	76	86	p	as well as
137	90	115	n	additional layer ( + 2L )
137	121	128	p	lead to
137	129	158	n	improved transfer performance
138	3	10	p	observe
138	11	16	n	gains
138	17	19	p	of
138	20	31	n	1.1 - 2.0 %
138	32	34	p	on
138	39	69	n	sentiment classification tasks
138	72	74	n	MR
138	77	79	n	CR
138	82	86	n	SUBJ
138	89	93	n	MPQA
138	96	100	p	over
138	101	110	n	Infersent
143	27	48	n	0.2-0.5 % improvement
143	49	53	p	over
143	58	86	n	decomposable attention model
139	3	14	p	demonstrate
139	15	32	n	substantial gains
139	33	35	p	on
139	36	40	n	TREC
139	43	46	n	6 %
139	47	51	p	over
139	52	61	n	Infersent
139	66	77	n	roughly 2 %
139	78	82	p	over
139	87	97	n	CNN - LSTM
139	102	115	n	outperforming
139	123	154	n	competitive supervised baseline
140	3	6	p	see
140	7	20	n	similar gains
140	23	28	n	2.3 %
140	31	33	p	on
140	34	68	n	paraphrase identification ( MPRC )
140	71	78	p	closing
140	83	86	n	gap
140	87	89	p	on
140	90	111	n	supervised approaches
140	112	124	p	trained from
140	125	132	n	scratch
141	4	15	p	addition of
141	16	36	n	constituency parsing
141	37	45	p	improves
141	46	57	n	performance
141	58	60	p	on
141	61	94	n	sentence relatedness ( SICK - R )
141	99	122	n	entailment ( SICK - E )
142	8	17	p	show that
142	25	33	n	training
142	37	40	n	MLP
142	41	50	p	on top of
142	51	85	n	our fixed sentence representations
142	86	97	n	outperforms
142	98	144	n	several strong & complex supervised approaches
142	145	153	p	that use
142	154	174	n	attention mechanisms
144	0	10	p	When using
144	18	32	n	small fraction
144	33	35	p	of
144	40	53	n	training data
144	101	108	p	able to
144	109	119	n	outperform
144	124	159	n	Siamese and Multi - Perspective CNN
144	160	165	p	using
144	166	177	n	roughly 6 %
144	178	180	p	of
144	185	207	n	available training set
145	8	18	n	outperform
145	23	39	n	Deconv LVM model
150	0	15	n	Representations
150	16	35	p	learned solely from
150	36	39	n	NLI
150	43	59	p	appear to encode
150	60	66	n	syntax
150	71	89	p	incorporation into
150	90	114	n	our multi-task framework
150	115	123	p	does not
150	124	131	n	amplify
150	137	143	n	signal
147	30	42	p	observe that
147	47	70	n	learned word embeddings
147	71	74	p	are
147	75	86	n	competitive
147	87	91	p	with
147	92	107	n	popular methods
147	108	115	p	such as
147	116	121	n	GloVe
147	124	132	n	word2vec
147	139	147	n	fasttext
151	28	52	n	sentence characteristics
151	53	60	p	such as
151	61	82	n	length and word order
151	83	86	p	are
151	87	101	n	better encoded
151	111	122	p	addition of
151	123	130	n	parsing
