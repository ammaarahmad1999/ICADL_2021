title
abstract
Introduction
Related Works
Background
Additive Attention
Dot-product Attention
Proposed Model
Overall Architecture
Sentence Encoder
Experiments and Results
Dataset
Training Details
SNLI Results
MultiNLI Results
Case Study
Conclusion
