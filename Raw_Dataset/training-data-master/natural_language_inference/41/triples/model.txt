(Contribution||has||Model)
(Model||has||Pretraining)
(Pretraining||has||two stages)
(two stages||has||sequence - to - sequence model)
(sequence - to - sequence model||is||learned)
(learned||to reconstruct||original text)
(two stages||has||text)
(text||corrupted with||arbitrary noising function)
(Model||has||BART)
(BART||is||denoising autoencoder)
(denoising autoencoder||built with||sequence - to - sequence model)
(denoising autoencoder||applicable to||very wide range of end tasks)
(BART||uses||standard Tranformer - based neural machine translation architecture)
(standard Tranformer - based neural machine translation architecture||seen as||generalizing)
(generalizing||has||BERT)
(BERT||due to||bidirectional encoder)
(generalizing||has||GPT)
(GPT||with||left - to - right decoder)
(Model||present||BART)
(BART||pre-trains||model)
(model||combining||Bidirectional and Auto - Regressive Transformers)
