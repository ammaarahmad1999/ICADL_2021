193	0	8	p	Removing
193	13	48	n	independent span loss ( indep - I )
193	49	59	p	results in
193	62	78	n	performance drop
193	79	82	p	for
193	83	118	n	all answerable questions ( HasAns )
194	0	8	p	Ablating
194	9	52	n	independent no - answer loss ( indep - II )
194	75	81	p	causes
194	82	98	n	little influence
194	55	57	p	on
194	102	108	n	HasAns
194	115	123	p	leads to
194	126	140	n	severe decline
194	99	101	p	on
194	144	178	n	no - answer accuracy ( NoAns ACC )
212	9	29	n	two auxiliary losses
212	42	50	p	leads to
212	54	74	n	over all degradation
212	75	77	p	on
212	82	87	n	curve
212	103	114	n	outperforms
212	119	127	n	baseline
212	128	130	p	by
212	133	145	n	large margin
196	10	18	p	deleting
196	19	37	n	both of two losses
196	38	44	p	causes
196	47	58	n	degradation
196	59	61	p	of
196	62	82	n	more than 1.5 points
196	83	85	p	on
196	90	110	n	over all performance
196	111	122	p	in terms of
196	123	125	n	F1
196	128	143	p	with or without
196	144	159	n	ELMo embeddings
200	0	6	p	Adding
200	7	22	n	ELMo embeddings
200	35	43	p	does not
200	44	49	n	boost
200	54	65	n	performance
204	3	12	p	find that
204	17	28	n	improvement
204	29	31	p	on
204	32	49	n	noanswer accuracy
204	50	52	p	is
204	53	64	n	significant
210	3	10	p	observe
210	16	37	n	RMR + ELMo + Verifier
210	38	46	p	achieves
210	51	65	n	best precision
210	66	70	p	when
210	75	81	n	recall
210	82	84	p	is
210	85	97	n	less than 80
177	3	6	p	run
177	9	20	n	grid search
177	32	37	p	among
177	38	71	n	[ 0.1 , 0.3 , 0.5 , 0.7 , 1 , 2 ]
179	80	83	p	For
179	84	94	n	Model - II
179	101	138	n	Adam optimizer ( Kingma and Ba 2014 )
179	139	143	p	with
179	146	159	n	learning rate
179	160	162	p	of
179	163	169	n	0.0008
179	184	195	n	hidden size
179	199	205	p	set as
179	206	209	n	300
179	218	225	n	dropout
179	228	230	p	of
179	231	234	n	0.3
179	238	249	p	applied for
179	250	272	n	preventing overfitting
180	4	14	n	batch size
180	15	17	p	is
180	18	20	n	48
180	21	24	p	for
180	29	35	n	reader
180	38	40	n	64
180	41	44	p	for
180	45	55	n	Model - II
180	62	64	n	32
180	65	68	p	for
180	69	78	n	Model - I
180	90	101	n	Model - III
181	3	6	p	use
181	11	33	n	Glo Ve 100D embeddings
181	34	37	p	for
181	42	48	n	reader
181	55	70	n	300D embeddings
181	71	74	p	for
181	75	85	n	Model - II
181	90	101	n	Model - III
182	3	10	p	utilize
182	15	29	n	nltk tokenizer
182	32	45	p	to preprocess
182	46	68	n	passages and questions
182	82	87	p	split
182	88	97	n	sentences
26	32	39	p	propose
26	42	69	n	read - then - verify system
26	75	85	p	aims to be
26	86	92	n	robust
26	93	95	p	to
26	96	118	n	unanswerable questions
27	14	24	n	our system
27	25	36	p	consists of
27	37	51	n	two components
27	62	78	n	no-answer reader
27	79	93	p	for extracting
27	94	111	n	candidate answers
27	116	125	p	detecting
27	126	148	n	unanswerable questions
27	164	179	n	answer verifier
27	180	192	p	for deciding
27	212	231	n	extracted candidate
27	232	234	p	is
27	235	245	n	legitimate
29	11	18	p	augment
29	19	35	n	existing readers
29	36	40	p	with
29	41	61	n	two auxiliary losses
32	25	36	p	introducing
32	40	61	n	independent span loss
32	67	74	p	aims to
32	75	86	n	concentrate
32	87	89	p	on
32	94	116	n	answer extraction task
33	57	65	p	leverage
33	68	94	n	multi-head pointer network
33	95	106	p	to generate
33	107	131	n	two pairs of span scores
33	134	139	p	where
33	140	148	n	one pair
33	149	151	p	is
33	152	162	n	normalized
33	25	29	p	with
33	172	188	n	no -answer score
33	197	202	n	other
33	206	214	p	used for
33	219	233	n	auxiliary loss
34	13	20	p	present
34	21	54	n	another independent noanswer loss
34	55	75	p	to further alleviate
34	80	91	n	confliction
34	97	108	p	focusing on
34	113	137	n	no-answer detection task
34	138	145	p	without
34	162	182	n	shared normalization
34	183	185	p	of
34	186	203	n	answer extraction
35	56	65	p	introduce
35	69	102	n	additional answer verifying phase
35	111	118	p	aims at
35	119	126	n	finding
35	127	143	n	local entailment
35	149	157	p	supports
35	162	168	n	answer
39	71	82	p	investigate
39	83	112	n	three different architectures
39	113	116	p	for
39	121	142	n	answer verifying task
40	4	13	n	first one
40	14	16	p	is
40	19	35	n	sequential model
40	41	46	p	takes
40	47	60	n	two sentences
40	61	63	p	as
40	64	78	n	along sequence
40	91	101	n	second one
40	114	121	p	capture
40	122	134	n	interactions
40	135	142	p	between
40	143	156	n	two sentences
41	4	12	n	last one
41	13	15	p	is
41	18	30	n	hybrid model
41	31	44	p	that combines
41	49	65	n	above two models
2	16	45	n	Machine Reading Comprehension
187	16	26	n	our system
187	27	34	p	obtains
187	35	63	n	state - of the - art results
187	121	123	p	on
187	128	136	n	test set
187	64	76	p	by achieving
187	80	88	n	EM score
187	89	91	p	of
187	92	96	n	71.7
187	103	112	n	F 1 score
187	113	115	p	of
187	116	120	n	74.2
188	0	11	p	Notice that
188	12	18	n	SLQA +
188	23	30	p	reached
188	33	50	n	comparable result
188	51	62	p	compared to
188	63	75	n	our approach
