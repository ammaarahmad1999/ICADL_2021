103	51	96	n	https://github.com/jingli9111/GORU-tensorflow
29	3	10	p	propose
29	13	29	n	new architecture
29	36	76	n	Gated Orthogonal Recurrent Unit ( GORU )
29	85	93	p	combines
29	156	163	n	ability
29	164	174	p	to capture
29	175	197	n	long term dependencies
29	198	206	p	by using
29	207	226	n	orthogonal matrices
29	250	252	p	to
29	255	261	n	forget
29	264	272	p	by using
29	275	288	n	GRU structure
30	3	14	p	demonstrate
30	20	24	n	GORU
30	28	41	p	able to learn
30	42	64	n	long term dependencies
30	65	76	n	effectively
31	18	26	p	focus on
31	27	41	n	implementation
31	42	44	p	of
31	45	75	n	orthogonal transition matrices
31	76	84	p	which is
31	92	98	n	subset
31	99	101	p	of
31	106	122	n	unitary matrices
4	19	51	n	recurrent neural network ( RNN )
11	0	43	n	Recurrent Neural Networks with gating units
13	42	84	n	gating units for Recurrent Neural Networks
14	49	53	n	RNNs
105	45	64	n	Copying Memory Task
112	24	27	p	use
112	28	48	n	RMSProp optimization
112	49	53	p	with
112	56	69	n	learning rate
112	70	72	p	of
112	73	78	n	0.001
112	85	95	n	decay rate
112	96	98	p	of
112	99	102	n	0.9
113	4	14	n	batch size
113	18	24	p	set to
113	25	28	n	128
115	0	18	n	Hidden state sizes
115	23	29	p	set to
115	30	50	n	128 , 100 , 90 , 512
115	66	74	p	to match
115	75	87	n	total number
115	88	90	p	of
115	91	118	n	hidden to hidden parameters
120	4	8	n	GORU
120	9	11	p	is
120	16	35	n	only gated - system
120	36	38	p	to
120	39	57	n	successfully solve
120	63	67	n	task
121	0	12	n	Denoise Task
128	40	43	p	use
128	44	77	n	RM - SProp optimization algorithm
128	78	82	p	with
128	85	98	n	learning rate
128	99	101	p	of
128	102	106	n	0.01
128	113	123	n	decay rate
128	124	126	p	of
128	127	130	n	0.9
129	4	14	n	batch size
129	18	24	p	set to
129	25	28	n	128
131	0	18	n	Hidden state sizes
131	23	29	p	set to
131	30	50	n	128 , 100 , 90 , 512
131	66	74	p	to match
131	75	118	n	total number of hidden to hidden parameters
132	80	92	n	GORU and GRU
132	93	111	n	successfully solve
132	116	120	n	task
136	0	16	n	Parenthesis Task
142	24	42	n	total input length
142	46	52	p	set to
142	53	56	n	200
143	3	7	p	used
143	8	18	n	batch size
143	19	22	n	128
143	27	44	n	RMSProp Optimizer
143	45	49	p	with
143	52	65	n	learning rate
143	66	71	n	0.001
143	74	84	n	decay rate
143	85	88	n	0.9
146	4	8	n	GORU
146	12	19	p	able to
146	20	43	n	successfully outperform
146	44	47	n	GRU
146	50	54	n	LSTM
146	59	64	n	EURNN
146	65	76	p	in terms of
146	82	96	n	learning speed
146	101	119	n	final performances
147	8	16	p	analyzed
147	40	52	n	update gates
147	53	56	p	for
147	57	69	n	GORU and GRU
152	0	16	n	Algorithmic Task
156	3	7	p	used
156	8	18	n	batch size
156	19	21	n	50
156	26	37	n	hidden size
156	38	41	n	128
157	4	8	n	RNNs
157	13	25	p	trained with
157	26	43	n	RMSProp optimizer
157	44	48	p	with
157	51	64	n	learning rate
157	65	67	p	of
157	68	73	n	0.001
157	78	88	n	decay rate
157	89	91	p	of
157	92	95	n	0.9
159	3	13	p	found that
159	18	22	n	GORU
159	23	31	p	performs
159	32	48	n	averagely better
159	49	53	p	than
159	54	74	n	GRU / LSTM and EURNN
