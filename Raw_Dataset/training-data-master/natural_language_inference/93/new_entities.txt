132	86	107	n	positively contribute
132	108	110	p	to
132	115	128	n	final results
132	129	131	p	of
132	132	162	n	gated self - matching networks
132	0	45	n	attention - based recurrent network ( GARNN )
132	50	85	n	self - matching attention mechanism
134	0	25	n	Characterlevel embeddings
134	26	44	p	contribute towards
134	49	69	n	model 's performance
134	79	82	p	can
134	83	96	n	better handle
134	97	124	n	out - ofvocab or rare words
137	0	28	n	Character - level embeddings
137	33	36	p	not
137	37	45	n	utilized
138	18	22	n	gate
138	23	36	p	introduced in
138	37	72	n	question and passage matching layer
138	76	83	n	helpful
138	84	87	p	for
138	93	105	n	GRU and LSTM
133	0	8	p	Removing
133	9	24	n	self - matching
133	25	35	p	results in
133	36	53	n	3.5 point EM drop
108	3	6	p	use
108	11	20	n	tokenizer
108	21	25	p	from
108	26	42	n	Stanford CoreNLP
108	43	56	p	to preprocess
108	57	82	n	each passage and question
112	4	24	n	hidden vector length
112	28	34	p	set to
112	35	37	n	75
112	38	41	p	for
112	42	52	n	all layers
113	4	15	n	hidden size
113	21	31	p	to compute
113	32	48	n	attention scores
113	49	51	p	is
113	57	59	n	75
109	4	32	n	Gated Recurrent Unit variant
109	33	35	p	of
109	36	40	n	LSTM
109	44	59	p	used throughout
109	60	69	n	our model
115	4	9	n	model
115	13	27	p	optimized with
115	28	54	n	AdaDelta ( Zeiler , 2012 )
115	55	59	p	with
115	63	84	n	initial learning rate
115	85	87	p	of
115	88	89	n	1
110	0	3	p	For
110	4	18	n	word embedding
110	24	27	p	use
110	28	72	n	pretrained case - sensitive GloVe embeddings
110	104	107	p	for
110	113	135	n	questions and passages
110	148	153	n	fixed
110	154	160	p	during
110	161	169	n	training
110	179	191	n	zero vectors
110	192	204	p	to represent
110	205	231	n	all out - of - vocab words
111	3	10	p	utilize
111	11	40	n	1 layer of bi-directional GRU
111	41	51	p	to compute
111	52	80	n	character - level embeddings
111	85	115	n	3 layers of bi-directional GRU
111	116	125	p	to encode
111	126	148	n	questions and passages
111	155	196	n	gated attention - based recurrent network
111	197	200	p	for
111	201	230	n	question and passage matching
111	239	246	p	encoded
111	247	262	n	bidirectionally
114	8	13	p	apply
114	14	21	n	dropout
114	22	29	p	between
114	30	36	n	layers
114	37	41	p	with
114	44	56	n	dropout rate
114	57	59	p	of
114	60	63	n	0.2
116	10	17	p	used in
116	18	26	n	AdaDelta
116	27	30	p	are
116	31	46	n	0.95 and 1e ? 6
22	17	26	p	introduce
22	29	58	n	gated self - matching network
22	81	116	n	end - to - end neural network model
22	117	120	p	for
22	121	165	n	reading comprehension and question answering
23	10	21	p	consists of
23	22	32	n	four parts
24	8	33	n	recurrent network encoder
24	34	42	p	to build
24	43	57	n	representation
24	58	61	p	for
24	62	84	n	questions and passages
24	106	126	n	gated matching layer
24	127	135	p	to match
24	140	160	n	question and passage
24	171	192	n	self - matching layer
24	193	205	p	to aggregate
24	206	217	n	information
24	218	222	p	from
24	227	240	n	whole passage
24	255	308	n	pointernetwork based answer boundary prediction layer
2	35	79	n	Reading Comprehension and Question Answering
4	63	109	n	reading comprehension style question answering
