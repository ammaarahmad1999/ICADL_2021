24	19	26	p	explore
24	30	50	n	alternative approach
24	51	59	p	based on
24	60	69	n	enriching
24	74	97	n	document representation
25	36	41	p	train
25	44	74	n	sequence - to - sequence model
25	82	87	p	given
25	90	98	n	document
25	101	110	p	generates
25	111	129	n	possible questions
25	77	81	p	that
25	139	147	n	document
25	148	153	p	might
25	154	160	n	answer
25	0	11	p	Focusing on
25	12	30	n	question answering
68	0	4	n	BM25
68	10	13	p	use
68	18	51	n	Anserini open - source IR toolkit
68	54	62	p	to index
68	67	103	n	original ( non -expanded ) documents
68	113	120	p	to rank
68	125	133	n	passages
70	0	16	n	BM25 + Doc2query
71	9	15	p	expand
71	20	29	n	documents
71	30	35	p	using
71	40	65	n	proposed Doc2query method
72	8	22	p	index and rank
72	27	45	n	expanded documents
72	46	59	p	exactly as in
72	64	75	n	BM25 method
76	0	3	n	RM3
77	3	10	p	compare
77	11	29	n	document expansion
77	30	34	p	with
77	35	50	n	query expansion
77	56	63	p	applied
77	68	97	n	RM3 query expansion technique
79	0	11	n	BM25 + BERT
79	17	35	p	index and retrieve
79	36	45	n	documents
79	46	51	p	as in
79	56	70	n	BM25 condition
79	75	90	p	further re-rank
79	95	104	n	documents
79	105	109	p	with
79	110	114	n	BERT
80	0	23	n	BM25 + Doc2query + BERT
80	29	58	p	expand , index , and retrieve
80	59	68	n	documents
80	69	74	p	as in
80	79	105	n	BM25 + Doc2query condition
80	110	125	p	further re-rank
80	130	139	n	documents
80	140	144	p	with
80	145	149	n	BERT
2	0	18	n	Document Expansion
86	0	18	n	Document expansion
86	19	23	p	with
86	24	55	n	our method ( BM25 + Doc2query )
86	56	64	p	improves
86	65	88	n	retrieval effectiveness
86	89	91	p	by
86	94	98	n	15 %
86	99	102	p	for
86	103	116	n	both datasets
89	0	57	n	Our full re-ranking condition ( BM25 + Doc2query + BERT )
89	58	63	p	beats
89	64	81	n	BM25 + BERT alone
113	0	51	n	Our method without a re-ranker ( BM25 + Doc2query )
113	52	56	p	adds
113	59	72	n	small latency
113	82	86	p	over
113	87	120	n	baseline BM25 ( 50 ms vs. 90 ms )
113	125	127	p	is
113	142	160	n	seven times faster
113	161	165	p	than
113	168	184	n	neural re-ranker
113	185	193	p	that has
113	196	222	n	three points higher MRR@10
87	8	15	p	combine
87	16	34	n	document expansion
87	35	39	p	with
87	42	102	n	state - of - the - art re-ranker ( BM25 + Doc2query + BERT )
87	108	115	p	achieve
87	120	140	n	best - known results
87	149	151	p	on
87	152	160	n	TREC CAR
87	163	166	p	for
87	167	175	n	MS MARCO
87	185	189	p	near
87	194	210	n	state of the art
99	3	14	p	notice that
99	19	24	n	model
99	34	38	p	copy
99	39	49	n	some words
99	50	54	p	from
99	59	73	n	input document
100	30	38	p	produces
100	39	44	n	words
100	45	59	p	not present in
100	64	78	n	input document
100	128	144	p	characterized as
100	145	154	n	expansion
100	155	157	p	by
100	158	190	n	synonyms and other related terms
103	6	12	p	expand
103	13	31	n	MS MARCO documents
103	32	37	p	using
103	38	52	n	only new words
103	57	65	p	retrieve
103	70	93	n	development set queries
103	94	98	p	with
103	99	103	n	BM25
103	109	115	p	obtain
103	119	125	n	MRR@10
103	126	128	p	of
103	129	133	n	18.8
104	0	14	p	Expanding with
104	15	27	n	copied words
104	28	33	p	gives
104	37	43	n	MRR@10
104	44	46	p	of
104	47	51	n	19.7
105	3	10	p	achieve
105	13	26	n	higher MRR@10
105	27	29	p	of
105	30	34	n	21.5
105	35	39	p	when
105	40	49	n	documents
105	54	67	p	expanded with
105	68	87	n	both types of words
107	3	12	p	find that
107	17	28	n	Recall@1000
107	29	31	p	of
107	36	60	n	MS MARCO development set
107	61	75	p	increased from
107	76	89	n	85.3 ( BM25 )
107	90	92	p	to
107	93	118	n	89.3 ( BM25 + Doc2query )
109	42	57	n	query expansion
109	58	62	p	with
109	63	66	n	RM3
109	67	72	n	hurts
109	73	75	p	in
109	76	89	n	both datasets
111	12	22	p	shows that
111	23	41	n	document expansion
111	49	68	p	more effective than
111	69	84	n	query expansion
