arX1v:1810.02720v1 [cs.CL] 5 Oct 2018

TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic
Parsing and Code Generation

Pengcheng Yin, Graham Neubig
Language Technologies Institute
Carnegie Mellon University
{pcyin, gneubig}@cs.cmu.edu

Abstract

We present TRANX, a transition-based neural semantic parser that maps natural language
(NL) utterances into formal meaning representations (MRs). TRANX uses a transition
system based on the abstract syntax description language for the target MR, which gives
it two major advantages: (1) it is highly accurate, using information from the syntax of
the target MR to constrain the output space
and model the information flow, and (2) it is
highly generalizable, and can easily be applied
to new types of MR by just writing a new abstract syntax description corresponding to the
allowable structures in the MR. Experiments
on four different semantic parsing and code
generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural
semantic parsers.!

1 Introduction

Semantic parsing 1s the task of transducing natural language (NL) utterances into formal meaning representations (MRs). The target MRs can
be defined according to a wide variety of formalisms. This include linguistically-motivated semantic representations that are designed to capture the meaning of any sentence such as )calculus (Zettlemoyer and Collins, 2005) or the
abstract meaning representations (Banarescu et al.,
2013). Alternatively, for more task-driven approaches to semantic parsing, it is common for
meaning representations to represent executable
programs such as SQL queries (Zhong et al.,
2017), robotic commands (Artzi and Zettlemoyer,
2013), smart phone instructions (Quirk et al.,
2015), and even general-purpose programming
languages like Python (Yin and Neubig, 2017; Rabinovich et al., 2017) and Java (Ling et al., 2016).

' Available at https: //github.com/pcyin/tranX. An
earilier version is used in Yin et al. (2018).

Because of these varying formalisms for MRs,
the design of semantic parsers, particularly neural network-based ones has generally focused on
a small subset of tasks — in order to ensure the
syntactic well-formedness of generated MRs, a
parser is usually specifically designed to reflect
the domain-dependent grammar of MRs in the
structure of the model (Zhong et al., 2017; Xu
et al., 2017). To alleviate this issue, there have
been recent efforts in neural semantic parsing with
general-purpose grammar models (Xiao et al.,
2016; Dong and Lapata, 2018). Yin and Neubig
(2017) put forward a neural sequence-to-sequence
model that generates tree-structured MRs using a
series of tree-construction actions, guided by the
task-specific context free grammar provided to the
model a priori. Rabinovich et al. (2017) propose the abstract syntax networks (ASNs), where
domain-specific MRs are represented by abstract
syntax trees (ASTs, Fig. 2 Left) specified under
the abstract syntax description language (ASDL)
framework (Wang et al., 1997). An ASN employs
a modular architecture, generating an AST using specifically designed neural networks for each
construct in the ASDL grammar.

Inspired by this existing research, we have developed TRANX, a TRANSsition-based abstract
syntaX parser for semantic parsing and code generation. TRANX is designed with the following
principles in mind:

e Generalization ability TRANX employs ASTs
as a general-purpose intermediate meaning representation, and the task-dependent grammar is
provided to the system as external knowledge to
guide the parsing process, therefore decoupling
the semantic parsing procedure with specificities of grammars.

e Extensibility TRANX uses a simple transition system to parse NL utterances into treeASDL Grammar

stmt / Expr(expr value)

Transition System
(actions to construct AST)

keyword* keywords )

expr’ Call(expr func, expr* args, t ApplyConstr (Expr )

| Attribute(expr value,

identifier attr)
| Name(identifier id)

| Str(string s)

    

Input Utterance

pandas read top 100 lines in file.csv

ApplyConstr(Call)

ApplyConstr(Attr.)

GenToken(sorted)

Abstract Syntax Tree Meaning Representation

(general-purpose intermediate MR)

(to domain-specific MR)

AST to MR( a )

pandas.read_csv(
file.csv,
nrows=100)

 

Figure 1: Workflow of TRANX

structured ASTs. The transition system is designed to be easy to extend, requiring minimal
engineering to adapt to tasks that need to handle
extra domain-specific information.

e Effectiveness We test TRANX on four semantic parsing (ATIS, GEO) and code generation
(DJANGO, WIKISQL) tasks, and demonstrate
that TRANX is capable of generalizing to different domains while registering strong performance, out-performing existing neural networkbased approaches on three of the four datasets
(GEO, ATIS, DJANGO).

2 Methodology

Given an NL utterance, TRANX parses the utterance into a formal meaning representation, typically represented as A-calculus logical forms,
domain-specific, or general-purpose programming
languages (e.g., Python). In the following description we use Python code generation as a running
example, where a programmer’s natural language
intents are mapped to Python source code. Fig. 1
depicts the workflow of TRANX. We will present
more use cases of TRANX in 8 3.

The core of TRANX is a transition system.
Given an input NL utterance x, TRANX employs the transition system to map the utterance x into an AST z using a series of treeconstruction actions (8 2.2). TRANX employs
ASTs as the intermediate meaning representation to abstract over domain-specific structure of
MRs. This parsing process is guided by the userdefined, domain-specific grammar specified under
the ASDL formalism (§ 2.1). Given the generated AST z, the parser calls the user-defined function, AST_to_MR(-), to convert the intermediate
AST into a domain-specific meaning representation y, completing the parsing process. TRANX
uses a probabilistic model p(z|x), parameterized
by a neural network, to score each hypothesis AST

(§ 2.3).

2.1 Modeling ASTs using ASDL Grammar

TRANX uses ASTs as the general-purpose, intermediate semantic representation for MRs. ASTs
are commonly used to represent programming languages, and can also be used to represent other
tree-structured MRs (e.g., A-calculus). The ASDL
framework is a grammatical formalism to define
ASTs. See Fig. 1 for an excerpt of the Python
ASDL grammar. TRANX provides APIs to read
such a grammar from human-readable text files.

An ASDL grammar has two basic constructs:
types and constructors. A composite type is defined by the set of constructors under that type.
For example, the stmt and expr composite types
in Fig. | refer to Python statements and expressions, repectively, each defined by a series of constructors. A constructor specifies a language construct of a particular type using its fields. For instance, the Call constructor under the composite type expr denotes function call expressions,
and has three fields: func, args and keywords.
Each field in a constructor is also strongly typed,
which specifies the type of value the field can
hold. A field with a composite type can be instantiated by constructors of the same type. For example, the func field above can hold a constructor
of type expr. There are also fields with primitive types, which store values. For example, the
id field of Name constructor has a primitive type
identifier, and is used to store identifier names.
And the field s in the Str (string) constructor hold
string literals. Finally, each field has a cardinality
(single, optional ? and sequential *), denoting the
number of values the field holds.

An AST is then composed of multiple constructors, where each node on the tree corresponds to
a typed field in a constructor (except for the root
node, which denotes the root constructor). Depending on the cardinality of the field, a node
can hold one or multiple constructors as its values. For instance, the func field with single carty

fivalue

      
 

to

Jo func

Attribute

fs value

J4 keywords

13

ei

t4

Sr ea a rrr

fiid| gts ttnsczeees

Se ee

——sscaoe ee

ee

ae

; Action
tj; root Expr(expr value)

 

to fi Call(expr func, expr* args,
keyword* keywords)
tz fo Attribute(expr value, identifier attr)
ta fs Name(identifier id)
ts f7 | GENTOKEN|pandas|
te fe | GENTOKEN|read_csv|
tr fs Str(string s)
tg fg | GENTOKEN|file.csv]
to fg | GENTOKEN|</f>]
tio f3 REDUCE (close the frontier field fs)
ti, fa keyword(identifier arg, expr value)
ti2 fo GENTOKEN[nrows|
tig fio Num(object n)
tia fi: GENTOKEN|1000]
tis fa REDUCE (close the frontier field f4)

Figure 2: Left The ASDL AST for the target Python code in Fig. 1. Field names are labeled on upper arcs, and indexed as fi.
Purple squares denote fields with sequential cardinality. Grey nodes denote primitive identifier fields. Fields are labeled with
time steps at which they are generated. Right The action sequence used to construct the AST. Each action is labeled with its
frontier field n ¢,. APPLYCONSTR actions are represented by their constructors.

dinality in the ASDL grammar in Fig. | is instantiated with one Name constructor, while the
args field with sequential cardinality have multiple child constructors.

2.2 Transition System

Inspired by Yin and Neubig (2017) (hereafter
YN17), we develop a transition system that decomposes the generation procedure of an AST into
a sequence of tree-constructing actions. We now
explain the transition system using our running example. Fig. 2 Right lists the sequence of actions
used to construct the example AST. In high level,
the generation process starts from an initial derivation AST with a single root node, and proceeds according to a top-down, left-to-right order traversal
of the AST. At each time step, one of the following three types of actions is evoked to expand the
opening frontier field nz, of the derivation:

APPLYCONSTR|c] actions apply a constructor
c to the opening composite frontier field which has
the same type as c, populating the opening node
using the fields in c. If the frontier field has sequential cardinality, the action appends the constructor to the list of constructors held by the field.

REDUCE actions mark the completion of the
generation of child values for a field with optional
(?) or multiple (*) cardinalities.

GENTOKEN|v] actions populate a (empty)
primitive frontier field with a token v. For example, the field f7 on Fig. 2 has type identifier,
and is instantiated using a single GENTOKEN action. For fields of string type, like fg, whose
value could consists of multiple tokens (only
one shown here), it can be filled using a sequence of GENTOKEN actions, with a special

GENTOKEN|</f>] action to terminate the generation of token values.

The generation completes once there is no frontier field on the derivation. TRANX then calls
the user specified function AST_to_MR(-) to convert the generated intermediate AST z into the
target domain-specific MR y. TRANX provides
various helper functions to ease the process of
writing conversion functions. For example, our
example conversion function to transform ASTs
into Python source code contains only 32 lines of
code. TRANX also ships with several built-in conversion functions to handle MRs commonly used
in semantic parsing and code generation, like Acalculus logical forms and SQL queries.

2.3 Computing Action Probabilities p(z|z)
Given the transition system, the probability of an
z 1s decomposed into the probabilities of the sequence of actions used to generate z

p(z\x) = | | plarlace, x),

Following YN17, we parameterize the transitionbased parser p(z|a) using a neural encoderdecoder network with augmented recurrent connections to reflect the topology of ASTs.

Encoder The encoder is a standard bidirectional Long Short-term Memory (LSTM) network,
which encodes the input utterance x of n tokens,
{a;}"_, into vectorial representations {h}""_,.

Decoder The decoder is also an LSTM network,
with its hidden state s; at each time temp given by
Ss; = fiustm([at—1 : St—1 : Pe], St—-1),

where /Lstm is the LSTM transition function, and
[:| denotes vector concatenation. a,—, is the emexpr
= Variable(var variable)

Entity(ent entity)

Number (num number)

Apply(pred predicate, expr* arguments)
Argmax(var variable, expr domain, expr body)
Argmin(var variable, expr domain, expr body)
Count (var variable, expr body)

Exists(var variable, expr body)

Lambda(var variable, var_type type, expr body)
Max(var variable, expr body)

Min(var variable, expr body)

Sum(var variable, expr domain, expr body)
The(var variable, expr body)

Not(expr argument)

And(expr* arguments)

Or(expr* arguments)

Compare(cmp_op op, expr left, expr right)

 

cmp_op = Equal | LessThan | GreaterThan

Figure 3: The A-calculus ASDL grammar for GEO and
ATIS, defined in Rabinovich et al. (2017)

bedding of the previous action. We maintain an
embedding vector for each action. s; is the attentional vector defined as in Luong et al. (2015)

St = tanh(W,.|c; : S;|).

where c; is the context vector retrieved from input
encodings {h;}""_, using attention.

Parent Feeding pp; is a vector that encodes the
information of the parent frontier field nz, on the
derivation, which is a concatenation of two vectors: the embedding of the frontier field n;,, and
Sp,» the decoder’s state at which the constructor
of ny, 1S generated by the APPLYCONSTR action. Parent feeding reflects the topology of treestructured ASTs, and gives better performance on
generating complex MRs like Python code (8 3).

Action Probabilities
PLYCONSTR|c] action with embedding a, is

The probability of an AP2

p(az = APPLYCONSTR|c]|a<;, X)
= softmax(al Ws;)

(1)

For GENTOKEN actions, we employ a hybrid
approach of generation and copying, allowing
for out-of-vocabulary variable names and literals
(e.g., “file.csv” in Fig. 1) in & to be directly copied
to the derivation. Specifically, the action probability is defined to be the marginal probability

p(az = GENTOKEN|v]|a<z, ©)
= p(genjaz, x) p(v|gen, az, 2)+
p(copy|ae, x )p(v|copy, Qt, x)

°REDUCE is treated as a special APPLYCONSTR action.

stmt = Select(agg_op? agg, idx column_idx,
cond_expr* conditions)

cond_expr = Condition(cmp_op op, idx column_idx,
string value)
agg_op = Max | Min | Count | Sum | Avg

cmp_op = Equal | GreaterThan | LessThan | Other

Figure 4: The ASDL grammar for WIKISQL

The binary probability p(gen|-) and p(copy]-) is
given by softmax(Ws;,). The probability of generating v from a closed-set vocabulary, p(vu|gen, -)
is defined similarly as Eq. (1). The copy probability of copying the 2-th word in & is defined using
a pointer network (Vinyals et al., 2015)

p(x;|copy, act, %) = softmax(h} Ws;).

3 Experiments

3.1 Datasets

To demonstrate the generalization and extensibility of TRANX, we deploy our parser on four semantic parsing and code generation tasks.

3.1.1 Semantic Parsing

We evaluate on GEO and ATIS datasets. GEO
is a collection of 880 U.S. geographical questions (e.g., “Which states border Texas?”), and
ATIS is a set of 5,410 inquiries of flight information (e.g., “Show me flights from Dallas to Baltimore’’). The MRs in the two
datasets are defined in A-calculus logical forms
(e.g., “lambda x (and (state x) (next_to
x texas))” and “lambda x (and (flight zx
dallas) (to x baltimore))’’). We use the
pre-processed datasets released by Dong and Lapata (2016). We use the ASDL grammar defined
in Rabinovich et al. (2017), as listed in Fig. 3.

3.1.2 Code Generation
We evaluate TRANX on both general-purpose
(Python, DJANGO) and domain-specific (SQL,
WIKISQL) code generation tasks. The DJANGO
dataset (Oda et al., 2015) consists of 18,805 lines
of Python source code extracted from the Django
Web framework, with each line paired with an NL
description. Code in this dataset covers various
real-world use cases of Python, like string manipulation, I/O operation, exception handling, etc.
WIKISQL (Zhong et al., 2017) is a code
generation task for domain-specific languages
(i.e., SQL). It consists of 80,654 examples of
NL questions (e.g., “What position did Calvin
Mccarty play?”) and annotated SQL queries
(e.g., “SELECT Position FROM Table WHERE
Methods GEO ATIS
ZH15 (Zhao and Huang, 2015) 88.9 84.2
ZCO07 (Zettlemoyer and Collins, 2007) 89.0 84.6
WKZ14 (Wang et al., 2014) 90.4 91.3

Neural Network-based Models
SEQ2TREE (Dong and Lapata, 2016) 87.1 84.6

ASN (Rabinovich et al., 2017) 85.7 = 85.3

+ supervised attention 87.1 85.9
TRANX (w/o parent feeding) 88.2 86.2
TRANX (w/ parent feeding) 87.7 86.2

Table 1: Semantic parsing accuracies on GEO and ATIS

 

Methods ACC.
Phrasal Statistical MT (Ling et al., 2016) 31.5
SEQ2TREE (Dong and Lapata, 2016) 39.4
NMT (Neubig, 2015) 45.1
LPN (Ling et al., 2016) 62.3
YN17 (Yin and Neubig, 2017) 71.6
TRANX (w/o parent feeding) 72.7
TRANX (w parent feeding) 73.7

Table 2: Code generation accuracies on DIANGO

Player = Calvin Mccarty”). Different from
other datasets, each example also has a table extracted from Wikipedia, and the SQL query is executed against the table to get an answer.

Extending TRANX for WIKISQL In order to
achieve strong results, existing parsers, like most
models in Tab. 3, use specifically designed architectures to reflect the syntactic structure of
SQL queries. We show that the transition system used by TRANX can be easily extended for
WIKISQL with minimal engineering, while registering strong performance. First, we use define a simple ASDL grammar following the syntax of SQL (Fig. 4). We then augment the transition system with a special GENTOKEN action,
SELCOLUMN[k].. A SELCOLUMN|K] action is
used to populate a primitive column_idx field in
Select and Condition constructors in the grammar by selecting the k-th column in the table. To
compute the probability of SELCOLUMN|k] actions, we use a pointer network over column encodings, where the column encodings are given by
a bidirectional LSTM network over column names
in an input table. This can be simply implemented
by overriding the base Parser class in TRANX
and modifying the functions that compute action
probabilities.

3.2 Results
In this section we discuss our experimental results.

All results are averaged over three runs with different random seeds.

Semantic Parsing ‘Tab. | lists the results for
semantic parsing tasks. We test TRANX with

Methods ACCrEmM ACCrx
Seq2Seq (Zhong et al., 2017) 23.4 35.9
SEQ2TREE (Dong and Lapata, 2016) 23.4 35.9
Seq2SQL (Zhong et al., 2017) 48.3 59.4
SQLNet (Xu et al., 2017) - 68.0
PT-MAML (Huang et al., 2018) 62.8 68.0
TypeSQL (Yu et al., 2018) — 73.5
TRANX

w/ parent feeding 62.6 71.6

w/o parent feeding 62.9 71.7
PointSQL (Wang et al., 2017)! 61.5 66.8
TypeSQL+TC (Yu et al., 2018)! = 82.6
STAMP (Sun et al., 2018)" 60.7 74.4
STAMP+RL (Sun et al., 2018)! 61.0 74.6
TRANX

w par. feed. + answer pruning! 68.4 78.6

w/o par. feed. + answer pruning! 68.6 78.6

Table 3: Exact match (EM) and execution (EX) accuracies
on WIKISQL. ' Methods that use the contents of input tables.

two configurations, with or without parent feeding (§ 2.3). Our system outperforms existing neural network-based approaches. This demonstrates
the effectiveness of TRANX in closed-domain semantic parsing. Interestingly, we found the model
without parent feeding achieves slightly better accuracy on GEO, probably because that its relative
simple grammar does not require extra handling of
parent information.

Code Generation ‘Tab. 2 lists the results on
DJANGO. TRANX achieves state-of-the-art results
on DJANGO. We also find parent feeding yields
+1 point gain in accuracy, suggesting the importance of modeling parental connections in ASTs
with complex domain grammars (e.g., Python).
Tab. 3 shows the results on WIKISQL. We first
discuss our standard model which only uses information of column names and do not use the contents of input tables during inference, as listed in
the top two blocks in Tab. 3. We find TRANX, although just with simple extensions to adapt to this
dataset, achieves impressive results and outperforms many task-specific methods. This demonstrates that TRANX is easy to extend to incorporate task-specific information, while maintaining
its effectiveness. We also extend TRANX with a
very simple answer pruning strategy, where we
execute the candidate SQL queries in the beam
against the input table, and prune those that yield
empty execution results. Results are listed in the
bottom two-blocks in Tab. 3, where we compare
with systems that also use the contents of tables.
Surprisingly, this (frustratingly) simple extension
yields significant improvements, outperforming
many task-specific models that use specifically designed, heavily-engineered neural networks to incorporate information of table contents.

4 Conclusion

We present TRANX, a transition-based abstract
syntax parser. TRANX is generalizable, extensible
and effective, achieving strong results on semantic
parsing and code generation tasks.

Acknowledgements

This material is based upon work supported by
the National Science Foundation under Grant No.
1815287. PY would like to thank Junxian He and
Li Dong for helpful discussions.

References

Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping
instructions to actions. Transaction of ACL.

Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of LAW-ID@ACL.

Li Dong and Mirella Lapata. 2016. Language to logical
form with neural attention. In Proceedings of ACL.

Li Dong and Mirella Lapata. 2018. Coarse-to-fine decoding for neural semantic parsing. In Proceedings
of ACL.

Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen
tau Yih, and Xiaodong He. 2018. Natural language
to structured query generation via meta-learning. In
Proceedings of NAACL-HLT.

Wang Ling, Phil Blunsom, Edward Grefenstette,
Karl Moritz Hermann, Tomas Kocisky, Fumin
Wang, and Andrew Senior. 2016. Latent predictor
networks for code generation. In Proceedings of
ACL.

Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attentionbased neural machine translation. In Proceedings
of EMNLP.

Graham Neubig. 2015. lamtram: A toolkit for language and translation modeling using neural networks. http://www.github.com/neubig/lamtram.

Yusuke Oda, Hiroyuki Fudaba, Graham Neubig,
Hideaki Hata, Sakriani Sakti, Tomoki Toda, and
Satoshi Nakamura. 2015. Learning to generate
pseudo-code from source code using statistical machine translation (T). In Proceedings of ASE.

Chris Quirk, Raymond J. Mooney, and Michel Galley.
2015. Language to code: Learning semantic parsers
for if-this-then-that recipes. In Proceedings of ACL.

Maxim Rabinovich, Mitchell Stern, and Dan Klein.
2017. Abstract syntax networks for code generation
and semantic parsing. In Proceedings of ACL.

Yibo Sun, Duyu Tang, Nan Duan, Jianshu Ji, Guihong Cao, Xiaocheng Feng, Bing Qin, Ting Liu,
and Ming Zhou. 2018. Semantic parsing with
syntax- and table-aware SQL generation. CoRR,
abs/1804.08338.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. In Proceedings of NIPS.

Adrienne Wang, Tom Kwiatkowski, and Luke Zettlemoyer. 2014. Morpho-syntactic lexical generalization for CCG semantic parsing. In Proceedings of
EMNLP.

Chenglong Wang, Marc Brockschmidt, and Rishabh
Singh. 2017. Pointing out SQL queries from text.
Technical report.

Daniel C. Wang, Andrew W. Appel, Jeffrey L. Korn,
and Christopher S. Serra. 1997. The Zephyr abstract syntax description language. In Proceedings
of DSL.

Chunyang Xiao, Marc Dymetman, and Claire Gardent.
2016. Sequence-based structured prediction for semantic parsing. In Proceedings of ACL.

Xiaojun Xu, Chang Liu, and Dawn Song. 2017. SQLNet: Generating structured queries from natural
language without reinforcement learning. arXiv
preprint arXiv: 1711.04436.

Pengcheng Yin and Graham Neubig. 2017. A syntactic
neural model for general-purpose code generation.
In Proceedings of ACL.

Pengcheng Yin, Chunting Zhou, Junxian He, and Graham Neubig. 2018. StructVAE: Tree-structured latent variable models for semi-supervised semantic
parsing. In Proceedings of ACL.

Tao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and
Dragomir R. Radev. 2018. TypeSQL: Knowledgebased type-aware neural text-to-sql generation.

Luke Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form structured classification with probabilistic categorial grammars. In
Proceedings of UAI.

Luke S. Zetthemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of EMNLP-CoNLL.

Kai Zhao and Liang Huang. 2015. Type-driven incremental semantic parsing with polymorphism. In
Proceedings of NAACL-HLT.

Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2SQL: Generating structured queries
from natural language using reinforcement learning.
arXiv preprint arXiv: 1709.00103.
