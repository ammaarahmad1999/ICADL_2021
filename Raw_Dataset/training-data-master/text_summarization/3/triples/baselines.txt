(Contribution||has||Baselines)
(Baselines||has||lvt5 k - lsent)
(lvt5 k - lsent||uses||temporal attention)
(temporal attention||to keep track of||past attentive weights)
(past attentive weights||of||decoder)
(temporal attention||restrains||repetition)
(repetition||in||later sequences)
(Baselines||has||CGU)
(CGU||for||global encoding)
(global encoding||sets||convolutional gated unit)
(global encoding||sets||self - attention)
(Baselines||has||Seq2seq + att)
(Seq2seq + att||is||two - layer BiLSTM encoder)
(Seq2seq + att||is||one - layer LSTM decoder)
(one - layer LSTM decoder||equipped with||attention)
(Baselines||has||Pointer - generator)
(Pointer - generator||is||integrated pointer network)
(Pointer - generator||is||seq2seq model)
(Baselines||has||RAS - Elman)
(RAS - Elman||is a||convolution encoder)
(RAS - Elman||is a||Elman RNN decoder)
(Elman RNN decoder||with||attention)
(Baselines||has||ABS +)
(ABS +||is a||tuned ABS model)
(Baselines||has||SEASS)
(SEASS||includes||additional selective gate)
(additional selective gate||to control||information flow)
(information flow||from||encoder)
(encoder||to||decoder)
