Cutting-off Redundant Repeating Generations
for Neural Abstractive Summarization

Jun Suzuki and Masaaki Nagata
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{suzuki.jun, nagata.masaakis@lab.ntt.co.jp

Abstract

This paper tackles the reduction of redundant repeating generation that is often
observed in RNN-based encoder-decoder
models. Our basic idea is to jointly estimate the upper-bound frequency of each
target vocabulary in the encoder and control the output words based on the estimation in the decoder. Our method shows
significant improvement over a strong
RNN-based encoder-decoder baseline and
achieved its best results on an abstractive
summarization benchmark.

1 Introduction

The RNN-based encoder-decoder (EncDec) approach has recently been providing  significant progress in various natural language generation (NLG) tasks, ie., machine translation
(MT) (Sutskever et al., 2014; Cho et al., 2014)
and abstractive summarization (ABS) (Rush et al.,
2015). Since a scheme in this approach can be
interpreted as a conditional language model, it is
suitable for NLG tasks. However, one potential
weakness is that it sometimes repeatedly generates
the same phrase (or word).

This issue has been discussed in the neural MT
(NMT) literature as a part of a coverage problem (Tu et al., 2016; Mi et al., 2016). Such repeating generation behavior can become more severe in some NLG tasks than in MT. The very
short ABS task in DUC-2003 and 2004 (Over et
al., 2007) is a typical example because it requires
the generation of a summary in a pre-defined limited output space, such as ten words or 75 bytes.
Thus, the repeated output consumes precious limited output space. Unfortunately, the coverage approach cannot be directly applied to ABS tasks
since they require us to optimally find salient ideas

291

from the input in a lossy compression manner, and
thus the summary (output) length hardly depends
on the input length; an MT task is mainly Joss-less
generation and nearly one-to-one correspondence
between input and output (Nallapati et al., 2016a).
From this background, this paper tackles this issue and proposes a method to overcome it in ABS
tasks. The basic idea of our method is to jointly
estimate the upper-bound frequency of each target vocabulary that can occur in a summary during
the encoding process and exploit the estimation to
control the output words in each decoding step.
We refer to our additional component as a wordfrequency estimation (WFE) sub-model. The
WFE sub-model explicitly manages how many
times each word has been generated so far and
might be generated in the future during the decoding process. Thus, we expect to decisively prohibit
excessive generation. Finally, we evaluate the effectiveness of our method on well-studied ABS
benchmark data provided by Rush et al. (2015),
and evaluated in (Chopra et al., 2016; Nallapati
et al., 2016b; Kikuchi et al., 2016; Takase et al.,
2016; Ayana et al., 2016; Gulcehre et al., 2016).

2 Baseline RNN-based EncDec Model

The baseline of our proposal is an RNN-based
EncDec model with an attention mechanism (Luong et al., 2015). In fact, this model has already been used as a strong baseline for ABS
tasks (Chopra et al., 2016; Kikuchi et al., 2016) as
well as in the NMT literature. More specifically,
as a case study we employ a 2-layer bidirectional
LSTM encoder and a 2-layer LSTM decoder with a
global attention (Bahdanau et al., 2014). We omit
a detailed review of the descriptions due to space
limitations. The following are the necessary parts
for explaining our proposed method.

Let X = (a;)/_, and Y = (yj) 74 be input
and output sequences, respectively, where x; and

Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 291-297,

Valencia, Spain, April 3-7, 2017. ©2017 Association for Computational Linguistics
Input: H Bt es (h5)5_4 > list of hidden states generated by encoder
Initialize: s — 0
Y — ‘BOS’
Ht - H®
lhe (s, Y, A *) > triplet of (minimal) info for decoding process
2: O, — push(Q,, h)

> s: cumulative log-likelihood
> Y: list of generated words
[> Hr: hidden states to process decoder

> set initial triplet h to priority queue O,

3: O, — {} > prepare queue to store complete sentences
4: Repeat
5: O <— () > prepare empty list
6: Repeat
7: he pop( O.,) > pop a candidate history
8: Oo calcLL(h) > see Eq. 2
9: O — append(O, O) > append likelihood vector
10: Until O, = 0 > repeat until O,, is empty
ll: {(th, k)z}25° < findkBest(O)
12: {h,}55° — makeTriplet({ (rm, k)-}55°)
13: OQ’ — selectTopK(Q., fhz}85°)

14: (Qy, Q.) = SepComp(Q")
15: Until O, = @
Output: OQ.

> separate QO’ into Q, or QO,
> finish if O, is empty

Figure 1: Algorithm for a -best beam search decoding typically used in EncDec approach.

y; are one-hot vectors, which correspond to the
a-th word in the input and the j-th word in the output. Let V* denote the vocabulary (set of words)
of output. For simplification, this paper uses the
following four notation rules:

(1) (a;)j_, is a short notation for representing
a list of (column) vectors, i.e, (@1,...,a@7) =
(wij.

(2) v(a, D) represents a D-dimensional (column)
vector whose elements are all a, ie, v(1,3) =
(1,1,1)'.

(3) a[i] represents the i-th element of x, i.e., 2 =
(0.1, 0.2, 0.3)", then «[2] = 0.2.

(4) M =|V"| and, m always denotes the index of
output vocabulary, namely, m € {1,..., WW}, and
o|m| represents the score of the m-th word in V*,
where o € R™.

Encoder: Let (2°(-) denote the overall process
of our 2-layer bidirectional LSTM encoder. The
encoder receives input X and returns a list of final
hidden states H® = (h’)/_,:

H® = 08(X). (1)

Decoder: We employ a K -best beam-search decoder to find the (approximated) best output Y
given input X. Figure | shows a typical Kbest beam search algorithm used in the decoder
of EncDec approach. We define the (minimal) required information h shown in Figure 1 for the jth decoding process is the following triplet, h =
(sj-1, ¥j-1, H'_,), where s;—1 is the cumula
tive log-likelihood from step 0 to 7 — 1, Yj-1

292

is a (candidate of) output word sequence generated so far from step 0 to 7 — 1, that is, Yj-1 =
(yo,---,Yj-1) and H;_, is the all the hidden
states for calculating the j-th decoding process.
Then, the function calcLL in Line 8 can be written as follows:

6; = v(s;-1,M) + log (Softmax(o;))

0; = 0 (H*, Hy_,9j-1). (2)
where Softmax(-) is the softmax function for a
given vector and ()*(-) represents the overall process of a single decoding step.

Moreover, O in Line 11 is a (M x (K —C))matrix, where C’ is the number of complete sentences in Q,. The (m,k)-element of O represents a likelihood of the m-th word, namely 0;|mJ,
that is calculated using the k-th candidate in Q,,
at the (7 — 1)-th step. In Line 12, the function
makeTriplet constructs a set of triplets based
on the information of index (7, &). Then, in Line
13, the function select TopkK selects the top-k
candidates from union of a set of generated triplets
at current step {h,}%7Cand a set of triplets of
complete sentences in Q,. Finally, the function
sepComp in Line 13 divides a set of triplets Q’ in
two distinct sets whether they are complete sentences, Q., or not, O,,. If the elements in O’
are all complete sentences, namely, Q, = Q’ and
O,, = ), then the algorithm stops according to the
evaluation of Line 15.

3 Word Frequency Estimation

This section describes our proposed method,
which roughly consists of two parts: (1) a submodel that estimates the upper-bound frequencies
of the target vocabulary words in the output, and
(2) architecture for controlling the output words in
the decoder using estimations.

3.1 Definition

Let a denote a vector representation of the frequency estimation. © denotes element-wise product. @ is calculated by:

g=Sigmoid(g), (3)

where Sigmoid(-) and ReLu(-) represent the
element-wise sigmoid and ReLU (Glorot et al.,

2011), respectively. Thus, * € [0,+00]”, g €
(0, 1], and @€ [0, +00].
We incorporate two separated components, fF
and g, to improve the frequency fitting. The purpose of g is to distinguish whether the target words
occur or not, regardless of their frequency. Thus,
g can be interpreted as a gate function that resembles estimating the fertility in the coverage (Tu
et al., 2016) and a switch probability in the copy
mechanism (Gulcehre et al., 2016). These ideas
originated from such gated recurrent networks as
LSTM (Hochreiter and Schmidhuber, 1997) and
GRU (Chung et al., 2014). Then, 7 can much focus on to model frequency equal to or larger than
1. This separation can be expected since r|m|] has
no influence if g/m] =0.

3.2 Effective usage

The technical challenge of our method is effectively leveraging WFE a. Among several possible
choices, we selected to integrate it as prior knowIedge in the decoder. To do so, we re-define 0; in
Eq. 2 as:

6; = v(s;-1, M) + log (Softmax(o,;)) + a).

The difference is the additional term of a;, which
is an adjusted likelihood for the j-th step originally calculated from @. We define a; as:

@; = log(ClipReLU,(rj)Og). 4)

ClipReLU,(-) is a function that receives a vector and performs an element-wise calculation:
x'|m] = max (0, min(1, #[m])) for all m if it receives x. We define the relation between 7; in
Eq. 4 and F in Eq. 3 as follows:

ri = ” A

Tj-1 — Yj-1
Eq. 5 is updated from r;_1 to 7; with the estimated
output of previous step y;_1. Since y; € {0, yi
for all 7, all of the elements in 7; are monotonically non-increasing. If r;[m] < 0 at 7’, then
0;'[m] = —on regardless of o|m]. This means that
the m-th word will never be selected any more at
step j’ < j for all 7. Thus, the interpretation of
r; 1s that it directly manages the upper-bound frequency of each target word that can occur in the
current and future decoding time steps. As a result,
decoding with our method never generates words
that exceed the estimation 7, and thus we expect

to reduce the redundant repeating generation.
Note here that our method never requires
r;|m] <0 (or F;|m] =0) for all m at the last decoding time step 7, as 1s generally required in the

ifj=1
otherwise —

(5)

293

Input: H 2 (hi )j—1 > list of hidden states generated by encoder
Parameters: W/,W2¢ RY’*", WZ ce RY*", Wee

 

 

 

 

 

 

 

RMx2H
1: H 1 <— Wr H* > linear transformation for frequency model
2: hi — Hyv(1, M) DAT CR!, HT cR?*!
3. re WwW; hi > frequency estimation
4: H - <— Ww? H* > linear transformation for occurrence model
5: h3* — RowMax(H?) pngt eR” and HY € RY*!
6: h3- — RowMin(H7) ph” €R”,andH? cR®*!

 

7: g — WS (concat(h§$*, hZ”))
Output: (g, 7)

> occurrence estimation

Figure 2: Procedure for calculating the components of our WFE sub-model.

coverage (Tu et al., 2016; Mi et al., 2016; Wu et
al., 2016). This is why we say upper-bound frequency estimation, not just (exact) frequency.

3.3. Calculation

Figure 2 shows the detailed procedure for calculating g and r in Eq. 3. For r, we sum up all of the
features of the input given by the encoder (Line 2)
and estimate the frequency. In contrast, for g, we
expect Lines 5 and 6 to work as a kind of voting
for both positive and negative directions since g
needs just occurrence information, not frequency.
For example, g may take large positive or negative values if a certain input word (feature) has
a strong influence for occurring or not occurring
specific target word(s) in the output. This idea is
borrowed from the Max-pooling layer (Goodfellow et al., 2013).

3.4 Parameter estimation (Training)

Given the training data, let a* € P™ bea
vector representation of the true frequency of
the target words given the input, where P =
{0,1,...,+oo}. Clearly a* can be obtained by
counting the words in the corresponding output.
We define loss function UY for estimating our
WEE sub-model as follows:

ww’ (X,a*,W) =d-v(1,M) (6)
d = c; max (v(0, M),a@ — a* — v(e,M))”
+ cgmax (v(0,M),a" — @— v(¢,M))’,

where YV represents the overall parameters. The
form of Y(-) is closely related to that used
in support vector regression (SVR) (Smola and
Schélkopf, 2004). We allow estimation a@|m] for
all m to take a value in the range of [a*|m] —
€,a*|m] + €] with no penalty (the loss is zero). In
our case, we select « = 0.25 since all the elements
Source vocabulary + 119,507

Target vocabulary | 68,887
Dim. of embedding D 200
Dim. of hidden state 1 400

 

Encoder RNN unit 2-layer bi-LSTM
Decoder RNN unit 2-layer LSTM with attention
Optimizer Adam (first 5 epoch)

+ SGD (remaining epoch) x
0.001 (Adam) / 0.01 (SGD)
256 (shuffled at each epoch)

10 (Adam) / 5 (SGD)

max 15 epoch w/ early stopping
based on the val. set

Dropout = 0.3

Initial learning rate
Mini batch size
Gradient clipping
Stopping criterion

 

Other opt. options

Table 1: Model and optimization configurations in
our experiments. +: including special BOS, EOS,
and UNK symbols. «: as suggested in (Wu et al.,
2016)

of a* are an integer. The remaining 0.25 for both
the positive and negative sides denotes the margin
between every integer. We select 6 = 2 to penalize larger for more distant error, and c, < ¢,
1.é., Cy = 0.2,co = 1, since we aim to obtain
upper-bound estimation and to penalize the underestimation below the true frequency a”.

Finally, we minimize Eq. 6 with a standard negative log-likelihood objective function to estimate
the baseline EncDec model.

4 Experiments

We investigated the effectiveness of our method
on ABS experiments, which were first performed
by Rush et al., (2015). The data consist of approximately 3.8 million training, 400,000 validation and 400,000 test data, respectively”. Generally, 1951 test data, randomly extracted from the
test data section, are used for evaluation*. Additionally, DUC-2004 evaluation data (Over et al.,
2007)* were also evaluated by the identical models
trained on the above Gigaword data. We strictly
followed the instructions of the evaluation setting
used in previous studies for a fair comparison. Table 1 summarizes the model configuration and the
parameter estimation setting in our experiments.

4.1 Main results: comparison with baseline

Table 2 shows the results of the baseline EncDec
and our proposed EncDec+WFE. Note that the

°*The data can be created by the data construction scripts
in the author’s code: https://github.com/facebook/NAMAS.

As previously described (Chopra et al., 2016) we removed the ill-formed (empty) data for Gigaword.

“http://duc.nist.gov/duc2004/tasks.html

294

china success at youth world
championship shows preparation for
#H#Ht# Olympics
china germany germany germany
germany and germany at world youth
championship
:china faces germany at world youth
championship

>:British and Spanish governments leave
extradition of Pinochet to courts
Spain britain seek shelter from
Pinochet ’s pinochet case over
pinochet ’s
:spain britain seek shelter over
pinochet ’s possible extradition from
spain
torn UNK : plum island juniper duo
now just a lone tree
black women black women black in
black code
B:in plum island of the ancient

A:

 

 

 

 

G:

A:

 

Figure 3: Examples of generated summary. G:
reference summary, A: baseline EncDec, and
B: EncDec+WFE. (underlines indicate repeating
phrases and words)

DUC-2004 data was evaluated by recall-based
ROUGE scores, while the Gigaword data was
evaluated by F-score-based ROUGE, respectively. For a validity confirmation of our EncDec
baseline, we also performed OpenNMT tool.
The results on Gigaword data with B 5D
were, 33.65, 16.12, and 31.37 for ROUGE1(F), ROUGE-2(F) and ROUGE-L(P), respectively, which were almost similar results (but
slightly lower) with our implementation. This supports that our baseline worked well as a strong
baseline. Clearly, EncDec+WFE significantly outperformed the strong EncDec baseline by a wide
margin on the ROUGE scores. Thus, we conclude
that the WFE sub-model has a positive impact
to gain the ABS performance since performance
gains were derived only by the effect of incorporating our WFE sub-model.

4.2 Comparison to current top systems

Table 3 lists the current top system results. Our
method EncDec+WFE successfully achieved the
current best scores on most evaluations. This result also supports the effectiveness of incorporating our WFE sub-model.

MRT (Ayana et al., 2016) previously provided
the best results. Note that its model structure is
nearly identical to our baseline. On the contrary,
MRT trained a model with a sequence-wise min
http://opennmt.net
 
 

 

 

DUC-2004 (w/ 75-byte limit) Gigaword (w/o length limit)

Method Beam || ROUGE-1(R)|ROUGE-2(R)|ROUGE-L(R) | ROUGE-1(F)|ROUGE-2(F)|ROUGE-L(F)
EncDec B=1 29.23 25.27 . 31.63
(baseline) B=5 29.52 732.14
our impl.) B=10 + 29.60 31.97

EncDec+WFE B=1 31.92 33.55
(proposed) B=5 «32.28 «33.88
B=10 31.70 33.73

(perf. gainfromjtox)|  ——s-+2.68] 40.92) s+. 83 || +2.03 +0.63 +1.78

 

Table 2: Results on DUC-2004 and Gigaword data: ROUGE-2x(R): recall-based ROUGE-x, ROUGEx(F): Fl-based ROUGE-z, where x € {1, 2, L}, respectively.

 

 

 

Gigaword (w/o Tength Timi)
Meihod ROUGE-2F) | ROUGE-L)
ABS (Rush et al., 2015) 26.55 7.06 22.05 30.88 12.22 27.77
RAS (Chopra et al., 2016) 15.97 31.15
BWL (Nallapati et al., 2016a) 28.35 9.46 24.59 32.67 15.59 30.64
(words-lvt5k-1sentT) 28.61 9.42 25.24 35.30 116.64 32.62
MRT (Ayana et al., 2016) 730.41 110.87 {26.79 {36.54 16.59 133.44
EncDec+WFE [This Paper] 32.28 10.54 27.80 36.30 17.31 33.88
(perf. gain from 7) +1.87 -0.33 +0.72 +0.44

 

6

Table 3: Results of current top systems:

 

 

*°: previous best score for each evaluation. 7: using a larger

vocab for both encoder and decoder, not strictly fair configuration with other results.

 

 

 

 

 

 

True a* \ Estimation @ 0 1 2| 3|4>
1 7,014|7,064)1,784)16| 4
2 51 95} 60} O| O
32> 2 4 1; O} O

 

 

Table 4: Confusion matrix of WFE on Gigaword
data: only evaluated true frequency > 1.

imum risk estimation, while we trained all the
models in our experiments with standard (pointwise) log-likelihood maximization. MRT essentially complements our method. We expect to further improve its performance by applying MRT
for its training since recent progress of NMT has
suggested leveraging a sequence-wise optimization technique for improving performance (Wiseman and Rush, 2016; Shen et al., 2016). We leave
this as our future work.

4.3 Generation examples

Figure 3 shows actual generation examples. Based
on our motivation, we specifically selected the redundant repeating output that occurred in the baseline EncDec. It is clear that EncDec+WFE successfully reduced them. This observation offers
further evidence of the effectiveness of our method
in quality.

4.4 Performance of the WFE sub-model

To evaluate the WFE sub-model alone, Table 4
shows the confusion matrix of the frequency esti
295

mation. We quantized @ by | @/m]+0.5] for all m,
where 0.5 was derived from the margin in VU",
Unfortunately, the result looks not so well. There
seems to exist an enough room to improve the estimation. However, we emphasize that it already has
an enough power to improve the overall quality as
shown in Table 2 and Figure 3. We can expect to
further gain the overall performance by improving
the performance of the WFE sub-model.

5 Conclusion

This paper discussed the behavior of redundant
repeating generation often observed in neural
EncDec approaches. We proposed a method for
reducing such redundancy by incorporating a submodel that directly estimates and manages the frequency of each target vocabulary in the output.
Experiments on ABS benchmark data showed the
effectiveness of our method, EncDec+WFE, for
both improving automatic evaluation performance
and reducing the actual redundancy. Our method
is suitable for lossy compression tasks such as image caption generation tasks.

Acknowledgement

We thank three anonymous reviewers for their
helpful comments.
References

Ayana, Shiqi Shen, Zhiyuan Liu, and Maosong Sun.
2016. Neural headline generation with minimum
risk training. CoRR, abs/1604.01904.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of
the 3rd International Conference on Learning Representations (ICLR 2015).

Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder-—
Decoder for Statistical Machine Translation. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2014), pages 1724-1734.

Sumit Chopra, Michael Auli, and Alexander M. Rush.
2016. Abstractive Sentence Summarization with Attentive Recurrent Neural Networks. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
93-98, San Diego, California, June. Association for
Computational Linguistics.

Junyoung Chung, Caglar Giilcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical Evaluation
of Gated Recurrent Neural Networks on Sequence
Modeling. CoRR, abs/1412.3555.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep Sparse Rectifier Neural Networks.
In Geoffrey J. Gordon and David B. Dunson, editors, Proceedings of the Fourteenth International
Conference on Artificial Intelligence and Statistics
(AISTATS-11), volume 15, pages 315-323. Journal
of Machine Learning Research - Workshop and Conference Proceedings.

Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza,
Aaron C. Courville, and Yoshua Bengio. 2013.
Maxout Networks. In Proceedings of the 30th International Conference on Machine Learning, ICML
2013, Atlanta, GA, USA, 16-21 June 2013, pages
1319-1327.

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati,
Bowen Zhou, and Yoshua Bengio. 2016. Pointing the Unknown Words. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
140-149, Berlin, Germany, August. Association for
Computational Linguistics.

Sepp Hochreiter and Jiirgen Schmidhuber. 1997. Long
Short-Term Memory. Neural Comput., 9(8):1735-—
1780, November.

Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, and Manabu Okumura. 2016. Controlling output length in neural encoder-decoders.

296

In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages
1328-1338, Austin, Texas, November. Association
for Computational Linguistics.

Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attentionbased Neural Machine Translation. In Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pages 1412-1421,
Lisbon, Portugal, September. Association for Computational Linguistics.

Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe
Ittycheriah. 2016. Coverage embedding models
for neural machine translation. In Proceedings of
the 2016 Conference on Empirical Methods in Natural Language Processing, pages 955—960, Austin,
Texas, November. Association for Computational
Linguistics.

Ramesh Nallapati, Bing Xiang, and Bowen Zhou.
2016a. Sequence-to-sequence rnns for text summarization. CoRR, abs/1602.06023.

Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Caglar Gulcehre, and Bing Xiang. 2016b. Abstractive Text Summarization using Sequence-tosequence RNNs and Beyond. In Proceedings of The
20th SIGNLL Conference on Computational Natural
Language Learning, pages 280-290, Berlin, Germany, August. Association for Computational Linguistics.

Paul Over, Hoa Dang, and Donna Harman. 2007. DUC
in context. Information Processing and Management, 43(6):1506—1520.

Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A Neural Attention Model for Abstractive Sentence Summarization. In Proceedings of the
2015 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2015), pages 379389.

Shigi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum risk training for neural machine translation. In
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pages 1683-1692, Berlin, Germany,
August. Association for Computational Linguistics.

Alex J Smola and Bernhard Scholkopf. 2004. A tutorial on support vector regression. Statistics and
computing, 14(3):199-222.

Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to Sequence Learning with Neural Networks. In Advances in Neural Information Processing Systems 27 (NIPS 2014), pages 3104-3112.

Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu
Hirao, and Masaaki Nagata. 2016. Neural headline generation on abstract meaning representation.
In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages
1054-1059, Austin, Texas, November. Association
for Computational Linguistics.

Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,
and Hang Li. 2016. Modeling Coverage for Neural
Machine Translation. In Proceedings of the 54th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 76-85,
Berlin, Germany, August. Association for Computational Linguistics.

Sam Wiseman and Alexander M. Rush. 2016.
Sequence-to-sequence learning as beam-search optimization. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing, pages 1296-1306, Austin, Texas, November.
Association for Computational Linguistics.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin
Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan
Gouws, Yoshikiyo Kato, Taku Kudo, Hideto
Kazawa, Keith Stevens, George Kurian, Nishant
Patil, Wei Wang, Cliff Young, Jason Smith, Jason
Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2016. Google’s
neural machine translation system: Bridging the gap
between human and machine translation. CoRR,
abs/1609.08 144.

297
