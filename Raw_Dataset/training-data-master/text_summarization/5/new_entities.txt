30	93	100	p	combine
30	105	156	n	seq2seq and template based summarization approaches
31	3	7	p	call
31	12	32	n	summarization system
31	33	41	n	Re 3 Sum
31	50	61	p	consists of
31	62	75	n	three modules
31	78	86	n	Retrieve
31	89	95	n	Rerank
31	100	107	n	Rewrite
32	3	10	p	utilize
32	13	64	n	widely - used Information Retrieval ( IR ) platform
32	65	76	p	to find out
32	77	101	n	candidate soft templates
32	102	106	p	from
32	111	126	n	training corpus
33	10	16	p	extend
33	21	34	n	seq2seq model
33	35	51	p	to jointly learn
33	52	92	n	template saliency measurement ( Rerank )
33	97	133	n	final summary generation ( Rewrite )
34	17	57	n	Recurrent Neural Network ( RNN ) encoder
34	61	71	p	applied to
34	72	79	n	convert
34	127	131	p	into
34	132	145	n	hidden states
34	84	98	n	input sentence
34	103	126	n	each candidate template
35	0	2	p	In
35	3	9	n	Rerank
35	15	22	p	measure
35	27	42	n	informativeness
35	43	45	p	of
35	48	66	n	candidate template
35	67	79	p	according to
35	84	106	n	hidden state relevance
35	107	109	p	to
35	114	128	n	input sentence
36	23	27	p	with
36	32	65	n	highest predicted informativeness
36	66	80	p	is regarded as
36	85	105	n	actual soft template
37	3	10	n	Rewrite
37	28	37	p	generated
37	17	24	n	summary
37	38	50	p	according to
37	55	68	n	hidden states
37	69	76	p	of both
37	81	102	n	sentence and template
151	0	7	n	OpenNMT
152	8	17	p	implement
152	22	56	n	standard attentional seq2seq model
156	0	5	n	FTSum
156	6	13	p	encoded
156	18	23	n	facts
156	24	38	p	extracted from
156	43	58	n	source sentence
156	59	69	p	to improve
156	115	134	n	generated summaries
156	70	74	p	both
156	79	91	n	faithfulness
156	96	111	n	informativeness
157	108	116	n	PIPELINE
159	13	19	p	trains
159	24	37	n	Rerank module
159	42	56	n	Rewrite module
43	33	71	n	http://www4.comp.polyu.edu.hk/cszqcao/
139	3	6	p	use
139	11	36	n	popular seq2seq framework
139	37	47	n	Open - NMT
140	36	42	p	retain
140	47	63	n	default settings
140	64	66	p	of
140	67	75	n	Open NMT
140	76	84	p	to build
140	89	109	n	network architecture
141	19	29	p	dimensions
141	33	56	n	word embeddings and RNN
141	57	65	p	are both
141	66	69	n	500
141	80	110	n	encoder and decoder structures
141	111	114	p	are
141	115	182	n	two - layer bidirectional Long Short Term Memory Networks ( LSTMs )
144	0	2	p	On
144	3	15	n	our computer
144	18	21	p	GPU
144	24	32	n	GTX 1080
144	35	41	p	Memory
144	44	47	n	16G
144	50	53	p	CPU
144	56	65	n	i7-7700 K
144	74	89	p	training spends
144	90	102	n	about 2 days
145	0	11	p	During test
145	21	32	n	beam search
145	33	40	p	of size
145	41	42	n	5
145	43	54	p	to generate
145	55	64	n	summaries
146	3	6	p	add
146	11	19	n	argument
146	22	33	n	replace unk
146	36	46	p	to replace
146	51	74	n	generated unknown words
146	75	79	p	with
146	84	95	n	source word
146	96	106	p	that holds
146	111	135	n	highest attention weight
147	74	83	p	introduce
147	87	121	n	additional length penalty argument
147	124	131	n	alpha 1
147	134	146	p	to encourage
147	147	164	n	longer generation
2	52	72	n	Neural Summarization
12	69	103	n	abstractive sentence summarization
163	8	15	p	examine
163	20	31	n	performance
163	32	53	p	of directly regarding
163	54	68	n	soft templates
163	69	71	p	as
163	72	88	n	output summaries
164	3	12	p	introduce
164	13	51	n	five types of different soft templates
176	18	32	p	performance of
176	33	39	n	Random
176	40	42	p	is
176	43	51	n	terrible
177	0	6	n	Rerank
177	7	26	p	largely outperforms
177	27	32	n	First
180	12	19	n	Optimal
180	20	35	p	greatly exceeds
180	36	77	n	all the state - of - the - art approaches
179	11	20	p	comparing
179	21	34	n	Max and First
179	40	52	p	observe that
179	57	75	n	improving capacity
179	76	78	p	of
179	83	98	n	Retrieve module
179	99	101	p	is
179	102	106	n	high
183	8	15	p	measure
183	20	38	n	linguistic quality
183	39	41	p	of
183	42	61	n	generated summaries
184	62	76	p	performance of
184	77	85	n	Re 3 Sum
184	89	107	p	almost the same as
184	116	130	n	soft templates
204	21	32	p	investigate
204	37	51	n	soft templates
204	52	58	p	affect
204	59	68	n	our model
206	58	66	p	provided
206	24	53	n	more high - quality templates
206	97	105	p	achieved
206	73	92	n	higher ROUGE scores
210	10	26	p	manually inspect
210	31	40	n	summaries
210	41	53	p	generated by
210	54	71	n	different methods
211	3	7	p	find
211	12	31	n	outputs of Re 3 Sum
211	44	74	p	longer and more flu - ent than
211	79	97	n	outputs of OpenNMT
222	17	21	p	with
222	22	47	n	different templates given
222	50	59	n	our model
222	63	81	p	likely to generate
222	82	102	n	dissimilar summaries
