100	10	13	p	for
100	14	19	n	LCSTS
100	20	23	p	are
101	0	21	n	RNN and RNN - context
101	30	53	n	RNNbased seq2seq models
101	56	72	p	without and with
101	73	92	n	attention mechanism
102	0	10	n	Copy - Net
102	11	13	p	is
102	18	49	n	attention - based seq2seq model
102	50	54	p	with
102	59	73	n	copy mechanism
103	0	3	n	SRB
103	20	28	p	improves
103	29	47	n	semantic relevance
103	48	55	p	between
103	56	79	n	source text and summary
104	0	4	n	DRGD
104	5	7	p	is
104	12	32	n	conventional seq2seq
104	33	37	p	with
104	40	73	n	deep recurrent generative decoder
105	24	32	n	Gigaword
105	35	48	n	ABS and ABS +
105	49	52	p	are
105	57	63	n	models
105	64	68	p	with
105	69	109	n	local attention and handcrafted features
106	0	5	n	Feats
106	6	8	p	is
106	11	34	n	fully RNN seq2seq model
106	35	39	p	with
106	40	61	n	some specific methods
106	62	72	p	to control
106	77	92	n	vocabulary size
107	0	26	n	RAS - LSTM and RAS - Elman
107	27	30	p	are
107	31	45	n	seq2seq models
107	46	50	p	with
107	53	74	n	convolutional encoder
107	82	94	n	LSTM decoder
107	102	119	n	Elman RNN decoder
108	0	5	n	SEASS
108	6	10	p	is a
108	11	24	n	seq2seq model
108	25	29	p	with
108	32	56	n	selective gate mechanism
109	0	4	n	DRGD
109	15	27	p	baseline for
109	28	36	n	Gigaword
88	3	12	p	implement
88	13	28	n	our experiments
88	29	31	p	in
88	32	64	n	PyTorch on an NVIDIA 1080 Ti GPU
89	4	59	p	word embedding dimension and the number of hidden units
89	69	72	n	512
90	26	36	n	batch size
90	40	46	p	set to
90	47	49	n	64
92	4	17	n	learning rate
92	21	27	p	halved
92	28	39	n	every epoch
93	0	17	n	Gradient clipping
93	21	33	p	applied with
93	34	53	n	range [ - 10 , 10 ]
91	3	6	p	use
91	7	46	n	Adam optimizer ( Kingma and Ba , 2014 )
91	47	51	p	with
91	56	121	n	default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8
23	44	46	p	of
23	47	62	n	global encoding
23	63	66	p	for
23	67	92	n	abstractive summarization
24	3	6	p	set
24	9	33	n	convolutional gated unit
24	34	44	p	to perform
24	45	60	n	global encoding
24	61	63	p	on
24	68	82	n	source context
25	4	8	n	gate
25	9	17	p	based on
25	18	54	n	convolutional neural network ( CNN )
25	55	62	p	filters
25	63	82	n	each encoder output
25	83	91	p	based on
25	96	110	n	global context
25	111	117	p	due to
25	122	139	n	parameter sharing
2	20	45	n	Abstractive Summarization
10	64	96	n	neural abstractive summarization
115	19	21	p	on
115	26	38	n	two datasets
115	41	50	n	our model
115	51	59	p	achieves
115	60	70	n	advantages
115	71	73	p	of
115	74	85	n	ROUGE score
115	86	90	p	over
115	95	104	n	baselines
115	141	143	p	on
115	148	153	n	LCSTS
115	154	157	p	are
115	158	169	n	significant
118	0	13	p	Compared with
118	18	44	n	conventional seq2seq model
118	57	74	p	owns an advantage
118	47	56	n	our model
118	75	77	p	of
118	78	105	n	ROUGE - 2 score 3.7 and 1.5
118	106	108	p	on
118	113	131	n	LCSTS and Gigaword
