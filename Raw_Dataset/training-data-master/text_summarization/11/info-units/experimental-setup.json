{
  "has" : {
    "Experimental setup" : {
      "implement" : {
        "our experiments" : {
          "in" : "PyTorch on an NVIDIA 1080 Ti GPU"
        },
        "from sentence" : "We implement our experiments in PyTorch on an NVIDIA 1080 Ti GPU ."
      },
      "word embedding dimension and the number of hidden units" : ["512", {"from sentence" : "The word embedding dimension and the number of hidden units are both 512 ."}],
      "has" : {
        "batch size" : {
          "set to" : "64",
          "from sentence" : "In both experiments , the batch size is set to 64 ."
        },
        "learning rate" : {
          "halved" : "every epoch",
          "from sentence" : "The learning rate is halved every epoch ."
        },
        "Gradient clipping" : {
          "applied with" : "range [ - 10 , 10 ]",
          "from sentence" : "Gradient clipping is applied with range [ - 10 , 10 ] ."
        }
      },
      "use" : {
        "Adam optimizer ( Kingma and Ba , 2014 )" : {
          "with" : "default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8",
          "from sentence" : "We use Adam optimizer ( Kingma and Ba , 2014 ) with the default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 10 ?8 ."
        }
      }
    }
  }
}