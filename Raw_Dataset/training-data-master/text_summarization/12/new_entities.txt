178	0	5	n	ABS +
178	6	14	p	Based on
178	15	24	n	ABS model
178	27	39	p	further with
178	40	57	n	two - layer LSTMs
178	58	61	p	for
178	66	83	n	encoder - decoder
178	84	88	p	with
178	89	105	n	500 hidden units
178	106	108	p	in
178	109	119	n	each layer
179	0	10	n	s 2 s+ att
180	8	17	p	implement
180	20	48	n	sequence - to sequence model
180	49	53	p	with
180	54	63	n	attention
164	3	13	p	initialize
164	14	39	n	model parameters randomly
164	40	45	p	using
164	48	69	n	Gaussian distribution
164	70	74	p	with
164	75	88	n	Xavier scheme
165	3	6	p	use
165	7	11	n	Adam
165	12	14	p	as
165	19	39	n	optimizing algorithm
166	55	68	p	learning rate
166	73	78	n	0.001
166	81	104	p	two momentum parameters
166	105	130	n	? 1 = 0.9 and ? 2 = 0.999
170	59	77	n	mini-batch size 64
170	78	80	p	by
170	81	92	n	grid search
170	0	15	p	To both speedup
170	20	49	n	training and converge quickly
167	0	6	p	During
167	7	15	n	training
167	21	25	p	test
167	30	64	n	model performance ( ROUGE - 2 F1 )
167	65	67	p	on
167	68	83	n	development set
167	84	87	p	for
167	88	107	n	every 2,000 batches
169	8	13	p	apply
169	14	31	n	gradient clipping
169	32	36	p	with
169	37	54	n	range [ ? 5 , 5 ]
169	55	61	p	during
169	62	70	n	training
35	17	24	p	propose
35	25	92	n	Selective Encoding for Abstractive Sentence Summarization ( SEASS )
36	3	8	p	treat
36	13	35	n	sentence summarization
36	36	40	p	as a
36	41	56	n	threephase task
36	59	67	n	encoding
36	70	79	n	selection
36	86	94	n	decoding
37	3	14	p	consists of
37	17	33	n	sentence encoder
37	38	60	n	selective gate network
37	69	84	n	summary decoder
38	0	5	p	First
38	12	28	n	sentence encoder
38	29	34	p	reads
38	39	50	n	input words
38	51	58	p	through
38	62	70	n	RNN unit
38	71	83	p	to construct
38	88	123	n	first level sentence representation
39	9	31	n	selective gate network
39	32	39	p	selects
39	44	63	n	encoded information
39	64	76	p	to construct
39	81	117	n	second level sentence representation
40	4	23	n	selective mechanism
40	24	32	p	controls
40	37	53	n	information flow
40	54	58	p	from
40	59	77	n	encoder to decoder
40	78	89	p	by applying
40	92	104	n	gate network
40	105	117	p	according to
40	122	142	n	sentence information
41	14	42	n	attention - equipped decoder
41	43	52	p	generates
41	57	64	n	summary
41	65	70	p	using
41	75	111	n	second level sentence representation
2	23	57	n	Abstractive Sentence Summarization
8	48	70	n	sentence summarization
189	4	15	n	SEASS model
189	16	20	p	with
189	21	32	n	beam search
189	33	44	p	outperforms
189	45	64	n	all baseline models
189	65	67	p	by
189	70	82	n	large margin
190	0	8	p	Even for
190	9	22	n	greedy search
190	35	61	p	still performs better than
190	62	75	n	other methods
191	0	15	p	For the popular
191	16	32	n	ROUGE - 2 metric
191	51	59	p	achieves
191	60	74	n	17.54 F1 score
191	79	99	p	performs better than
191	104	118	n	previous works
192	0	11	p	Compared to
192	16	25	n	ABS model
192	44	75	n	6.22 ROUGE - 2 F1 relative gain
193	16	39	n	highest CAs 2s baseline
193	52	60	p	achieves
193	61	90	n	1.57 ROUGE - 2 F1 improvement
193	95	101	p	passes
193	106	122	n	significant test
193	123	135	p	according to
193	140	161	n	official ROUGE script
196	0	8	n	DUC 2004
199	23	28	n	SEASS
199	29	40	p	outperforms
199	41	65	n	all the baseline methods
199	70	78	p	achieves
199	79	101	n	29.21 , 9.56 and 25.51
199	102	105	p	for
199	106	130	n	ROUGE 1 , 2 and L recall
200	161	177	n	English Gigaword
200	0	11	p	Compared to
200	16	27	n	ABS + model
200	37	48	p	tuned using
200	49	62	n	DUC 2003 data
200	65	74	n	our model
200	75	83	p	performs
200	84	104	n	significantly better
200	105	107	p	by
200	108	135	n	1.07 ROUGE - 2 recall score
