(Contribution||has||Baselines)
(Baselines||has||lvt2k - 1sent and lvt5k - 1sent)
(lvt2k - 1sent and lvt5k - 1sent||utilize||trick)
(trick||to control||vocabulary size)
(vocabulary size||to improve||training efficiency)
(Baselines||has||RNN and RNN - context)
(RNN and RNN - context||are||two seq2seq architectures)
(Baselines||has||ASC+ FSC)
(ASC+ FSC||uses||generative model)
(generative model||with||attention mechanism)
(attention mechanism||to conduct||sentence compression problem)
(Baselines||has||ABS and ABS +)
(ABS and ABS +||with||local attention modeling)
(local attention modeling||for||abstractive sentence summarization)
(Baselines||has||TOPIARY)
(TOPIARY||for||compressive text summarization)
(compressive text summarization||combines||system using linguistic based transformations)
(compressive text summarization||combines||an unsupervised topic detection algorithm)
(Baselines||has||LenEmb)
(LenEmb||control||summary length)
(summary length||by considering||length embedding vector)
(length embedding vector||as||input)
(Baselines||has||Copy Net)
(Copy Net||integrates||copying mechanism)
(copying mechanism||into||sequence - to sequence framework)
(Baselines||has||RAS - LSTM and RAS - Elman)
(RAS - LSTM and RAS - Elman||use||convolutional encoders)
(convolutional encoders||to handle||source information)
(RAS - LSTM and RAS - Elman||consider||words and word positions)
(words and word positions||as||input)
(Baselines||has||MOSES +)
(MOSES +||uses||phrasebased statistical machine translation system)
(phrasebased statistical machine translation system||trained on||Gigaword)
(Gigaword||to produce||summaries)
(Baselines||has||RNN - distract)
(RNN - distract||uses||new attention mechanism)
(new attention mechanism||by distracting||historical attention)
(historical attention||in||decoding steps)
(Baselines||has||ABS +)
(ABS +||combined with||additional log - linear extractive summarization model)
(additional log - linear extractive summarization model||with||handcrafted features)
(ABS +||trained on||Gigaword corpus)
