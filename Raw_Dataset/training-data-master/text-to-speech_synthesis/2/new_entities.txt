18	16	27	p	to decouple
18	28	44	n	speaker modeling
18	45	49	p	from
18	50	66	n	speech synthesis
18	67	92	p	by independently training
18	95	137	n	speaker - discriminative embedding network
18	143	151	p	captures
18	156	188	n	space of speaker characteristics
18	193	201	p	training
18	204	226	n	high quality TTS model
18	227	229	p	on
18	232	247	n	smaller dataset
18	248	262	p	conditioned on
18	267	310	n	representation learned by the first network
20	3	8	p	train
20	13	38	n	speaker embedding network
20	39	41	p	on
20	44	69	n	speaker verification task
20	70	82	p	to determine
20	86	142	n	two different utterances were spoken by the same speaker
21	58	68	p	trained on
21	69	89	n	untranscribed speech
21	90	100	p	containing
21	101	135	n	reverberation and background noise
21	136	140	p	from
21	143	167	n	large number of speakers
2	60	88	n	Text - To - Speech Synthesis
4	48	84	n	text - to - speech ( TTS ) synthesis
10	28	46	n	build a TTS system
115	0	18	n	Speech naturalness
123	4	18	n	proposed model
123	19	27	p	achieved
123	28	41	n	about 4.0 MOS
123	42	44	p	in
123	45	57	n	all datasets
125	23	38	n	audio generated
125	52	55	p	for
125	56	71	n	unseen speakers
125	72	110	p	is deemed to be at least as natural as
125	116	143	n	generated for seen speakers
126	19	41	n	MOS on unseen speakers
126	42	56	p	is higher than
126	65	78	n	seen speakers
126	81	94	p	by as much as
126	95	105	n	0.2 points
126	106	108	p	on
126	109	120	n	LibriSpeech
131	0	18	n	Speaker similarity
135	4	14	p	scores for
135	19	29	n	VCTK model
135	30	62	p	tend to be higher than those for
135	63	74	n	LibriSpeech
135	77	87	p	reflecting
135	92	121	n	cleaner nature of the dataset
137	0	3	p	For
137	4	25	n	seen speakers on VCTK
137	32	46	n	proposed model
137	47	72	p	performs about as well as
137	77	85	n	baseline
137	92	96	p	uses
137	100	147	n	embedding lookup table for speaker conditioning
152	0	20	n	Speaker verification
160	45	55	p	trained on
160	103	114	n	LibriSpeech
160	121	139	n	synthesized speech
160	153	168	p	most similar to
160	173	192	n	ground truth voices
164	0	2	p	On
164	8	36	n	20 voice discrimination task
164	40	46	p	obtain
164	50	63	n	EER of 2.86 %
166	0	23	n	Speaker embedding space
169	4	21	n	PCA visualization
169	31	36	p	shows
169	42	64	n	synthesized utterances
169	65	90	p	tend to lie very close to
169	91	124	n	real speech from the same speaker
169	125	127	p	in
169	132	147	n	embedding space
170	114	135	n	t - SNE visualization
170	94	106	p	demonstrated
170	10	30	n	synthetic utterances
170	41	68	p	easily distinguishable from
170	73	90	n	real human speech
170	146	151	p	where
170	152	162	n	utterances
170	163	167	p	from
170	168	190	n	each synthetic speaker
170	256	260	p	from
170	198	214	n	distinct cluster
170	215	226	p	adjacent to
170	229	255	n	cluster of real utterances
170	265	286	n	corresponding speaker
173	0	43	n	Number of speaker encoder training speakers
185	79	100	p	improve significantly
185	47	78	n	both naturalness and similarity
185	0	2	p	As
185	35	44	n	increases
185	7	34	n	number of training speakers
190	0	19	n	Fictitious speakers
191	137	141	p	from
191	0	9	p	Bypassing
191	14	37	n	speaker encoder network
191	42	54	p	conditioning
191	59	70	n	synthesizer
191	71	73	p	on
191	74	87	n	random points
191	88	90	p	in
191	95	118	n	speaker embedding space
