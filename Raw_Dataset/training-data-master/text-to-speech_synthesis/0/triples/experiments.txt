(Contribution||has||Experiments)
(Experiments||has||Experimental setup)
(Experimental setup||use||WER ( word error rate ) and PER ( phoneme error rate ))
(WER ( word error rate ) and PER ( phoneme error rate )||to measure||accuracy of G2P conversion)
(Experimental setup||use||Adam optimizer)
(Adam optimizer||for||all models)
(Adam optimizer||follow||learning rate schedule)
(Experimental setup||use||beam search)
(beam search||set||beam size to 10)
(beam search||during||inference)
(Experimental setup||has||residual dropout , attention dropout and ReLU dropout)
(residual dropout , attention dropout and ReLU dropout||for||Transformer models)
(Transformer models||is||0.2 , 0.4 , 0.4)
(Experimental setup||has||dropout)
(dropout||is||0.3)
(0.3||for||Bi - LSTM and CNN models)
(Experimental setup||implement experiments with||fairseq - py 4 library in Py-Torch)
(Experimental setup||train||each model)
(each model||on||8 NVIDIA M40 GPUs)
(8 NVIDIA M40 GPUs||contains||roughly 4000 tokens)
(roughly 4000 tokens||in||one mini-batch)
(Experiments||has||Model)
(Model||has||Ensemble Model)
(Ensemble Model||use||4 Transformer models)
(Ensemble Model||use||3 CNN models)
(Ensemble Model||use||3 Bi - LSTM models)
(Ensemble Model||has||4 Transformer models)
(4 Transformer models||vary in||number of the encoder - decoder layers)
(4 Transformer models||share||same hidden size ( 256 ))
(Ensemble Model||has||3 Bi - LSTM models)
(3 Bi - LSTM models||share||same number of encoder - decoder layers ( 1 - 1 ))
(3 Bi - LSTM models||with different||hidden sizes ( 256 , 384 and 512 ))
(Ensemble Model||has||3 CNN models)
(3 CNN models||vary in||number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 ))
(3 CNN models||share||same hidden size ( 256 ))
(Model||has||Student Model)
(Student Model||use||default configurations ( 256 hidden size and 6 - 6 layers of encoder - decoder ))
(Student Model||choose||Transformer)
