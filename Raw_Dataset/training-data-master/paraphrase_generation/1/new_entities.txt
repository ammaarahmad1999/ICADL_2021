122	0	13	n	Residual LSTM
122	14	16	p	is
122	26	56	n	current state - of - the - art
122	57	59	p	on
122	64	78	n	MSCOCO dataset
123	69	76	p	compare
123	98	116	n	standard VAE model
123	117	121	p	i.e.
123	128	148	n	unsupervised version
123	163	193	n	" supervised " variant VAE - S
123	194	196	p	of
123	201	219	n	unsupervised model
131	4	13	n	dimension
131	14	16	p	of
131	21	37	n	embedding vector
131	41	47	p	set to
131	48	51	n	300
131	71	95	n	both encoder and decoder
131	38	40	p	is
131	99	102	n	600
131	113	135	n	latent space dimension
131	96	98	p	is
131	139	143	n	1100
134	0	10	n	Batch size
134	14	21	p	kept at
134	22	24	n	32
139	0	15	n	Number of units
139	16	18	p	in
139	19	23	n	LSTM
139	28	37	p	set to be
139	42	56	n	maximum length
139	57	59	p	of
139	64	72	n	sequence
139	73	75	p	in
139	80	93	n	training data
132	4	20	n	number of layers
132	21	23	p	in
132	28	35	n	encoder
132	36	38	p	is
132	39	40	n	1
132	48	55	n	decoder
133	0	1	n	2
133	15	27	p	trained with
133	28	55	n	stochastic gradient descent
133	56	60	p	with
133	61	74	n	learning rate
133	75	83	p	fixed at
133	86	103	n	value of 5 10 ? 5
133	109	121	n	dropout rate
133	122	124	p	of
133	125	129	n	30 %
135	11	22	p	trained for
135	25	56	n	predefined number of iterations
135	59	70	p	rather than
135	73	95	n	fixed number of epochs
25	19	26	p	present
25	29	54	n	deep generative framework
25	55	83	p	for automatically generating
25	84	95	n	paraphrases
25	98	103	p	given
25	106	114	n	sentence
30	42	51	n	mechanism
30	52	64	p	to condition
30	65	78	n	our VAE model
30	79	81	p	on
30	86	103	n	original sentence
30	122	133	p	to generate
30	138	149	n	paraphrases
26	4	13	n	framework
26	14	22	p	combines
26	27	32	n	power
26	33	35	p	of
26	36	64	n	sequenceto - sequence models
26	67	79	p	specifically
26	84	117	n	long short - term memory ( LSTM )
26	124	146	n	deep generative models
26	149	161	p	specifically
26	166	197	n	variational autoencoder ( VAE )
26	200	210	p	to develop
26	213	262	n	novel , end - to - end deep learning architecture
26	263	266	p	for
26	271	275	n	task
26	276	278	p	of
26	279	300	n	paraphrase generation
32	111	121	n	our method
32	122	132	p	conditions
32	133	147	n	both the sides
32	150	154	p	i.e.
32	155	174	n	encoder and decoder
32	34	36	p	of
32	180	183	n	VAE
32	184	186	p	on
32	81	108	n	intermediate representation
32	177	179	p	of
32	226	240	n	input question
32	241	257	p	obtained through
32	258	262	n	LSTM
35	18	39	n	deep generative model
35	40	46	p	enjoys
35	49	78	n	simple , modular architecture
35	85	97	p	can generate
35	109	115	n	single
35	120	166	n	multiple , semantically sensible , paraphrases
38	27	42	n	proposed method
38	43	48	p	where
38	49	63	n	all variations
38	72	74	p	of
38	75	100	n	relatively better quality
38	112	115	p	are
38	120	144	n	top beam - search result
38	147	165	p	generated based on
38	166	177	n	different z
38	178	190	p	sampled from
38	193	205	n	latent space
2	32	53	n	Paraphrase Generation
5	42	78	n	generating paraphrases automatically
172	18	29	n	paraphrases
172	30	42	p	generated by
172	43	53	n	our system
172	54	57	p	are
172	58	71	n	well - formed
172	74	95	n	semantically sensible
172	102	123	n	grammatically correct
196	74	88	n	average metric
196	44	46	p	of
196	96	111	n	VAE - SVG model
196	115	127	p	able to give
196	130	173	n	10 % absolute point performance improvement
196	174	177	p	for
196	182	192	n	TER metric
187	112	115	p	for
187	116	130	n	MSCOCO dataset
189	19	23	p	have
189	26	49	n	significant improvement
189	50	56	p	w.r.t.
189	61	70	n	baselines
190	0	15	n	Both variations
190	16	18	p	of
190	23	39	n	supervised model
190	40	44	p	i.e.
190	47	56	n	VAE - SVG
190	61	75	n	VAE - SVG - eq
190	76	83	p	perform
190	84	90	n	better
190	91	95	p	than
190	100	122	n	state - of - the - art
190	123	127	p	with
190	128	137	n	VAE - SVG
190	138	148	p	performing
190	149	164	n	slightly better
190	165	169	p	than
190	170	184	n	VAE - SVG - eq
197	0	3	p	For
197	8	23	n	BLEU and METEOR
197	30	42	n	best results
197	43	46	p	are
197	47	87	n	4.7 % and 4 % absolute point improvement
197	88	92	p	over
197	97	119	n	state - of - the - art
198	31	44	n	Quora dataset
199	16	31	n	both variations
199	32	34	p	of
199	39	44	n	model
199	45	52	p	perform
199	53	73	n	significantly better
199	74	78	p	than
199	79	95	n	unsupervised VAE
199	100	107	n	VAE - S
200	81	89	p	increase
200	94	112	n	training data size
200	19	26	n	results
200	123	130	n	improve
201	0	9	p	Comparing
201	14	21	n	results
201	22	28	p	across
201	29	47	n	different variants
201	48	50	p	of
201	51	67	n	supervised model
201	70	84	n	VAE - SVG - eq
201	85	93	p	performs
201	98	102	n	best
203	8	25	p	experimented with
203	26	48	n	generating paraphrases
203	49	56	p	through
203	57	70	n	beam - search
203	98	112	p	turns out that
203	113	124	n	beam search
203	125	133	p	improves
203	138	145	n	results
203	146	159	n	significantly
205	5	14	p	comparing
205	19	44	n	best variant of our model
205	45	49	p	with
205	50	76	n	unsupervised model ( VAE )
205	86	97	p	able to get
205	98	155	n	more than 27 % absolute point ( more than 3 times ) boost
205	156	158	p	in
205	159	169	n	BLEU score
205	176	233	n	more than 19 % absolute point ( more than 2 times ) boost
205	234	236	p	in
205	237	243	n	METEOR
205	270	277	n	VAE - S
205	287	298	p	able to get
205	301	337	n	boost of almost 19 % absolute points
205	261	263	p	in
205	341	357	n	BLEU ( 2 times )
205	362	392	n	more than 10 % absolute points
205	393	395	p	in
205	396	416	n	METEOR ( 1.5 times )
