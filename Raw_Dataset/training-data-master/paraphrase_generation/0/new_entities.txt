155	26	61	n	proposed EDD - LG ( shared ) method
155	62	67	p	works
155	68	78	n	way better
155	79	83	p	than
155	88	102	n	other variants
155	103	114	p	in terms of
155	115	138	n	BLEU and METEOR metrics
155	142	151	p	achieving
155	155	166	n	improvement
155	167	169	p	of
155	170	181	n	8 % and 6 %
155	182	184	p	in
155	189	195	n	scores
155	234	237	p	for
155	238	250	n	50 K dataset
155	273	285	n	10 % and 7 %
155	286	288	p	in
155	293	299	n	scores
155	313	316	p	for
155	317	330	n	100 K dataset
148	3	13	p	start with
148	14	28	n	baseline model
148	38	45	p	take as
148	48	82	n	simple encoder and decoder network
148	83	87	p	with
148	88	122	n	only the local loss ( ED - Local )
149	16	33	p	experimented with
149	34	79	n	encoder - decoder and a discriminator network
149	80	84	p	with
149	85	118	n	only global loss ( EDD - Global )
149	119	133	p	to distinguish
149	138	161	n	ground truth paraphrase
149	162	166	p	with
149	171	184	n	predicted one
150	8	30	n	variation of our model
150	34	38	p	used
150	39	82	n	both the global and local loss ( EDD - LG )
152	13	17	p	make
152	22	35	n	discriminator
152	36	41	p	share
152	42	49	n	weights
152	50	54	p	with
152	59	66	n	encoder
152	71	76	p	train
152	82	89	n	network
152	90	94	p	with
152	95	134	n	both the losses ( EDD - LG ( shared ) )
22	10	21	p	consists of
22	24	52	n	sequential encoder - decoder
22	58	60	p	is
22	61	76	n	further trained
22	77	82	p	using
22	85	107	n	pairwise discriminator
23	4	34	n	encoder - decoder architecture
23	44	59	p	widely used for
23	60	79	n	machine translation
23	84	105	n	machine comprehension
24	23	30	p	ensures
24	33	47	n	' local ' loss
24	56	68	p	incurred for
24	69	93	n	each recurrent unit cell
27	0	9	p	To ensure
27	19	33	n	whole sentence
27	34	36	p	is
27	37	54	n	correctly encoded
27	60	79	p	make further use of
27	82	107	n	pair - wise discriminator
27	108	120	p	that encodes
27	125	139	n	whole sentence
27	144	151	p	obtains
27	155	164	n	embedding
28	3	22	p	further ensure that
28	31	36	n	close
28	37	39	p	to
28	44	77	n	desired ground - truth embeddings
28	90	93	n	far
28	94	98	p	from
28	99	143	n	other ( sentences in the corpus ) embeddings
29	16	24	p	provides
29	27	42	n	' global ' loss
29	48	55	p	ensures
29	60	78	n	sentence embedding
29	93	101	p	close to
29	102	148	n	other semantically related sentence embeddings
2	0	37	n	Learning Semantic Sentence Embeddings
4	40	77	n	obtaining sentence - level embeddings
