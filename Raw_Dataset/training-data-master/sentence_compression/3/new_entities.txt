72	58	90	n	dependency - tree - based method
72	96	105	p	considers
72	110	135	n	sentence compression task
72	136	138	p	as
72	142	162	n	optimization problem
72	163	171	p	by using
72	172	198	n	integer linear programming
82	25	68	n	long short - term memory networks ( LSTMs )
82	75	81	p	showed
82	82	96	n	strong promise
82	97	99	p	in
82	100	120	n	sentence compression
99	2	44	n	https://github.com/code4conference/code4sc
90	4	18	n	embedding size
90	83	85	p	is
90	86	89	n	128
90	19	22	p	for
90	23	27	n	word
90	30	52	n	part - of - speech tag
90	63	82	n	dependency relation
92	4	21	n	mini - batch size
92	26	37	p	chosen from
92	38	54	n	[ 5 , 50 , 100 ]
93	0	15	n	Vocabulary size
93	16	19	p	was
93	20	26	n	50,000
94	4	17	n	learning rate
94	18	21	p	for
94	22	43	n	neural language model
94	44	46	p	is
94	47	56	n	2.5 e - 4
94	63	70	n	1e - 05
94	71	74	p	for
94	79	93	n	policy network
91	3	11	p	employed
91	16	27	n	vanilla RNN
91	28	32	p	with
91	35	46	n	hidden size
91	47	49	p	of
91	50	53	n	512
91	54	57	p	for
91	67	81	n	policy network
91	86	107	n	neural language model
95	0	3	p	For
95	4	19	n	policy learning
95	25	29	p	used
95	34	53	n	REINFORCE algorithm
95	54	63	p	to update
95	68	78	n	parameters
95	79	81	p	of
95	86	100	n	policy network
95	105	109	p	find
95	113	119	n	policy
95	125	134	p	maximizes
95	139	145	n	reward
19	34	70	n	syntax - based neural language model
19	74	84	p	trained on
19	85	107	n	large - scale datasets
19	108	110	p	as
19	113	134	n	readability evaluator
20	4	25	n	neural language model
20	38	46	p	to learn
20	51	76	n	correct word collocations
20	77	88	p	in terms of
20	94	100	n	syntax
20	105	114	n	semantics
22	4	18	n	policy network
22	59	66	p	to form
22	69	80	n	compression
22	19	34	p	performs either
22	35	41	n	RETAIN
22	45	51	n	REMOVE
22	87	95	p	receives
22	98	133	n	reward ( e.g. , readability score )
22	134	143	p	to update
22	148	155	n	network
21	18	27	p	formulate
21	32	66	n	deletionbased sentence compression
21	72	81	p	series of
21	82	118	n	trialand - error deletion operations
21	119	126	p	through
21	129	161	n	reinforcement learning framework
2	37	57	n	Sentence Compression
4	56	93	n	deletion - based sentence compression
107	24	51	n	Evaluator - SLMbased method
107	52	58	p	yields
107	61	78	n	large improvement
107	79	83	p	over
107	88	97	n	baselines
109	9	12	p	for
109	13	32	n	Google news dataset
109	35	59	n	LSTMs ( LSTM + pos+dep )
109	68	70	p	is
109	73	99	n	relatively strong baseline
109	118	131	p	incorporating
109	132	152	n	dependency relations
109	157	180	n	part - of - speech tags
112	55	65	n	perplexity
112	88	90	p	is
112	179	183	n	76.5
112	21	25	p	with
112	137	158	n	0.2 million instances
112	0	3	p	For
112	4	20	n	Gigaword dataset
112	132	136	p	with
112	26	48	n	1.02 million instances
112	165	175	n	perplexity
112	66	68	p	of
112	73	87	n	language model
112	176	178	p	is
112	91	95	n	20.3
110	13	21	p	applying
110	22	37	n	Evaluator - SLM
110	67	75	p	observed
110	40	63	n	only a tiny improvement
114	12	22	p	shows that
114	23	41	n	small improvements
114	46	57	p	observed on
114	58	70	n	two datasets
