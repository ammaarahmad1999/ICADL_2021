{
  "has" : {
    "Experiments" : {
      "has" : {
        "Tasks" : {
          "has" : {
            "Character Prediction" : {
              "has" : {
                "Hyperparameters" : {
                  "has" : {
                    "ByteNet Decoder" : {
                      "has" : {
                        "30 residual blocks" : {
                          "split into": "six sets of five blocks"
                        }
                      },
                      "for" : {
                        "five blocks" : {
                          "dilation rates" : "1 , 2 , 4 , 8 and 16"
                        }
                      }
                    },
                    "from sentence" : "Character Prediction
                    The ByteNet Decoder that we use for the result has 30 residual blocks split into six sets of five blocks each ; for the five blocks in each set the dilation rates are , respectively , 1 , 2 , 4 , 8 and 16 ."
                    
                  },
                  "masked kernel" : ["size 3", {"from sentence" : "The masked kernel has size 3 ."}],
                  "number of hidden units" : ["dis 512", {"from sentence" : "The number of hidden units dis 512 ."}],
                  "optimization" : {
                    "Adam" : {
                      "learning rate" : "0.0003",
                      "weight decay" : "0.0001"
                    },
                    "from sentence" : "For the optimization we use Adam with a learning rate of 0.0003 and a weight decay term of 0.0001 ."
                  },
                  "apply" : {
                    "dropout" :{
                      "to" : {
                        "ReLU layer before the softmax dropping units" : {
                          "with" : "probability of 0.1"
                        }
                      }
                    },
                    "from sentence" : "We apply dropout to the last ReLU layer before the softmax dropping units with a probability of 0.1 ."
                  },
                  "sample" : {
                    "batch of sequences" : {
                      "of" : {
                        "500 characters each" : {
                          "use" : {
                            "first 100 characters" : {
                              "as" : "the minimum context"
                            }
                          },
                          "predict" : "latter 400 characters"
                        }
                      }
                    },
                    "from sentence" : "At each step we sample a batch of sequences of 500 characters each , use the first 100 characters as the minimum context and predict the latter 400 characters ."
                  }
                },
                "Results" : {
                  "on" : {
                    "test set" : {
                      "achieves" : "1.31 bits / character"
                    }
                  },
                  "from sentence" : "The ByteNet decoder achieves 1.31 bits / character on the test set ."
                }
              }
            },
            "Character - Level Machine Translation" : {
              "has" : {
                "Hyperparameters" : {
                  "has" : {
                    "ByteNet" : {
                      "has" : {
                        "30 residual blocks" : {
                          "in" : ["in the encoder", "in the decoder"]
                        }
                      },
                      "from sentence" : "Character - Level Machine Translation
                      The ByteNet used in the experiments has 30 residual blocks in the encoder and 30 residual blocks in the decoder ."                      
                    },
                    "optimization" : {
                      "use" : {
                        "Adam" : {
                          "with" : "learning rate of 0.0003"
                        }
                      },
                      "from sentence" : "For the optimization we use Adam with a learning rate of 0.0003 ."
                    },
                    "sentence" : {
                      "padded with" : {
                        "special characters" : {
                          "nearest greater multiple" : "50"
                        }
                      },
                      "from sentence" : "Each sentence is padded with special characters to the nearest greater multiple of 50 ; 20 % of further padding is ap - plied to each source sentence as apart of dynamic unfolding ( eq. 2 ) ."                      
                    }
                  },
                  "residual blocks" : {
                    "arranged in sets of five" : {
                      "with" : {
                        "dilation rates" : {
                          "of" : "1 , 2 , 4 , 8 and 16"
                        }
                      }
                    },
                    "from sentence" : "As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding dilation rates of 1 , 2 , 4 , 8 and 16 ."
                  },
                  "use" : {
                    "residual blocks" : {
                      "with" : "ReLUs",
                      "from sentence" : "For this task we use the residual blocks with ReLUs ( ."
                    },
                    "vanilla beam search" : {
                      "according to" : "total likelihood of the generated candidate",
                      "accept" : "candidates which end in a end -of - sentence token",
                      "from sentence" : "We use vanilla beam search according to the total likelihood of the generated candidate and accept only candidates which end in a end -of - sentence token ."
                    },
                    "beam" : {
                      "of" : "size 12",
                      "from sentence" : "We use a beam of size 12 ."
                    }
                  },
                  "hidden units" : ["dis 800", {"from sentence" : "The number of hidden units dis 800 ."}],
                  "in" : {
                    "source network" : {
                      "size of the kernel" : "3"
                    },
                    "target network" : {
                      "size of the masked kernel" : "3"
                    },
                    "from sentence" : "The size of the kernel in the source network is 3 , whereas the size of the masked kernel in the target network is 3 ."
                  },
                  "mapped" : {
                    "pair of sentences" : {
                      "to" : {
                        "bucket" : {
                          "based on" : {
                            "pair of padded lengths" : {
                              "for" : "efficient batching during training"
                            }
                          }
                        }
                      }
                    },
                    "from sentence" : "Each pair of sentences is mapped to a bucket based on the pair of padded lengths for efficient batching during training ."
                  }
                },
                "Results" : {
                  "On" : {
                    "NewsTest 2014" : {
                      "achieves" : {
                        "highest performance" : {
                          "in" : "character - level and subword - level neural machine translation"
                        }
                      },
                      "is" : {
                        "second" : {
                          "compared to" : {
                            "word - level systems" : {
                              "to" : {
                                "version of GNMT" : {
                                  "uses" : "word - pieces"
                                }
                              }                              
                            }
                          }
                        }
                      },
                      "from sentence" : "On NewsTest 2014 the ByteNet achieves the highest performance in character - level and subword - level neural machine translation , and compared to the word - level systems it is second only to the version of GNMT that uses word - pieces ."
                    },
                    "NewsTest 2015" : {
                      "achieves" : "best published results to date",
                      "from sentence" : "On NewsTest 2015 , to our knowledge , ByteNet achieves the best published results to date ."
                    }
                  }
                }
              }
            }
          }          
        }
      }
    }
  }
}