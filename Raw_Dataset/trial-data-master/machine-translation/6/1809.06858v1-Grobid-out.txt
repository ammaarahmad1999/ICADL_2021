title
FRAGE: Frequency-Agnostic Word Representation
abstract
Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of highfrequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop a neat, simple yet effective way to learn FRequency-AGnostic word Embedding (FRAGE) using adversarial training. We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.
Introduction
Word embeddings, which are distributed and continuous vector representations for word tokens, have been one of the basic building blocks for many neural network-based models used in natural language processing (NLP) tasks, such as language modeling, text classification and machine translation. Different from classic one-hot representation, the learned word embeddings contain semantic information which can measure the semantic similarity between words, and can also be transferred into other learning tasks.
In deep learning approaches for NLP tasks, word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters. As the inputs of the neural network, word embeddings carryall the information of words that will be further processed by the network, and the quality of embeddings is critical and highly impacts the final performance of the learning task. Unfortunately, we find the word embeddings learned by many deep learning approaches are far from perfect. As shown in(a) and 1(b), in the embedding space learned by word2vec model, the nearest neighbors of word "Peking" includes "quickest", "multicellular", and "epigenetic", which are not semantically similar, while semantically related words such as "Beijing" and "China" are far from it. Similar phenomena are observed from the word embeddings learned from translation tasks.
With a careful study, we find a more general problem which is rooted in low-frequency words in the text corpus. Without any confusion, we also call high-frequency words as popular words and call low-frequency words as rare words. As is well known, the frequency distribution of words roughly follows a simple mathematical form known as Zipf's law. When the size of a text corpus grows, the frequency of rare words is much smaller than popular words while the number of unique rare words is much larger than popular words. Interestingly, the learned embeddings of rare words and popular words behave differently. In the embedding space, a popular word usually has semantically related neighbors, while a rare word usually does not. Moreover, the nearest neighbors of more than 85% rare words are rare words. Word embeddings encode frequency information. As shown in(a) and 1(b), the embeddings of rare words and popular words actually lie in different subregions of the space. Such a phenomenon is also observed in.
We argue that the different behaviors of the embeddings of popular words and rare words are problematic. First, such embeddings will affect the semantic understanding of words. We observe more than half of the rare words are nouns or variants of popular words. Those rare words should have similar meanings or share the same topics with popular words. Second, the neighbors of a large number of rare words are semantically unrelated rare words. To some extent, those word embeddings encode more frequency information than semantic information which is not good from the view of semantic understanding. It will consequently limit the performance of down-stream tasks using the embeddings. For example, in text classification, it cannot be well guaranteed that the label of a sentence does not change when you replace one popular/rare word in the sentence by its rare/popular alternatives.
To address this problem, in this paper, we propose an adversarial training method to learn FRequency-AGnostic word Embedding (FRAGE). For a given NLP task, in addition to minimize the task-specific loss by optimizing the task-specific parameters together with word embeddings, we introduce another discriminator, which takes a word embedding as input and classifies whether it is a popular/rare word. The discriminator optimizes its parameters to maximize its classification accuracy, while word embeddings are optimized towards a low task-dependent loss as well as fooling the discriminator to mis-classify the popular and rare words. When the whole training process converges and the system achieves an equilibrium, the discriminator cannot well differentiate popular words from rare words. Consequently, rare words lie in the same region as and are mixed with popular words in the embedding space. Then FRAGE will catch better semantic information and help the task-specific model to perform better.
We conduct experiments on four types of NLP tasks, including three word similarity tasks, two language modeling tasks, three sentiment classification tasks and two machine translation tasks to test our method. In all tasks, FRAGE outperforms the baselines. Specifically, in language modeling and machine translation, we achieve better performance than the state-of-the-art results on PTB, WT2 and WMT14 English-German datasets.

Background

Word Representation
Words are the basic units of natural languages, and distributed word representations (i.e., word embeddings) are the basic units of many models in NLP tasks including language modeling and machine translation. It has been demonstrated that word representations learned from one task can be transferred to other tasks and achieve competitive performance.
While word embeddings play an important role in neural network-based models in NLP and achieve great success, one technical challenge is that the embeddings of rare words are difficult to train due to their low frequency of occurrences. develops a novel way to split word into sub-word units which is widely used in neural machine translation. However, the low-frequency sub-word units are still difficult to train: provides a comprehensive study which shows that the rare (sub)words are usually under-estimated in neural machine translation: during inference step, the model tends to choose popular words over their rare alternatives.

Adversarial Training
The basic idea of our work to address the above problem is adversarial training, in which two or more models learn together by pursuing competing goals. A representative example of adversarial training is Generative Adversarial Networks (GANs) for image generation, in which a discriminator and a generator compete with each other: the generator aims to generate images similar to the natural ones, and the discriminator aims to detect the generated ones from the natural ones. Recently, adversarial training has been successfully applied to NLP tasks. introduce an additional discriminator to differentiate the semantics learned from different languages in non-parallel bilingual data. develops a discriminator to classify whether a sentence is created by human or generated by a model.
Our proposed method is under the adversarial training framework but not exactly the conventional generator-discriminator approach since there is no generator in our scenario. For an NLP task and its neural network model (including word embeddings), we introduce a discriminator to differentiate embeddings of popular words and rare words; while the NN model aims to fool the discriminator and minimize the task-specific loss simultaneously.
Our work is also weakly related to adversarial domain adaptation which attempts to mitigate the negative effects of domain shift between training and testing. The difference between this work and adversarial domain adaptation is that we do not target at the mismatch between training and testing; instead, we aim to improve the effectiveness of word embeddings and consequently improve the performance of end-to-end NLP tasks.

Empirical Study
In this section, we study the embeddings of popular words and rare words based on the models trained from Google News corpora using word2vec 1 and trained from WMT14 English-German translation task using Transformer. The implementation details can be found in the supplementary material (part A).

Experimental Design
In both tasks, we simply set the top 20% frequent words in vocabulary as popular words and denote the rest as rare words (roughly speaking, we set a word as a rare word if its relative frequency is lower than 10 ?6 in WMT14 dataset and 10 ?7 in Google News dataset). We have tried other thresholds such as 10% or 25% and found the observations are similar.
We study whether the semantic relationship between two words is reasonable. To achieve this, we randomly sampled some rare/popular words and checked the embeddings trained from different tasks. For each sampled word, we determined its nearest neighbors based on the cosine similarity between its embeddings and others'. We also manually chose words which are semantically similar to it. For simplicity, for each word, we call the nearest words predicted from the embeddings as model-predicted neighbors, and call our chosen words as semantic neighbors.
Observation To visualize word embeddings, we reduce their dimensionalities by SVD and plot two cases in. More cases and other studies without dimensionality reduction can be found in the supplementary material (part C).
We find that the embeddings trained from different tasks share some common patterns. For both tasks, more than 90% of model-predicted neighbors of rare words are rare words. For each rare word, the model-predicted neighbor is usually not semantically related to this word, and semantic neighbors we chose are faraway from it in the embedding space. In contrast, the model-predicted neighbors of popular words are very reasonable.
As the patterns in rare words are different from that of popular words, we further check the whole embedding matrix to make a general understanding. We also visualize the word embeddings using SVD by keeping the two directions with top-2 largest eigenvalues as in and plot them in

Input Tokens Word Embeddings
Task-specific Outputs

Task-specific Model

Loss
Rare/Popular Labels Discriminator Loss predict predict: The proposed learning framework includes a task-specific predictor and a discriminator, whose function is to classify rare and popular words. Both modules use word embeddings as the input. a certain degree: the rare words and popular words lie in different regions after this linear projection, and thus they occupy different regions in the original embedding space. This strange phenomenon is also observed in other learned embeddings (e.g.CBOW and GLOVE) and mentioned in.
Explanation From the empirical study above, we can see that the occupied spaces of popular words and rare words are different and here we intuitively explain a possible reason. We simply take word2vec as an example which is trained by stochastic gradient descent. During training, the sample rate of a popular word is high and the embedding of a popular word updates frequently. For a rare word, the sample rate is low and its embedding rarely updates. According to our study, on average, the moving distance of the embedding for a popular word is twice longer than that of a rare word during training. As all word embeddings are usually initialized around the origin with a small variance, we observe in the final model, the embeddings of rare words are still around the origin and the popular words have moved faraway.

Discussion
We have strong evidence that the current phenomena are problematic. First, according to our study, in both tasks, more than half of the rare words are nouns, e.g., company names, city names. They may share some similar topics to popular entities, e.g., big companies and cities; around 10% percent of rare words include a hyphen (which is usually used to join popular words), and over 30% rare words are different PoSs of popular words. These words should have mixed or similar semantics to some popular words. These facts show that rare words and popular words should lie in the same region of the embedding space, which is different from what we observed. Second, as we can see from the cases, for rare words, model-predicted neighbors are usually not semantically related words but frequency-related words (rare words). This shows, for rare words, the embeddings encode more frequency information than semantic information. It is not good to use such word embeddings into semantic understanding tasks, e.g., text classification, language modeling, language understanding and translation.

Our Method
In this section, we present our method to improve word representations. As we have a strong prior that many rare words should share the same region in the embedding space as popular words, the basic idea of our algorithm is to train the word embeddings in an adversarial framework: We introduce a discriminator to categorize word embeddings into two classes: popular ones or rare ones. We hope the learned word embeddings not only minimize the task-specific training loss but also fool the discriminator. By doing so, the frequency information is removed from the embedding and we call our method frequency-agnostic word embedding (FRAGE).
We first define some notations and then introduce our algorithm. We develop three types of notations: embeddings, task-specific parameters/loss, and discriminator parameters/loss.
Denote ? emb ? R d?|V | as the word embedding matrix to be learned, where d is the dimension of the embedding vectors and |V | is the vocabulary size. Let V pop denote the set of popular words and V rare = V \ V pop denote the set of rare words. Then the embedding matrix ? emb can be divided into two parts: ? emb pop for popular words and ? emb rare for rare words. Let ? emb w denote the embedding of word w. Let ? model denote all the other task-specific parameters except word embeddings. For instance, for language modeling, ? model is the parameters of the RNN or LSTM; for neural machine translation, ? model is the parameters of the encoder, attention module and decoder.
Let L T (S; ? model , ? emb ) denote the task-specific loss over a dataset S. Taking language modeling as an example, the loss L T (S; ? model , ? emb ) is defined as the negative log likelihood of the data:
where y is a sentence.
Let f ? D denote a discriminator with parameters ? D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word. Let L D (V ; ? D , ? emb ) denote the loss of the discriminator:
Following the principle of adversarial training, we develop a minimax objective to train the taskspecific model (? model and ? emb ) and the discriminator (? D ) as below:
where ? is a coefficient to trade off the two loss terms. We can see that when the model parameter ? model and the embedding ? emb are fixed, the optimization of the discriminator ? D becomes
which is to minimize the classification error of popular and rare words. When the discriminator ? Dis fixed, the optimization of ? model and ? emb becomes
i.e., to optimize the task performance as well as fooling the discriminator. We train ? model , ? emb and ? D iteratively by stochastic gradient descent or its variants. The general training process is shown in Algorithm 1.

Experiment
We test our method on a wide range of tasks, including word similarity, language modeling, machine translation and text classification. For each task, we choose the state-of-the-art architecture together with the state-of-the-art training method as our baseline .
Sample a minibatch? from S.

4:
Sample a minibatchV =V pop ?V rare from V .

5:
Update ? model , ? emb by gradient descent according to Eqn. (5) with data?. Update ? D by gradient ascent according to Eqn. (4) with vocabularyV . 7: until Converge 8: Output: ? model , ? emb , ? D .
For fair comparisons, for each task, our method shares the same model architecture as the baseline. The only difference is that we use the original task-specific loss function with an additional adversarial loss as in Eqn.. Due to space limitations, we put dataset description, model description, hyperparameter configuration into supplementary material (part A).

Settings
We conduct experiments on the following tasks.
Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity: it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground-truth, in terms of Spearman's rank correlation. We use the skip-gram model as our baseline model , and train the embeddings using Enwik9 6 . We test the baseline and our method on three datasets: RG65, WS and RW. The RW dataset is a dataset for the evaluation of rare words. Following common practice, we use cosine distance while computing the similarity between two word embeddings.
Language Modeling is a basic task in natural language processing. The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity. We do experiments on two widely used datasets, Penn Treebank (PTB) and WikiText-2 (WT2). We choose two recent works as our baselines: the AWD-LSTM model and the AWD-LSTM-MoS model, which achieves state-of-the-art performance.
Machine Translation is a popular task in both deep learning and natural language processing. We choose two datasets: WMT14 English-German and IWSLT14 German-English datasets, which are evaluated in terms of BLEU score . We use Transformer as the baseline model, which achieves state-of-the-art accuracy on multiple translation datasets. We use transformer_base and transformer_big configurations following tensor2tensor .
Text Classification is a conventional machine learning task and is evaluated by accuracy. Following the setting in, we implement a Recurrent CNN-based model and test it on AG's news corpus (AGs), IMDB movie review dataset (IMDB) and 20 Newsgroups (20NG).
In all tasks, we simply set the top 20% frequent words in vocabulary as popular words and denote the rest as rare words, which is the same as our empirical study. For all the tasks except word embedding, we use full-batch gradient descent to update the discriminator. For word embedding, mini-batch stochastic gradient descent is used to update the discriminator with a batch size 3000, since the vocabulary size is large. For language modeling and machine translation tasks, we use logistic regression as the discriminator. For other tasks, we find using a shallow neural network with 5 https://github.com/tensorflow/models/blob/master/tutorials/embedding 6 http://mattmahoney.net/dc/textdata.html 7 https://github.com/salesforce/awd-lstm-lm 8 https://github.com/zihangdai/mos 9 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl To improve the training for imbalanced labeled data, a common method is to adjust loss function by reweighting the training samples; To regularize the parameter space, a common method is to use l2 regularization. We tested these methods in machine translation and found the performance is not good. Detailed analysis is provided in the supplementary material (part B). https://github.com/brightmart/text_classification one hidden layer is more efficient and we set the number of nodes in the hidden layer as 1.5 times embedding size. In all tasks, we set the hyper-parameter ? to 0.1. We list other hyper-parameters related to different task-specific models in the supplementary material (part A). In this subsection, we provide the experimental results of all tasks. For simplicity, we use "with FRAGE" as our proposed method in the tables.

Results

RG65

Word Similarity
The results on three word similarity tasks are listed in. "Paras" denotes the number of model parameters.

Language Modeling
The results of language modeling on PTB and WT2 datasets are presented in. We test our model and the baselines at several checkpoints used in the baseline papers: without finetune, with finetune, with post-process (continuous cache pointer or dynamic evaluation). In all these settings, our method outperforms the two baselines. On PTB dataset, our method improves the AWD-LSTM and AWD-LSTM-MoS baseline by 0.8/1.2/1.0 and 0.76/1.13/1.15 points in test set at different checkpoints. On WT2 dataset, which contains more rare words, our method achieves larger improvements. We improve the results of AWD-LSTM and AWD-LSTM-MoS by 2.3/2.4/2.7 and 1.15/1.72/1.54 in terms of test perplexity, respectively.

Machine Translation
The results of neural machine translation on WMT14 English-German and IWSLT14 German-English tasks are shown in. We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English-German: BLEU scores on test set on WMT2014 English-German and IWSLT German-English tasks.
task, respectively. The model learned from adversarial training also outperforms original one in IWSLT14 German-English task by 0.85. These results show improving word embeddings can achieve better results in more complicated tasks and larger datasets.

Text Classification
The results are listed in. Our method outperforms the baseline method for 1.26%/0.66%/0.44% on three different datasets.  As a summary, our experiments on four different tasks with 10 datasets verify the effectiveness of our method. We provide some case studies and visualizations of our method in the supplementary material (part C), which show that the semantic similarities are reasonable and the popular/rare words are better mixed together in the embedding space.

Conclusion
In this paper, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. We propose a neat, simple yet effective adversarial training method to improve the model performance which is verified in a wide range of tasks.
We will explore several directions in the future. First, we will investigate the theoretical aspects of word embedding learning and our adversarial training method. Second, we will study more applications which have the similar problem even beyond NLP.

A Experimental settings A.1 Dataset Description
For word similarity, we use three test datasets. WordSim-353 (WS) dataset consists of 353 pairs of commonly used verbs and nouns; The rare-words (RW) dataset contains rarely used words; The RG65 dataset contains 65 word pairs, and the similarity values in the dataset are the means of judgments made by 51 subjects.
For language modeling tasks, we use Penn Treebank dataset and WikiText-2 dataset. The details of the datasets are provided in.: Statistics of the Penn Treebank, and WikiText-2 dataset used in language modeling. The out of vocabulary (OOV) words will be replaced by <unk> during training and testing.
For machine translation, we use WMT14 English-German and IWSLT14 German-English datasets.
The training set of WMT14 English-German task consists of 4.5M sentence pairs. Source and target tokens are processed into 37K shared sub-word units based on byte-pair encoding (BPE). We use the concatenation of newstest2012 and newstest2013 as the validation set and use newstest2014 as the test set following all previous works. IWSLT14 German-English dataset contains 160K training sentence pairs and 7K validation sentence pairs. Tokens are processed using BPE and eventually we obtain a shared vocabulary of about 32K tokens. We use the concatenation of dev2010, tst2010, tst2011 and tst2011 as the test set, which is widely adopted in.
For text classification tasks, we use three datasets: AG's News, IMDB and 20NG. AG's news corpus is a news article corpus with categorized articles from more than 2,000 news. IMDB movie review dataset is a sentiment classification dataset. It consists of movie review comments with binary sentiment labels. 20 Newsgroups is an email collection dataset, in which the emails are categorized into 20 different groups. We use the bydate version and select 4 major categories (comp, politics, rec, and religion) following.: Detailed statistics about text classification datasets.

A.2 Hyper-parameter configurations
The hyper-parameters used for AWD-LSTM with/without MoS in language modeling experiment is shown in.
For machine translation tasks, we choose Adam optimizer with ? 1 = 0.9, ? 2 = 0.98, ? = 10 ?9 , and follow the learning rate schedule in. For evaluation, we use the case-sensitive tokenized BLEU score for WMT14 English-German and case-insensitive tokenized BLEU score for IWSLT14 German-English. The hyper-parameters used in machine translation task are summarized in. The hyper-parameters used in word embedding task are summarized in.
For all text classification tasks, we use convolutional kernel with size 2, 3, 5. We implement batch normalization and shortcut connection, and use Adam optimizer with ? 1 = 0.9, ? 2 = 0.99, ? = 10 ?8 .: Hyper-parameter used for word embedding training.

AWD-LSTM + MoS

A.3 Models Description
We use task-specific baseline models. In language modeling, AWD-LSTM is a weight-dropped LSTM which uses Drop Connect on hidden-to-hidden weights as a means of recurrent regularization. The model is trained by NT-ASGD, which is a variant of the averaged stochastic gradient method. The training process has two steps, in the second step, the model is finetuned using another configuration of NT-ASGD. AWD-LSTM-MoS uses the Mixture of Softmaxes structure to the vanilla AWD-LSTM and achieves the state-of-the-art result on PTB and WT2.
For machine translation, Transformer is a recently developed architecture in which the selfattention network is used during encoding and decoding step. It achieves the best performances on several machine translation tasks, e.g. WMT14 English-German, WMT14 English-French datasets.
Word2vec is one of the pioneer works on using deep learning to NLP tasks. Based on the co-occurrence of words, it produces distributed representations of words (word embeddings).
RCNN contains both recurrent and convolutional layers to catch the key components in texts, and is widely used in text classification tasks.

B Additional Comparisons
We compare some other simple methods with ours on machine translation tasks, which include reweighting method and l 2 regularization (weight decay). Results are listed in. We notice that those simple methods do notwork for the tasks, even have negative effects.: BLEU scores on test set of the WMT14 English-German task and IWSLT14 German-English task.

WMT
Our method is denoted as "FRAGE", "Reweighting" denotes reweighting the loss of each word by reciprocal of its frequency, and "Weight Decay" denotes putting weight decay rate (0.2) on embeddings.

C Case Study on Original Models and Qualitative Analysis of Our Method
We provide more word similarity cases in to justify our statement in Section 3. We also present the effectiveness of our method by showcase and embedding visualizations. From the cases and visualizations in and, we find the word similarities are improved and popular/rare words are better mixed together.
(a) (b): These figures show that, in different tasks, the embeddings of rare and popular words are better mixed together after applying our method.