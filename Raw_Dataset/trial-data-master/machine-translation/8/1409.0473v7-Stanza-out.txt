title
Published as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE
abstract
Neural machine translation is a recently proposed approach to machine translation .
Unlike the traditional statistical machine translation , the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance .
The models proposed recently for neural machine translation often belong to a family of encoder - decoders and encode a source sentence into a fixed - length vector from which a decoder generates a translation .
In this paper , we conjecture that the use of a fixed - length vector is a bottleneck in improving the performance of this basic encoder - decoder architecture , and propose to extend this by allowing a model to automatically ( soft - ) search for parts of a source sentence that are relevant to predicting a target word , without having to form these parts as a hard segment explicitly .
With this new approach , we achieve a translation performance comparable to the existing state - of - the - art phrase - based system on the task of English - to - French translation .
Furthermore , qualitative analysis reveals that the ( soft - ) alignments found by the model agree well with our intuition .
INTRODUCTION
Neural machine translation is a newly emerging approach to machine translation , recently proposed by , and .
Unlike the traditional phrase - based translation system ( see , e.g. , which consists of many small sub-components thatare tuned separately , neural machine translation attempts to build and train a single , large neural network that reads a sentence and outputs a correct translation .
Most of the proposed neural machine translation models belong to a family of encoderdecoders , with an encoder and a decoder for each language , or involve a language - specific encoder applied to each sentence whose outputs are then compared ) .
An encoder neural network reads and encodes a source sentence into a fixed - length vector .
A decoder then outputs a translation from the encoded vector .
The whole encoder - decoder system , which consists of the encoder and the decoder for a language pair , is jointly trained to maximize the probability of a correct translation given a source sentence .
A potential issue with this encoder - decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed - length vector .
This may make it difficult for the neural network to cope with long sentences , especially those thatare longer than the sentences in the training corpus .
showed that indeed the performance of a basic encoder - decoder deteriorates rapidly as the length of an input sentence increases .
In order to address this issue , we introduce an extension to the encoder - decoder model which learns to align and translate jointly .
Each time the proposed model generates a word in a translation , it ( soft - ) searches for a set of positions in a source sentence where the most relevant information is concentrated .
The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words .
The most important distinguishing feature of this approach from the basic encoder - decoder is that it does not attempt to encode a whole input sentence into a single fixed - length vector .
Instead , it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation .
This frees a neural translation model from having to squash all the information of a source sentence , regardless of its length , into a fixed - length vector .
We show this allows a model to cope better with long sentences .
In this paper , we show that the proposed approach of jointly learning to align and translate achieves significantly improved translation performance over the basic encoder - decoder approach .
The improvement is more apparent with longer sentences , but can be observed with sentences of any length .
On the task of English - to - French translation , the proposed approach achieves , with a single model , a translation performance comparable , or close , to the conventional phrase - based system .
Furthermore , qualitative analysis reveals that the proposed model finds a linguistically plausible ( soft - ) alignment between a source sentence and the corresponding target sentence .
BACKGROUND : NEURAL MACHINE TRANSLATION
From a probabilistic perspective , translation is equivalent to finding a target sentence y that maximizes the conditional probability of y given a source sentence x , i.e. , arg max y p ( y | x ) .
In neural machine translation , we fit a parameterized model to maximize the conditional probability of sentence pairs using a parallel training corpus .
Once the conditional distribution is learned by a translation model , given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability .
Recently , a number of papers have proposed the use of neural networks to directly learn this conditional distribution ( see , e.g. , .
This neural machine translation approach typically consists of two components , the first of which encodes a source sentence x and the second decodes to a target sentence y .
For instance , two recurrent neural networks ( RNN ) were used by and to encode a variable - length source sentence into a fixed - length vector and to decode the vector into a variable - length target sentence .
Despite being a quite new approach , neural machine translation has already shown promising results .
reported that the neural machine translation based on RNNs with long shortterm memory ( LSTM ) units achieves close to the state - of - the - art performance of the conventional phrase - based machine translation system on an English - to - French translation task .
1 Adding neural components to existing translation systems , for instance , to score the phrase pairs in the phrase table or to re-rank candidate translations , has allowed to surpass the previous state - of - the - art performance level .
RNN ENCODER - DECODER
Here , we describe briefly the underlying framework , called RNN Encoder - Decoder , proposed by and upon which we build a novel architecture that learns to align and translate simultaneously .
In the Encoder - Decoder framework , an encoder reads the input sentence , a sequence of vectors x = ( x 1 , , x Tx ) , into a vector c. 2 The most common approach is to use an RNN such that
( 1 ) and c = q ( {h 1 , , h Tx } ) , where ht ?
Rn is a hidden state at time t , and c is a vector generated from the sequence of the hidden states .
f and q are some nonlinear functions .
used an LSTM as f and q ( {h 1 , , h T }) = h T , for instance .
The decoder is often trained to predict the next wordy t given the context vector c and all the previously predicted words {y 1 , , y t ?1 }.
In other words , the decoder defines a probability over the translation y by decomposing the joint probability into the ordered conditionals :
where y = y 1 , , y Ty .
With an RNN , each conditional probability is modeled as
where g is a nonlinear , potentially multi-layered , function that outputs the probability of y t , and st is the hidden state of the RNN .
It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used .
LEARNING TO ALIGN AND TRANSLATE
In this section , we propose a novel architecture for neural machine translation .
The new architecture consists of a bidirectional RNN as an encoder ( Sec. 3.2 ) and a decoder that emulates searching through a source sentence during decoding a translation ( Sec. 3.1 ) .
DECODER : GENERAL DESCRIPTION
x 1 x 2 x 3 x T :
The graphical illustration of the proposed model trying to generate the t-th target wordy t given a source sentence ( x 1 , x 2 , . . . , x T ) .
In a new model architecture , we define each conditional probability in Eq .
( 2 ) as :
where s i is an RNN hidden state for time i , computed by
It should be noted that unlike the existing encoder - decoder approach ( see Eq.
( 2 ) ) , here the probability is conditioned on a distinct context vector c i for each target wordy i .
The context vector c i depends on a sequence of annotations ( h 1 , , h Tx ) to which an encoder maps the input sentence .
Each annotation hi contains information about the whole input sequence with a strong focus on the parts surrounding the i - th word of the input sequence .
We explain in detail how the annotations are computed in the next section .
The context vector c i is , then , computed as a weighted sum of these annotations hi :
The weight ?
ij of each annotation h j is computed by
where e ij = a (s i?1 , h j ) is an alignment model which scores how well the inputs around position j and the output at position i match .
The score is based on the RNN hidden state s i ?1 ( just before emitting y i , Eq. ( 4 ) ) and the j - th annotation h j of the input sentence .
We parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system .
Note that unlike in traditional machine translation , the alignment is not considered to be a latent variable .
Instead , the alignment model directly computes a soft alignment , which allows the gradient of the cost function to be backpropagated through .
This gradient can be used to train the alignment model as well as the whole translation model jointly .
We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation , where the expectation is over possible alignments .
Let ?
ij be a probability that the target wordy i is aligned to , or translated from , a source word x j .
Then , the i - th context vector c i is the expected annotation over all the annotations with probabilities ? ij .
The probability ?
ij , or it s associated energy e ij , reflects the importance of the annotation h j with respect to the previous hidden state s i ?1 in deciding the next state s i and generating y i .
Intuitively , this implements a mechanism of attention in the decoder .
The decoder decides parts of the source sentence to pay attention to .
By letting the decoder have an attention mechanism , we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector .
With this new approach the information can be spread throughout the sequence of annotations , which can be selectively retrieved by the decoder accordingly .
ENCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
The usual RNN , described in Eq. ( 1 ) , reads an input sequence x in order starting from the first symbol x 1 to the last one x Tx .
However , in the proposed scheme , we would like the annotation of each word to summarize not only the preceding words , but also the following words .
Hence , we propose to use a bidirectional RNN ( BiRNN , , which has been successfully used recently in speech recognition ( see , e.g. , .
A BiRNN consists of forward and backward RNN 's .
The forward RNN ? ?
f reads the input sequence as it is ordered ( from x 1 to x Tx ) and calculates a sequence of forward hidden states (
The backward RNN
? ?
f reads the sequence in the reverse order ( from x Tx to x 1 ) , resulting in a sequence of backward hidden states (
We obtain an annotation for each word x j by concatenating the forward hidden state ? ?
h j and the backward one
In this way , the annotation h j contains the summaries of both the preceding words and the following words .
Due to the tendency of RNNs to better represent recent inputs , the annotation h j will be focused on the words around x j .
This sequence of annotations is used by the decoder and the alignment model later to compute the context vector ( Eqs. ( 5 ) - ) .
See for the graphical illustration of the proposed model .
EXPERIMENT SETTINGS
We evaluate the proposed approach on the task of English - to - French translation .
We use the bilingual , parallel corpora provided by ACL WMT ' 14 .
3
As a comparison , we also report the performance of an RNN Encoder - Decoder which was proposed recently by .
We use the same training procedures and the same dataset for both models .
4
DATASET
WMT ' 14 contains the following English - French parallel corpora : Europarl ( 61 M words ) , news commentary ( 5.5 M ) , UN ( 421M ) and two crawled corpora of 90 M and 272.5 M words respectively , totaling 850M words .
Following the procedure described in , we reduce the size of the combined corpus to have 348M words using the data selection method by .
We do not use any monolingual data other than the mentioned parallel corpora , although it maybe possible to use a much larger monolingual corpus to pretrain an encoder .
We concatenate news - test - After a usual tokenization 6 , we use a shortlist of 30,000 most frequent words in each language to train our models .
Any word not included in the shortlist is mapped to a special token ( [ UNK ] ) .
We do not apply any other special preprocessing , such as lowercasing or stemming , to the data .
MODELS
We train two types of models .
The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .
We train each model twice : first with the sentences of length up to 30 words ( RNNencdec - 30 , RNNsearch - 30 ) and then with the sentences of length up to 50 word ( RNNencdec - 50 , RNNsearch - 50 ) .
The encoder and decoder of the RNNencdec have 1000 hidden units each .
The encoder of the RNNsearch consists of forward and backward recurrent neural networks ( RNN ) each having 1000 hidden units .
It s decoder has 1000 hidden units .
In both cases , we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word .
We use a minibatch stochastic gradient descent ( SGD ) algorithm together with Adadelta to train each model .
Each SGD update direction is computed using a minibatch of 80 sentences .
We trained each model for approximately 5 days .
Once a model is trained , we use a beam search to find a translation that approximately maximizes the conditional probability ( see , e.g. , .
used this approach to generate translations from their neural machine translation model .
For more details on the architectures of the models and training procedure used in the experiments , see Appendices A and B.
RESULTS
QUANTITATIVE RESULTS
In : Four sample alignments found by RNNsearch - 50 .
The x - axis and y-axis of each plot correspond to the words in the source sentence ( English ) and the generated translation ( French ) , respectively .
Each pixel shows the weight ?
ij of the annotation of the j - th source word for the i - th target word ( see Eq. ) , in grayscale ( 0 : black , 1 : white ) .
( a ) an arbitrary sentence .
( b - d ) three randomly selected samples among the sentences without any unknown words and of length between 10 and 20 words from the test set .
One of the motivations behind the proposed approach was the use of a fixed - length context vector in the basic encoder - decoder approach .
We conjectured that this limitation may make the basic encoder - decoder approach to underperform with long sentences .
In , we see that the performance of RNNencdec dramatically drops as the length of the sentences increases .
On the other hand , both RNNsearch - 30 and RNNsearch - 50 are more robust to the length of the sentences .
RNNsearch - 50 , especially , shows no performance deterioration even with sentences of length 50 or more .
This superiority of the proposed model over the basic encoder - decoder is further confirmed by the fact that the RNNsearch - 30 even outperforms RNNencdec - 50 ( see ) .
tokens when only the sentences having no unknown words were evaluated ( last column ) .
QUALITATIVE ANALYSIS
ALIGNMENT
The proposed approach provides an intuitive way to inspect the ( soft - ) alignment between the words in a generated translation and those in a source sentence .
This is done by visualizing the annotation weights ?
ij from Eq. , as in .
Each row of a matrix in each plot indicates the weights associated with the annotations .
From this we see which positions in the source sentence were considered more important when generating the target word .
We can see from the alignments in that the alignment of words between English and French is largely monotonic .
We see strong weights along the diagonal of each matrix .
However , we also observe a number of non-trivial , non-monotonic alignments .
Adjectives and nouns are typically ordered differently between French and English , and we see an example in The strength of the soft - alignment , opposed to a hard - alignment , is evident , for instance , from ].
We observe similar behaviors in all the presented cases in .
An additional benefit of the soft alignment is that it naturally deals with source and target phrases of different lengths , without requiring a counter - intuitive way of mapping some words to or from nowhere ( [ NULL ] ) ( see , e.g. , Chapters 4 and 5 of .
LONG SENTENCES
As clearly visible from the proposed model ( RNNsearch ) is much better than the conventional model ( RNNencdec ) at translating long sentences .
This is likely due to the fact that the RNNsearch does not require encoding along sentence into a fixed - length vector perfectly , but only accurately encoding the parts of the input sentence that surround a particular word .
As an example , consider this source sentence from the test set :
An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .
The RNNencdec - 50 translated this sentence into :
Un privilge d'admission est le droit d'un mdecin de reconnatre un patient l'hpital ou un centre mdical d'un diagnostic ou de prendre un diagnostic en fonction de sontat de sant .
On the other hand , the RNNsearch - 50 generated the following correct translation , preserving the whole meaning of the input sentence without omitting any details :
Un privilge d'admission est le droit d'un mdecin d'admettre un patient un hpital ou un centre mdical pour effectuer un diagnostic ou une procdure , selon son statut de travailleur des soins de sant l'hpital .
Let us consider another sentence from the test set :
This kind of experience is part of Disney 's efforts to " extend the lifetime of its series and build new relationships with audiences via digital platforms thatare becoming evermore important , " he added .
The translation by the RNNencdec - 50 is
Ce type d'exprience fait partie des initiatives du Disney pour " prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .
As with the previous example , the RNNencdec began deviating from the actual meaning of the source sentence after generating approximately 30 words ( see the underlined phrase ) .
After that point , the quality of the translation deteriorates , with basic mistakes such as the lack of a closing quotation mark .
Again , the RNNsearch - 50 was able to translate this long sentence correctly :
Ce genre d'exprience fait partie des efforts de Disney pour " prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes " , a-t - il ajout .
In conjunction with the quantitative results presented already , these qualitative observations confirm our hypotheses that the RNNsearch architecture enables far more reliable translation of long sentences than the standard RNNencdec model .
In Appendix C , we provide a few more sample translations of long source sentences generated by the RNNencdec - 50 , RNNsearch - 50 and Google Translate along with the reference translations .
6 RELATED WORK
LEARNING TO ALIGN
A similar approach of aligning an output symbol with an input symbol was proposed recently by in the context of handwriting synthesis .
Handwriting synthesis is a task where the model is asked to generate handwriting of a given sequence of characters .
In his work , he used a mixture of Gaussian kernels to compute the weights of the annotations , where the location , width and mixture coefficient of each kernel was predicted from an alignment model .
More specifically , his alignment was restricted to predict the location such that the location increases monotonically .
The main difference from our approach is that , in , the modes of the weights of the annotations only move in one direction .
In the context of machine translation , this is a severe limitation , as ( long - distance ) reordering is often needed to generate a grammatically correct translation ( for instance , English - to - German ) .
Our approach , on the other hand , requires computing the annotation weight of every word in the source sentence for each word in the translation .
This drawback is not severe with the task of translation in which most of input and output sentences are only 15 - 40 words .
However , this may limit the applicability of the proposed scheme to other tasks .
NEURAL NETWORKS FOR MACHINE TRANSLATION
Since introduced a neural probabilistic language model which uses a neural network to model the conditional probability of a word given a fixed number of the preceding words , neural networks have widely been used in machine translation .
However , the role of neural networks has been largely limited to simply providing a single feature to an existing statistical machine translation system or to re-rank a list of candidate translations provided by an existing system .
For instance , proposed using a feedforward neural network to compute the score of a pair of source and target phrases and to use the score as an additional feature in the phrase - based statistical machine translation system .
More recently , and reported the successful use of the neural networks as a sub-component of the existing translation system .
Traditionally , a neural network trained as a target - side language model has been used to rescore or rerank a list of candidate translations ( see , e.g. , .
Although the above approaches were shown to improve the translation performance over the stateof - the - art machine translation systems , we are more interested in a more ambitious objective of designing a completely new translation system based on neural networks .
The neural machine translation approach we consider in this paper is therefore a radical departure from these earlier works .
Rather than using a neural network as apart of the existing system , our model works on its own and generates a translation from a source sentence directly .
CONCLUSION
The conventional approach to neural machine translation , called an encoder - decoder approach , encodes a whole input sentence into a fixed - length vector from which a translation will be decoded .
We conjectured that the use of a fixed - length context vector is problematic for translating long sentences , based on a recent empirical study reported by and .
In this paper , we proposed a novel architecture that addresses this issue .
We extended the basic encoder - decoder by letting a model ( soft - ) search for a set of input words , or their annotations computed by an encoder , when generating each target word .
This frees the model from having to encode a whole source sentence into a fixed - length vector , and also lets the model focus only on information relevant to the generation of the next target word .
This has a major positive impact on the ability of the neural machine translation system to yield good results on longer sentences .
Unlike with the traditional machine translation systems , all of the pieces of the translation system , including the alignment mechanism , are jointly trained towards a better log-probability of producing correct translations .
We tested the proposed model , called RNNsearch , on the task of English - to - French translation .
The experiment revealed that the proposed RNNsearch outperforms the conventional encoder - decoder model ( RNNencdec ) significantly , regardless of the sentence length and that it is much more robust to the length of a source sentence .
From the qualitative analysis where we investigated the ( soft - ) alignment generated by the RNNsearch , we were able to conclude that the model can correctly align each target word with the relevant words , or their annotations , in the source sentence as it generated a correct translation .
Perhaps more importantly , the proposed approach achieved a translation performance comparable to the existing phrase - based statistical machine translation .
It is a striking result , considering that the proposed architecture , or the whole family of neural machine translation , has only been proposed as recently as this year .
We believe the architecture proposed here is a promising step toward better machine translation and a better understanding of natural languages in general .
One of challenges left for the future is to better handle unknown , or rare words .
This will be required for the model to be more widely used and to match the performance of current state - of - the - art machine translation systems in all contexts .
A MODEL ARCHITECTURE
A.1 ARCHITECTURAL CHOICES
The proposed scheme in Section 3 is a general framework where one can freely define , for instance , the activation functions f of recurrent neural networks ( RNN ) and the alignment model a .
Here , we describe the choices we made for the experiments in this paper .
A.1.1 RECURRENT NEURAL NETWORK
For the activation function f of an RNN , we use the gated hidden unit recently proposed by .
The gated hidden unit is an alternative to the conventional simple units such as an element - wise tanh .
This gated unit is similar to along short - term memory ( LSTM ) unit proposed earlier by , sharing with it the ability to better model and learn long - term dependencies .
This is made possible by having computation paths in the unfolded RNN for which the product of derivatives is close to 1 .
These paths allow gradients to flow backward easily without suffering too much from the vanishing effect .
It is therefore possible to use LSTM units instead of the gated hidden unit described here , as was done in a similar context by .
The new state s i of the RNN employing n gated hidden units 8 is computed by
where is an element - wise multiplication , and z i is the output of the update gates ( see below ) .
The proposed updated states i is computed b ?
where e ( y
i?1 ) ?
R m is an m-dimensional embedding of a wordy i ?1 , and r i is the output of the reset gates ( see below ) .
When y i is represented as a 1 - of - K vector , e ( y i ) is simply a column of an embedding matrix E ?
R mK .
Whenever possible , we omit bias terms to make the equations less cluttered .
The update gates z i allow each hidden unit to maintain its previous activation , and the reset gates r i control how much and what information from the previous state should be reset .
We compute them by
where ? ( ) is a logistic sigmoid function .
At each step of the decoder , we compute the output probability ( Eq. ( 4 ) ) as a multi -layered function .
We use a single hidden layer of maxout units and normalize the output probabilities ( one for each word ) with a softmax function ( see Eq. ) .
A.1.2 ALIGNMENT MODEL
The alignment model should be designed considering that the model needs to be evaluated T x Ty times for each sentence pair of lengths T x and Ty .
In order to reduce computation , we use a singlelayer multilayer perceptron such that
where W a ?
R nn , U a ?
R n 2n and v a ?
Rn are the weight matrices .
Since
U ah j does not depend on i , we can pre-compute it in advance to minimize the computational cost .
A.2 DETAILED DESCRIPTION OF THE MODEL
A.2.1 ENCODER
In this section , we describe in detail the architecture of the proposed model ( RNNsearch ) used in the experiments ( see .
From hereon , we omit all bias terms in order to increase readability .
The model takes a source sentence of 1 - of - K coded word vectors as input
and outputs a translated sentence of 1 - of - K coded word vectors y = ( y 1 , . . . , y Ty ) , y i ? R Ky ,
where K x and Ky are the vocabulary sizes of source and target languages , respectively .
T x and Ty respectively denote the lengths of source and target sentences .
First , the forward states of the bidirectional recurrent neural network ( BiRNN ) are computed :
are weight matrices .
m and n are the word embedding dimensionality and the number of hidden units , respectively . ? ( ) is as usual a logistic sigmoid function .
The backward states ( ? ? h 1 , , ? ? h Tx ) are computed similarly .
We share the word embedding matrix E between the forward and backward RNNs , unlike the weight matrices .
We concatenate the forward and backward states to to obtain the annotations ( h 1 , h 2 , , h Tx ) , where
A.
DECODER
The hidden state s i of the decoder given the annotations from the encoder is computed by
E is the word embedding matrix for the target language .
W , W z , W r ?
R nm , U , U z , Ur ? R nn , and C , C z , Cr ?
R n 2n are weights .
Again , m and n are the word embedding dimensionality and the number of hidden units , respectively .
The initial hidden state s 0 is computed by
The context vector c i are recomputed at each step by the alignment model : :
Learning statistics and relevant information .
Each update corresponds to updating the parameters once using a single minibatch .
One epoch is one pass through the training set .
NLL is the average conditional log-probabilities of the sentences in either the training set or the development set .
Note that the lengths of the sentences differ .
where
and h j is the j - th annotation in the source sentence ( see Eq. ) .
v a ?
Rn , W a ?
Rn n and U a ?
Rn 2n are weight matrices .
Note that the model becomes RNN Encoder - Decoder
With the decoder state s i ?1 , the context c i and the last generated wordy i ? 1 , we define the probability of a target wordy i as
where ti = max t i ,2 j?1 ,t i , 2 j j=1 , ... , l andt i , k is the k - th element of a vectort i which is computed b ?
and Co ?
R 2 l 2n are weight matrices .
This can be understood as having a deep output with a single maxout hidden layer .
A.2.3 MODEL SIZE
For all the models used in this paper , the size of a hidden layer n is 1000 , the word embedding dimensionality m is 620 and the size of the maxout hidden layer in the deep output l is 500 .
The number of hidden units in the alignment model n is 1000 .
and ? ?
Ur as random orthogonal matrices .
For W a and U a , we initialized them by sampling each element from the Gaussian distribution of mean 0 and variance 0.001 2 .
All the elements of Va and all the bias vectors were initialized to zero .
Any other weight matrix was initialized by sampling from the Gaussian distribution of mean 0 and variance 0.01 2 .
B.2 TRAINING
We used the stochastic gradient descent ( SGD ) algorithm .
Adadelta was used to automatically adapt the learning rate of each parameter ( = 10 ?6 and ? = 0.95 ) .
We explicitly normalized the L 2 - norm of the gradient of the cost function each time to beat most a predefined threshold of 1 , when the norm was larger than the threshold .
Each SGD update direction was computed with a minibatch of 80 sentences .
At each update our implementation requires time proportional to the length of the longest sentence in a minibatch .
Hence , to minimize the waste of computation , before every 20 - th update , we retrieved 1600 sentence pairs , sorted them according to the lengths and split them into 20 minibatches .
The training data was shuffled once before training and was traversed sequentially in this manner .
In Tables 2 we present the statistics related to training all the models used in the experiments .
C TRANSLATIONS OF LONG SENTENCES
Source
An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure , based on his status as a healthcare worker at a hospital .
Reference
Le privilge d'admission est le droit d'un mdecin , en vertu de son statut de membre soignant d'un hpital , d'admettre un patient dans un hpital ou un centre mdical afin d 'y dlivrer un diagnostic ou un traitement .
RNNenc - 50
Ce type d'exprience fait partie des initiatives du Disney pour " prolonger la dure de vie de ses nouvelles et de dvelopper des liens avec les lecteurs numriques qui deviennent plus complexes .
RNNsearch - 50
Ce genre d'exprience fait partie des efforts de Disney pour " prolonger la dure de vie de ses sries et crer de nouvelles relations avec des publics via des plateformes numriques de plus en plus importantes " , a-t - il ajout .
Google Translate
Ce genre d'exprience fait partie des efforts de Disney " tendre la dure de vie de sa srie et construire de nouvelles relations avec le public par le biais des plates - formes numriques qui deviennent de plus en plus important " , at - il ajout .
Source
In a press conference on Thursday , Mr Blair stated that there was nothing in this video that might constitute a " reasonable motive " that could lead to criminal charges being brought against the mayor .
Reference
En confrence de presse , jeudi , M. Blair a affirm qu'il n'y avait rien dans cette vido qui puisse constituer des " motifs raisonnables " pouvant mener au dpt d'une accusation criminelle contre le maire .
RNNenc - 50
Lors de la confrence de presse de jeudi , M. Blair a dit qu'il n'y avait rien dans cette vido qui pourrait constituer une " motivation raisonnable " pouvant entraner des accusations criminelles portes contre le maire .
RNNsearch - 50
Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un " motif raisonnable " qui pourrait conduire des accusations criminelles contre le maire .
Google Translate
Lors d'une confrence de presse jeudi , M. Blair a dclar qu'il n'y avait rien dans cette vido qui pourrait constituer un " motif raisonnable " qui pourrait mener des accusations criminelles portes contre le maire . :
The translations generated by RNNenc - 50 and RNNsearch - 50 from long source sentences ( 30 words or more ) selected from the test set .
For each source sentence , we also show the goldstandard translation .
The translations by Google Translate were made on 27 August 2014 .
Reference
Ce type d'exprience entre dans le cadre des efforts de Disney pour " tendre la dure de vie de ses sries et construire de nouvelles relations avec son public grce des plateformes numriques qui so nt de plus en plus importantes " , a-t - il ajout .
