(Contribution||has||Tasks)
(Tasks||has||MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR ))
(MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )||has||Hyperparameters)
(Hyperparameters||Each||MoE layer)
(MoE layer||contained up to||2048 experts each)
(2048 experts each||with||about two million parameters)
(Hyperparameters||inserted||MoE layers)
(MoE layers||in both||decoder)
(decoder||between||layers 1 and 2)
(MoE layers||in both||encoder)
(encoder||between||layers 2 and 3)
(Hyperparameters||decreased||LSTM layers)
(LSTM layers||in||encoder and decoder)
(encoder and decoder||from||9 and 8 to 3 and 2 respectively)
(LSTM layers||To reduce||computation)
(Hyperparameters||model||GNMT model)
(GNMT model||was a||modified version)
(MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )||has||Results)
(Results||BLEU scores||40.56 and 26.03)
(40.56 and 26.03||on||WMT ' 14 En?Fr and En ? De benchmarks)
(Results||test BLEU score||1.01 higher)
(1.01 higher||On||Google Production dataset)
(Tasks||has||MULTILINGUAL MACHINE TRANSLATION)
(MULTILINGUAL MACHINE TRANSLATION||has||Results)
(Results||on||dev set)
(dev set||achieves||19 % lower perplexity)
(19 % lower perplexity||than||multilingual GNMT model)
(Results||On||BLEU score)
(BLEU score||significantly beats||multilingual GNMT model)
(multilingual GNMT model||on||11 of the 12 language pairs)
(11 of the 12 language pairs||by as much as||5.84 points)
(BLEU score||even beats||monolingual GNMT models)
(monolingual GNMT models||on||8 of 12 language pairs)
(Tasks||has||100 BILLION WORD GOOGLE NEWS CORPUS)
(100 BILLION WORD GOOGLE NEWS CORPUS||has||Hyperparameters)
(Hyperparameters||MoE layers||32 , experts)
(100 BILLION WORD GOOGLE NEWS CORPUS||has||Results)
(Results||When||training over the full 100 billion words)
(training over the full 100 billion words||test perplexity||improves significantly)
(improves significantly||up to||65536 experts ( 68 billion parameters ))
