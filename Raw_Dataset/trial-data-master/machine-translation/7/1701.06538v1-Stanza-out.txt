title
OUTRAGEOUSLY LARGE NEURAL NETWORKS : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER
abstract
The capacity of a neural network to absorb information is limited by its number of parameters .
Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .
In practice , however , there are significant algorithmic and performance challenges .
In this work , we address these challenges and finally realize the promise of conditional computation , achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters .
We introduce a Sparsely - Gated Mixture - of - Experts layer ( MoE ) , consisting of up to thousands of feed - forward sub - networks .
A trainable gating network determines a sparse combination of these experts to use for each example .
We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora .
We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers .
On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .
* Equally major contributors Work done as a member of the Google Brain Residency program ( g.co/ brainresidency )
INTRODUCTION AND RELATED WORK 1 .
CONDITIONAL COMPUTATION
Exploiting scale in both training data and model size has been central to the success of deep learning .
When datasets are sufficiently large , increasing the capacity ( number of parameters ) of neural networks can give much better prediction accuracy .
This has been shown in domains such as text , images , and audio .
For typical deep learning models , where the entire model is activated for every example , this leads to a roughly quadratic blow - up in training costs , as both the model size and the number of training examples increase .
Unfortunately , the advances in computing power and distributed computation fall short of meeting such demand .
Various forms of conditional computation have been proposed as away to increase model capacity without a proportional increase in computational costs .
In these schemes , large parts of a network are active or inactive on a per-example basis .
The gating decisions maybe binary or sparse and continuous , stochastic or deterministic .
Various forms of reinforcement learning and back - propagation are proposed for trarining the gating decisions .
While these ideas are promising in theory , no work to date has yet demonstrated massive improvements in model capacity , training time , or model quality .
We blame this on a combination of the following challenges :
Modern computing devices , especially GPUs , are much faster at arithmetic than at branching .
Most of the works above recognize this and propose turning on / off large chunks of the network with each gating decision .
Large batch sizes are critical for performance , as they amortize the costs of parameter transfers and updates .
Conditional computation reduces the batch sizes for the conditionally active chunks of the network .
Network bandwidth can be a bottleneck .
A cluster of GPUs may have computational power thousands of times greater than the aggregate inter - device network bandwidth .
To be computationally efficient , the relative computational versus network demands of an algorithm must exceed this ratio .
Embedding layers , which can be seen as a form of conditional computation , are handicapped by this very problem .
Since the embeddings generally need to be sent across the network , the number of ( example , parameter ) interactions is limited by network bandwidth instead of computational capacity .
Depending on the scheme , loss terms maybe necessary to achieve the desired level of sparsity per-chunk and / or per example .
use three such terms .
These issues can affect both model quality and load - balancing .
Model capacity is most critical for very large data sets .
The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600,000 images .
It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters .
In this work , we for the first time address all of the above challenges and finally realize the promise of conditional computation .
We obtain greater than 1000x improvements in model capacity with only minor losses in computational efficiency and significantly advance the state - of - the - art results on public language modeling and translation data sets .
OUR APPROACH : THE SPARSELY - GATED MIXTURE - OF - EXPERTS LAYER
Our approach to conditional computation is to introduce a new type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .
The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .
All parts of the network are trained jointly by back - propagation .
While the introduced technique is generic , in this paper we focus on language modeling and machine translation tasks , which are known to benefit from very large models .
In particular , we apply a MoE convolutionally between stacked LSTM layers , as in .
The MoE is called once for each position in the text , selecting a potentially different combination of experts at each position .
The different experts tend to become highly specialized based on syntax and semantics ( see Appendix E ) .
On both language modeling and machine translation benchmarks , we improve on best published results at a fraction of the computational cost .
RELATED WORK ON MIXTURES OF EXPERTS
Since its introduction more than two decades ago , the mixture - of - experts approach has been the subject of much research .
Different types of expert architectures hae been proposed such as SVMs , Gaussian Processes , Dirichlet Processes , and deep networks .
Other work has focused on different expert configurations such as a hierarchical structure , infinite numbers of experts , and adding experts sequentially .
suggest an ensemble model in the format of mixture of experts for machine translation .
The gating network is trained on a pre-trained ensemble NMT model .
The works above concern top - level mixtures of experts .
The mixture of experts is the whole model .
introduce the idea of using multiple
MoEs with their own gating networks as parts of a deep model .
It is intuitive that the latter approach is more powerful , since complex problems may contain many sub-problems each requiring different experts .
They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation .
Our work builds on this use of MoEs as a general purpose neural network component .
While uses two stacked MoEs allowing for two sets of gating decisions , our convolutional application of the MoE allows for different gating decisions at each position in the text .
We also realize sparse gating and demonstrate its use as a practical way to massively increase model capacity .
THE STRUCTURE OF THE MIXTURE - OF - EXPERTS LAYER
The Mixture - of - Experts ( MoE ) layer consists of a set of n " expert networks " E 1 , , E n , and a " gating network " G whose output is a sparse n-dimensional vector .
shows an overview of the MoE module .
The experts are themselves neural networks , each with their own parameters .
Although in principle we only require that the experts accept the same sized inputs and produce the same - sized outputs , in our initial investigations in this paper , we restrict ourselves to the case where the models are feed - forward networks with identical architectures , but with separate parameters .
Let us denote by G ( x ) and E i ( x ) the output of the gating network and the output of the i - th expert network for a given input x .
The output y of the MoE module can be written as follows :
We save computation based on the sparsity of the output of G ( x ) .
Wherever G (x ) i = 0 , we need not compute E i ( x ) .
In our experiments , we have up to thousands of experts , but only need to evaluate a handful of them for every example .
If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of " experts " , each of which is itself a secondary mixture - of - experts with its own gating network .
In the following we focus on ordinary MoEs .
We provide more details on hierarchical MoEs in Appendix B.
Our implementation is related to other models of conditional computation .
A MoE whose experts are simple weight matrices is similar to the parameterized weight matrix proposed in .
A MoE whose experts have one hidden layer is similar to the block - wise dropout described in , where the dropped - out layer is sandwiched between fully - activated layers .
GATING NETWORK
Softmax Gating :
A simple choice of non-sparse gating function is to multiply the input by a trainable weight matrix W g and then apply the Sof tmax function .
Noisy Top - K
Gating :
We add two components to the Softmax gating network : sparsity and noise .
Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to ??
( which causes the corresponding gate values to equal 0 ) .
The sparsity serves to save computation , as described above .
While this form of sparsity creates some theoretically scary discontinuities in the output of gating function , we have not yet observed this to be a problem in practice .
The noise term helps with load balancing , as will be discussed in Appendix A .
The amount of noise per component is controlled by a second trainable weight matrix W noise .
Training the Gating Network
We train the gating network by simple back - propagation , along with the rest of the model .
If we choose k > 1 , the gate values for the top k experts have nonzero derivatives with respect to the weights of the gating network .
This type of occasionally - sensitive behavior is described in with respect to noisy rectifiers .
Gradients also backpropagate through the gating network to its inputs .
Our method differs here from who use boolean gates and a REINFORCE - style approach to train the gating network .
ADDRESSING PERFORMANCE
CHALLENGES
THE SHRINKING BATCH PROBLEM
On modern CPUs and GPUs , large batch sizes are necessary for computational efficiency , so as to amortize the overhead of parameter loads and updates .
If the gating network chooses k out of n experts for each example , then for a batch of b examples , each expert receives a much smaller batch of approximately kb n b examples .
This causes a naive MoE implementation to become very inefficient as the number of experts increases .
The solution to this shrinking batch problem is to make the original batch size as large as possible .
However , batch size tends to be limited by the memory necessary to store activations between the forwards and backwards passes .
We propose the following techniques for increasing the batch size :
Mixing Data Parallelism and Model Parallelism :
In a conventional distributed training setting , multiple copies of the model on different devices asynchronously process distinct batches of data , and parameters are synchronized through a set of parameter servers .
In our technique , these different batches run synchronously so that they can be combined for the MoE layer .
We distribute the standard layers of the model and the gating network according to conventional data - parallel schemes , but keep only one shared copy of each expert .
Each expert in the MoE layer receives a combined batch consisting of the relevant examples from all of the data - parallel input batches .
The same set of devices function as data - parallel replicas ( for the standard layers and the gating networks ) and as model - parallel shards ( each hosting a subset of the experts ) .
If the model is distributed over d devices , and each device processes a batch of size b , each expert receives a batch of approximately kbd n examples .
Thus , we achieve a factor of d improvement inexpert batch size .
In the case of a hierarchical MoE ( Section B ) , the primary gating network employs data parallelism , and the secondary MoEs employ model parallelism .
Each secondary MoE resides on one device .
This technique allows us to increase the number of experts ( and hence the number of parameters ) by proportionally increasing the number of devices in the training cluster .
The total batch size increases , keeping the batch size per expert constant .
The memory and bandwidth requirements per device also remain constant , as do the step times , as does the amount of time necessary to process a number of training examples equal to the number of parameters in the model .
It is our goal to train a trillionparameter model on a trillion - word corpus .
We have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware .
Taking Advantage of Convolutionality :
In our language models , we apply the same MoE to each time step of the previous layer .
If we wait for the previous layer to finish , we can apply the MoE to all the time steps together as one big batch .
Doing so increases the size of the input batch to the MoE layer by a factor of the number of unrolled time steps .
Increasing Batch Size for a
Recurrent MoE :
We suspect that even more powerful models may involve applying a MoE recurrently .
For example , the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly , such models break the convolutional trick from the last paragraph , since the input to the MoE atone timestep depends on the output of the MoE at the previous timestep .
Gruslys et al . ( 2016 ) describe a technique for drastically reducing the number of stored activations in an unrolled RNN , at the cost of recomputing forward activations .
This would allow for a large increase in batch size .
NETWORK BANDWIDTH
Another major performance concern in distributed computing is network bandwidth .
Since the experts are stationary ( see above ) and the number of gating parameters is small , most of the communication involves sending the inputs and outputs of the experts across the network .
To maintain computational efficiency , the ratio of an expert 's computation to the size of its input and output must exceed the ratio of computational to network capacity of the computing device .
For GPUs , this maybe thousands to one .
In our experiments , we use experts with one hidden layer containing thousands of RELU - activated units .
Since the weight matrices in the expert have sizes input_sizehidden_size and hidden_size output_size , the ratio of computation to input and output is equal to the size of the hidden layer .
Conveniently , we can increase computational efficiency simply by using a larger hidden layer , or more hidden layers .
BALANCING EXPERT UTILIZATION
We have observed that the gating network tends to converge to a state where it always produces large weights for the same few experts .
This imbalance is self - reinforcing , as the favored experts are trained more rapidly and thus are selected even more by the gating network .
describe the same phenomenon , and use a hard constraint at the beginning of training to avoid this local minimum .
include a soft constraint on the batch - wise average of each gate .
We take a soft constraint approach .
We define the importance of an expert relative to a batch of training examples to be the batchwise sum of the gate values for that expert .
We define an additional loss L importance , which is added to the over all loss function for the model .
This loss is equal to the square of the coefficient of variation of the set of importance values , multiplied by a hand - tuned scaling factor w importance .
This additional loss encourages all experts to have equal importance .
L importance ( X ) = w importance CV ( Importance ( X ) )
2 . The number of parameters in the LSTM layers of these models vary from 2 million to 151 million .
Quality increases greatly with parameter count , as do computational costs .
Results for these models form the top line of - right .
MoE Models :
Our models consist of two stacked LSTM layers with a MoE layer between them ( see ) .
We vary the sizes of the layers and the number of experts .
For full details on model architecture , training regimen , additional baselines and results , see Appendix C .
The results of these models are shown in - left .
The model with 4 always - active experts performed ( unsurprisingly ) similarly to the computationally - matched baseline models , while the largest of the models ( 4096 experts ) achieved an impressive 24 % lower perplexity on the test set .
Varied Computation , High Capacity :
In addition to the largest model from the previous section , we trained two more MoE models with similarly high capacity ( 4 billion parameters ) , but higher computation budgets .
These models had larger LSTMs , and fewer but larger and experts .
Details can be found in Appendix C.2 .
Results of these three models form the bottom line of - right .
compares the results of these models to the best previously - published result on this dataset .
Even the fastest of these models beats the best published result ( when controlling for the number of training epochs ) , despite requiring only 6 % of the computation .
Computational
Efficiency : We trained our models using TensorFlow on clusters containing 16 - 32 Tesla K40 GPUs .
For each of our models , we determine computational efficiency in TFLOPS / GPU by dividing the number of floating point operations required to process one training batch by the observed step time and the number of GPUs in the cluster .
The operation counts used here are higher than the ones we report in our ops / timestep numbers in that we include the backwards pass , we include the importance - sampling - based training of the softmax layer , and we count a multiply - and - add as two separate operations .
For all of our MoE models , the floating point operations involved in the experts represent between 37 % and 46 % of the total .
For our baseline models wtih no MoE , observed computational efficiency ranged from 1.07 - 1.29 TFLOPS / GPU .
For our low-computation MoE models , computation efficiency ranged from 0.74 - 0.90 TFLOPS / GPU , except for the 4 - expert model which did not make full use of the available parallelism .
Our highest - computation MoE model was more efficient at 1.56 TFLOPS / GPU , likely due to the larger matrices .
These numbers represent a significant fraction of the theoretical maximum of 4.29 TFLOPS / GPU claimed by NVIDIA .
Detailed results are in Appendix C , .
On the 1 - billion - word corpus , adding additional capacity seems to produce diminishing returns as the number of parameters in the MoE layer exceeds 1 billion , as can be seen in - left .
We hypothesized that for a larger training set , even higher capacities would produce significant quality improvements .
100 BILLION WORD GOOGLE NEWS CORPUS
We constructed a similar training set consisting of shuffled unique sentences from Google 's internal news corpus , totalling roughly 100 billion words .
Similarly to the previous section , we tested a series of models with similar computational costs of about 8 million ops / timestep .
In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts .
This corresponds to up to 137 billion parameters in the MoE layer .
Details on architecture , training , and results are given in Appendix D.
Results : shows test perplexity as a function of capacity after training on 10 billion words ( top line ) and 100 billion words ( bottom line ) .
When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity .
The widening gap between the two lines demonstrates ( unsurprisingly ) that increased model capacity helps more on larger training sets .
Even at 65536 experts ( 99.994 % layer sparsity ) , computational efficiency for the model stays at a respectable 0.72 TFLOPS / GPU .
MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )
Model Architecture :
Our model was a modified version of the GNMT model described in .
To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .
We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .
Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models .
Further details on model architecture , testing procedure and results can be found in Appendix E.
Datasets :
We benchmarked our method on the WMT ' 14 En? Fr and En ?
De corpora , whose training sets have 36M sentence pairs and 5 M sentence pairs , respectively .
The experimental protocols were also similar to those in : newstest2014 was used as the test set to compare against previous work , while the combination of newstest2012 and newstest2013 was used as the development set .
We also tested the same model on a Google 's Production English to French data . 2.79 39.22 214M 278M 6 days/96 k 80s GNMT+RL 2.96 39.92 214M 278M 6 days/96 k80s PBMT 37.0 LSTM ( 6-layer ) 31.5 LSTM ( 6-layer + PosUnk ) 33.1 DeepAtt 37.7 DeepAtt+PosUnk 39.2 5.25 24.91 214M 278M 1 day/96 k80s GNMT + RL 8.08 24.66 214M 278M 1 day/96 k80s PBMT 20.7 DeepAtt 20.6
Results : show the results of our largest models , compared with published results .
Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks .
As our models did not use RL refinement , these results constitute significant gains of 1.34 and 1.12 BLEU score on top of the strong baselines in .
The perplexity scores are also better .
2
On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time .
MULTILINGUAL MACHINE TRANSLATION
Results :
Results for the single - pair GNMT models , the multilingual GNMT model and the multilingual MoE model are given in .
The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model .
On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs .
The poor performance on English ?
Korean seems to be a result of severe overtraining , as for the rarer language pairs a small number of real examples were highly oversampled in the training corpus .
CONCLUSION
This work is the first to demonstrate major wins from conditional computation in deep networks .
We carefully identified the design considerations and challenges of conditional computing and addressed them with a combination of algorithmic and engineering solutions .
While we focused on text , conditional computation may help in other domains as well , provided sufficiently large training sets .
We look forward to seeing many novel implementations and applications of conditional computation in the years to come .
APPENDICES A LOAD - BALANCING LOSS
As discussed in section 4 , for load - balancing purposes , we want to define an additional loss function to encourage experts to receive roughly equal numbers of training examples .
Unfortunately , the number of examples received by an expert is a discrete quantity , so it can not be used in backpropagation .
Instead , we define a smooth estimator Load ( X ) of the number of examples assigned to each expert for a batch X of inputs .
The smoothness allows us to back - propagate gradients through the estimator .
This is the purpose of the noise term in the gating function .
We define P ( x , i ) as the probability that G (x ) i is nonzero , given a new random choice of noise on element i , but keeping the already - sampled choices of noise on the other elements .
To compute P ( x , i ) , we note that the G ( x ) i is nonzero if and only if H ( x ) i is greater than the k th - greatest element of H ( x ) excluding itself .
The probability works out to be :
Where kth_excluding ( v , k , i ) means the kth highest component of v , excluding component i .
Simplifying , we get :
Where ?
is the CDF of the standard normal distribution .
We can now define the load loss to be the square of the coefficient of variation of the load vector , multiplied by a hand - tuned scaling factor w load .
L load ( X ) = w load CV ( Load ( X ) )
2 ( 11 ) Initial Load Imbalance :
To avoid out - of - memory errors , we need to initialize the network in a state of approximately equal expert load ( since the soft constraints need sometime to work ) .
To accomplish this , we initialize the matrices W g and W noise to all zeros , which yields no signal and some noise .
Experiments :
We trained a set of models with identical architecture ( the MoE - 256 model described in Appendix C ) , using different values of w importance and w load .
We trained each model for 10 epochs , then measured perplexity on the test set .
We also measured the coefficients of variation in Importance and Load , as well as ratio of the load on the most overloaded expert to the average load .
This last value is significant for load balancing purposes on distributed hardware .
All of these metrics were averaged over several training batches .
Results :
Results are reported in .
All the combinations containing at least one the two losses led to very similar model quality , where having no loss was much worse .
Models with higher values of w load had lower loads on the most overloaded expert .
B HIERACHICAL MIXTURE OF EXPERTS
If the number of experts is very large , we can reduce the branching factor by using a two - level hierarchical MoE. In a hierarchical MoE , a primary gating network chooses a sparse weighted combination of " experts " , each of which is itself a secondary mixture - of - experts with its own gating network .
3
If the hierarchical MoE consists of a groups of b experts each , we denote the primary gating network by G primary , the secondary gating networks by , and the expert networks by ( E 0 , 0 , E 0 , 1 ..E a , b ) .
The output of the MoE is given by :
Our metrics of expert utilization change to the following :
Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively .
X ( i ) denotes the subset of X for which G primary ( x ) i >
0 .
It would seem simpler to let Load H ( X ) i , j = Load i ( X i ) j , but this would not have a gradient with respect to the primary gating network , so we use the formulation above .
C 1 BILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS
C.1 8- MILLION - OPERATIONS - PER - TIMESTEP MODELS
Model Architecture :
Our model consists of five layers : a word embedding layer , a recurrent Long Short - Term Memory ( LSTM ) layer , a MoE layer , a second LSTM layer , and a softmax layer .
The dimensionality of the embedding layer , the number of units in each LSTM layer , and the input and output dimensionality of the MoE layer are all equal to 512 .
For every layer other than the softmax , we apply drouput to the layer output , dropping each activation with probability DropP rob , otherwise dividing by ( 1 ? DropP rob ) .
After dropout , the output of the previous layer is added to the layer output .
This residual connection encourages gradient flow .
For the hierarchical MoE layers , the first level branching factor was 16 , corresponding to the number of GPUs in our cluster .
We use Noisy - Top - K Gating ( see Section 2.1 ) with k = 4 for the ordinary MoE layers and k = 2 at each level of the hierarchical MoE layers .
Thus , each example is processed by exactly 4 experts for a total of 4M ops / timestep .
The two LSTM layers contribute 2M ops / timestep each for the desired total of 8 M .
Computationally - Matched Baselines :
The MoE - 4 model does not employ sparsity , since all 4 experts are always used .
In addition , we trained four more computationally - matched baseline models with no sparsity :
MoE - 1 - Wide :
The MoE layer consists of a single " expert " containing one ReLU - activated hidden layer of size 4096 .
MoE - 1 - Deep :
The MoE layer consists of a single " expert " containing four ReLU - activated hidden layers , each with size 1024 .
4xLSTM - 512 : We replace the MoE layer with two additional 512 - unit LSTM layers .
LSTM - 2048-512 :
The model contains one 2048 - unit LSTM layer ( and no MoE ) .
The output of the LSTM is projected down to 512 dimensions .
The next timestep of the LSTM receives the projected output .
This is identical to one of the models published in .
We re-ran it to account for differences in training regimen , and obtained results very similar to the published ones .
Training :
The models were trained on a cluster of 16 K40 GPUs using the synchronous method described in Section 3 .
Each batch consisted of a set of sentences totaling roughly 300,000 words .
In the interest of time , we limited training to 10 epochs , ( 27,000 steps ) .
Training took 12 - 16 hours for all models , except for MoE - 4 , which took 18 hours ( since all the expert computation was performed on only 4 of 16 GPUs ) .
We used the Adam optimizer .
The base learning rate was increased linearly for the first 1000 training steps , and decreased after that so as to be proportional to the inverse square root of the step number .
The Softmax output layer was trained efficiently using importance sampling similarly to the models in .
For each model , we performed a hyper - parmeter search to find the best dropout probability , in increments of 0.1 .
To ensure balanced expert utilization we set w importance = 0.1 and w load = 0.1 , as described in Section 4 and Appendix A.
Results :
We evaluate our model using perplexity on the holdout dataset , used by .
We follow the standard procedure and sum over all the words including the end of sentence symbol .
Results are reported in .
For each model , we report the test perplexity , the computational budget , the parameter counts , the value of DropP rob , and the computational efficiency .
We implement several memory optimizations in order to fit up to 1 billion parameters per GPU .
First , we do not store the activations of the hidden layers of the experts , but instead recompute them on the backwards pass .
Secondly , we modify the optimizer on the expert parameters to require less auxiliary storage :
The Adam optimizer keeps first and second moment estimates of the perparameter gradients .
This triples the required memory .
To avoid keeping a first - moment estimator , we set ?
1 = 0 . To reduce the size of the second moment estimator , we replace it with a factored approximation .
For a matrix of parameters , instead of maintaining a full matrix of second - moment estimators , we maintain vectors of row - wise and column - wise averages of that matrix .
At each step , the matrix of estimators is taken to be the outer product of those two vectors divided by the mean of either one .
This technique could similarly be applied to Adagrad .
Results :
We evaluate our model using perplexity on a holdout dataset .
Results are reported in .
Perplexity after 100 billion training words is 39 % lower for the 68 - billion - parameter MoE model than for the baseline model .
It is notable that the measured computational efficiency of the largest model ( 0.30 TFLOPS / GPU ) is very low compared to the other models .
This is likely a result of the fact that , for purposes of comparison to the other models , we did not increase the training batch size proportionally to the number of GPUs .
For comparison , we include results for a computationally matched baseline model consisting of 4 LSTMs , and for an unpruned 5 - gram model with Kneser - Ney smoothing .
4
E MACHINE TRANSLATION - EXPERIMENTAL DETAILS
Model Architecture for Single Language
Pair MoE Models :
Our model is a modified version of the GNMT model described in .
To reduce computation , we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .
We insert MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .
We use an attention mechanism between the encoder and decoder , with the first decoder LSTM receiving output from and providing input for the attention 5 .
All of the layers in our model have input and output dimensionality of 512 .
Our LSTM layers have 2048 hidden units , with a 512 - dimensional output projection .
We add residual connections around all LSTM and MoE layers to encourage gradient flow .
Similar to GNMT , to effectively deal with rare words , we used subword units ( also known as " wordpieces " )
( Schuster & Nakajima , 2012 ) for inputs and outputs in our system .
We use a shared source and target vocabulary of 32 K wordpieces .
We also used the same beam search technique as proposed in Model Architecture for Multilingual MoE Model :
We used the same model architecture as for the single - language - pair models , with the following exceptions :
We used noisy - top - k gating as described in Section 2.1 , not the scheme from Appendix F. The MoE layers in the encoder and decoder are non-hierarchical MoEs with n = 512 experts , and k = 2 .
Each expert has a larger hidden layer of size 8192 .
This doubles the amount of computation in the MoE layers , raising the computational budget of the entire model from 85 M to 102M ops / timestep .
Training :
We trained our networks using the Adam optimizer .
The base learning rate was increased linearly for the first 2000 training steps , held constant for an additional 8000 steps , and decreased after that so as to be proportional to the inverse square root of the step number .
For the single - language - pair models , similarly to , we applied dropout to the output of all embedding , LSTM and MoE layers , using DropP rob = 0.4 .
Training was done synchronously on a cluster of up to 64 GPUs as described in section 3 .
Each training batch consisted of a set of sentence pairs containing roughly 16000 words per GPU .
To ensure balanced expert utilization we set w importance = 0.01 and w load = 0.01 , as described in Section 4 and Appendix A.
Metrics : We evaluated our models using the perplexity and the standard BLEU score metric .
We reported tokenized BLEU score as computed by the multi -bleu.pl script , downloaded from the public implementation of Moses ( on Github ) , which was also used in .
Results : and 4 in Section 5.3 show comparisons of our results to other published methods .
shows test perplexity as a function of number of words in the ( training data 's ) source sentences processed for models with different numbers of experts .
As can be seen from the as we increased the number of experts to approach 2048 , the test perplexity of our model continued to improve .
We found that the experts indeed become highly specialized by syntax and / or semantics , as can be seen in .
For example , one expert is used when the indefinite article " a " introduces the direct object in a verb phrase indicating importance or leadership .
F STRICTLY BALANCED GATING
Due to some peculiarities in our infrastructure which have since been fixed , at the time we ran some of the machine translation experiments , our models ran faster if every expert received exactly the same batch size .
To accommodate this , we used a different gating function which we describe below .
Recall that we define the softmax gating function to be :
Sparse Gating ( alternate formulation ) :
To obtain a sparse gating vector , we multiply G ?
( x ) component - wise with a sparse mask M ( G ? ( x ) ) and normalize the output .
The mask itself is a function of G ? ( x ) and specifies which experts are assigned to each input example : M batchwise ( X , m ) j , i = 1 if X j, i is in the top m values for to expert i 0 otherwise
As our experiments suggest and also observed in , using a batchwise function during training ( such as M batchwise ) requires modifications to the inference when we may not have a large batch of examples .
Our solution to this is to train a vector T of per-expert threshold values to approximate the effects of the batchwise mask .
We use the following mask at inference time :
To learn the threshold values , we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical .
L batchwise ( X , T , m ) = | X | j=1 n i=1 ( M threshold ( x , T ) i ?
M batchwise ( X , m ) j, i ) ( X j , i ? Ti ) ( 20 )
G ATTENTION FUNCTION
The attention mechanism described in GNMT involves a learned " Attention Function " A ( x i , y j ) which takes a " source vector " x i and a " target vector " y j , and must be computed for every source time step i and target time step j .
In GNMT , the attention function is implemented as a feed forward neural network with a hidden layer of size n.
It can be expressed as :
Where U and Ware trainable weight matrices and V is a trainable weight vector .
For performance reasons , in our models , we used a slightly different attention function :
With our attention function , we can simultaneously compute the attention function on multiple source time steps and multiple target time steps using optimized matrix multiplications .
We found little difference in quality between the two functions .
