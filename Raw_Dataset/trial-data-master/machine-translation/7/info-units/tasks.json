{
  "has" : {
    "Tasks" : {
      "has" : {
        "100 BILLION WORD GOOGLE NEWS CORPUS" : {
          "has" : {
            "Hyperparameters" : {
              "MoE layers" : "32 , experts",
              "from sentence" : "100 BILLION WORD GOOGLE NEWS CORPUS
              In addition to a baseline LSTM model , we trained models augmented with MoE layers containing 32 , experts ."
              
            },
            "Results" : {
              "When" : {
                "training over the full 100 billion words" : {
                  "test perplexity" : {
                    "improves significantly" : {
                      "up to" : "65536 experts ( 68 billion parameters )"
                    }
                  }
                },
                "from sentence" : "When training over the full 100 billion words , test perplexity improves significantly up to 65536 experts ( 68 billion parameters ) , dropping 39 % lower than the computationally matched baseline , but degrades at 131072 experts , possibly a result of too much sparsity ."
              }
            }
          }
        },
        "MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )" : {
          "has" : {
            "Hyperparameters" : {
              "model" : {
                "GNMT model" : {
                  "was a" : "modified version"
                },
                "from sentence" : "MACHINE TRANSLATION ( SINGLE LANGUAGE PAIR )
Our model was a modified version of the GNMT model described in ."

              },
              "decreased" : {
                "LSTM layers" : {
                  "in" : {
                    "encoder and decoder" : {
                      "from" : "9 and 8 to 3 and 2 respectively"
                    }
                  },
                  "To reduce" : "computation"
                },
                "from sentence" : "To reduce computation , we decreased the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively ."
              },
              "inserted" : {
                "MoE layers" : {
                  "in both" : {
                    "encoder" : {
                      "between" : "layers 2 and 3"
                    },
                    "decoder" : {
                      "between" : "layers 1 and 2"
                    }
                  }
                },
                "from sentence" : "We inserted MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) ."
              },
              "Each" : {
                "MoE layer" : {
                  "contained up to" : {
                    "2048 experts each" : {
                      "with" : "about two million parameters"
                    }
                  }
                },
                "from sentence" : "Each MoE layer contained up to 2048 experts each with about two million parameters , adding a total of about 8 billion parameters to the models ."
              }
            },
            "Results" : {
              "BLEU scores" : {
                "40.56 and 26.03" : {
                  "on" : "WMT ' 14 En?Fr and En ? De benchmarks"
                },
                "from sentence" : "Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT ' 14 En?Fr and En ? De benchmarks ."                
              },
              "test BLEU score" : {
                "1.01 higher" : {
                  "On" : "Google Production dataset"
                },
                "from sentence" : "On the Google Production dataset , our model achieved 1.01 higher test BLEU score even after training for only one sixth of the time ."
              }
            }
          }
        },
        "MULTILINGUAL MACHINE TRANSLATION" : {
          "has" : {
            "Results" : {
              "on" : {
                "dev set" : {
                  "achieves" : {
                    "19 % lower perplexity" : {
                      "than" : "multilingual GNMT model"
                    }
                  }
                },
                "from sentence" : "MULTILINGUAL MACHINE TRANSLATION
The MoE model achieves 19 % lower perplexity on the dev set than the multilingual GNMT model ."

              },
              "On" : {
                "BLEU score" : {
                  "significantly beats" : {
                    "multilingual GNMT model" : {
                      "on" : {
                        "11 of the 12 language pairs" : {
                          "by as much as" : "5.84 points"
                        }
                      }
                    }
                  },
                  "even beats" : {
                    "monolingual GNMT models" : {
                      "on" : "8 of 12 language pairs"
                    }
                  }
                },
                "from sentence" :  "On BLEU score , the MoE model significantly beats the multilingual GNMT model on 11 of the 12 language pairs ( by as much as 5.84 points ) , and even beats the monolingual GNMT models on 8 of 12 language pairs ."
              }
            }
          }
        }
      }
    }
  }
}