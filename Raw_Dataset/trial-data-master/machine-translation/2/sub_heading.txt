title
abstract
Introduction
Background
Model Architecture
Encoder and Decoder Stacks
Attention
Position-wise Feed-Forward Networks
Embeddings and Softmax
Positional Encoding
Why Self-Attention
Training
Training Data and Batching
Hardware and Schedule
Optimizer
Regularization
Results
Machine Translation
Model Variations
English Constituency Parsing
Conclusion
