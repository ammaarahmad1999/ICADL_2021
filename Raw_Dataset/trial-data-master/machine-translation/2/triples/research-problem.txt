(Contribution||has research problem||attention mechanisms)
(Contribution||has research problem||push the boundaries of recurrent language models and encoder - decoder architectures)
(Contribution||has research problem||model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output)
