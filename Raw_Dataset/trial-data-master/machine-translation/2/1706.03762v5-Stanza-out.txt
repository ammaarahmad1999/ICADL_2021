title
Attention Is All You Need
abstract
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .
The best performing models also connect the encoder and decoder through an attention mechanism .
We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .
Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train .
Our model achieves 28.4 BLEU on the WMT 2014 Englishto - German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .
On the WMT 2014 English - to - French translation task , our model establishes a new single - model state - of - the - art BLEU score of 41.8 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .
We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data .
* Equal contribution .
Listing order is random .
Jakob proposed replacing RNNs with self - attention and started the effort to evaluate this idea .
Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .
Noam proposed scaled dot-product attention , multi-head attention and the parameter - free position representation and became the other person involved in nearly every detail .
Niki designed , implemented , tuned and evaluated countless model variants in our original codebase and tensor2tensor .
Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .
Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .
Work performed while at Google Brain .
Introduction
Recurrent neural networks , long short - term memory and gated recurrent neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation .
Numerous efforts have since continued to push the boundaries of recurrent language models and encoder - decoder architectures .
Recurrent models typically factor computation along the symbol positions of the input and output sequences .
Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state h t?1 and the input for position t.
This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .
Recent work has achieved significant improvements in computational efficiency through factorization tricks and conditional computation , while also improving model performance in case of the latter .
The fundamental constraint of sequential computation , however , remains .
Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences .
In all but a few cases , however , such attention mechanisms are used in conjunction with a recurrent network .
In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .
The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .
Background
The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU , ByteNet and ConvS2S , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .
In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .
This makes it more difficult to learn dependencies between distant positions .
In the Transformer this is reduced to a constant number of operations , albeit at the cost of reduced effective resolution due to averaging attention - weighted positions , an effect we counteract with Multi - Head Attention as described in section 3.2 .
Self - attention , sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .
Self - attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task - independent sentence representations .
End - to - end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple - language question answering and language modeling tasks .
To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self - attention to compute representations of its input and output without using sequencealigned RNNs or convolution .
In the following sections , we will describe the Transformer , motivate self - attention and discuss its advantages over models such as and .
Model Architecture
Most competitive neural sequence transduction models have an encoder - decoder structure .
Here , the encoder maps an input sequence of symbol representations ( x 1 , ... , x n ) to a sequence of continuous representations z = ( z 1 , ... , z n ) .
Given z , the decoder then generates an output sequence ( y 1 , ... , y m ) of symbols one element at a time .
At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next .
The Transformer follows this over all architecture using stacked self - attention and point - wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of
Encoder and Decoder Stacks
Encoder :
The encoder is composed of a stack of N = 6 identical layers .
Each layer has two sub-layers .
The first is a multi-head self - attention mechanism , and the second is a simple , positionwise fully connected feed - forward network .
We employ a residual connection around each of the two sub-layers , followed by layer normalization .
That is , the output of each sub - layer is LayerNorm ( x + Sublayer ( x ) ) , where Sublayer ( x ) is the function implemented by the sub - layer itself .
To facilitate these residual connections , all sub- layers in the model , as well as the embedding layers , produce outputs of dimension d model = 512 .
Decoder :
The decoder is also composed of a stack of N = 6 identical layers .
In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs multi-head attention over the output of the encoder stack .
Similar to the encoder , we employ residual connections around each of the sub-layers , followed by layer normalization .
We also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions .
This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i.
Attention
An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .
The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .
Scaled Dot - Product Attention
We call our particular attention " Scaled Dot -Product Attention " ) .
The input consists of queries and keys of dimension d k , and values of dimension d v .
We compute the dot products of the query with all keys , divide each by ? d k , and apply a softmax function to obtain the weights on the values .
In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .
The keys and values are also packed together into matrices K and V .
We compute the matrix of outputs as :
The two most commonly used attention functions are additive attention , and dot-product ( multiplicative ) attention .
Dot-product attention is identical to our algorithm , except for the scaling factor of 1
Additive attention computes the compatibility function using a feed - forward network with a single hidden layer .
While the two are similar in theoretical complexity , dot-product attention is much faster and more space - efficient in practice , since it can be implemented using highly optimized matrix multiplication code .
While for small values of d k the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of d k.
We suspect that for large values of d k , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients
4 .
To counteract this effect , we scale the dot products by 1
Multi - Head Attention
Instead of performing a single attention function with d model - dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to d k , d k and d v dimensions , respectively .
On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding d v - dimensional output values .
These are concatenated and once again projected , resulting in the final values , as depicted in .
Multi - head attention allows the model to jointly attend to information from different representation subspaces at different positions .
With a single attention head , averaging inhibits this .
Where the projections are parameter matrices
In this work we employ h = 8 parallel attention layers , or heads .
For each of these we use
Due to the reduced dimension of each head , the total computational cost is similar to that of single - head attention with full dimensionality .
Applications of Attention in our Model
The Transformer uses multi-head attention in three different ways :
In " encoder - decoder attention " layers , the queries come from the previous decoder layer , and the memory keys and values come from the output of the encoder .
This allows every position in the decoder to attend over all positions in the input sequence .
This mimics the typical encoder - decoder attention mechanisms in sequence - to - sequence models such as . The encoder contains self - attention layers .
In a self - attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .
Each position in the encoder can attend to all positions in the previous layer of the encoder .
Similarly , self - attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .
We need to prevent leftward information flow in the decoder to preserve the auto - regressive property .
We implement this inside of scaled dot-product attention by masking out ( setting to ?? ) all values in the input of the softmax which correspond to illegal connections .
See.
Position - wise Feed - Forward Networks
In addition to attention sub - layers , each of the layers in our encoder and decoder contains a fully connected feed - forward network , which is applied to each position separately and identically .
This consists of two linear transformations with a ReLU activation in between .
While the linear transformations are the same across different positions , they use different parameters from layer to layer .
Another way of describing this is as two convolutions with kernel size
1 .
The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality d ff = 2048 .
Embeddings and Softmax
Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model .
We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next - token probabilities .
In our model , we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation , similar to .
In the embedding layers , we multiply those weights by ? d model .
Positional Encoding
Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the : Maximum path lengths , per-layer complexity and minimum number of sequential operations for different layer types .
n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self - attention .
Layer Type Complexity per Layer Sequential Maximum Path Length Operations
tokens in the sequence .
To this end , we add " positional encodings " to the input embeddings at the bottoms of the encoder and decoder stacks .
The positional encodings have the same dimension d model as the embeddings , so that the two can be summed .
There are many choices of positional encodings , learned and fixed .
In this work , we use sine and cosine functions of different frequencies : where pos is the position and i is the dimension .
That is , each dimension of the positional encoding corresponds to a sinusoid .
The wavelengths form a geometric progression from 2 ? to 10000 2 ?.
We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PE pos+k can be represented as a linear function of PE pos .
We also experimented with using learned positional embeddings instead , and found that the two versions produced nearly identical results ( see row ( E ) ) .
We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .
Why Self - Attention
In this section we compare various aspects of self - attention layers to the recurrent and convolutional layers commonly used for mapping one variable - length sequence of symbol representations ( x 1 , ... , x n ) to another sequence of equal length ( z 1 , ... , z n ) , with x i , z i ?
Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .
Motivating our use of self - attention we consider three desiderata .
One is the total computational complexity per layer .
Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .
The third is the path length between long - range dependencies in the network .
Learning long - range dependencies is a key challenge in many sequence transduction tasks .
One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .
The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long - range dependencies .
Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types .
As noted in , a self - attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O ( n ) sequential operations .
In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece and byte - pair representations .
To improve computational performance for tasks involving very long sequences , self - attention could be restricted to considering only a neighborhood of sizer in the input sequence centered around the respective output position .
This would increase the maximum path length to O ( n / r ) .
We plan to investigate this approach further in future work .
A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions .
Doing so requires a stack of O ( n / k ) convolutional layers in the case of contiguous kernels , or O ( log k ( n ) ) in the case of dilated convolutions , increasing the length of the longest paths between any two positions in the network .
Convolutional layers are generally more expensive than recurrent layers , by a factor of k.
Separable convolutions , however , decrease the complexity considerably , to O ( k n d + n d 2 ) .
Even with k = n , however , the complexity of a separable convolution is equal to the combination of a self - attention layer and a point - wise feed - forward layer , the approach we take in our model .
As side benefit , self - attention could yield more interpretable models .
We inspect attention distributions from our models and present and discuss examples in the appendix .
Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .
Training
This section describes the training regime for our models .
Training Data and Batching
We trained on the standard WMT 2014 English - German dataset consisting of about 4.5 million sentence pairs .
Sentences were encoded using byte - pair encoding , which has a shared sourcetarget vocabulary of about 37000 tokens .
For English - French , we used the significantly larger WMT 2014 English - French dataset consisting of 36M sentences and split tokens into a 32000 word - piece vocabulary .
Sentence pairs were batched together by approximate sequence length .
Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .
Hardware and Schedule
We trained our models on one machine with 8 NVIDIA P100 GPUs .
For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .
We trained the base models for a total of 100,000 steps or 12 hours .
For our big models , ( described on the bottom line of table 3 ) , step time was 1.0 seconds .
The big models were trained for 300,000 steps ( 3.5 days ) .
Optimizer
We used the Adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9 .
We varied the learning rate over the course of training , according to the formula :
This corresponds to increasing the learning rate linearly for the first warmup_steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .
We used warmup_steps = 4000 .
Regularization
We employ three types of regularization during training :
Residual Dropout
We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .
In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .
For the base model , we use a rate of P drop = 0.1 .
Label Smoothing
During training , we employed label smoothing of value ls = 0.1 .
This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .
Results
Machine Translation
On the WMT 2014 English - to - German translation task , the big transformer model ( Transformer ( big ) in ) outperforms the best previously reported models ( including ensembles ) by more than 2.0 BLEU , establishing a new state - of - the - art BLEU score of 28.4 .
The configuration of this model is listed in the bottom line of .
Training took 3.5 days on 8 P100 GPUs .
Even our base model surpasses all previously published models and ensembles , at a fraction of the training cost of any of the competitive models .
On the WMT 2014 English - to - French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 / 4 the training cost of the previous state - of - the - art model .
The Transformer ( big ) model trained for English - to - French used dropout rate P drop = 0.1 , instead of 0.3 .
For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 - minute intervals .
For the big models , we averaged the last 20 checkpoints .
We used beam search with a beam size of 4 and length penalty ? = 0.6 .
These hyperparameters were chosen after experimentation on the development set .
We set the maximum output length during inference to input length + 50 , but terminate early when possible .
summarizes our results and compares our translation quality and training costs to other model architectures from the literature .
We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single - precision floating - point capacity of each GPU 5 .
Model Variations
To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English - to - German translation on the development set , newstest2013 .
We used beam search as described in the previous section , but no checkpoint averaging .
We present these results in .
In rows ( A ) , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .
While single - head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .
In rows ( B ) , we observe that reducing the attention key size d k hurts model quality .
This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product maybe beneficial .
We further observe in rows ( C ) and ( D ) that , as expected , bigger models are better , and dropout is very helpful in avoiding over-fitting .
In row ( E ) we replace our sinusoidal positional encoding with learned positional embeddings , and observe nearly identical results to the base model .
English Constituency Parsing
To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing .
This task presents specific challenges : the output is subject to strong structural constraints and is significantly longer than the input .
Furthermore , RNN sequence - to - sequence models have not been able to attain state - of - the - art results in small - data regimes .
We trained a 4 - layer transformer with d model = 1024 on the Wall Street Journal ( WSJ ) portion of the Penn Treebank , about 40 K training sentences .
We also trained it in a semi-supervised setting , using the larger high - confidence and BerkleyParser corpora from with approximately 17M sentences .
We used a vocabulary of 16 K tokens for the WSJ only setting and a vocabulary of 32 K tokens for the semi-supervised setting .
We performed only a small number of experiments to select the dropout , both attention and residual ( section 5.4 ) , learning rates and beam size on the Section 22 development set , all other parameters remained unchanged from the English - to - German base translation model .
During inference , we increased the maximum output length to input length + 300 .
We used a beam size of 21 and ? = 0.3 for both WSJ only and the semi-supervised setting .
Our results in show that despite the lack of task - specific tuning our model performs surprisingly well , yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar .
In contrast to RNN sequence - to - sequence models , the Transformer outperforms the Berkeley - Parser even when training only on the WSJ training set of 40K sentences .
Conclusion
In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder - decoder architectures with multi-headed self - attention .
For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .
On both WMT 2014 English - to - German and WMT 2014 English - to - French translation tasks , we achieve a new state of the art .
In the former task our best model outperforms even all previously reported ensembles .
We are excited about the future of attention - based models and plan to apply them to other tasks .
We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .
Making generation less sequential is another research goals of ours .
The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.
