(Contribution||has||Experimental setup)
(Experimental setup||in||recurrent computation)
(recurrent computation||used||larger learning rate)
(larger learning rate||l r||5 10 ? 4)
(Experimental setup||use||word embeddings)
(word embeddings||for||both the source and target languages)
(word embeddings||is||256 dimensional)
(Experimental setup||use||4 ? 8 GPU machines)
(4 ? 8 GPU machines||running for||10 days)
(4 ? 8 GPU machines||to train||full model with parallelization at the data batch level)
(Experimental setup||dropout ratio||0.1)
(Experimental setup||have||512 memory cells)
(512 memory cells||has||LSTM layers)
(LSTM layers||including||2n e layers in the encoder and then d layers in the decoder)
(Experimental setup||For||each LSTM layer)
(each LSTM layer||activation functions for gates , inputs and outputs||sigmoid , tanh , and tanh)
(Experimental setup||for||feed - forward computation)
(feed - forward computation||used||smaller learning rate)
(smaller learning rate||l f||4 10 ? 5)
