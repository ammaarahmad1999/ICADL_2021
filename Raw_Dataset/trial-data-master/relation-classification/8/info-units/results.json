{
  "has" : {
    "Results" : {
      "on" : {
        "ACE 2005" : {
          "first observation" : {
            "our model architecture" : {
              "achieves" : {
                "much better results" : {
                  "compared to" : "previous state - of - the - art methods"
                }
              }
            },
            "from sentence" : "Results on ACE 2005
The first observation is that our model architecture achieves much better results compared to the previous state - of - the - art methods ."

          },
          "has" : {
            "Our full model" : {
              "with" : {
                "structured fine - tuning of attention layers" : {
                  "brings" : {
                    "further improvement" : { 
                      "of" : "about 5.5 %",
                      "in" : "MRE one - pass setting"
                    }
                  }
                }
              },
              "from sentence" : "Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation ."              
            }
          }
        },
        "SemEval 2018 Task 7" : {
          "has" : {
            "Our Entity - Aware BERT SP" : {
              "gives comparable results to" : {
                "top - ranked system in the shared task" : {
                  "with" : "slightly lower Macro - F1"
                },
                "from sentence" : "The results on SemEval 2018 Task 7 are shown in .
Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 ."

              },
              "When predicting multiple relations in one - pass" : {
                "0.9 % drop" : {
                  "on" : "Macro - F1"
                },
                "from sentence" : "When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 ."                
              }
            }  
          },
          "compared to" : {
            "top singlemodel result" : {
              "which makes use of" : {
                "additional word and entity embeddings" : {
                  "pretrained on" : "in - domain data"  
                }
              },
              "demonstrate" : {
                "our methods" : {
                  "clear advantage as a" : "single model"
                }
              }
            },
            "from sentence" : "On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model ."
          }
        }
      }
    }
  }
}