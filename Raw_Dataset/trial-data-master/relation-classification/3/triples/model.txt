(Contribution||has||Model)
(Model||use||character level embeddings)
(character level embeddings||to implicitly capture||morphological features ( e.g. , prefixes and suffixes ))
(morphological features ( e.g. , prefixes and suffixes )||representing each||character by a vector ( embedding ))
(Model||use||pre-trained word embeddings)
(Model||For||NER task)
(NER task||adopt||BIO ( Beginning , Inside , Outside ) encoding scheme)
(Model||model||relation extraction task)
(relation extraction task||as a||multi-label head selection problem)
(Model||has||Adversarial training ( AT ))
(Adversarial training ( AT )||exploit||idea of AT)
(idea of AT||as a||regularization method)
(regularization method||to make||our model robust to input perturbations)
(Model||has||input)
(input||is a||sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n)
(Model||has||character embeddings)
(character embeddings||fed to||bidirectional LSTM ( BiLSTM ))
(bidirectional LSTM ( BiLSTM )||to obtain||character - based representation of the word)
(Model||has||Word and character embeddings)
(Word and character embeddings||concatenated to form||final token representation)
(final token representation||then fed to||BiLSTM layer)
(BiLSTM layer||to extract||sequential information)
(Model||has||baseline model)
(baseline model||aims to detect||type and the boundaries of the entities)
(baseline model||aims to detect||relations between them)
