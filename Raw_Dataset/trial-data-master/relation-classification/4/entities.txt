179	92	94	on
179	99	113	TACRED dev set
180	3	7	find
180	19	64	entity representations and feedforward layers
180	65	75	contribute
180	76	83	1.0 F 1
181	14	20	remove
181	25	45	dependency structure
181	76	90	score drops by
181	91	98	3.2 F 1
182	43	122	feedforward layers , the LSTM component and the dependency structure altogether
182	6	18	F 1 drops by
182	19	23	10.3
183	6	14	Removing
183	19	26	pruning
183	36	41	using
183	42	61	full trees as input
183	72	91	hurts the result by
183	92	107	another 9.7 F 1
124	0	25	Dependency - based models
126	8	45	logistic regression ( LR ) classifier
126	52	60	combines
126	61	85	dependencybased features
126	86	90	with
126	91	113	other lexical features
127	6	50	Shortest Dependency Path LSTM ( SDP - LSTM )
127	59	66	applies
127	69	90	neural sequence model
127	91	93	on
127	98	111	shortest path
127	112	119	between
127	124	151	subject and object entities
127	152	154	in
127	159	174	dependency tree
128	0	11	Tree - LSTM
128	20	24	is a
128	25	40	recursive model
128	41	57	that generalizes
128	62	95	LSTM to arbitrary tree structures
132	0	21	Neural sequence model
133	10	19	presented
133	22	48	competitive sequence model
133	49	61	that employs
133	64	100	position - aware attention mechanism
133	101	105	over
133	106	132	LSTM outputs ( PA - LSTM )
133	151	165	it outperforms
133	166	207	several CNN and dependency - based models
133	208	210	by
133	213	231	substantial margin
29	10	17	encodes
29	22	42	dependency structure
29	43	47	over
29	52	66	input sentence
29	67	71	with
29	72	110	efficient graph convolution operations
29	113	126	then extracts
29	127	159	entity - centric representations
29	160	167	to make
29	168	195	robust relation predictions
30	8	13	apply
30	16	54	novel path - centric pruning technique
30	55	64	to remove
30	65	101	irrelevant information from the tree
30	102	125	while maximally keeping
30	126	142	relevant content
2	56	75	Relation Extraction
4	49	93	capture long - range relations between words
18	71	111	capture long - range syntactic relations
149	8	10	on
149	15	29	TACRED Dataset
151	3	10	observe
151	16	29	our GCN model
151	66	77	outperforms
151	78	107	all dependency - based models
151	108	110	by
151	111	127	at least 1.6 F 1
152	3	8	using
152	9	44	contextualized word representations
152	51	64	C - GCN model
152	65	84	further outperforms
152	89	111	strong PA - LSTM model
152	112	114	by
152	115	122	1.3 F 1
153	32	40	improves
153	41	74	upon other dependencybased models
153	75	82	in both
153	83	103	precision and recall
154	0	9	Comparing
154	14	27	C - GCN model
154	28	32	with
154	37	46	GCN model
154	52	61	find that
154	66	70	gain
154	71	88	mainly comes from
154	89	104	improved recall
156	36	45	find that
156	46	60	our GCN models
156	61	65	have
156	66	89	complementary strengths
156	90	106	when compared to
156	111	120	PA - LSTM
163	15	30	SemEval Dataset
165	13	18	under
165	23	59	conventional with- entity evaluation
165	66	79	C - GCN model
165	80	91	outperforms
165	92	137	all existing dependency - based neural models
166	61	82	our model outperforms
166	87	149	previous shortest dependency path - based model ( SDP - LSTM )
166	10	35	by properly incorporating
166	36	58	off - path information
167	0	5	Under
167	10	34	mask - entity evaluation
167	41	54	C - GCN model
167	55	71	also outperforms
167	72	81	PA - LSTM
167	82	84	by
167	87	105	substantial margin
168	0	32	Effect of Path - centric Pruning
169	57	64	compare
169	69	83	two GCN models
169	92	103	Tree - LSTM
169	104	108	when
169	113	131	pruning distance K
169	132	134	is
169	135	141	varied
171	18	32	performance of
171	33	49	all three models
171	50	60	peaks when
171	61	66	K = 1
171	69	82	outperforming
171	89	145	respective dependency path - based counterpart ( K = 0 )
175	3	7	find
175	13	29	all three models
175	30	33	are
175	34	48	less effective
175	49	53	when
175	58	80	entire dependency tree
175	81	83	is
175	84	91	present
176	23	38	contextualizing
176	43	46	GCN
176	47	55	makes it
176	56	70	less sensitive
176	71	84	to changes in
176	89	113	tree structures provided
