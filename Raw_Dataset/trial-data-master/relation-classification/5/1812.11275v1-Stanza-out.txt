title
End - to - end neural relation extraction using deep biaffine attention
abstract
We propose a neural network model for joint extraction of named entities and relations between them , without any hand - crafted features .
The key contribution of our model is to extend a BiLSTM - CRF - based entity recognition model with a deep biaffine attention layer to model second - order interactions between latent features for relation classification , specifically attending to the role of an entity in a directional relationship .
On the benchmark " relation and entity recognition " dataset CoNLL04 , experimental results show that our model outperforms previous models , producing new state - of - the - art performances .
Introduction
Extracting entities and their semantic relations from raw text is a key information extraction task .
For example , given the sentence " David Foster is the AP 's Northwest regional reporter , based in Seattle " in the CoNLL04 dataset , our goal is to recognize " David Foster " as person , " AP " as organization , and " Northwest " and " Seattle " as location entities , then classifiy entity pairs to extract structured information : Work For ( David Foster , AP ) , OrgBased In ( AP , Northwest ) and OrgBased In ( AP , Seattle ) .
Such information is useful in many other NLP tasks .
Especially in IR applications such as entity search , structured search and question answering , it helps provide end users with significantly better search experience .
A common relation extraction approach is to construct pipeline systems with separate sub-systems for the two tasks of named entity recognition and relation classification .
More recently , end - to - end systems which jointly learn to extract entities and relations have been proposed with strong potential to obtain high performance .
Traditional joint approaches are feature - based supervised learning methods which employ numerous syntactic and lexical features based on external NLP tools as well as knowledge base resources .
State - of - the - art relation extraction performance has been obtained by end - to - end models based on neural networks .
Specifically , proposed a RNNbased model which achieved top results on the CoNLL04 dataset .
Their approach relies on various manually extracted features .
Other neural models employ dependency parsing - based information .
In particular , applied bottom - up and top - down tree - structured LSTMs to model dependency paths between entities .
integrated implicit syntactic information by using latent feature representations extracted from a pre-trained BiLSTM - based dependency parser .
entity recognition , and a CNN on top of the BiLSTM for classifying relations .
Adel and Schtze ( 2017 ) assumed that entity boundaries are given , and trained a CNN to extract context features around the entities , and using these features for entity and relation classification .
Recently , formulated the joint entity and relation extraction problem as a directed graph and proposed a BiLSTM - and transition - based approach to generate the graph incrementally . [ 4 ] extended the multi-head selection - based joint model with adversarial training .
In , the joint task is formulated as a sequence tagging problem , and a BiLSTM with a softmax output layer can then be used for joint prediction .
In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .
As illustrated in , our model architecture can be viewed as a mixture of a named entity recognition ( NER ) component and a relation classification ( RC ) component .
Our NER component employs a BiLSTM - CRF architecture to predict entities from input word tokens .
Based on both the input words and the predicted NER labels , the RC component uses another BiLSTM to learn latent features relevant for relation classification .
In most previous neural joint models , the relation classification part relies on a common " linear " concatenation - based mechanism over the latent features associated with entity pairs , i.e. the latent features are first concatenated into a single feature vector which is then linearly transformed before being fed into a softmax classifier .
In contrast , our RC component takes into account second - order interactions over the latent features via a tensor .
In particular , for relation classification we propose a novel use of the deep biaffine attention mechanism which was first introduced in dependency parsing .
Experimental results on the benchmark " relation and entity recognition " dataset CoNLL04 show that our model outperforms previous models , obtaining new stateof - the - art scores .
In addition , using the biaffine attention improves the performance compared to using the linear mechanism significantly .
We also provide an ablation study to investigate effects of different contributing factors in our model .
Our proposed model
This section details our end - to - end relation extraction model .
Given an input sequence of n word tokens w 1 , w 2 , ... , w n , we use a vector vi to represent each i th word w i by concatenating word embedding e
Here , for each word type w , we use a one - layer BiLSTM ( BiLSTM char ) to learn its character - level word embedding e ( C ) w.
Named entity recognition ( NER ) :
The NER component feeds the sequence of vectors v 1:n with an additional context position index i into another BiLSTM ( BiLSTM NER ) to learn a " latent " feature vector representing the i th word token .
Then the NER component performs linear transformation of each latent feature vector by using a single - layer feed - forward network ( FFNN NER ) :
The output layer size of FFNN NER is the number of BIOLU - based NER labels .
The NER component feeds the output vectors h 1:n into a linear - chain CRF layer for NER label prediction .
A cross-entropy loss L NER is computed during training , while the Viterbi algorithm is used for decoding .
Our NER component thus is the BiLSTM - CRF model with additional LSTM - based character - level word embeddings .
Relation classification ( RC ) : Assume that t 1 , t 2 , ... , tn are NER labels predicted by the NER component for the input words .
We represent each i th predicted label by a vector embedding e ti .
We create a sequence of vectors x 1:n in which each x i is computed as :
As for NER , the RC component also uses a BiLSTM ( BiLSTM RC ) to learn another set of latent feature vectors , but from the sequence x 1:n :
The RC component further uses these latent vectors r i for relation classification .
We propose a novel use of the deep biaffine attention mechanism for relation classification .
The biaffine attention mechanism was proposed for dependency parsing , helping to produce the best reported parsing performance to date .
First , to encode the directionality of a relation , we use two single - layer feed - forward networks to project each r i into head and tail vector representations which correspond to whether the i th word serves as the head or tail argument of the relation :
Following , our RC component incrementally constructs relation candidates using all possible combinations of the last word tokens of predicted entities , i.e. words with L or U labels .
We assign an entity pair to a negative relation class ( NEG ) when the pair has no relation or when the predicted entities are not correct .
For example , for , we would have two relation candidates : NEG ( Paris , International ) and OrgBased In ( International , Paris ) .
Then for each head - tail candidate pair ( w j , wk ) , we apply the biaffine attention operator :
3 Experiments
Experimental setup
Evaluation scenarios :
We evaluate our joint model on two evaluation setup scenarios :
( 1 ) NER&RC :
A realistic scenario where entity boundaries are not given .
( 2 ) EC&RC : A less realistic scenario where the entity boundaries are given ] .
Thus the NER task which identifies both entity boundaries and classes reduces to the entity classification ( EC ) task .
Following , we encode the gold entity boundaries in the BILOU scheme .
Then we represent each B , I , O , L or U boundary tag as a vector embedding .
As a result , the vector vi in Equation 1 now also includes the boundary tag embedding in addition to the word embedding and character - level word embedding .
Dataset : We use the benchmark " entity and relation recognition " dataset CoNLL04 from .
Following , we use the 64%/16%/20 % training / development / test presplit available from Adel and Schtze ( 2017 ) , in which the test set was previously also used by .
Implementation :
Our model is implemented using DYNET v 2.0 .
We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .
We compute the average of NER / EC score and RC score after each training epoch .
We choose the model with the highest average score on the development set , which is then applied to the test set for the final evaluation phase .
More details of the implementation as well as optimal hyper - parameters are in the Appendix .
Our code is available at : https : //github.com/datquocnguyen/jointRE
Metric : Similar to previous works in , we use the macro -averaged F1 - score over the entity classes to score NER / EC and over the relation classes to score RC .
More details of the metric are also in the Appendix .
Unlike previous neural models , we report results as mean and standard deviation of the scores over 10 runs with 10 random seeds .
Main results
End - to - end results :
The first six rows in compare our results with previous state - of - the - art published results on the same test set .
In particular , our model obtains 2 + % absolute higher NER and RC scores ( Setup 1 ) than the BiLSTM - CRF - based multihead selection model .
We also obtain 7 + % higher EC and RC scores ( Setup 2 ) than Adel and Schtze ( 2017 ) .
Note that Gupta et al. ( 2016 ) use the same test set as we do , however they report final results on a 80/0 / 20 training / development / test split rather than our 64/16 /20 , i.e. Gupta et al. ( 2016 ) use a larger training set , but producing about 1.5 % lower EC score and similar RC score against ours .
These results show that our model performs better than previous state - of - the - art models , using the same setup .
In , the last two rows present results reported in and on the dataset CoNLL04 .
However , these results are not comparable due to their random sampling of the test set , i.e. using different train - test splits .
Both and employ additional extra features based on external NLP tools and use larger training sets than ours .
Specifically , integrate syntactic features by using a pre-trained BiLSTM - based dependency parser to extract BiLSTM - based latent feature representations for words in the input sentence , and then using these latent representations directly as part of the input embeddings in their model .
We plan to extend our model with their syntactic integration approach to further improve our model performance in future work .
Ablation analysis :
We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .
Here , the RC network uses gold NER labels when training , and uses predicted labels produced by the NER network when decoding .
We find that the joint approach does slightly better than the pipeline approach in relation classification , although the .
Ablation results on the development set . * and ** denote the statistically significant differences against the full results at p < 0.05 and p < 0.01 , respectively ( using the two - tailed paired t- test ) .
differences are not significant .
A similar observation is also found in .
Also , in preliminary experiments , we do not find any significant difference in performance of our joint model when feeding gold NER labels instead of predicted NER labels into the RC component during training .
This is not surprising as the training NER score is at 99 +% .
also presents ablation tests over 5 factors of our joint model on the development set .
In particular , Setup 1 performances significantly degrade by 4 + % absolutely , when not using the character - level word embeddings .
The performances also decrease when using a softmax classifier for NER label prediction rather than a CRF layer ( here , the decrease is significant ) .
In contrast , we do not find any significant difference in Setup 2 scores when not using either the character - level embeddings or the CRF layer , clearly showing the usefulness of the given gold entity boundaries .
The 3 remaining factors , including removing NER label embeddings and not taking either the Bilinear or Linear part ( in Equation 8 ) into the Biaffine attention layer , do not affect the NER / EC score .
However , they significantly decrease the RC score .
This is reasonable because those 3 factors are part of the RC component only , thus helpful in predicting relations .
More specifically , using the Biaffine attention produces about 1.5 % significant improvements to a common Linear transformation mechanism in relation classification , i.e. , " w / o Bilinear " results against the full results in : 65.4 % vs. 66.9 % and 72.0 % vs. 73.3 % ( although using Biaffine increases training time over using Linear by 35 % , relatively ) .
Conclusion
In this paper , we have presented an end - to - end neural network - based relation extraction model .
Our model employs a BiLSTM - CRF architecture for entity recognition and a biaffine attention mechanism for relation classification .
On the benchmark CoNLL04 dataset , our model produces new state - of - the - art performance .
