{
  "has" : {
    "Model" : {
      "introduce" : {
        "novel recurrent neural model" : {
          "incorporate" : {
            "entity - aware attention mechanism" : {
              "with" : "LET method"
            }
          }
        },
        "from sentence" : "In this section , we introduce a novel recurrent neural model that incorporate an entity - aware attention mechanism with a LET method in detail ."        
      },
      "consists of" : {
        "four main components" : {
          "has" : {
            "Word Representation" : {
              "maps" : {
                "each word in a sentence" : {
                  "into" : "vector representations"
                }  
              }
            },
            "Self Attention" : {
              "captures" : {
                "meaning of the correlation between words" : {
                  "based on" : "multi-head attention"
                }
              }
            },
            "BLSTM" : {
              "sequentially encodes" : "representations of self attention layer"
            },
            "Entity - aware Attention" : {
              "calculates" : {
                "attention weights" : {
                  "with respect to" : "entity pairs , word positions relative to these pairs , and their latent types obtained by LET"
                }
              }
            }            
          },
          "from sentence" : "As shown inure 2 , our model consists of four main components : Word Representation that maps each word in a sentence into vector representations ; ( 2 ) Self Attention that captures the meaning of the correlation between words based on multi-head attention ; ( 3 ) BLSTM which sequentially encodes the representations of self attention layer ; ( 4 ) Entity - aware Attention that calculates attention weights with respect to the entity pairs , word positions relative to these pairs , and their latent types obtained by LET ."          
        }
      }
    }
  }
}