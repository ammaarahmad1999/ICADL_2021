title
Universal Sentence Encoder
abstract
We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks .
The models are efficient and result in accurate performance on diverse transfer tasks .
Two variants of the encoding models allow for trade - offs between accuracy and compute resources .
For both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance .
Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning .
We find that transfer learning using sentence embeddings tends to outperform word level transfer .
With transfer learning via sentence embeddings , we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task .
We obtain encouraging results on Word Embedding Association Tests ( WEAT ) targeted at detecting model bias .
Our pre-trained sentence encoding models are made freely available for download and on TF Hub .
Introduction
Limited amounts of training data are available for many NLP tasks .
This presents a challenge for data hungry deep learning methods .
Given the high cost of annotating supervised training data , very large training sets are usually not available for most research or industry NLP tasks .
Many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or Glo Ve .
However , recent work has demonstrated strong transfer task performance using pre-trained sentence level embeddings .
In this paper , we present two models for producing sentence embeddings that demonstrate good transfer to a number of other of other NLP tasks .
We include experiments with varying amounts of transfer task training data to illustrate the relationship between transfer task performance and training set size .
We find that our sentence embeddings can be used to obtain surprisingly good task performance with remarkably little task specific training data .
The sentence encoding models are made publicly available on TF Hub .
Engineering characteristics of models used for transfer learning are an important consideration .
We discuss modeling trade - offs regarding memory requirements as well as compute time on CPU and GPU .
Resource consumption comparisons are made for sentences of varying lengths .
import tensorflow_hub as hub embed = hub.Module ( " https://tfhub.dev/google/ " " universal- sentence - encoder / 1 " ) embedding = embed ( [
" The quick brown fox jumps over the lazy dog . " ] )
Listing 1 : Python example code for using the universal sentence encoder .
Model Toolkit
We make available two new models for encoding sentences into embedding vectors .
One makes use of the transformer architecture , while the other is formulated as a deep averaging network ( DAN ) .
Both models are implemented in TensorFlow and are available to download from TF Hub : 1 https://tfhub.dev/google/universal-sentence-encoder/1
The models take as input English strings and produce as output a fixed dimensional embedding representation of the string .
Listing 1 provides a minimal code snippet to convert a sentence into a tensor containing its sentence embedding .
The embedding tensor can be used directly or incorporated into larger model graphs for specific tasks .
As illustrated in , the sentence embeddings can be trivially used to compute sentence level semantic similarity scores that achieve excellent performance on the semantic textual similarity ( STS ) Benchmark .
When included within larger models , the sentence encoding models can be fine tuned for specific tasks using gradient based updates .
Encoders
We introduce the model architecture for our two encoding models in this section .
Our two encoders have different design goals .
One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption .
The other targets efficient inference with slightly reduced accuracy .
Transformer
The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture .
This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words .
The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .
The encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional vector as the sentence embedding .
The encoding model is designed to be as general purpose as possible .
This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks .
The supported tasks include : a Skip - Thought like task for the unsupervised learning from arbitrary running text ; a conversational input - response task for the inclusion of parsed conversational data ; and classification tasks for training on supervised data .
The Skip - Thought task replaces the LSTM used in the original formulation with a model based on the Transformer architecture .
As will be shown in the experimental results below , the transformer based encoder achieves the best over all transfer task performance .
However , this comes at the cost of compute time and memory usage scaling dramatically with sentence length .
Deep Averaging Network ( DAN )
The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings .
Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .
The DAN encoder is trained similarly to the Transformer based encoder .
We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .
The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence .
Similar to , our results demonstrate that DANs achieve strong baseline performance on text classification tasks .
Encoder Training Data
Unsupervised training data for the sentence encoding models are drawn from a variety of web sources .
The sources are Wikipedia , web news , web question - answer pages and discussion forums .
We augment unsupervised learning with training on supervised data from the Stanford Natural Language Inference ( SNLI ) corpus .
Similar to the findings of , we observe that training to SNLI improves transfer performance .
Transfer Tasks
This section presents an overview of the data used for the transfer learning experiments and the Word Embedding Association Test ( WEAT ) data used to characterize model bias .
summarizes the number of samples provided by the test portion of each evaluation set and , when available , the size of the dev and training data .
MR : Movie review snippet sentiment on a five star scale .
CR : Sentiment of sentences mined from customer reviews .
SUBJ : Subjectivity of sentences from movie reviews and plot summaries .
MPQA : Phrase level opinion polarity from news data .
TREC : Fine grained question classification sourced from TREC .
SST : Binary phrase level sentiment classification .
STS Benchmark : Semantic textual similarity ( STS ) between sentence pairs scored by Pearson correlation with human judgments .
WEAT : Word pairs from the psychology literature on implicit association tests ( IAT ) that are used to characterize model bias .
For sentence classification transfer tasks , the output of the transformer and DAN sentence encoders are provided to a task specific DNN .
For the pairwise semantic similarity task , we directly assess the similarity of the sentence embeddings produced by our two encoders .
As shown Eq. 1 , we first compute the cosine similarity of the two sentence embeddings and then use arccos to convert the cosine similarity into an angular distance .
5 sim ( u , v ) = 1 ? arccos u v | | u || | | v|| /?
( 1 )
Baselines
For each transfer task , we include baselines that only make use of word level transfer and baselines that make use of no transfer learning at all .
For word level transfer , we use word embeddings from a word2 vec skip - gram model trained on a corpus of news data .
The pretrained word embeddings are included as input to two model types : a convolutional neural network models ( CNN ) ; a DAN .
The baselines that use pretrained word embeddings allow us to contrast word versus sentence level transfer .
Additional baseline CNN and DAN models are trained without using any pretrained word or sentence embeddings .
Combined Transfer Models
We explore combining the sentence and word level transfer models by concatenating their representations prior to feeding the combined representation :
Model performance on transfer tasks .
USE
T is the universal sentence encoder ( USE ) using Transformer .
USE
Dis the universal encoder DAN model .
Models tagged with w2 v w.e. make use of pre-training word2vec skip - gram embeddings for the transfer task model , while models tagged with lrn w.e. use randomly initialized word embeddings that are learned only on the transfer task data .
Accuracy is reported for all evaluations except STS Bench where we report the Pearson correlation of the similarity scores with human judgments .
Pairwise similarity scores are computed directly using the sentence embeddings from the universal sentence encoder as in Eq. ( 1 ) .
to the transfer task classification layers .
For completeness , we also explore concatenating the representations from sentence level transfer models with the baseline models that do not make use of word level transfer learning .
Experiments
Transfer task model hyperparamaters are tuned using a combination of Vizier and light manual tuning .
When available , model hyperparameters are tuned using task dev sets .
Otherwise , hyperparameters are tuned by crossvalidation on the task training data when available or the evaluation test data when neither training nor dev data are provided .
Training repeats ten times for each transfer task model with different randomly initialized weights and we report evaluation results by averaging across runs .
Transfer learning is critically important when training data for a target task is limited .
We explore the impact on task performance of varying the amount of training data available for the task both with and without the use of transfer learning .
Contrasting the transformer and DAN based encoders , we demonstrate trade - offs in model complexity and the amount of data required to reach a desired level of accuracy on a task .
To assess bias in our encoding models , we evaluate the strength of various associations learned by our model on WEAT word lists .
We compare our result to those of who discovered that word embeddings could be used to reproduce human performance on implicit association tasks for both benign and potentially undesirable associations .
Results
Transfer task performance is summarized in Table 2 . 
We observe that transfer learning from the transformer based sentence encoder usually performs as good or better than transfer learning from the DAN encoder .
Hoewver , transfer learning using the simpler and fast DAN encoder can for some tasks perform as well or better than the more sophisticated transformer encoder .
Models that make use of sentence level transfer learning tend to perform better than models that only use word level transfer .
The best performance on most tasks is obtained by models that make use of both sentence and word level transfer .
illustrates transfer task performance for varying amounts of training data .
We observe that , for smaller quantities of data , sentence level transfer learning can achieve surprisingly good task performance .
As the training set size increases , models that do not make use of transfer learning approach the performance of the other models .
contrasts 's findings on bias within GloVe embeddings with the DAN variant of the universal encoder .
Similar to GloVe , our model reproduces human associations between flowers vs. insects and pleasantness vs. unpleasantness .
However , our model demonstrates weaker associations than GloVe for probes targeted at revealing at ageism , racism and sexism .
The differences in word association patterns can be attributed to differences in the training data composition and the mixture of tasks used to train the sentence embeddings .
Discussion
Transfer learning leads to performance improvements on many tasks .
Using transfer learning is more critical when less training data is available .
When task performance is close , the correct modeling choice should take into account engineering trade - offs regarding the memory and compute 6 Researchers and developers are strongly encouraged to independently verify whether biases in their over all model or model components impacts their use case .
For resources on ML fairness visit https://developers.google.com/machinelearning/fairness-overview/.
resource requirements introduced by the different models that could be used .
Resource Usage
This section describes memory and compute resource usage for the transformer and DAN sentence encoding models for different sentence lengths .
Figure 2 plots model resource usage against sentence length .
Compute Usage
The transformer model time complexity is O ( n 2 ) in sentence length , while the DAN model is O ( n ) .
As seen in ( a - b ) , for short sentences , the transformer encoding model is only moderately slower than the much simpler DAN model .
However , compute time for transformer increases noticeably as sentence length increases .
In contrast , the compute time for the DAN model stays nearly constant as sentence length is increased .
Since the DAN model is remarkably computational efficient , using GPUs over CPUs will often have a much larger practical impact for the transformer based encoder .
Memory Usage
The transformer model space complexity also scales quadratically , O ( n 2 ) , in sentence length , while the DAN model space complexity is constant in the length of the sentence . Similar to compute usage , memory usage for the transformer model increases quickly with sentence length , while the memory usage for the DAN model remains constant .
We note that , for the DAN model , memory usage is dominated by the parameters used to store the model unigram and bigram embeddings .
Since the transformer model only needs to store unigram embeddings , for short sequences it requires nearly half as much memory as the DAN model .
Conclusion
Both the transformer and DAN based universal encoding models provide sentence level embeddings that demonstrate strong transfer performance on a number of NLP tasks .
The sentence level embeddings surpass the performance of transfer learning using word level embeddings alone .
Models that make use of sentence and word level transfer achieve the best over all performance .
We observe that transfer learning is most helpful when limited training data is available for the transfer task .
The encoding models make different trade - offs regarding accuracy and model complexity that should be considered when choosing the best model for a particular application .
The pre-trained encoding models will be made publicly available for research and use in applications that can benefit from a better understanding of natural language .
