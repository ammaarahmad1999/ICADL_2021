(Contribution||has||Model)
(Model||has||Transformer)
(Transformer||based||sentence encoding model)
(sentence encoding model||constructs||sentence embeddings)
(sentence embeddings||using||encoding sub - graph of the transformer architecture)
(Transformer||converted||context aware word representations)
(context aware word representations||to a||fixed length sentence encoding vector)
(fixed length sentence encoding vector||by computing||element - wise sum)
(element - wise sum||of||representations at each word position)
(Transformer||uses||attention)
(attention||to compute||context aware representations)
(context aware representations||of||words in a sentence)
(words in a sentence||that take into account||ordering and identity of all the other words)
(Transformer||has||encoding model)
(encoding model||designed to be||general purpose)
(general purpose||accomplished by||using multi-task learning)
(using multi-task learning||whereby||a single encoding model)
(a single encoding model||is used to feed||multiple downstream tasks)
(Model||has||Deep Averaging Network ( DAN ))
(Deep Averaging Network ( DAN )||takes as input||lowercased PTB tokenized string)
(lowercased PTB tokenized string||outputs||512 dimensional sentence embedding)
(Deep Averaging Network ( DAN )||primary advantage||compute time)
(compute time||is||linear in the length)
(linear in the length||of||input sequence)
(Deep Averaging Network ( DAN )||use||mul-titask learning)
(mul-titask learning||whereby||single DAN encoder)
(single DAN encoder||used to supply||sentence embeddings)
(sentence embeddings||for||multiple downstream tasks)
(Deep Averaging Network ( DAN )||whereby||input embeddings for words and bi-grams)
(input embeddings for words and bi-grams||then passed through||feedforward deep neural network ( DNN ))
(feedforward deep neural network ( DNN )||to produce||sentence embeddings)
(input embeddings for words and bi-grams||are first||averaged together)
