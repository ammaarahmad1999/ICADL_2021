{
  "has" : {
    "Model" : {
      "has" : {
        "Transformer" : {
          "based" : {
            "sentence encoding model" : {
              "constructs" : {
                "sentence embeddings" : {
                  "using" : "encoding sub - graph of the transformer architecture"
                }
              },
              "from sentence" : "The transformer based sentence encoding model constructs sentence embeddings using the encoding sub - graph of the transformer architecture ."              
            }
          },
          "uses" : {
            "attention" : {
              "to compute" : {
                "context aware representations" : {
                  "of" : {
                    "words in a sentence" : {
                      "that take into account" : "ordering and identity of all the other words"                      
                    }
                  }
                }
              }
            },
            "from sentence" : "This sub - graph uses attention to compute context aware representations of words in a sentence that take into account both the ordering and identity of all the other words ."
          },
          "converted" : {
            "context aware word representations" : {
              "to a" : {
                "fixed length sentence encoding vector" : {
                  "by computing" : {
                    "element - wise sum" : {
                      "of" : "representations at each word position"
                    }
                  }
                }
              }
            },
            "from sentence" : "The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position ."
          },
          "has" : {
            "encoding model" : {
              "designed to be" : {
                "general purpose" : {
                  "accomplished by" : {
                    "using multi-task learning" : {
                      "whereby" : {
                        "a single encoding model" : {
                          "is used to feed" : "multiple downstream tasks"
                        }
                      }
                    }
                  }
                }
              }
            },
            "from sentence" : "The encoding model is designed to be as general purpose as possible .
This is accomplished by using multi-task learning whereby a single encoding model is used to feed multiple downstream tasks ."

          }
        },
        "Deep Averaging Network ( DAN )" : {
          "whereby" : {
            "input embeddings for words and bi-grams" : {
              "are first" : "averaged together",
              "then passed through" : {
                "feedforward deep neural network ( DNN )" : {
                  "to produce" : "sentence embeddings"
                }
              }
            },
            "from sentence" : "Deep Averaging Network ( DAN )
The second encoding model makes use of a deep averaging network ( DAN ) whereby input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network ( DNN ) to produce sentence embeddings ."

          },
          "takes as input" : {
            "lowercased PTB tokenized string" : {
              "outputs" : "512 dimensional sentence embedding"
            },
            "from sentence" : "Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding ."
          },
          "use" : {
            "mul-titask learning" : {
              "whereby" : {
                "single DAN encoder" : {
                  "used to supply" : {
                    "sentence embeddings" : {
                      "for" : "multiple downstream tasks"
                    }
                  }
                }
              }
            },
            "from sentence" : "We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks ."
          },
          "primary advantage" : {
            "compute time" : {
              "is" : {
                "linear in the length" : {
                  "of" : "input sequence"
                }
              }
            },
            "from sentence" : "The primary advantage of the DAN encoder is that compute time is linear in the length of the input sequence ."
          }
        }
      }
    }
  }
}