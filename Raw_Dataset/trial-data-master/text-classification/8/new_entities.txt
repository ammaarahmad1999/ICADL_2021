21	19	26	p	conduct
21	30	66	n	extensive experimental investigation
21	67	80	p	to understand
21	81	95	n	when , and why
21	98	123	p	simple pooling strategies
21	126	161	n	operated over word embeddings alone
21	164	204	p	already carry sufficient information for
21	205	235	n	natural language understanding
22	0	16	p	To ac- count for
22	21	102	n	distinct nature of various NLP tasks that may require different semantic features
22	108	115	p	compare
22	116	135	n	SWEM - based models
22	136	140	p	with
22	141	186	n	existing recurrent and convolutional networks
22	187	191	p	in a
22	192	214	n	pointby - point manner
10	50	85	n	https://github.com/dinghanshen/SWEM
117	3	6	p	use
117	7	29	n	Glo Ve word embeddings
117	30	34	p	with
117	35	42	n	K = 300
117	43	45	p	as
117	46	60	n	initialization
117	61	64	p	for
117	65	79	n	all our models
118	0	35	n	Out - Of - Vocabulary ( OOV ) words
118	40	56	p	initialized from
118	59	79	n	uniform distribution
118	80	90	p	with range
118	91	108	n	[ ? 0.01 , 0.01 ]
119	4	21	n	Glo Ve embeddings
119	26	37	p	employed in
119	38	46	n	two ways
119	47	55	p	to learn
119	56	79	n	refined word embeddings
119	97	105	p	updating
119	106	125	n	each word embedding
119	126	132	p	during
119	133	141	p	training
119	155	163	p	training
119	166	217	n	300 dimensional Multilayer Perceptron ( MLP ) layer
119	218	222	p	with
119	223	238	n	ReLU activation
119	246	263	n	Glo Ve embeddings
119	264	275	p	as input to
119	280	283	n	MLP
119	293	299	p	output
119	313	336	n	refined word embeddings
124	0	4	p	Adam
124	18	37	n	optimize all models
2	30	68	n	Simple Word - Embedding - Based Models
4	55	99	n	model the compositionality in text sequences
14	90	152	n	model the compositionality in variable - length text sequences
125	15	17	p	on
125	18	40	n	topic prediction tasks
125	47	57	n	SWEM model
125	58	66	p	exhibits
125	67	88	n	stronger performances
125	91	107	p	relative to both
125	108	148	n	LSTM and CNN compositional architectures
125	156	174	p	by leveraging both
125	179	213	n	average and max - pooling features
125	214	218	p	from
125	219	234	n	word embeddings
127	0	2	p	On
127	7	38	n	ontology classification problem
127	64	71	p	observe
127	94	98	n	SWEM
127	99	107	p	exhibits
127	108	143	n	comparable or even superior results
127	146	157	p	relative to
127	158	176	n	CNN or LSTM models
81	16	19	p	for
81	24	42	n	sentiment analysis
81	51	55	p	both
81	56	92	n	CNN and LSTM compositional functions
81	93	112	p	perform better than
81	113	117	n	SWEM
149	0	22	n	Text Sequence Matching
152	15	17	p	on
152	18	67	n	most of the datasets considered ( except WikiQA )
152	70	74	n	SWEM
152	75	87	p	demonstrates
152	92	104	n	best results
152	105	118	p	compared with
152	130	153	n	CNN or the LSTM encoder
153	13	25	n	SNLI dataset
153	31	43	p	observe that
153	44	54	n	SWEM - max
153	55	72	p	performs the best
153	73	96	n	among all SWEM variants
176	0	34	n	SWEM - hier for sentiment analysis
177	35	59	n	word - order information
177	60	82	p	plays a vital role for
177	83	107	n	sentiment analysis tasks
178	50	77	p	most important features for
178	78	98	n	sentiment prediction
178	99	104	p	maybe
178	105	135	n	some key n-gram phrase / words
189	12	31	p	greatly outperforms
189	36	61	n	other three SWEM variants
189	72	96	n	corresponding accuracies
189	101	114	p	comparable to
189	119	141	n	results of CNN or LSTM
191	0	25	n	Short Sentence Processing
195	51	55	n	SWEM
195	56	62	p	yields
195	63	82	n	inferior accuracies
195	83	85	p	on
195	86	113	n	sentiment analysis datasets
196	15	23	p	exhibits
196	24	46	n	comparable performance
196	47	49	p	on
196	54	69	n	other two tasks
196	78	82	p	with
196	83	123	n	much less parameters and faster training
