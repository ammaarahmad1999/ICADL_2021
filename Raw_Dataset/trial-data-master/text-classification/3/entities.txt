23	17	37	focus on the role of
23	38	66	preprocessing the input text
23	69	84	particularly in
23	85	88	how
23	95	105	split into
23	106	145	individual ( meaning - bearing ) tokens
23	157	183	affects the performance of
23	184	226	standard neural text classification models
32	94	141	github.com/pedrada88/preproc-textclassification
63	3	8	tried
63	14	39	two classification models
64	4	16	first one is
64	19	37	standard CNN model
64	38	48	similar to
64	59	103	using ReLU as non-linear activation function
65	7	13	second
65	14	19	model
65	25	28	add
65	31	46	recurrent layer
65	49	64	specifically an
65	65	69	LSTM
65	72	86	before passing
65	91	106	pooled features
65	107	118	directly to
65	123	152	fully connected softmax layer
68	4	19	embedding layer
68	24	41	initialized using
68	42	84	300 - dimensional CBOW Word2vec embeddings
68	85	95	trained on
68	100	129	3B - word UMBC WebBase corpus
68	130	134	with
68	135	159	standard hyperparameters
2	15	65	Text Preprocessing in Neural Network Architectures
4	0	18	Text preprocessing
94	0	35	Experiment 1 : Preprocessing effect
95	19	25	use of
95	26	63	more complex preprocessing techniques
95	64	71	such as
95	72	108	lemmatization and multiword grouping
95	109	117	does not
95	118	122	help
97	0	34	Experiment 2 : Cross-preprocessing
100	22	29	observe
100	32	80	different trend , with multiwordenhanced vectors
100	81	91	exhibiting
100	94	112	better performance
100	113	120	both on
100	125	141	single CNN model
100	144	172	best over all performance in
100	173	199	seven of the nine datasets
100	213	229	CNN + LSTM model
100	232	251	best performance in
100	252	265	four datasets
100	277	293	same ballpark as
100	298	310	best results
100	311	321	in four of
100	326	349	remaining five datasets
109	16	21	using
109	22	49	multiword - wise embeddings
109	50	52	on
109	57	72	vanilla setting
109	73	81	leads to
109	82	109	consistently better results
109	110	128	than using them on
109	133	175	same multiwordgrouped preprocessed dataset
109	176	178	in
109	179	205	eight of the nine datasets
111	50	56	use of
111	61	71	embeddings
111	72	82	trained on
111	85	125	simple tokenized corpus ( i.e. vanilla )
