(Contribution||has||Results)
(Results||has||Semi-supervised experiments)
(Semi-supervised experiments||tested||wv - 2 LSTMp)
(wv - 2 LSTMp||difference from||oh - 2 LSTMp)
(oh - 2 LSTMp||is||input to the LSTM layers)
(input to the LSTM layers||is||pre-trained word vectors)
(wv - 2 LSTMp||name||word - vector bidirectional LSTM with pooling)
(Semi-supervised experiments||review||performance)
(performance||of||one - hot CNN)
(one - hot CNN||with||one 200 - dim CNN tv-embedding)
(one 200 - dim CNN tv-embedding||comparable with||our LSTM)
(our LSTM||with||two 100 - dim LSTM tv-embeddings)
(two 100 - dim LSTM tv-embeddings||in terms of||dimensionality of tv-embeddings)
(Results||has||Experiments ( supervised ))
(Experiments ( supervised )||Comparing||two types of LSTM)
(two types of LSTM||see that||one - hot bidirectional LSTM with pooling)
(one - hot bidirectional LSTM with pooling||name||oh - 2 LSTMp)
(one - hot bidirectional LSTM with pooling||outperforms||word - vector LSTM)
(word - vector LSTM||name||wv - LSTM)
(word - vector LSTM||on||all the datasets)
(Experiments ( supervised )||on||three out of the four datasets)
(three out of the four datasets||has||oh - 2 LSTMp)
(oh - 2 LSTMp||outperforms||SVM and the CNN)
