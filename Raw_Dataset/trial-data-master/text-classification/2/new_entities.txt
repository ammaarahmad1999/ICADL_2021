22	0	5	p	shows
22	8	27	n	simple linear model
22	28	32	p	with
22	33	48	n	rank constraint
23	10	25	n	weight matrix A
23	26	30	p	is a
23	31	46	n	look - up table
23	47	51	p	over
23	56	61	n	words
24	4	24	n	word representations
24	25	47	p	are then averaged into
24	50	69	n	text representation
24	81	95	p	in turn fed to
24	98	115	n	linear classifier
27	3	6	p	use
27	11	29	n	softmax function f
27	30	40	p	to compute
27	45	69	n	probability distribution
27	70	74	p	over
27	79	97	n	predefined classes
45	19	33	n	bag of n-grams
45	34	36	p	as
45	37	56	n	additional features
45	57	67	p	to capture
45	68	92	n	some partial information
45	93	98	p	about
45	103	119	n	local word order
46	23	34	p	in practice
46	8	22	n	very efficient
46	41	50	p	achieving
46	51	69	n	comparable results
46	70	100	p	to methods that explicitly use
46	105	110	n	order
47	3	11	p	maintain
47	14	47	n	fast and memory efficient mapping
47	48	50	p	of
47	55	62	n	n-grams
47	63	71	p	by using
47	76	89	n	hashing trick
47	90	94	p	with
47	99	126	n	same hashing function as in
47	131	139	n	10M bins
47	146	155	p	only used
47	156	163	n	bigrams
34	9	19	p	to improve
34	24	36	n	running time
34	42	45	p	use
34	48	68	n	hierarchical softmax
34	71	79	p	based on
34	84	103	n	Huffman coding tree
2	18	47	n	Efficient Text Classification
4	56	75	n	text classification
53	0	18	n	Sentiment analysis
60	10	22	p	hidden units
60	7	9	n	10
60	27	30	p	run
60	31	39	n	fastText
60	40	43	p	for
60	44	52	n	5 epochs
60	53	57	p	with
60	60	73	n	learning rate
60	74	85	p	selected on
60	88	102	n	validation set
60	103	107	p	from
60	108	135	n	{ 0.05 , 0.1 , 0.25 , 0.5 }
61	15	21	p	adding
61	22	40	n	bigram information
61	41	68	p	improves the performance by
61	69	76	n	1 - 4 %
62	8	20	n	our accuracy
62	24	44	p	slightly better than
62	45	71	n	char - CNN and char - CRNN
62	80	94	p	bit worse than
62	95	100	n	VDCNN
63	17	25	p	increase
63	30	47	n	accuracy slightly
63	51	56	p	using
63	57	69	n	more n-grams
63	72	88	p	for example with
63	89	97	n	trigrams
63	104	118	p	performance on
63	119	124	n	Sogou
63	125	135	p	goes up to
63	136	142	n	97.1 %
69	0	14	n	Tag prediction
84	0	12	p	At test time
84	15	23	n	Tagspace
84	24	40	p	needs to compute
84	45	51	n	scores
84	52	55	p	for
84	56	71	n	all the classes
84	78	86	p	makes it
84	87	102	n	relatively slow
84	111	129	n	our fast inference
84	130	135	p	gives
84	138	160	n	significant speed - up
84	161	165	p	when
84	170	221	n	number of classes is large ( more than 300 K here )
85	17	48	p	more than an order of magnitude
85	49	55	n	faster
85	56	65	p	to obtain
85	66	71	n	model
85	72	76	p	with
85	79	93	n	better quality
