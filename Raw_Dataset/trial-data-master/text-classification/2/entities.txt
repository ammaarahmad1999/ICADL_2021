22	0	5	shows
22	8	27	simple linear model
22	28	32	with
22	33	48	rank constraint
23	10	25	weight matrix A
23	26	30	is a
23	31	46	look - up table
23	47	51	over
23	56	61	words
24	4	24	word representations
24	25	47	are then averaged into
24	50	69	text representation
24	81	95	in turn fed to
24	98	115	linear classifier
27	3	6	use
27	11	29	softmax function f
27	30	40	to compute
27	45	69	probability distribution
27	70	74	over
27	79	97	predefined classes
45	19	33	bag of n-grams
45	34	36	as
45	37	56	additional features
45	57	67	to capture
45	68	92	some partial information
45	93	98	about
45	103	119	local word order
46	23	34	in practice
46	8	22	very efficient
46	41	50	achieving
46	51	69	comparable results
46	70	100	to methods that explicitly use
46	105	110	order
47	3	11	maintain
47	14	47	fast and memory efficient mapping
47	48	50	of
47	55	62	n-grams
47	63	71	by using
47	76	89	hashing trick
47	90	94	with
47	99	126	same hashing function as in
47	131	139	10M bins
47	146	155	only used
47	156	163	bigrams
34	9	19	to improve
34	24	36	running time
34	42	45	use
34	48	68	hierarchical softmax
34	71	79	based on
34	84	103	Huffman coding tree
2	18	47	Efficient Text Classification
4	56	75	text classification
53	0	18	Sentiment analysis
60	10	22	hidden units
60	7	9	10
60	27	30	run
60	31	39	fastText
60	40	43	for
60	44	52	5 epochs
60	53	57	with
60	60	73	learning rate
60	74	85	selected on
60	88	102	validation set
60	103	107	from
60	108	135	{ 0.05 , 0.1 , 0.25 , 0.5 }
61	15	21	adding
61	22	40	bigram information
61	41	68	improves the performance by
61	69	76	1 - 4 %
62	8	20	our accuracy
62	24	44	slightly better than
62	45	71	char - CNN and char - CRNN
62	80	94	bit worse than
62	95	100	VDCNN
63	17	25	increase
63	30	47	accuracy slightly
63	51	56	using
63	57	69	more n-grams
63	72	88	for example with
63	89	97	trigrams
63	104	118	performance on
63	119	124	Sogou
63	125	135	goes up to
63	136	142	97.1 %
69	0	14	Tag prediction
84	0	12	At test time
84	15	23	Tagspace
84	24	40	needs to compute
84	45	51	scores
84	52	55	for
84	56	71	all the classes
84	78	86	makes it
84	87	102	relatively slow
84	111	129	our fast inference
84	130	135	gives
84	138	160	significant speed - up
84	161	165	when
84	170	221	number of classes is large ( more than 300 K here )
85	17	48	more than an order of magnitude
85	49	55	faster
85	56	65	to obtain
85	66	71	model
85	72	76	with
85	79	93	better quality
