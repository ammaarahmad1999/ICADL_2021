{
  "has" : {
    "Ablation analysis" : {
      "has" : {
        "Low - shot learning" : {
          "On" : {
            "IMDb and AG" : {
              "has" : {
                "supervised ULMFiT" : {
                  "with only" : "100 labeled examples",
                  "matches the performance of" : {
                    "training from scratch" : {
                      "with" : "10 and 20 more data"
                    }
                  },
                  "from sentence" : "Low - shot learning
On IMDb and AG , supervised ULMFiT with only 100 labeled examples matches the performance of training from scratch with 10 and 20 more data respectively , clearly demonstrating the benefit of general - domain LM pretraining ."

                }
              }
            },
            "TREC - 6" : {
              "has" : {
                "ULMFiT" : {
                  "significantly improves upon" : "training from scratch",
                  "from sentence" : "On TREC - 6 , ULMFiT significantly improves upon training from scratch ; as examples are shorter and fewer , supervised and semi-supervised ULMFiT achieve similar results ."                  
                }
              }
            }
          }
        },
        "Pretraining" : {
          "most useful for" : "small and medium - sized datasets",
          "from sentence" : "Pretraining is most useful for small and medium - sized datasets , which are most common in commercial applications ."
        },
        "Impact of LM quality" : {
          "Using" : {
            "our fine - tuning techniques" : {
              "reaches" : {
                "surprisingly good performance" : {
                  "on" : "larger datasets"
                }
              },
              "from sentence" : "Impact of LM quality
Using our fine - tuning techniques , even a regular LM reaches surprisingly good performance on the larger datasets ."

            }
          },
          "On" : {
            "smaller TREC - 6" : {
              "has" : {
                "vanilla LM without dropout" : {
                  "runs the risk of" : "overfitting"
                }
              },
              "from sentence" : "On the smaller TREC - 6 , a vanilla LM without dropout runs the risk of overfitting , which decreases performance ."  
            }  
          }
        },
        "Fine - tuning" : {
		  "has" : {
			"LM" : {
				"most beneficial for" : "larger datasets"
			}
		  },
          "from sentence" : "Fine - tuning the LM is most beneficial for larger datasets ."
        },
        "Fine - tuning" : {
		  "has" : {
		     "classifier" : {
				"significantly improves over" : {
					"training from scratch" : {
					  "particularly on" : "small TREC - 6"
					}
				}			 
			 }
		  },
          "from sentence" : "Fine - tuning the classifier significantly improves over training from scratch , particularly on the small TREC - 6 . ' Last ' , the standard fine - tuning method in CV , severely underfits and is never able to lower the training error to 0 . ' Chainthaw ' achieves competitive performance on the smaller datasets , but is outperformed significantly on the large AG ."
        },
        "forward and backwards LM - classifier" : {
          "brings" : {
            "performance boost" : {
              "of" : "0.5 - 0.7"
            },
            "from sentence" : "At the cost of training a second model , ensembling the predictions of a forward and backwards LM - classifier brings a performance boost of around 0.5 - 0.7 ."
          },
          "On" : {
            "IMD b" : {
              "lower" : {
                "test error" : {
                  "from" : {
                    "5.30" : {
                      "of" : "single model"
                    }
                  },
                  "to" : {
                    "4.58" : {
                      "for" : "bidirectional model"
                    }
                  }
                }
              }
            },
            "from sentence" : "On IMD b we lower the test error from 5.30 of a single model to 4.58 for the bidirectional model ."
          }
        }
      }
    }
  }
}