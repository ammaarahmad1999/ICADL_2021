127	15	28	trained using
127	29	33	Adam
128	3	8	chose
128	9	29	two one - layer LSTM
128	30	33	for
128	34	54	read / write modules
128	55	57	on
128	62	81	tasks other than QA
129	77	89	obtained for
129	94	109	word embeddings
129	4	42	pre-trained 300 - D Glove 840B vectors
129	47	71	100 - D Glove 6B vectors
132	3	14	crop or pad
132	19	33	input sequence
132	34	36	to
132	39	51	fixed length
134	16	30	regularized by
134	37	45	dropouts
134	53	69	l 2 weight decay
18	17	24	propose
18	25	74	a novel class of memory augmented neural networks
18	75	81	called
18	82	114	Neural Semantic Encoders ( NSE )
18	115	118	for
18	119	149	natural language understanding
20	0	3	NSE
20	10	40	variable sized encoding memory
20	64	73	to access
20	74	95	entire input sequence
20	96	102	during
20	107	122	reading process
20	135	157	efficiently delivering
20	158	191	long - term dependencies overtime
22	4	26	sequentially processes
22	27	36	the input
22	41	49	supports
22	50	71	word compositionality
23	8	30	read from and write to
23	33	81	set of relevant encoding memories simultaneously
23	103	109	access
23	112	134	shared encoding memory
23	147	157	supporting
23	158	194	knowledge and representation sharing
21	4	19	encoding memory
21	20	27	evolves
21	28	36	overtime
21	41	50	maintains
21	55	61	memory
21	62	64	of
21	69	83	input sequence
21	84	91	through
21	92	127	read , compose and write operations
4	49	79	natural language understanding
24	52	55	NLU
135	0	26	Natural Language Inference
142	28	40	hidden layer
142	46	56	1024 units
142	41	45	with
142	62	77	ReLU activation
142	84	98	sof tmax layer
143	11	21	batch size
143	25	28	128
143	35	56	initial learning rate
143	60	66	3e - 4
143	71	95	l 2 regularizer strength
143	99	106	3 e - 5
143	113	118	train
143	134	143	40 epochs
158	4	29	MMA - NSE attention model
160	11	19	obtained
160	20	41	85.4 % accuracy score
162	0	25	Answer Sentence Selection
173	11	21	batch size
173	25	26	4
173	35	56	initial learning rate
173	60	67	1 e - 5
173	74	93	train the model for
173	94	103	10 epochs
174	3	7	used
174	8	21	40 % dropouts
174	22	31	afterword
174	32	42	embeddings
174	47	49	no
174	50	66	l 2 weight decay
175	4	19	word embeddings
175	20	23	are
175	24	62	pre-trained 300 - D Glove 840B vectors
176	18	38	linear mapping layer
176	39	49	transforms
176	54	77	300 - D word embeddings
176	78	80	to
176	85	103	512- D LSTM inputs
181	4	29	MMA - NSE attention model
181	30	37	exceeds
181	42	46	NASM
181	47	63	by approximately
181	64	67	1 %
181	68	70	on
181	71	74	MAP
181	79	84	0.8 %
181	85	87	on
181	88	91	MRR
186	0	23	Sentence Classification
191	4	26	first layer of the MLP
191	31	46	ReLU activation
191	51	68	1024 or 300 units
191	69	72	for
191	73	105	binary or fine - grained setting
192	4	16	second layer
192	22	36	sof tmax layer
193	4	24	read / write modules
193	29	49	two one - layer LSTM
193	50	54	with
193	55	71	300 hidden units
193	80	95	word embeddings
193	104	142	pre-trained 300 - D Glove 840B vectors
194	11	21	batch size
194	25	27	64
194	34	55	initial learning rate
194	59	65	3e - 4
194	70	94	l 2 regularizer strength
194	98	105	3 e - 5
194	112	117	train
194	133	142	25 epochs
199	0	9	Our model
199	10	22	outperformed
199	27	30	DMN
199	35	38	set
199	43	73	state - of - the - art results
199	74	76	on
199	77	90	both subtasks
200	0	27	Document Sentiment Analysis
204	3	8	stack
204	11	22	NSE or LSTM
204	23	36	on the top of
204	37	48	another NSE
204	49	52	for
204	53	70	document modeling
207	0	17	The whole network
207	21	55	trained jointly by backpropagating
207	60	78	cross entropy loss
208	3	7	used
208	8	24	one - layer LSTM
208	25	29	with
208	30	46	100 hidden units
208	47	50	for
208	55	75	read / write modules
208	84	120	pre-trained 100 - D Glove 6B vectors
209	11	21	batch size
209	25	27	32
209	34	55	initial learning rate
209	59	65	3e - 4
209	70	94	l 2 regularizer strength
209	98	105	1 e - 5
209	112	119	trained
209	135	144	50 epochs
221	0	19	Machine Translation
241	16	35	trained to minimize
241	36	67	word - level cross entropy loss
241	77	91	regularized by
241	92	111	20 % input dropouts
241	120	140	30 % output dropouts
242	11	21	batch size
242	25	28	128
242	35	56	initial learning rate
242	60	66	1e - 3
242	67	70	for
242	71	82	LSTM - LSTM
242	87	93	3e - 4
242	94	97	for
242	102	114	other models
242	119	143	l 2 regularizer strength
242	147	154	3 e - 5
242	161	166	train
242	182	191	40 epochs
