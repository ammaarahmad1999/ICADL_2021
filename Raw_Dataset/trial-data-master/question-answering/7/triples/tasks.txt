(Contribution||has||Tasks)
(Tasks||Answer Sentence Selection||Hyperparameters)
(Hyperparameters||initial learning rate||1 e - 5)
(Hyperparameters||batch size||4)
(Hyperparameters||train the model for||10 epochs)
(Hyperparameters||used||40 % dropouts)
(40 % dropouts||no||l 2 weight decay)
(40 % dropouts||afterword||embeddings)
(Hyperparameters||has||word embeddings)
(word embeddings||are||pre-trained 300 - D Glove 840B vectors)
(word embeddings||has||linear mapping layer)
(linear mapping layer||transforms||300 - D word embeddings)
(300 - D word embeddings||to||512- D LSTM inputs)
(Tasks||Answer Sentence Selection||Results)
(Results||has||MMA - NSE attention model)
(MMA - NSE attention model||exceeds||NASM)
(NASM||by approximately||1 %)
(1 %||on||MAP)
(NASM||by approximately||0.8 %)
(0.8 %||on||MRR)
(Tasks||Machine Translation||Hyperparameters)
(Hyperparameters||regularized by||20 % input dropouts)
(Hyperparameters||regularized by||30 % output dropouts)
(Hyperparameters||initial learning rate||3e - 4)
(3e - 4||for||other models)
(Hyperparameters||initial learning rate||1e - 3)
(1e - 3||for||LSTM - LSTM)
(Hyperparameters||trained to minimize||word - level cross entropy loss)
(Hyperparameters||l 2 regularizer strength||3 e - 5)
(Hyperparameters||batch size||128)
(Hyperparameters||train||40 epochs)
(Tasks||Natural Language Inference||Hyperparameters)
(Hyperparameters||initial learning rate||3e - 4)
(Hyperparameters||hidden layer||1024 units)
(1024 units||with||ReLU activation)
(1024 units||with||sof tmax layer)
(Hyperparameters||l 2 regularizer strength||3 e - 5)
(Hyperparameters||batch size||128)
(Hyperparameters||train||40 epochs)
(Tasks||Natural Language Inference||Results)
(Results||has||MMA - NSE attention model)
(MMA - NSE attention model||obtained||85.4 % accuracy score)
(Tasks||Sentence Classification||Hyperparameters)
(Hyperparameters||second layer||sof tmax layer)
(Hyperparameters||read / write modules||two one - layer LSTM)
(two one - layer LSTM||with||300 hidden units)
(Hyperparameters||initial learning rate||3e - 4)
(Hyperparameters||l 2 regularizer strength||3 e - 5)
(Hyperparameters||batch size||64)
(Hyperparameters||word embeddings||pre-trained 300 - D Glove 840B vectors)
(Hyperparameters||first layer of the MLP||ReLU activation)
(Hyperparameters||first layer of the MLP||1024 or 300 units)
(1024 or 300 units||for||binary or fine - grained setting)
(Hyperparameters||train||25 epochs)
(Tasks||Sentence Classification||Results)
(Results||has||Our model)
(Our model||outperformed||DMN)
(Our model||set||state - of - the - art results)
(state - of - the - art results||on||both subtasks)
(Tasks||Document Sentiment Analysis||Hyperparameters)
(Hyperparameters||stack||NSE or LSTM)
(NSE or LSTM||on the top of||another NSE)
(another NSE||for||document modeling)
(Hyperparameters||trained||50 epochs)
(Hyperparameters||initial learning rate||3e - 4)
(Hyperparameters||l 2 regularizer strength||1 e - 5)
(Hyperparameters||batch size||32)
(Hyperparameters||has||The whole network)
(The whole network||trained jointly by backpropagating||cross entropy loss)
(Hyperparameters||used||one - layer LSTM)
(one - layer LSTM||with||100 hidden units)
(100 hidden units||for||read / write modules)
(Hyperparameters||used||pre-trained 100 - D Glove 6B vectors)
