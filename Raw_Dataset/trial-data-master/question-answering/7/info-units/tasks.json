{
  "has" : {
    "Tasks" : {
      "Natural Language Inference" : {
        "Hyperparameters" : {
          "hidden layer" : {
            "1024 units" : {
              "with" : ["ReLU activation", "sof tmax layer"]
            },
            "from sentence" : "Natural Language Inference
In addition , the MLP has a hidden layer with 1024 units with ReLU activation and a sof tmax layer ."

          },
          "batch size" : "128",
          "initial learning rate" : "3e - 4",
          "l 2 regularizer strength" : "3 e - 5",
          "train" : ["40 epochs", {"from sentence" : "We set the batch size to 128 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs ."}]
        },
        "Results" : {
          "has" : {
            "MMA - NSE attention model" : {
              "obtained" : "85.4 % accuracy score",
              "from sentence" : "Our MMA - NSE attention model is similar to the LSTM attention model .
This model obtained 85.4 % accuracy score ."

            }
          }
        }
      },
      "Answer Sentence Selection" : {
        "Hyperparameters" : {
          "batch size" : "4",
          "initial learning rate" : "1 e - 5",
          "train the model for" : ["10 epochs", {"from sentence" : "Answer Sentence Selection
We set the batch size to 4 and the initial learning rate to 1 e - 5 , and train the model for 10 epochs ."

}],
          "used" : {
            "40 % dropouts" : {
              "afterword" : "embeddings",
              "no" : "l 2 weight decay"
            },
            "from sentence" : "We used 40 % dropouts afterword embeddings and no l 2 weight decay ." 
          },
          "has" : {
            "word embeddings" : {
              "are" : "pre-trained 300 - D Glove 840B vectors",
              "has" : {
                "linear mapping layer" : {
                  "transforms" : {
                    "300 - D word embeddings" : {
                      "to" : "512- D LSTM inputs"
                    }
                  }
                }
              },
              "from sentence" : "The word embeddings are pre-trained 300 - D Glove 840B vectors .
For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs ."   

            }
          }
        },
        "Results" : {
          "has" : {
            "MMA - NSE attention model" : {
              "exceeds" : {
                "NASM" : {
                  "by approximately" : [{"1 %" : {"on" : "MAP"}}, {"0.8 %" : {"on" : "MRR"}}]
                }
              },
              "from sentence" : "Our MMA - NSE attention model exceeds the NASM by approximately 1 % on MAP and 0.8 % on MRR for this task ."
            }
          }
        }
      },
      "Sentence Classification" : {
        "Hyperparameters" : {
          "first layer of the MLP" : ["ReLU activation", {"1024 or 300 units" : {"for" : "binary or fine - grained setting"}}, 
		  {"from sentence": "Sentence Classification
		  The first layer of the MLP has ReLU activation and 1024 or 300 units for binary or fine - grained setting ."
		  
		  }],
          "second layer" : ["sof tmax layer", {"from sentence" : "The second layer is a sof tmax layer ."}],
          "read / write modules" : {
            "two one - layer LSTM" : {
              "with" : "300 hidden units"
            }
          },
          "word embeddings" : ["pre-trained 300 - D Glove 840B vectors", {"from sentence" : "The read / write modules are two one - layer LSTM with 300 hidden units and the word embeddings are the pre-trained 300 - D Glove 840B vectors ."}],
          "batch size" : "64",
          "initial learning rate" : "3e - 4",
          "l 2 regularizer strength" : "3 e - 5",
          "train" : ["25 epochs", {"from sentence" : "We set the batch size to 64 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 3 e - 5 , and train each model for 25 epochs ."}]
        },
        "Results" : {
          "has" : {
            "Our model" : {
              "outperformed" : "DMN",
              "set" : {
                "state - of - the - art results" : {
                  "on" : "both subtasks"
                }
              },
              "from sentence" : "Our model outperformed the DMN and set the state - of - the - art results on both subtasks ."              
            }
          }
        }
      },
      "Document Sentiment Analysis" : {
        "Hyperparameters" : {
          "stack" : {
            "NSE or LSTM" : {
              "on the top of" : {
                "another NSE" : {
                  "for" : "document modeling"
                }
              }
            },
            "from sentence" : "Document Sentiment Analysis
			We stack a NSE or LSTM on the top of another NSE for document modeling ."
			
          },
          "has" : {
            "The whole network" : {
              "trained jointly by backpropagating" : "cross entropy loss",
              "from sentence" : "The whole network is trained jointly by backpropagating the cross entropy loss ."
            }
          },
          "used" : [
            {"one - layer LSTM" : {
              "with" : {
                "100 hidden units" : {
                  "for" : "read / write modules"
                }
              }
            }},
            "pre-trained 100 - D Glove 6B vectors",
            {"from sentence" : "We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task ."}],
          "batch size" : "32",
          "initial learning rate" : "3e - 4",
          "l 2 regularizer strength" : "1 e - 5",
          "trained" : ["50 epochs", {"from sentence" : "We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs ."}]
        }
      },
      "Machine Translation" : {
        "Hyperparameters" : {
          "trained to minimize" : "word - level cross entropy loss",
          "regularized by" : ["20 % input dropouts", "30 % output dropouts", 
		  {"from sentence" : "Machine Translation
The models were trained to minimize word - level cross entropy loss and were regularized by 20 % input dropouts and the 30 % output dropouts ."

}],
          "batch size" : "128",
          "initial learning rate" : {
            "1e - 3" : {
              "for" : "LSTM - LSTM"
            },
            "3e - 4" : {
              "for" : "other models"
            }
          },
          "l 2 regularizer strength" : "3 e - 5",
          "train" : ["40 epochs", {"from sentence" : "We set the batch size to 128 , the initial learning rate to 1e - 3 for LSTM - LSTM and 3e - 4 for the other models and l 2 regularizer strength to 3 e - 5 , and train each model for 40 epochs ."}]
        }
      }
    }
  }
}