{
  "has" : {
    "Baselines" : {
      "has" : {
        "WORDEMBED" : {
          "represent" : {
            "each short - text" : {
              "as" : "sum of the embedding of the words it contains"
            },
            "from sentence" : "WORDEMBED : We first represent each short - text as the sum of the embedding of the words it contains ."
          },
          "calculated" : {
            "matching score" : {
              "of" : {
                "two short - texts" : {
                  "with" : {
                    "MLP" : {
                      "with" : "embedding of the two documents as input"
                    }
                  }
                }
              }
            }
          }
        },
        "DEEPMATCH" : {
          "take" : {
            "matching model" : {
              "train it on" : {
                "our datasets" : {
                  "with" : "3 hidden layers and 1,000 hidden nodes in the first hidden layer"
                }
              } 
            }
          }
        },
        "URAE+ MLP" : {
          "use" : {
            "Unfolding Recursive Autoencoder" : {
              "get" : {
                "100 dimensional vector representation" : {
                  "of" : "each sentence"
                }
              },
              "put" : {
                "an MLP" : {
                  "on the top as in" : "WORDEMBED"
                }
              }
            }
          }
        },
        "SENNA + MLP / SIM" : {
          "use" : {
            "SENNA - type sentence model" : {
              "for" : "sentence representation"
            },
            "from sentence" : "The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :
We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :
We use the SENNA - type sentence model for sentence representation ;"

          }
        },
        "SENMLP" : {
          "take" : {
            "whole sentence" : {
              "as" : {
                "input" : {
                  "with" : "word embedding aligned sequentially"
                }
              }
            }
          },
          "use" : {
            "MLP" : {
              "to obtain" : "score of coherence"
            }
          },
          "from sentence" : "SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence ."
        }
      }
    }
  }
}