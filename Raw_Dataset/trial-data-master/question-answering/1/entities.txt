142	0	9	WORDEMBED
142	21	30	represent
142	31	48	each short - text
142	49	51	as
142	56	101	sum of the embedding of the words it contains
143	44	54	calculated
143	4	18	matching score
143	19	21	of
143	22	39	two short - texts
143	55	59	with
143	63	66	MLP
143	67	71	with
143	76	115	embedding of the two documents as input
143	118	127	DEEPMATCH
143	133	137	take
143	142	156	matching model
143	164	175	train it on
143	176	188	our datasets
143	189	193	with
143	194	258	3 hidden layers and 1,000 hidden nodes in the first hidden layer
143	261	270	URAE+ MLP
144	3	6	use
144	11	42	Unfolding Recursive Autoencoder
144	46	49	get
144	52	89	100 dimensional vector representation
144	90	92	of
144	93	106	each sentence
144	113	116	put
144	117	123	an MLP
144	124	140	on the top as in
144	141	150	WORDEMBED
144	153	170	SENNA + MLP / SIM
145	3	6	use
145	11	38	SENNA - type sentence model
145	39	42	for
145	43	66	sentence representation
146	0	6	SENMLP
146	12	16	take
146	21	35	whole sentence
146	36	38	as
146	39	44	input
146	47	51	with
146	52	87	word embedding aligned sequentially
146	96	99	use
146	103	106	MLP
146	107	116	to obtain
146	121	139	score of coherence
127	20	23	use
127	24	51	stochastic gradient descent
127	52	55	for
127	60	82	optimization of models
131	7	38	50 - dimensional word embedding
131	39	51	trained with
131	56	65	Word2 Vec
136	7	11	ReLu
136	12	14	as
136	19	38	activation function
136	39	42	for
136	43	80	all of models ( convolution and MLP )
136	89	95	yields
136	96	124	comparable or better results
136	125	127	to
136	128	152	sigmoid - like functions
136	159	168	converges
136	169	175	faster
134	7	22	3 - word window
134	23	33	throughout
134	34	49	all experiments
134	58	62	test
134	63	94	various numbers of feature maps
134	97	111	typically from
134	112	122	200 to 500
134	127	130	for
134	131	150	optimal performance
128	24	43	perform better with
128	44	77	mini-batch ( 100 ? 200 in sizes )
128	84	90	can be
128	98	110	parallelized
128	111	113	on
128	114	128	single machine
128	129	133	with
128	134	145	multi-cores
129	0	18	For regularization
129	59	73	early stopping
129	34	37	for
129	88	94	models
129	95	99	with
129	100	111	medium size
129	116	135	large training sets
129	138	142	with
129	143	162	over 500K instances
16	22	29	propose
16	30	56	deep neural network models
16	65	70	adapt
16	75	97	convolutional strategy
16	140	142	to
16	143	159	natural language
17	0	18	To further explore
17	23	31	relation
17	32	39	between
17	40	80	representing sentences and matching them
17	86	92	devise
17	95	106	novel model
17	243	247	with
17	252	283	same convolutional architecture
17	112	130	can naturally host
17	140	178	hierarchical composition for sentences
17	187	242	simple - to - comprehensive fusion of matching patterns
2	47	82	Matching Natural Language Sentences
4	0	17	Semantic matching
11	0	54	Matching two potentially heterogenous language objects
15	13	32	sentence - matching
173	0	8	ARC - II
173	9	46	outperforms others significantly when
173	51	93	training instances are relatively abundant
176	35	86	convolutional models ( ARC - I & II , SENNA + MLP )
176	87	109	perform favorably over
176	110	133	bag - of - words models
177	58	78	ARC - I and ARC - II
177	79	98	trained purely with
177	99	115	random negatives
177	116	166	automatically gain some ability in telling whether
177	171	196	words in a given sentence
177	201	203	in
177	204	226	right sequential order
179	16	61	simple sum of embedding learned via Word2 Vec
179	62	68	yields
179	69	92	reasonably good results
179	93	95	on
179	96	111	all three tasks
