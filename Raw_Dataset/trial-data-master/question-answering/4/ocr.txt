A Parallel-Hierarchical Model for Machine Comprehension on Sparse

Data
Adam Trischler Zheng Ye Xingdi Yuan Jing He Phillip Bachman
adam.trischler jeff.ye eric.yuan jJing.he phil.bachman

Kaheer Suleman
k.suleman@maluuba.com
Maluuba Research
Montreal, Québec, Canada

arX1v:1603.08884v1 [cs.CL] 29 Mar 2016

Abstract

Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions
based on short text passages to evaluate
such understanding. In this work, we investigate machine comprehension on the
challenging MCTest benchmark. Partly
because of its limited size, prior work
on MCTest has focused mainly on engineering better features. We tackle the
dataset with a neural approach, harnessing simple neural networks arranged in a
parallel hierarchy. The parallel hierarchy
enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed
to using a manually designed, rigid feature set. Perspectives range from the word
level to sentence fragments to sequences
of sentences; the networks operate only on
word-embedding representations of text.
When trained with a methodology designed to help cope with limited training
data, our Parallel-Hierarchical model sets
a new state of the art for MCTest, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over
15% absolute).

1 Introduction

Humans learn in a variety of ways—by communication with each other, and by study, the reading
of text. Comprehension of unstructured text by
machines, at a near-human level, is a major goal
for natural language processing. It has garnered
significant attention from the machine learning research community in recent years.

Machine comprehension (MC) is evaluated by
posing a set of questions based on a text passage (akin to the reading tests we all took in
school). Such tests are objectively gradable and
can be used to assess a range of abilities, from
basic understanding to causal reasoning to inference (Richardson et al., 2013). Given a text passage and a question about its content, a system is
tested on its ability to determine the correct answer (Sachan et al., 2015). In this work, we focus
on MCTest, a complex but data-limited comprehension benchmark, whose multiple-choice questions require not only extraction but also inference and limited reasoning
2013). Inference and reasoning are important human skills that apply broadly, beyond language.

We present a parallel-hierarchical approach to
machine comprehension designed to work well in
a data-limited setting. There are many use-cases in
which comprehension over limited data would be
handy: for example, user manuals, internal documentation, legal contracts, and so on. Moreover, work towards more efficient learning from
any quantity of data is important in its own right,
for bringing machines more in line with the way
humans learn. Typically, artificial neural networks
require numerous parameters to capture complex
patterns, and the more parameters, the more training data is required to tune them. Likewise, deep
models learn to extract their own features, but this
is a data-intensive process. Our model learns to
comprehend at a high level even when data is
sparse.

The key to our model is that it compares the
question and answer candidates to the text using
several distinct perspectives. We refer to a question combined with one of its answer candidates
as a hypothesis (to be detailed below). The semantic perspective compares the hypothesis to sentences in the text viewed as single, self-contained
thoughts; these are represented using a sum and
transformation of word embedding vectors, similarly to in|Weston et al. (2014). The word-by-word
perspective focuses on similarity matches between
individual words from hypothesis and text, at various scales. As in the semantic perspective, we
consider matches over complete sentences. We
also use a sliding window acting on a subsentential
scale (inspired by the work of {Hill et al. (2015)),
which implicitly considers the linear distance between matched words. Finally, this word-level
sliding window operates on two different views of
text sentences: the sequential view, where words
appear in their natural order, and the dependency
view, where words are reordered based on a linearization of the sentence’s dependency graph.
Words are represented throughout by embedding
vectors (Mikolov et al., 2013). These distinct perspectives naturally form a hierarchy that we depict in Figure/I| Language is hierarchical, so it
makes sense that comprehension relies on hierarchical levels of understanding.

The perspectives of our model can be considered a type of feature. However, they are implemented by parametric differentiable functions.
This is in contrast to most previous efforts on
MCTest, whose numerous hand-engineered features cannot be trained. Our model, significantly,
can be trained end-to-end with backpropagation.
To facilitate learning with limited data, we also
develop a unique training scheme. We initialize
the model’s neural networks to perform specific
heuristic functions that yield decent (thought not
impressive) performance on the dataset. Thus, the
training scheme gives the model a safe, reasonable
baseline from which to start learning. We call this
technique training wheels.

Computational models that comprehend (insofar as they perform well on MC datasets) have
developed contemporaneously in several research

groups
mar et al., 2015). Models designed specifically for
MCTest include those of|Richardson et al. (2013),
and more recently |Sachan et al. (2015),
(McAlester 20TS), and[Yin etal. (2016). In exper

iments, our Parallel-Hierarchical model achieves
state-of-the-art accuracy on MCTest, outperforming these existing methods.

Below we describe related work, the mathematical details of our model, and our experiments,

then analyze our results.

2 The Problem

In this section we borrow from |Sachan_et_al.
(2015), who laid out the MC problem nicely. Machine comprehension requires machines to answer
questions based on unstructured text. This can
be viewed as selecting the best answer from a set
of candidates. In the multiple-choice case, candidate answers are predefined, but candidate answers may also be undefined yet restricted (e.g., to

yes, no, or any noun phrase in the text)
al., 2015).

For each question q, let 7’ be the unstructured
text and A = {a;} the set of candidate answers
to g. The machine comprehension task reduces to
selecting the answer that has the highest evidence
given T. As in|Sachan et al. (2015), we combine
an answer and a question into a hypothesis, h; =
f(q, a;). To facilitate comparisons of the text with
the hypotheses, we also break down the passage
into sentences t;, T = {t;}. In our setting, gq,
a;, and t; each represent a sequence of embedding
vectors, one for each word and punctuation mark
in the respective item.

3 Related Work

Machine comprehension is currently a hot topic
within the machine learning community. In this
section we will focus on the best-performing models applied specifically to MCTest, since it 1s somewhat unique among MC datasets (see Section (5).
Generally, models can be divided into two categories: those that use fixed, engineered features,
and neural models. The bulk of the work on
MCTest falls into the former category.

Manually engineered features often require significant effort on the part of a designer, and/or
various auxiliary tools to extract them, and they
cannot be modified by training. On the other
hand, neural models can be trained end-to-end and
typically harness only a single feature: vectorrepresentations of words. Word embeddings are
fed into a complex and possibly deep neural network which processes and compares text to question and answer. Among deep models, mechanisms of attention and working memory are com
mon, as in|Weston et al. (2014) and/Hermann et al.|
(2015).
3.1 Feature-engineering models

Sachan et al. (2015) treated MCTest as a structured

prediction problem, searching for a latent answerentailing structure connecting question, answer,
and text. This structure corresponds to the best
latent alignment of a hypothesis with appropriate snippets of the text. The process of (latently)
selecting text snippets is related to the attention
mechanisms typically used in deep networks de
signed for MC and machine translation (Bahdanau
et al., 2014; ‘Weston et al., 2014; |Hill et al.,
2015; |Hermann et al., 2015). The model uses

event and entity coreference links across sentences
along with a host of other features. These include
specifically trained word vectors for synonymy;
antonymy and class-inclusion relations from external database sources; dependencies and semantic role labels. The model is trained using a latent
structural SVM extended to a multitask setting, so
that questions are first classified using a pretrained
top-level classifier. This enables the system to use
different processing strategies for different question categories. The model also combines question
and answer into a well-formed statement using the

rules of|Cucerzan and Agichtein (2005).

Our model is simpler than that of
al. (2015) in terms of the features it takes in, the

training procedure (stochastic gradient descent vs.
alternating minimization), question classification
(we use none), and question-answer combination
(simple concatenation or mean vs. a set of rules).

Wang and McAllester (2015) augmented the
baseline feature set from |Richardson et al. (2013)

with features for syntax, frame semantics, coreference chains, and word embeddings. They combined features using a linear latent-variable classifier trained to minimize a max-margin loss func
tion. As in|Sachan et al. (2015), questions and an
swers are combined using a set of manually written rules. The method of
(2015) achieved the previous state of the art, but
has significant complexity in terms of the feature
set.

Space does not permit a full description of all
models in this category, but see also ‘Smith et al.)
and|Narasimhan and Barzilay (2015).

Despite its relative lack of features, the ParallelHierarchical model improves upon the featureengineered state of the art for MCTest by a small
amount (about 1% absolute) as detailed in Sec
tion|5]

3.2 Neural models

Neural models have, to date, performed relatively
poorly on MCTest. This is because the dataset 1s
sparse and complex.

Yin et al. (2016) investigated deep-learning
approaches concurrently with the present work.
They measured the performance of the Attentive

Reader (Hermann et al., 2015) and the Neural Rea
soner (Peng et al., 2015), both deep, end-to-end
recurrent models with attention mechanisms, and

also developed an attention-based convolutional
network, the HABCNN. Their network operates
on a hierarchy similar to our own, providing further evidence of the promise of hierarchical perspectives. Specifically, the HABCNN processes
text at the sentence level and the snippet level,
where the latter combines adjacent sentences (as
we do through an n-gram input). Embedding vectors for the question and the answer candidates
are combined and encoded by a convolutional network. This encoding modulates attention over sentence and snippet encodings, followed by maxpooling to determine the best matches between
question, answer, and text. As in the present work,
matching scores are given by cosine similarity.
The HABCNN also makes use of a question classifier.

Despite the shared concepts between the
HABCNN and our approach, the ParallelHierarchical model performs significantly better
on MCTest (more than 15% absolute) as detailed
in Section[5] Other neural models tested in{Yin et!

fare even worse.
4 The Parallel-Hierarchical Model

Let us now define our machine comprehension
model in full. We first describe each of the perspectives separately, then describe how they are
combined. Below, we use subscripts to index elements of sequences, like word vectors, and superscripts to indicate whether elements come from
the text, question, or answer. In particular, we use
the subscripts k,m,n,p to index sequences from
the text, question, answer, and hypothesis, respectively, and superscripts t,q,a,h. We depict the
model schematically in Figure[I]

4.1 Semantic Perspective

The semantic perspective is similar to the Memory Networks approach for embedding inputs into

memory space (Weston et al., 2014). Each sen       
     

   

unigram fj
Gh by

t-1| ti lie

bigram

trigram

Figure 1: Schematic of the Parallel-Hierarchical
model. SW stands for “sliding window.” MLP
represents a general neural network.

tence of the text is a sequence of d-dimensional
word vectors: t; = {t,}, t, € R?. The semantic
vector s’ is computed by embedding the word vectors into a D-dimensional space using a two-layer
network that implements weighted sum followed
by an affine tranformation and a nonlinearity; 7.e.,

sh=f (ATS uxt, +b4 J. (1)
k

The matrix A’ € R”%*4, the bias vector b4, €
R?, and for f we use the leaky ReLU function. The scalar w; is a trainable weight associated to each word in the vocabulary. These scalar
weights implement a kind of exogenous or bottomup attention that depends only on the input stimulus (Mayer et al., 2004). They can, for example,
learn to perform the function of stopword lists in
a soft, trainable way, to nullify the contribution of
unimportant filler words.

The semantic representation of a hypothesis is
formed analogously, except that we combine the
question word vectors q,,, and answer word vectors a, as a single sequence {h,} = {Qm,an}.
For semantic vector s” of the hypothesis, we use
a unique transformation matrix A” € R?*? and
bias vector b”, € R?.

These transformations map a text sentence and
a hypothesis into a common space where they can
be compared. We compute the semantic match be
tween text sentence and hypothesis using the cosine similarity,

M*™ = cos(s‘,s"). (2)

4.2 Word-by-Word Perspective

The first step in building the word-by-word perspective is to transform word vectors from a
text sentence, question, and answer through respective neural functions. For the text, t, =
f (B't, + bi), where B‘ € R?**, bi, € R” and
f is again the leaky ReLU. We transform the question and the answer to q,, and a, analogously using distinct matrices and bias vectors. In contrast
with the semantic perspective, we keep the question and answer candidates separate in the wordby-word perspective. This is because matches
to answer words are inherently more important
than matches to question words, and we want our
model to learn to use this property.

4.2.1 Sentential

Inspired by the work of |Wang and Jiang (2015)

in paraphrase detection, we compute matches between hypotheses and text sentences at the word
level. This computation uses the cosine similarity
as before:

cl = cos(th, Gm); (3)

— cos(tp, a, ). (4)

The word-by-word match between a text sentence and question is determined by taking the
maximum over & (finding the text word that best
matches each question word) and then taking a
weighted mean over m (finding the average match
over the full question):

1
M!i = FZ DW MAX Cy (5)

Here, Ww, 1s the word weight for the question word
and Z normalizes these weights to sum to one over
the question. We define the match between a sentence and answer candidate, 1/“, analogously. Finally, we combine the matches to question and answer according to

MY" = a1M%+a9M%+a3M4M"._ (6)

Here the qa are trainable parameters that control
the relative importance of the terms.
4.2.2 Sequential Sliding Window

The sequential sliding window is related to the
original MCTest baseline by
(2013). Our sliding window decays from its focus
word according to a Gaussian distribution, which
we extend by assigning a trainable weight to each
location in the window. This modification enables
the window to use information about the distance
between word matches; the original baseline used
distance information through a predefined function.

The sliding window scans over the words of
the text as one continuous sequence, without sentence breaks. Each window is treated like a sentence in the previous subsection, but we include a
location-based weight A(k). This weight is based
on a word’s position in the window, which, given
a window, depends on its global position k. The
cosine similarity is adapted as

stn = A(k) cos(tk, Gm), (7)

for the question and analogously for the answer.
We initialize the location weights with a Gaussian and fine-tune them during training. The final
matching score, denoted as (/°”*, is computed as

in|(5)|and|(6)| with Si, replacing cj,,,,.

4.2.3 Dependency Sliding Window

The dependency sliding window operates identically to the linear sliding window, but on a different view of the text passage. The output of this
component is 1/4 and is formed analogously to
Apes

The dependency perspective uses the Stanford
Dependency Parser as
an auxiliary tool. Thus, the dependency graph can
be considered a fixed feature. Moreover, linearization of the dependency graph, because it relies
on an eigendecomposition, is not differentiable.
However, we handle the linearization in data preprocessing so that the model sees only reordered
word-vector inputs.

Specifically, we run the Stanford Dependency
Parser on each text sentence to build a dependency
graph. This graph has n,, vertices, one for each
word in the sentence. From the dependency graph
we form the Laplacian matrix L € R"’*”"” and
determine its eigenvectors. The second eigenvector ug of the Laplacian is known as the Fiedler

vector. It is the solution to the minimization

N
minimize } | nij(g(vi) — 9(v))", 8)
i,j=l

where v; are the vertices of the graph, and 7;;
is the weight of the edge from vertex 2 to vertex
4 (Golub and Van Loan, 2012). The Fiedler vector
maps a weighted graph onto a line such that connected nodes stay close, modulated by the connection weights|"] This enables us to reorder the words
of a sentence based on their proximity in the dependency graph. The reordering of the words is
given by the ordered index set

I = arg sort(ug). (9)

To give an example of how this works, consider the following sentence from MCTest and its
dependency-based reordering:

Jenny, Mrs. Mustard ’s helper, called the
police.

the police, called Jenny helper, Mrs. ’s
Mustard.

Sliding-window-based matching on the original
sentence will answer the question Who called the
police? with Mrs. Mustard. The dependency reordering enables the window to determine the correct answer, Jenny.

4.3 Combining Distributed Evidence

It is important in comprehension to synthesize information found throughout a document. MCTest
was explicitly designed to ensure that it could not
be solved by lexical techniques alone, but would
instead require some form of inference or limited
reasoning (Richardson et al., 2013). It therefore
includes questions where the evidence for an answer spans several sentences.

To perform synthesis, our model also takes in ngrams of sentences, i.e., sentence pairs and triples
strung together. The model treats these exactly as
it does single sentences, applying all functions detailed above. A later pooling operation combines
scores across all n-grams (including the singlesentence input). This is described in the next subsection.

'We experimented with assigning unique edge weights to
unique relation types in the dependency graph. However, this

had negligible effect. We hypothesize that this is because
dependency graphs are trees, without cycles.
With n-grams, the model can combine information distributed across contiguous sentences.
In some cases, however, the required evidence is
spread across distant sentences. To give our model
some capacity to deal with this scenario, we take
the top VV sentences as scored by all the preceding functions, and then repeat the scoring computations viewing these top NV as a single sentence.

The reasoning behind these approaches can be
explained well in a probabilistic setting. If we consider our similarity scores to model the likelihood
of a text sentence given a hypothesis, p(t,|hi),
then the n-gram and top N approaches model a
joint probability p(t;,,t;,,...,¢,;,|hs). We cannot
model the joint probability as a product of individual terms (score values) because distributed pieces
of evidence are likely not independent.

4.4 Combining Perspectives

We use a multilayer perceptron to combine M*™,
Mword swe and M®S as a final matching score
MM, for each answer candidate. This network also
pools and combines the separate n-gram scores,
and uses a linear activation function.

Our overall training objective is to minimize the
ranking loss

L(T,q, A) = max(0, po + max Mizi« — Mix),
(10)
where pu is a constant margin, 7* indexes the correct answer, and we take the maximum over 2 so
that we are ranking the correct answer over the
best-ranked incorrect answer (of which there are
three). This approach worked better than comparing the correct answer to the incorrect answers individually as in|Wang and McAlester (2015).
Our implementation of the Parallel-Hierarchical
model, using the Keras framework, 1s available on
GithubF|

4.5 Training Wheels

Before training, we initialized the neural-network
components of our model to perform sensible
heuristic functions. Training did not converge on
the small MCTest without this vital approach.
Empirically, we found that we could achieve
above 50% accuracy on MCTest using a simple
sum of word vectors followed by a dot product between the question sum and the hypothesis sum.

*http://www.hiddenwebsite.com

Therefore, we initialized the network for the semantic perspective to perform this sum, by initializing A” as the identity matrix and b%, as the
zero vector, x € {t,h}. Recall that the activation
function is a ReLU so that positive outputs are unchanged.

We also found basic word-matching scores to
be helpful, so we initialized the word-by-word
networks likewise. The network for perspectivecombination was initialized to perform a sum of
individual scores, using a zero bias-vector and a
weight matrix of ones, since we found that each
perspective contributed positively to the overall result.

This training wheels approach is related to other
techniques from the literature. For instance,
proposed the identity-matrix initialization in the context of recurrent neural networks in
order to preserve the error signal through backpropagation. In residual networks
2015), shortcut connections bypass certain layers
in the network so that a simpler function can be
trained in conjunction with the full model.

5 Experiments

5.1 The Dataset

MCTest is a collection of 660 elementary-level
children’s stories and associated questions, written by human subjects. The stories are fictional,
ensuring that the answer must be found in the text
itself, and carefully limited to what a young child
can understand (Richardson et al., 2013).

The more challenging variant consists of 500
stories with four multiple-choice questions each.
Despite the elementary level, stories and questions
are more natural and more complex than those
found in synthetic MC datasets like bAb/
and CNN (Hermann etal., 2013).

MCTest is challenging because it is both complicated and small. As per {Hill et al. (2015), “it
is very difficult to train statistical models only on
MCTest.” Its size limits the number of parameters that can be trained, and prevents learning any
complex language modeling simultaneously with
the capacity to answer questions.

5.2. Training and Model Details

In this section we describe important details of the
training procedure and model setup. For a complete list of hyperparameter settings, our stopword
list, and other minutiae, we refer interested readers
to our Github repository.

For word vectors we use Google’s publicly
available embeddings, trained with word2vec on
the 100-billion-word News corpus
2013). These vectors are kept fixed throughout
training, since we found that training them was
not helpful (likely because of MCTest’s size). The
vectors are 300-dimensional (d = 300).

We do not use a stopword list for the text passage, instead relying on the trainable word weights
to ascribe global importance ratings to words.
These weights are initialized with the inverse document frequency (IDF) statistic computed over the
MCTest corpus] However, we do use a short stopword list for questions. This list nullifies query
words such as { Who, what, when, where, how},
along with conjugations of the verbs to do and to
be.

Following earlier methods, we use a heuristic to improve performance on negation questions
2015). When a question contains the words which
and not, we negate the hypothesis ranking scores
so that the minimum becomes the maximum.

The most important technique for training the
model was the training wheels approach. Without
this, training was not effective at all. The identity initialization requires that the network weight
matrices are square (d = D).

We found dropout to be
particularly effective at improving generalization
from the training to the test set, and used 0.5 as
the dropout probability. Dropout occurs after all
neural-network transformations, if those transformations are allowed to change with training. Our
best performing model held networks at the wordby-word level fixed.

For combining distributed evidence, we used
up to trigrams over sentences and our bestperforming model reiterated over the top two sentences (NV = 2).

We used the Adam optimizer with the standard
settings and a learning
rate of 0.003. To determine the best hyperparameters we performed a grid search over 150 settings based on validation-set accuracy. MCTest’s
original validation set is too small for reliable
hyperparameter tuning, so, following |Wang and

>We override the IDF initialization for words like not,

 

which are frequent but highly informative.

McAllester (2015), we merged the training and

validation sets of MCTest-160 and MCTest-500,
then split them randomly into a 250-story training
set and a 200-story validation set.

5.3. Results

Table |1} presents the performance of featureengineered and neural methods on the MCTest test
set. Accuracy scores are divided among questions
whose evidence lies in a single sentence (single)
and across multiple sentences (multi), and among
the two variants. Clearly, MCTest-160 is easier.

The first three rows represent featureengineered methods. +
RTE 1s the best-performing variant of the original
baseline published along with MCTest. It uses
a lexical sliding window and distance-based
measure, augmented with rules for recognizing
textual entailment. We described the methods
of|Sachan et al. (015) and|Wang and McAlester
(2015) in Section |3] On MCTest-500, the Parallel
Hierarchical model significantly outperforms
these methods on single questions (> 2%) and
slightly outperforms the latter two on multi
questions (~ 0.3%) and overall (~ 1%). The
method of achieves
the best overall result on MCTest-160. We suspect
this is because our neural method suffered from
the relative lack of training data.

The last four rows in Table|1| are neural methods that we discussed in Section [3] Performance
measures are taken from [Yin et al. (2016). Here
we see our model outperforming the alternatives
by a large margin across the board (> 15%).
The Neural Reasoner and the Attentive Reader
are large, deep models with hundreds of thousands of parameters, so it is unsurprising that they
performed poorly on MCTest. The specificallydesigned HABCNN fared better, its convolutional
architecture cutting down on the parameter count.
Because there are similarities between our model
and the HABCNN, we hypothesize that much of
the performance difference is attributable to our
training wheels methodology.

6 Analysis and Discussion

We measure the contribution of each component of
the model by ablating it. Results are given in Table {2} Not surprisingly, the n-gram functionality
is important, contributing almost 5% accuracy improvement. Without this, the model has almost no
Method

 

Single (112)
Richardson et al. (2013) + RTE
Sachan et al. (2015)
Wang et al. (2015)
Attentive Reader
Neural Reasoner
HABCNN-TE
Parallel-Hierarchical

 

 

MCTest-160 accuracy (%)
Multiple (128) All

 

 

MCTest-500 accuracy (%)
59.45
67.99
67.94
39.5

63.33
67.83
69.94
41.9

 

 

Table 1: Experimental results on MCTest.

Ablated component | Test accuracy (%)

A

SW-dependency 70.00
Word weights 66.51

Table 2: Ablation study on MCTest-500 (all).

 

means for synthesizing distributed evidence. The
top V function contributes very little to the overall performance, suggesting that most multi questions have their evidence distributed across contiguous sentences. Ablating the sentential component made the most significant difference, reducing performance by more than 5%. Simple
word-by-word matching is obviously useful on
MCTest. The sequential sliding window makes a
3% contribution, highlighting the importance of
word-distance measures. On the other hand, the
dependency-based sliding window makes only a
minor contribution. We found this surprising. It
may be that linearization of the dependency graph
removes too much of its information. Finally, the
exogenous word weights make a significant contribution of almost 5%.

Analysis reveals that most of our system’s test
failures occur on questions about quantity (e.g.,
How many...?) and temporal order (e.g., Who
was invited last?). Quantity questions make up
9.5% of our errors on the validation set, while order questions make up 10.3%. This weakness is
not unexpected, since our architecture lacks any
capacity for counting or tracking temporal order.
Incorporating mechanisms for these forms of reasoning is a priority for future work (in contrast, the
Memory Network model is quite good at temporal

reasoning (Weston et al., 2014)).

The Parallel-Hierarchical model is simple. It
does no complex language or sequence modeling.
Its simplicity is a response to the limited data of

MCTest. Nevertheless, the model achieves stateof-the-art results on the multi questions, which
(putatively) require some limited reasoning. Our
model is able to handle them reasonably well just
by stringing important sentences together. Thus,
the model imitates reasoning with a heuristic. This
suggests that, to learn true reasoning abilities,
MCTest is too simple a dataset—and it is almost
certainly too small for this goal.

However, it may be that human language processing can be factored into separate processes of
comprehension and reasoning. If so, the ParallelHierarchical model is a good start on the former.
Indeed, if we train the method exclusively on single questions then its results become even more
impressive: we can achieve a test accuracy of
79.1% on MCTest-500.

7 Conclusion

We have presented the novel Parallel-Hierarchical
model for machine comprehension, and evaluated it on the small but complex MCTest. Our
model achieves state-of-the-art results, outperforming several feature-engineered and neural approaches.

Working with our model has emphasized to
us the following (not necessarily novel) concepts,
which we record here to promote further empirical
validation.

e Good comprehension of language is supported by hierarchical levels of understand
ing (Cf |Hill et al. (2015)).

e Exogenous attention (the trainable word
weights) may be broadly helpful for NLP.

e The training wheels approach, that is, initializing neural networks to perform sensible
heuristics, appears helpful for small datasets.

e Reasoning over language is challenging, but
easily simulated in some cases.
References

[Bahdanau et al.2014] Dzmitry Bahdanau, Kyunghyun
Cho, and Yoshua Bengio. 2014. Neural machine
translation by jointly learning to align and translate.
arXiv preprint arXiv: 1409.0473.

[Chen and Manning2014] Dangi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In EMNLP,
pages 740-750.

[Cucerzan and Agichtein2005] Silviu Cucerzan and
Eugene Agichtein. 2005. Factoid question answering over unstructured and structured web content.
In TREC, volume 72, page 90.

[Golub and Van Loan2012] Gene H Golub = and
Charles F Van Loan. 2012. Matrix computations,
volume 3. JHU Press.

[He et al.2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep residual learning for image recognition. arXiv preprint
arXiv: 1512.03385.

[Hermann et al.2015] Karl Moritz Hermann, Tomas
Kocisky, Edward Grefenstette, Lasse Espeholt, Will
Kay, Mustafa Suleyman, and Phil Blunsom. 2015.
Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems,
pages 1684-1692.

[Hill et al.2015] Felix Hill, Antoine Bordes, Sumit
Chopra, and Jason Weston. 2015. The goldilocks
principle: Reading children’s books with explicit memory representations. arXiv preprint
arXiv: 1511.02301.

[Kingma and Ba2014] Diederik Kingma and Jimmy
Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv: 1412.6980.

[Kumar et al.2015] Ankit Kumar, Ozan Irsoy, Jonathan
Su, James Bradbury, Robert English, Brian Pierce,
Peter Ondruska, Ishaan Gulrajani, and Richard
Socher. 2015. Ask me anything: Dynamic memory networks for natural language processing. arXiv
preprint arXiv: 1506.07285.

[Le et al.2015] Quoc V Le, Navdeep Jaitly, and Geoffrey E Hinton. 2015. A simple way to initialize
recurrent networks of rectified linear units. arXiv
preprint arXiv: 1504.0094 1.

[Mayer et al.2004] Andrew R Mayer, Jill M Dorflinger,
Stephen M Rao, and Michael Seidenberg. 2004.
Neural networks underlying endogenous and exogenous visual-spatial orienting. Neuroimage,
23(2):534—541.

[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg
Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv
preprint arXiv: 1301.3781.

[Narasimhan and Barzilay2015] Karthik Narasimhan
and Regina Barzilay. 2015. Machine comprehension with discourse relations. In 53rd Annual
Meeting of the Association for Computational
Linguistics.

[Peng et al.2015] Baolin Peng, Zhengdong Lu, Hang
Li, and Kam-Fai Wong. 2015. Towards neural network-based reasoning. arXiv preprint
arXiv: 1508.05508.

[Richardson et al.2013] Matthew Richardson, Christopher JC Burges, and Erin Renshaw. 2013. Mctest:
A challenge dataset for the open-domain machine
comprehension of text. In EMNLP, volume 1,
page 2.

[Sachan et al.2015] Mrinmaya Sachan, Avinava Dubey,
Eric P Xing, and Matthew Richardson. 2015.
Learning answerentailing structures for machine
comprehension. In Proceedings of ACL.

[Smith et al.2015] Ellery Smith, Nicola Greco, Matko
Bosnjak, and Andreas Vlachos. 2015. A strong lexical matching method for the machine comprehension test. In Proceedings of the 2015 Conference on
Empirical Methods in Natural Language Processing, pages 1693-1698, Lisbon, Portugal, September.
Association for Computational Linguistics.

[Srivastava et al.2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: A simple way to
prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):19291958.

[Sukhbaatar et al.2015] Sainbayar Sukhbaatar, Jason
Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in Neural Information
Processing Systems, pages 2431-2439.

[Wang and Jiang2015] Shuohang Wang and Jing Jiang.
2015. Learning natural language inference with
Istm. arXiv preprint arXiv: 1512.08849.

[Wang and McAllester2015] Hai Wang and Mohit
Bansal Kevin Gimpel David McAllester. 2015.
Machine comprehension with syntax, frames, and
semantics. Volume 2: Short Papers, page 700.

[Weston et al.2014] Jason Weston, Sumit Chopra, and
Antoine Bordes. 2014. Memory networks. arXiv
preprint arXiv: 1410.3916.

[Yin et al.2016] Wenpeng Yin, Sebastian Ebert, and
Hinrich Schiitze. 2016. Attention-based convolutional neural network for machine comprehension.
arXiv preprint arXiv: 1602.04341.
