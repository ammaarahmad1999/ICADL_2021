259	47	56	p	important
259	23	43	n	n-gram functionality
259	59	71	p	contributing
259	72	103	n	almost 5 % accuracy improvement
262	19	42	p	contributes very little
262	4	18	n	top N function
262	73	83	p	suggesting
262	89	169	n	most multi questions have their evidence distributed across contiguous sentences
263	13	33	n	sentential component
263	34	38	p	made
263	43	70	n	most significant difference
263	73	93	p	reducing performance
263	94	110	n	by more than 5 %
265	4	29	n	sequential sliding window
265	30	35	p	makes
265	38	54	n	3 % contribution
265	57	69	p	highlighting
265	74	112	n	importance of word - distance measures
264	53	55	p	on
264	56	62	n	MCTest
264	36	52	p	obviously useful
264	0	32	n	Simple word - by - word matching
266	64	89	p	only a minor contribution
266	24	57	n	dependency - based sliding window
269	44	55	p	significant
269	14	36	n	exogenous word weights
269	56	71	p	contribution of
269	72	82	n	almost 5 %
218	0	3	p	For
218	4	16	n	word vectors
218	20	23	p	use
218	24	63	n	Google 's publicly available embeddings
218	66	78	p	trained with
218	79	87	n	word2vec
218	88	90	p	on
218	95	127	n	100 - billion - word News corpus
231	3	8	p	found
231	9	16	n	dropout
231	36	58	p	effective at improving
231	59	107	n	generalization from the training to the test set
231	114	118	p	used
231	119	122	n	0.5
231	123	125	p	as
231	130	149	n	dropout probability
235	3	7	p	used
235	12	26	n	Adam optimizer
235	27	31	p	with
235	36	78	n	standard settings ( Kingma and Ba , 2014 )
235	85	98	p	learning rate
235	102	107	n	0.003
236	0	12	p	To determine
236	17	37	n	best hyperparameters
236	41	50	p	performed
236	53	64	n	grid search
236	65	69	p	over
236	70	82	n	150 settings
236	83	91	p	based on
236	92	117	n	validation - set accuracy
27	32	40	p	compares
27	45	75	n	question and answer candidates
27	8	10	p	to
27	83	87	n	text
27	88	93	p	using
27	94	123	n	several distinct perspectives
29	4	24	n	semantic perspective
29	25	33	p	compares
29	38	48	n	hypothesis
29	49	51	p	to
29	52	61	n	sentences
29	62	64	p	in
29	69	73	n	text
29	74	83	p	viewed as
29	84	118	n	single , self - contained thoughts
29	131	148	p	represented using
29	151	199	n	sum and transformation of word embedding vectors
30	4	32	n	word - by - word perspective
30	33	43	p	focuses on
30	44	62	n	similarity matches
30	63	70	p	between
30	71	132	n	individual words from hypothesis and text , at various scales
32	8	11	p	use
32	14	28	n	sliding window
32	97	117	p	implicitly considers
32	122	159	n	linear distance between matched words
33	43	54	p	operates on
33	55	92	n	two different views of text sentences
33	99	114	n	sequential view
33	117	122	p	where
33	123	158	n	words appear in their natural order
33	169	184	n	dependency view
33	187	192	p	where
33	193	212	n	words are reordered
33	213	221	p	based on
33	224	273	n	linearization of the sentence 's dependency graph
2	36	72	n	Machine Comprehension on Sparse Data
4	0	31	n	Understanding unstructured text
6	30	87	n	machine comprehension on the challenging MCTest benchmark
14	0	71	n	Comprehension of unstructured text by machines , at a near- human level
16	0	21	n	Machine comprehension
16	24	26	n	MC
247	0	2	p	On
247	3	15	n	MCTest - 500
247	22	49	n	Parallel Hierarchical model
247	90	92	p	on
247	93	109	n	single questions
247	110	119	p	( > 2 % )
247	50	75	n	significantly outperforms
247	124	144	p	slightly outperforms
247	163	190	n	multi questions ( ? 0.3 % )
247	195	213	n	over all ( ? 1 % )
