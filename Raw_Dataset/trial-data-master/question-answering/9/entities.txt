131	4	45	passage - aligned question representation
131	46	48	is
131	49	56	crucial
163	0	5	RASOR
163	6	17	outperforms
163	22	47	endpoint prediction model
163	48	50	by
163	51	69	1.1 in exact match
157	11	18	observe
157	19	39	general improvements
157	40	50	when using
157	51	90	labels that closely align with the task
162	12	37	observe the importance of
162	38	59	allowing interactions
162	60	67	between
162	68	81	the endpoints
162	82	87	using
162	92	106	spanlevel FFNN
106	3	12	represent
106	13	30	each of the words
106	31	33	in
106	38	59	question and document
106	60	65	using
106	66	98	300 dimensional GloVe embeddings
106	107	109	on
106	112	134	corpus of 840 bn words
108	3	9	couple
108	14	36	input and forget gates
108	37	39	in
108	44	49	LSTMs
108	77	80	use
108	83	102	single dropout mask
108	106	119	apply dropout
108	120	148	across all LSTM time - steps
109	0	13	Hidden layers
109	21	49	feed forward neural networks
109	50	53	use
109	54	76	rectified linear units
111	45	48	ran
111	49	62	grid searches
111	74	88	dimensionality
111	96	114	LSTM hidden states
111	121	136	width and depth
111	144	172	feed forward neural networks
111	175	182	dropout
111	191	196	LSTMs
111	243	259	decay multiplier
111	260	280	[ 0.9 , 0.95 , 1.0 ]
111	295	303	multiply
111	308	321	learning rate
111	322	327	every
111	328	338	10 k steps
112	4	14	best model
112	15	19	uses
112	20	35	50d LSTM states
112	38	57	two - layer BiLSTMs
112	58	61	for
112	66	132	span encoder and the passage - independent question representation
112	135	142	dropout
112	143	145	of
112	146	149	0.1
112	169	188	learning rate decay
112	189	191	of
112	192	195	5 %
112	196	201	every
112	202	212	10 k steps
113	15	32	implemented using
113	33	45	TensorFlow 3
113	50	60	trained on
113	65	83	SQUAD training set
113	84	89	using
113	94	108	ADAM optimizer
113	109	113	with
113	116	131	mini-batch size
113	132	134	of
113	135	136	4
113	141	154	trained using
113	155	187	10 asynchronous training threads
113	188	190	on
113	193	207	single machine
24	58	64	called
24	65	70	RASOR
24	76	82	builds
24	83	118	fixed - length span representations
24	121	128	reusing
24	129	151	recurrent computations
24	152	155	for
24	156	176	shared substructures
25	3	14	demonstrate
25	147	182	significant increase in performance
25	20	40	directly classifying
25	41	68	each of the competing spans
25	75	88	training with
25	89	109	global normalization
25	110	118	over all
25	119	133	possible spans
2	44	73	EXTRACTIVE QUESTION ANSWERING
4	4	25	reading comprehension
17	77	125	answer questions about the contents of documents
121	83	88	RASOR
121	89	97	achieves
121	101	116	error reduction
121	117	129	of more than
121	130	153	50 % over this baseline
125	24	56	efficiently and explicitly model
125	61	97	quadratic number of possible answers
125	106	114	leads to
125	117	137	14 % error reduction
125	138	142	over
125	147	181	best performing Match - LSTM model
