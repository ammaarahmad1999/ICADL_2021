{
  "has" : {
    "Experimental setup" : {
      "represent" : {
        "each of the words" : {
          "in" : {
            "question and document" : {
              "using" : {
                "300 dimensional GloVe embeddings" : {
                  "on" : "corpus of 840 bn words"
                }
              }
            }
          }
        },
        "from sentence" : "We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words ."
      },
      "couple" : {
        "input and forget gates" : {
          "in" : "LSTMs"
        }
      },
      "use" : {
        "single dropout mask" : {
          "apply dropout" : "across all LSTM time - steps"
        },
        "from sentence" : "We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by ."
      },
      "Hidden layers" : {
        "feed forward neural networks" : {
          "use" : "rectified linear units"
        },
        "from sentence" : "Hidden layers in the feed forward neural networks use rectified linear units ."
      },
      "ran" : {
        "grid searches" : {
          "dimensionality" : "LSTM hidden states",
          "width and depth" : "feed forward neural networks",
          "dropout" : "LSTMs",
          "decay multiplier" : {
            "[ 0.9 , 0.95 , 1.0 ]" : {
              "multiply" : {
                "learning rate" : {
                  "every" : "10 k steps"
                }
              }
            }
          }
        },
        "from sentence" : "To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps ."
      },
      "has" : {
        "best model" : {
          "uses" : ["50d LSTM states", {"two - layer BiLSTMs" : {
            "for" : "span encoder and the passage - independent question representation"}}, {"dropout" : {"of" : "0.1"}}, {"learning rate decay" : {"of" : {"5 %"  : {"every" : "10 k steps"}}}},
            {"from sentence" : "The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps ."}
          ]
        }
      },
      "implemented using" : "TensorFlow 3",
      "trained on" : {
        "SQUAD training set" : {
          "using" : {
            "ADAM optimizer" : {
              "with" : {
                "mini-batch size" : {
                  "of" : "4"
                }
              }
            }
          }
        }
      },
      "trained using" : {
        "10 asynchronous training threads" : {
          "on" : "single machine"
        },
        "from sentence" : "All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine ."
      }
    }
  }
}