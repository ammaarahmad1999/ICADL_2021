(Contribution||has||Experimental setup)
(Experimental setup||couple||input and forget gates)
(input and forget gates||in||LSTMs)
(Experimental setup||use||single dropout mask)
(single dropout mask||apply dropout||across all LSTM time - steps)
(Experimental setup||Hidden layers||feed forward neural networks)
(feed forward neural networks||use||rectified linear units)
(Experimental setup||implemented using||TensorFlow 3)
(Experimental setup||has||best model)
(best model||uses||50d LSTM states)
(best model||uses||two - layer BiLSTMs)
(two - layer BiLSTMs||for||span encoder and the passage - independent question representation)
(best model||uses||dropout)
(dropout||of||0.1)
(best model||uses||learning rate decay)
(learning rate decay||of||5 %)
(5 %||every||10 k steps)
(Experimental setup||trained on||SQUAD training set)
(SQUAD training set||using||ADAM optimizer)
(ADAM optimizer||with||mini-batch size)
(mini-batch size||of||4)
(Experimental setup||trained using||10 asynchronous training threads)
(10 asynchronous training threads||on||single machine)
(Experimental setup||ran||grid searches)
(grid searches||width and depth||feed forward neural networks)
(grid searches||dimensionality||LSTM hidden states)
(grid searches||dropout||LSTMs)
(grid searches||decay multiplier||[ 0.9 , 0.95 , 1.0 ])
([ 0.9 , 0.95 , 1.0 ]||multiply||learning rate)
(learning rate||every||10 k steps)
(Experimental setup||represent||each of the words)
(each of the words||in||question and document)
(question and document||using||300 dimensional GloVe embeddings)
(300 dimensional GloVe embeddings||on||corpus of 840 bn words)
