(Contribution||has||Hyperparameters)
(Hyperparameters||controlled||learning rate)
(learning rate||by||AdaGrad)
(AdaGrad||initial learning rate||0.5 ( 0.1 for QA 10 k ))
(Hyperparameters||withhold||10 % of the training)
(10 % of the training||for||development)
(Hyperparameters||use||hidden state size)
(hidden state size||of||50)
(50||by||deafult)
(Hyperparameters||used||Batch sizes)
(Batch sizes||of||128)
(128||for||bAbI QA 10 k)
(Batch sizes||of||32)
(32||for||bAbI story - based QA 1k)
(32||for||bAb I dialog)
(32||for||DSTC2 dialog)
(Hyperparameters||used||L2 weight decay)
(L2 weight decay||of||0.001 ( 0.0005 for QA 10 k ))
(Hyperparameters||has||loss function)
(loss function||is||cross entropy betweenv and the one - hot vector of the true answer)
(Hyperparameters||has||loss)
(loss||is minimized by||stochastic gradient descent)
(stochastic gradient descent||for||maximally 500 epochs)
(maximally 500 epochs||early stopped||training)
(training||if the loss on the development data||does not decrease)
(does not decrease||for||50 epochs)
