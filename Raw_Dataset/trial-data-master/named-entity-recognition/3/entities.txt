16	40	63	semisupervised approach
16	75	86	not require
16	87	110	additional labeled data
17	3	6	use
17	9	37	neural language model ( LM )
17	40	54	pre-trained on
17	57	81	large , unlabeled corpus
17	82	92	to compute
17	96	152	encoding of the context at each position in the sequence
17	168	180	LM embedding
17	187	196	use it in
17	201	234	supervised sequence tagging model
104	16	19	use
104	24	38	Adam optimizer
104	64	68	with
104	69	83	gradient norms
104	84	94	clipped at
104	95	98	5.0
106	61	75	early stopping
106	76	86	to prevent
106	87	99	over-fitting
105	24	33	fine tune
105	38	71	pre-trained Senna word embeddings
105	76	79	fix
105	80	91	all weights
105	92	94	in
105	99	126	pre-trained language models
107	9	19	train with
107	22	54	constant learning rate ? = 0.001
2	0	32	Semi-supervised sequence tagging
6	33	156	general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems
118	0	2	In
118	7	26	CoNLL 2003 NER task
118	39	45	scores
118	46	60	91.93 mean F 1
119	7	31	CoNLL 2000 Chunking task
119	41	49	achieves
119	50	64	96.37 mean F 1
75	0	14	CoNLL 2003 NER
80	7	29	two bidirectional GRUs
80	91	94	for
80	99	122	token character encoder
80	30	34	with
80	35	50	80 hidden units
80	55	90	25 dimensional character embeddings
81	4	18	sequence layer
81	19	23	uses
81	28	46	bidirectional GRUs
81	47	51	with
81	52	73	300 hidden units each
82	4	18	regularization
82	24	27	add
82	28	40	25 % dropout
82	41	56	to the input of
82	57	65	each GRU
83	0	19	CoNLL 2000 chunking
87	4	28	baseline sequence tagger
87	29	33	uses
87	34	69	30 dimensional character embeddings
87	76	79	CNN
87	80	84	with
87	85	117	30 filters of width 3 characters
87	118	129	followed by
87	132	182	tanh non-linearity for the token character encoder
88	4	18	sequence layer
88	19	23	uses
88	24	47	two bidirectional LSTMs
88	57	69	hidden units
88	53	56	200
89	19	31	50 % dropout
89	32	34	to
89	39	59	character embeddings
89	66	90	input to each LSTM layer
89	136	166	output of the final LSTM layer
