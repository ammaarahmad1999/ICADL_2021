13	20	26	p	differ
13	32	64	n	traditional word type embeddings
13	87	95	p	assigned
13	73	83	n	each token
13	98	112	p	representation
13	123	160	n	function of the entire input sentence
14	3	6	p	use
14	7	14	n	vectors
14	15	27	p	derived from
14	30	48	n	bidirectional LSTM
14	57	69	p	trained with
14	72	114	n	coupled lan - guage model ( LM ) objective
14	115	117	p	on
14	120	137	n	large text corpus
15	21	25	p	call
15	31	87	n	ELMo ( Embeddings from Language Models ) representations
17	23	28	p	learn
17	31	64	n	linear combination of the vectors
17	65	78	p	stacked above
17	79	94	n	each input word
17	95	98	p	for
17	99	112	n	each end task
17	121	138	p	markedly improves
17	139	150	n	performance
17	151	155	p	over
17	161	185	n	using the top LSTM layer
2	0	40	n	Deep contextualized word representations
108	0	18	n	Textual entailment
109	4	48	n	Stanford Natural Language Inference ( SNLI )
111	40	48	p	improves
111	49	57	n	accuracy
111	58	60	p	by
111	64	80	n	average of 0.7 %
111	81	87	p	across
111	88	105	n	five random seeds
111	10	16	p	adding
111	17	21	n	ELMo
111	22	24	p	to
111	29	39	n	ESIM model
117	12	34	n	Coreference resolution
120	19	23	p	with
120	28	61	n	OntoNotes coreference annotations
120	62	66	p	from
120	71	93	n	CoNLL 2012 shared task
120	108	116	p	improved
120	121	141	n	average F 1 by 3.2 %
120	142	146	p	from
120	147	159	n	67.2 to 70.4
120	96	102	p	adding
120	103	107	n	ELMo
