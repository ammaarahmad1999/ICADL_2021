{
  "has" : {
    "Approach" : {
      "differ" : "traditional word type embeddings",
      "assigned" : {
        "each token" : {
          "representation" : "function of the entire input sentence"
        },
        "from sentence" : "Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence ."
      },
      "use" : {
        "vectors" : {
          "derived from" : "bidirectional LSTM",
          "trained with" : {
            "coupled lan - guage model ( LM ) objective" :{
              "on" : "large text corpus"
            }
          }
        },
        "from sentence" : "We use vectors derived from a bidirectional LSTM that is trained with a coupled lan - guage model ( LM ) objective on a large text corpus ."
      },
      "call" : ["ELMo ( Embeddings from Language Models ) representations", {"from sentence" : "For this reason , we call them ELMo ( Embeddings from Language Models ) representations ."}],
      "learn" : {
        "linear combination of the vectors" : {
          "stacked above" : {
            "each input word" : {
              "for" : {
                "each end task" : {
                  "markedly improves" : {
                    "performance" : {
                      "over" : "using the top LSTM layer"
                    }
                  }
                }
              }
            }
          }
        },
        "from sentence" : "More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer ."
      }
    }
  }
}