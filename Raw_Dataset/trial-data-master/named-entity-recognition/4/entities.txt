13	20	26	differ
13	32	64	traditional word type embeddings
13	87	95	assigned
13	73	83	each token
13	98	112	representation
13	123	160	function of the entire input sentence
14	3	6	use
14	7	14	vectors
14	15	27	derived from
14	30	48	bidirectional LSTM
14	57	69	trained with
14	72	114	coupled lan - guage model ( LM ) objective
14	115	117	on
14	120	137	large text corpus
15	21	25	call
15	31	87	ELMo ( Embeddings from Language Models ) representations
17	23	28	learn
17	31	64	linear combination of the vectors
17	65	78	stacked above
17	79	94	each input word
17	95	98	for
17	99	112	each end task
17	121	138	markedly improves
17	139	150	performance
17	151	155	over
17	161	185	using the top LSTM layer
2	0	40	Deep contextualized word representations
108	0	18	Textual entailment
109	4	48	Stanford Natural Language Inference ( SNLI )
111	40	48	improves
111	49	57	accuracy
111	58	60	by
111	64	80	average of 0.7 %
111	81	87	across
111	88	105	five random seeds
111	10	16	adding
111	17	21	ELMo
111	22	24	to
111	29	39	ESIM model
117	12	34	Coreference resolution
120	19	23	with
120	28	61	OntoNotes coreference annotations
120	62	66	from
120	71	93	CoNLL 2012 shared task
120	108	116	improved
120	121	141	average F 1 by 3.2 %
120	142	146	from
120	147	159	67.2 to 70.4
120	96	102	adding
120	103	107	ELMo
