31	34	72	n	https://github.com/ leuchine /S - LSTM
148	4	15	n	experiments
148	20	35	p	conducted using
148	38	58	n	GeForce GTX 1080 GPU
148	59	63	p	with
148	64	75	n	8 GB memory
19	31	39	p	to model
19	44	85	n	hidden states of all words simultaneously
19	86	88	p	at
19	89	108	n	each recurrent step
20	19	23	p	view
20	28	42	n	whole sentence
20	43	45	p	as
20	48	60	n	single state
20	69	80	p	consists of
20	81	91	n	sub-states
20	92	95	p	for
20	96	112	n	individual words
20	120	151	n	over all sentence - level state
21	0	10	p	To capture
21	11	39	n	local and non-local contexts
21	53	72	p	updated recurrently
21	42	48	n	states
21	73	75	p	by
21	76	117	n	exchanging information between each other
25	0	22	p	At each recurrent step
25	25	45	n	information exchange
25	49	66	p	conducted between
25	67	100	n	consecutive words in the sentence
25	119	155	n	sentence - level state and each word
26	16	25	n	each word
26	26	34	p	receives
26	35	46	n	information
26	47	51	p	from
26	56	96	n	predecessor and successor simultaneously
2	0	45	n	Sentence - State LSTM for Text Representation
7	18	62	n	alternative LSTM structure for encoding text
18	18	64	n	alternative recurrent neural network structure
165	14	17	p	for
165	18	32	n	Classification
170	32	34	p	on
170	39	59	n	movie review dataset
170	112	120	n	S - LSTM
170	121	132	p	outperforms
170	133	142	n	BiL - STM
170	75	79	p	with
170	166	178	n	faster speed
173	14	19	p	among
173	24	35	n	16 datasets
173	50	55	p	gives
173	60	72	n	best results
173	73	75	p	on
173	76	78	n	12
174	21	23	p	of
174	24	32	n	S - LSTM
174	4	20	p	average accuracy
174	36	42	n	85.6 %
178	18	36	n	Sequence Labelling
187	0	3	p	For
187	4	7	n	NER
187	50	52	p	on
187	57	71	n	CoNLL test set
187	19	24	p	gives
187	10	18	n	S - LSTM
187	28	38	p	F1 - score
187	42	49	n	91.57 %
