250	0	20	n	Effect of Model Size
260	31	33	p	is
260	38	48	n	first work
260	49	63	p	to demonstrate
260	82	106	n	scaling to extreme model
260	118	126	p	leads to
260	127	145	n	large improvements
260	146	148	p	on
260	149	171	n	very small scale tasks
260	174	187	p	provided that
260	192	231	n	model has been sufficiently pre-trained
263	0	34	n	Feature - based Approach with BERT
275	0	10	n	BERT LARGE
275	11	33	p	performs competitively
275	39	69	n	state - of - the - art methods
277	5	17	p	demonstrates
277	23	40	n	BERT is effective
277	41	49	p	for both
277	50	60	n	finetuning
277	65	91	n	feature - based approaches
24	65	74	p	proposing
24	75	137	n	BERT : Bidirectional Encoder Representations from Transformers
25	0	4	n	BERT
25	5	15	p	alleviates
25	41	69	n	unidirectionality constraint
25	73	78	p	using
25	81	137	n	" masked language model " ( MLM ) pre-training objective
26	4	25	n	masked language model
26	26	40	p	randomly masks
26	41	59	n	some of the tokens
26	60	64	p	from
26	69	74	n	input
26	85	94	p	objective
26	101	135	n	predict the original vocabulary id
26	136	138	p	of
26	143	154	n	masked word
26	155	168	p	based only on
26	173	180	n	context
27	56	69	n	MLM objective
27	70	77	p	enables
27	82	135	n	representation to fuse the left and the right context
27	144	165	p	allows us to pretrain
27	168	198	n	deep bidirectional Transformer
28	51	54	p	use
28	57	90	n	" next sentence prediction " task
28	96	113	p	jointly pretrains
28	114	141	n	text - pair representations
2	59	81	n	Language Understanding
4	19	48	n	language representation model
14	0	27	n	Language model pre-training
155	0	4	n	GLUE
156	4	54	n	General Language Understanding Evaluation ( GLUE )
170	3	6	p	use
170	9	19	p	batch size
170	20	22	p	of
170	23	25	n	32
170	30	41	p	fine - tune
170	42	54	n	for 3 epochs
170	55	59	p	over
170	64	87	n	data for all GLUE tasks
171	19	27	p	selected
171	32	64	n	best fine - tuning learning rate
171	67	72	p	among
171	73	114	n	5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5
171	117	119	p	on
171	124	131	n	Dev set
172	15	18	p	for
172	19	29	n	BERT LARGE
172	104	107	p	ran
172	108	131	n	several random restarts
172	136	144	p	selected
172	149	174	n	best model on the Dev set
175	5	29	n	BERT BASE and BERT LARGE
175	30	40	p	outperform
175	41	65	n	all systems on all tasks
175	66	68	p	by
175	71	89	n	substantial margin
175	92	101	p	obtaining
175	102	157	n	4.5 % and 7.0 % respective average accuracy improvement
175	158	162	p	over
175	167	189	n	prior state of the art
179	13	23	n	BERT LARGE
179	24	49	p	significantly outperforms
179	50	59	n	BERT BASE
179	60	66	p	across
179	67	76	n	all tasks
181	0	11	n	SQuAD v 1.1
182	4	54	n	Stanford Question Answering Dataset ( SQuAD v1.1 )
195	3	14	p	fine - tune
195	19	27	n	3 epochs
195	35	48	p	learning rate
195	52	59	n	5 e - 5
195	66	76	p	batch size
195	80	82	n	32
199	4	26	n	best performing system
199	27	38	p	outperforms
199	43	65	n	top leaderboard system
199	66	68	p	by
199	69	77	n	+ 1.5 F1
199	78	80	p	in
199	81	91	n	ensembling
199	96	104	n	+ 1.3 F1
199	105	107	p	as
199	110	123	n	single system
203	0	11	n	SQuAD v 2.0
213	3	15	p	fine - tuned
213	20	28	n	2 epochs
213	36	49	p	learning rate
213	53	60	n	5 e - 5
213	67	77	p	batch size
213	81	83	n	48
215	3	10	p	observe
215	13	33	n	+ 5.1 F1 improvement
215	34	38	p	over
215	43	63	n	previous best system
216	0	4	n	SWAG
217	4	43	n	Situations With Adversarial Generations
221	3	14	p	fine - tune
221	29	37	n	3 epochs
221	45	58	p	learning rate
221	62	69	n	2 e - 5
221	76	86	p	batch size
221	90	92	n	16
223	0	10	n	BERT LARGE
223	11	22	p	outperforms
223	27	64	n	authors ' baseline ESIM + ELMo system
223	65	67	p	by
223	68	76	n	+ 27.1 %
223	81	91	n	OpenAI GPT
223	92	94	p	by
223	95	100	n	8.3 %
