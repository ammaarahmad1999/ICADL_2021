(Contribution||has||Approach)
(Approach||proposing||BERT : Bidirectional Encoder Representations from Transformers)
(Approach||use||" next sentence prediction " task)
(" next sentence prediction " task||jointly pretrains||text - pair representations)
(Approach||name||BERT)
(BERT||alleviates||unidirectionality constraint)
(unidirectionality constraint||using||" masked language model " ( MLM ) pre-training objective)
(Approach||has||MLM objective)
(MLM objective||enables||representation to fuse the left and the right context)
(representation to fuse the left and the right context||allows us to pretrain||deep bidirectional Transformer)
(Approach||has||masked language model)
(masked language model||randomly masks||some of the tokens)
(some of the tokens||from||input)
(masked language model||objective||predict the original vocabulary id)
(predict the original vocabulary id||of||masked word)
(masked word||based only on||context)
