{
  "has" : {
    "Approach" : {
      "proposing" : ["BERT : Bidirectional Encoder Representations from Transformers", {"from sentence" : "In this paper , we improve the fine - tuning based approaches by proposing BERT : Bidirectional Encoder Representations from Transformers ."}],
      "name" : {
        "BERT" : {
          "alleviates" : {
            "unidirectionality constraint" : {
              "using" : "\" masked language model \" ( MLM ) pre-training objective"
            }
          }
        },
        "from sentence" : "BERT alleviates the previously mentioned unidirectionality constraint by using a \" masked language model \" ( MLM ) pre-training objective , inspired by the Cloze task ."
      },
      "has" : {
        "masked language model" : {
          "randomly masks" : {
            "some of the tokens" : {
              "from" : "input"
            }
          },
          "objective" : {
            "predict the original vocabulary id" : {
              "of" : {
                "masked word" : {
                  "based only on" : "context"
                }
              }
            }
          },
          "from sentence" : "The masked language model randomly masks some of the tokens from the input , and the objective is to predict the original vocabulary id of the masked word based only on its context ."          
        },
        "MLM objective" : {
          "enables" : {
            "representation to fuse the left and the right context" :{
              "allows us to pretrain" : "deep bidirectional Transformer"
            }
          },
          "from sentence" : "Unlike left - toright language model pre-training , the MLM objective enables the representation to fuse the left and the right context , which allows us to pretrain a deep bidirectional Transformer ."          
        }
      },
      "use" : {
        "\" next sentence prediction \" task" : {
          "jointly pretrains" : "text - pair representations"
        },
        "from sentence" : "In addition to the masked language model , we also use a \" next sentence prediction \" task that jointly pretrains text - pair representations ."
      }
    }
  }
}