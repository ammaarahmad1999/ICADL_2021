{
  "has" : {
    "Tasks" : {
      "has" : {
        "GLUE" : {
          "name" : ["General Language Understanding Evaluation ( GLUE )", {"from sentence" : "GLUE
          The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks ."
          
          }],
          "has" : {
            "Hyperparameters" : {
              "use" : {
                "batch size" : {
                  "of" : "32"
                }
              },
              "fine - tune" : {
                "for 3 epochs" : {
                    "over" : "data for all GLUE tasks"
                },
                "from sentence" : "We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks ."
              },
              "selected" : {
                "best fine - tuning learning rate" : {
                  "among" : "5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5",
                  "on" : "Dev set"                  
                },
                "from sentence" : "For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set ."
              },
              "for" : {
                "BERT LARGE" : {
                  "ran" : {
                    "several random restarts" : {
                      "selected" : "best model on the Dev set"
                    }
                  }
                },
                "from sentence" : "Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set ."
              }
            },
            "Results" : {
              "has" : {
                "BERT BASE and BERT LARGE" : {
                  "outperform" : {
                    "all systems on all tasks" : {
                      "by" : {
                        "substantial margin" : {
                          "obtaining" : {
                            "4.5 % and 7.0 % respective average accuracy improvement" : {
                              "over" : "prior state of the art"
                            }
                          }
                        }
                      }
                    }
                  },
                  "from sentence" : "Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art ."
                },
                "BERT LARGE" : {
                  "significantly outperforms" : {
                    "BERT BASE" : {
                      "across" : "all tasks"
                    }
                  },
                  "from sentence" : "We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data ."
                }
              }
            }
          }
        },
        "SQuAD v 1.1" : {
          "name" : ["Stanford Question Answering Dataset ( SQuAD v1.1 )", {"from sentence" : "SQuAD v 1.1
          The Stanford Question Answering Dataset ( SQuAD v1.1 ) is a collection of 100 k crowdsourced question / answer pairs ."
          
          }],
          "has" : {
            "Hyperparameters" : {
              "fine - tune" : {
                "3 epochs" : {
                  "learning rate" : "5 e - 5",
                  "batch size" : "32"
                }
              },
              "from sentence" : "We fine - tune for 3 epochs with a learning rate of 5 e - 5 and a batch size of 32 ."
            }, 
            "Results" : {
              "has" : {
                "best performing system" : {
                  "outperforms" : {
                    "top leaderboard system" : {
                      "by" : {
                        "+ 1.5 F1" : {
                          "in" : "ensembling"
                        },
                        "+ 1.3 F1" : {
                          "as" : "single system"
                        }
                      }
                    }
                  },
                  "from sentence" : "Our best performing system outperforms the top leaderboard system by + 1.5 F1 in ensembling and + 1.3 F1 as a single system ."
                }
              }
            }
          }
        },
        "SQuAD v 2.0" : {
          "has" : {
            "Hyperparameters" : {
              "fine - tuned" : {
                "2 epochs" : {
                  "learning rate" : "5 e - 5",
                  "batch size" : "48"
                },
                "from sentence" : "SQuAD v 2.0
We fine - tuned for 2 epochs with a learning rate of 5 e - 5 and a batch size of 48 ."   

              }
            },
            "Results" : {
              "observe" : {
                "+ 5.1 F1 improvement" : {
                  "over" : "previous best system"
                },
                "from sentence" : "We observe a + 5.1 F1 improvement over the previous best system ."
              }
            }
          }
        },
        "SWAG" : {
          "name" : ["Situations With Adversarial Generations", {"from sentence" : "SWAG
The Situations With Adversarial Generations ( SWAG ) dataset contains 113 k sentence - pair completion examples that evaluate grounded commonsense inference ."
            
          }],
          "has" : {
            "Hyperparameters" : {
              "fine - tune" : {
                "3 epochs" : {
                  "learning rate" : "2 e - 5",
                  "batch size" : "16"
                },
                "from sentence" : "We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 ."
              }
            },
            "Results" : {
              "has" : {
                "BERT LARGE" : {
                  "outperforms" : {
                    "authors ' baseline ESIM + ELMo system" : {
                      "by" : "+ 27.1 %"
                    },
                    "OpenAI GPT" : {
                      "by" : "8.3 %"
                    }
                  },
                  "from sentence" : "BERT LARGE outperforms the authors ' baseline ESIM + ELMo system by + 27.1 % and OpenAI GPT by 8.3 % ."
                }  
              }
            }
          }
        }
      }
    }
  }
}