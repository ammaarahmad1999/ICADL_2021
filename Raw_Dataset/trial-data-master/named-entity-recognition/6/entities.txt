183	16	18	on
183	19	43	both CONLL and ONTONOTES
183	3	10	observe
183	50	61	SSKIP model
183	62	73	outperforms
183	74	101	our feature vector approach
183	102	104	by
183	105	130	0.65 F1 points on average
174	8	15	observe
174	21	54	models that use both feature sets
174	55	79	significantly outperform
174	80	100	other configurations
139	0	8	Training
139	12	26	carried out by
139	27	73	mini-batch stochastic gradient descent ( SGD )
139	74	78	with
139	81	96	momentum of 0.9
139	103	127	gradient clipping of 5.0
140	4	14	mini-batch
140	15	17	is
140	18	20	10
140	21	24	for
140	25	38	both datasets
140	45	59	learning rates
140	60	63	are
140	64	79	0.009 and 0.013
140	80	83	for
140	84	103	CONLL and ONTONOTES
147	3	9	varied
147	10	17	dropout
147	29	41	hidden units
147	46	60	capitalization
147	67	94	char ) embedding dimensions
147	97	110	learning rate
147	113	130	[ 0.001 , 0.015 ]
147	131	133	by
147	134	144	step 0.002
148	3	14	implemented
148	19	25	system
148	26	31	using
148	36	54	Tensorflow library
148	61	64	ran
148	69	75	models
148	76	78	on
148	81	105	GeForce GTX TITAN Xp GPU
24	76	83	propose
24	87	121	alternative lexical representation
24	131	138	trained
24	139	146	offline
24	163	171	added to
24	172	193	any neural NER system
25	19	24	embed
25	25	47	words and entity types
25	48	52	into
25	55	73	joint vector space
25	74	87	by leveraging
25	88	94	WiFiNE
2	37	78	Neural Network Named - Entity Recognition
4	0	55	Neural network approaches to Named - Entity Recognition
14	0	26	Named - Entity Recognition
14	29	32	NER
171	8	10	on
171	11	16	CONLL
155	34	59	significantly outperforms
155	60	66	models
155	72	75	use
155	76	114	extensive sets of handcrafted features
156	19	30	outperforms
156	39	54	other NN models
156	55	68	that only use
156	69	93	standard word embeddings
156	102	111	indicates
156	117	188	our lexical feature vector is complementary to standard word embeddings
157	19	26	matches
157	27	62	state - of - the - art performances
157	63	65	of
157	66	72	models
157	73	81	that use
157	89	115	more complex architectures
157	119	142	more elaborate features
172	11	20	ONTONOTES
167	27	52	significantly outperforms
167	57	116	Bi - LSTM - CNN - CRF models of ( Chiu and Nichols , 2016 )
167	121	123	by
167	124	140	an absolute gain
167	141	143	of
167	144	177	1.68 and 0.96 points respectively
168	23	32	surpasses
168	33	69	systems with hand - crafted features
168	72	81	including
168	87	101	use gazetteers
168	112	155	system of which uses coreference annotation
168	156	158	in
168	159	168	ONTONOTES
168	169	185	to jointly model
168	186	241	NER , entity linking , and coreference resolution tasks
