topic,paper_ID,text,main_heading,sub_heading,label,pos1,pos2,pos3,citation,head,ofs1,ofs2,ofs3,cleaned,length,prev_text,next_text,section
named-entity-recognition,5,"For the latter , we decide the number of recurrent stepson the respective development sets for sequence labelling .",result,Final Results for Sequence Labelling,0,182,18,5,0,result : Final Results for Sequence Labelling,0.8708133971291866,0.6206896551724138,0.3125,For the latter we decide the number of recurrent stepson the respective development sets for sequence labelling ,18,"We compare S - LSTM - CRF with BiLSTM - CRF for sequence labelling , using the same settings as decided on the movie review development experiments for both BiLSTMs and S - LSTMs .","The POS accuracies and NER F1 - scores against the number of recurrent steps are shown in ( a ) and ( b ) , respectively .",result
semantic_role_labeling,3,"Compared with the standard additive attention mechanism which is implemented using a one layer feed - forward neural network , the dot-product attention utilizes matrix production which allows faster computation .",system description,Self-Attention,0,63,25,7,0,system description : Self-Attention,0.23863636363636365,0.2293577981651377,0.2058823529411765,Compared with the standard additive attention mechanism which is implemented using a one layer feed forward neural network the dot product attention utilizes matrix production which allows faster computation ,30,"The center of the graph is the scaled dot-product attention , which is a variant of dot -product ( multiplicative ) attention .","Given a matrix of n query vectors Q ? R nd , keys K ? R nd and values V ? R nd , the scaled dot -product attention computes the attention scores based on the following equation :",method
natural_language_inference,54,"Similar to SECT , we use a window of size l to limit the amount of syntactic information for the learning models by choosing only the l-nearest dependents , which is again reported in Section 4.5 .",system description,Structural Embedding of Dependency Trees (SEDT),0,94,39,11,0,system description : Structural Embedding of Dependency Trees (SEDT),0.4104803493449782,0.6842105263157895,1.0,Similar to SECT we use a window of size l to limit the amount of syntactic information for the learning models by choosing only the l nearest dependents which is again reported in Section 4 5 ,37,"Similar to the syntactic encoding of C - Tree , we take the last hidden states as its embedding .", ,method
natural_language_inference,69,"To establish the quality of the data and analyze potential distant supervision errors , we sampled and annotated 100 samples from each development set .",analysis,Qualitative Analysis,0,155,2,2,0,analysis : Qualitative Analysis,0.4492753623188406,0.037037037037037035,0.2,To establish the quality of the data and analyze potential distant supervision errors we sampled and annotated 100 samples from each development set ,24, ,WIKIHOP lists characteristics along with the proportion of samples that exhibit them .,result
relation_extraction,8,"Our proposal is an extension of , in which a single max pooling operation is utilized to determine the most significant features .",introduction,introduction,0,43,30,30,0,introduction : introduction,0.15985130111524162,0.6666666666666666,0.6666666666666666,Our proposal is an extension of in which a single max pooling operation is utilized to determine the most significant features ,22,"To address the second problem , we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by .","Although this operation has been shown to be effective for textual feature representation , it reduces the size of the hidden layers too rapidly and can not capture the structural information between two entities .",introduction
named-entity-recognition,1,Recurrent neural networks ( RNNs ) are a family of neural networks that operate on sequential data .,model,LSTM,0,31,5,2,0,model : LSTM,0.1497584541062802,0.21739130434782608,0.10526315789473684,Recurrent neural networks RNNs are a family of neural networks that operate on sequential data ,16, ,"They take as input a sequence of vectors ( x 1 , x 2 , . . . , x n ) and return another sequence ( h 1 , h 2 , . . . , h n ) that represents some information about the sequence at every step in the input .",method
machine-translation,1,The resulting network has two core properties : it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.04477611940298507,0.7,0.7,The resulting network has two core properties it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization ,29,The ByteNet uses dilation in the convolutional layers to increase its receptive field .,The ByteNet decoder attains state - of - the - art performance on character - level language modelling and outperforms the previous best results obtained with recurrent networks .,abstract
text-to-speech_synthesis,0,The knowledge distillation loss with unlabeled source words is as follows :,system description,Knowledge Distillation with Unlabeled Source Words,0,88,53,9,0,system description : Knowledge Distillation with Unlabeled Source Words,0.5432098765432098,0.9298245614035088,0.6923076923076923,The knowledge distillation loss with unlabeled source words is as follows ,12,Denote D ? = {x ? X } as the corpus of unlabeled source words .,"where y ? is generated by the ensemble model ( Equation , Q ( y ? t = k|y ? <t , x ) is the probability distribution output of the ensemble model and is calculated by Equation 5 .",method
natural_language_inference,88,"LSTMs can produce a list of state representations during composition , however , the next state is always computed from the current state .",system description,Long Short-Term Memory-Network,0,85,13,3,0,system description : Long Short-Term Memory-Network,0.3441295546558704,0.19696969696969696,0.09375,LSTMs can produce a list of state representations during composition however the next state is always computed from the current state ,22,The first question that arises with LSTMs is the extent to which they are able to memorize sequences under recursive compression .,"That is to say , given the current state ht , the next state h t + 1 is conditionally independent of states h 1 h t?1 and tokens x 1 x t .",method
negation_scope_resolution,0,"Almost every NLP task benefitted from transfer learning , as training on massive corpora allowed these models to learn an understanding of language .",introduction,introduction,0,33,22,22,0,introduction : introduction,0.14347826086956522,0.7333333333333333,0.7333333333333333,Almost every NLP task benefitted from transfer learning as training on massive corpora allowed these models to learn an understanding of language ,23,"Recently , a number of architectures including BERT have applied this to NLP , contributing massively to the advancement of research in the field .","Motivated by the success of transfer learning , we apply BERT to negation detection and scope resolution .",introduction
question-answering,0,Our approach is based on learning low - dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space .,introduction,introduction,1,31,21,21,0,introduction : introduction,0.12015503875968993,0.6774193548387096,0.6774193548387096,Our approach is based on learning low dimensional vector embeddings of words and of KB triples so that representations of questions and corresponding answers end up being similar in the embedding space ,33,"In this task , the main difficulties come from lexical variability rather than from complex syntax , having multiple answers per question , and the absence of a supervised training signal .","Unfortunately , we do not have access to any human labeled ( query , answer ) supervision for this task .",introduction
machine-translation,5,Neural machine translation ( NMT ) is a rapidly changing research area .,introduction,introduction,1,9,2,2,0,introduction : introduction,0.06293706293706294,0.125,0.125,Neural machine translation NMT is a rapidly changing research area ,11, ,"Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation ( SMT ) systems , the dominant neural network ( NN ) architectures for NMT have changed on a yearly ( and even more frequent ) basis .",introduction
sentiment_analysis,16,This proposed technique associates the TCS interaction with an additional set of context representation .,approach,approach,0,168,46,46,0,approach : approach,0.5283018867924528,0.5609756097560976,0.5609756097560976,This proposed technique associates the TCS interaction with an additional set of context representation ,15,Coupled Interaction ( CI ) :,This representation is for capturing the global correlation between context and different sentiment classes .,method
sentiment_analysis,5,It seems temporal lobe associated more with emotion expression than frontal lobe for EEG emotion recognition ; C.,The activity maps of the paired EEG electrodes,Electrodes reduction,0,245,21,8,0,The activity maps of the paired EEG electrodes : Electrodes reduction,0.9245283018867924,0.5833333333333334,0.8888888888888888,It seems temporal lobe associated more with emotion expression than frontal lobe for EEG emotion recognition C ,18,"It is possible to consider utilizing fewer electrodes in EEG emotion recognition systems ; ( 2 ) Comparing between these two important brain regions , we can see the results based on temporal lobe electrodes outperform that based on frontal lobe .",It seems temporal lobe associated more with emotion expression than frontal lobe for EEG emotion recognition ; C.,others
sentence_classification,1,"While this enormous corpus provides us with the ability to conclusively accept or reject hypotheses and yields insight into promising research directions , it is getting harder and harder to extract useful information from the literature in an efficient and timely manner due to its sheer amount .",introduction,introduction,0,10,3,3,0,introduction : introduction,0.05780346820809248,0.10714285714285714,0.10714285714285714,While this enormous corpus provides us with the ability to conclusively accept or reject hypotheses and yields insight into promising research directions it is getting harder and harder to extract useful information from the literature in an efficient and timely manner due to its sheer amount ,47,"Since 1665 , over 50 million scholarly research articles have been published , with approximately 2.5 million new scientific papers coming out each year .","Therefore , an automatic and intelligent tool to help users locate the information of interest quickly and comprehensively is highly desired .",introduction
natural_language_inference,83,"For collecting these stories , Amazon Mechanical Turk workers were asked to compose novel five - sentence long stories on everyday topics .",dataset,Dataset,0,161,4,4,0,dataset : Dataset,0.5261437908496732,0.3076923076923077,0.3076923076923077,For collecting these stories Amazon Mechanical Turk workers were asked to compose novel five sentence long stories on everyday topics ,21,It consists of about 100K unannotated five - sentences long stories .,"They were prompted to write coherent stories with a specific beginning and ending , with something happening in between .",experiment
named-entity-recognition,4,"To match the CoVe training setup , we only train on phrases that contain four or more tokens .",model,model,0,270,49,49,0,model : model,0.9926470588235294,0.9607843137254902,0.9607843137254902,To match the CoVe training setup we only train on phrases that contain four or more tokens ,18,"BCN model with a batch - normalized maxout network reached significantly lower validation accuracies in our experiments , although there maybe discrepancies between our implementation and that of .",We use 300 -d hidden states for the biLSTM and optimize the model parameters with Adam ( Kingma and using a learning rate of 0.0001 .,method
natural_language_inference,55,"Since it contains few training samples , it is impossible to learn on it alone , and this section describes the various data sources that were used for training .",system description,Task Definition,0,32,7,7,0,system description : Task Definition,0.217687074829932,0.10606060606060606,0.875,Since it contains few training samples it is impossible to learn on it alone and this section describes the various data sources that were used for training ,28,We use WebQuestions as our evaluation bemchmark .,These are similar to those used in .,method
sentiment_analysis,3,Affectiveness : Affectiveness measures the emotion intensity of ck .,model,Dynamic Context-Aware Affective Graph Attention,0,133,68,27,0,model : Dynamic Context-Aware Affective Graph Attention,0.4554794520547945,0.5573770491803278,0.5869565217391305,Affectiveness Affectiveness measures the emotion intensity of ck ,9,The hierarchical pooling mechanism preserves word order information to certain degree and has demonstrated superior performance than average pooling or max - pooling on sentiment analysis tasks .,The affectiveness factor in wk is computed as,method
natural_language_inference,32,"Since there are unanswerable questions , we also calculate the no answer probabilities P ? i for the i - th question .",architecture,architecture,0,131,34,34,0,architecture : architecture,0.2997711670480549,0.8947368421052632,0.8947368421052632,Since there are unanswerable questions we also calculate the no answer probabilities P i for the i th question ,20,"We use the same answer span selection method to estimate the start and end probabilities PS i , j , PE i , j of the j - th context token for the i - th question .","Since there are unanswerable questions , we also calculate the no answer probabilities P ? i for the i - th question .",method
natural_language_inference,1,"On WebQuestions , not specifically designed as a simple QA dataset , 86 % of the questions can now be answered with a single supporting fact , and performance increases significantly ( from 36.2 % to 41.0 % F1-score ) .",result,WebQuestions SimpleQuestions,1,254,16,11,0,result : WebQuestions SimpleQuestions,0.9304029304029304,0.5333333333333333,0.9166666666666666,On WebQuestions not specifically designed as a simple QA dataset 86 of the questions can now be answered with a single supporting fact and performance increases significantly from 36 2 to 41 0 F1 score ,36,Grouping facts also allows us to scale much better and to train on FB5M .,"Using the bigger FB5M as KB does not change performance on SimpleQuestions because it was based on FB2M , but the results show that our model is robust to the addition of more entities than necessary .",result
sentiment_analysis,27,"The predicted probability of the i - th aspect with sentiment polarity j ? [ 1 , C ] is computed by :",methodology,Output layer,0,161,84,7,0,methodology : Output layer,0.5833333333333334,0.9882352941176472,0.875,The predicted probability of the i th aspect with sentiment polarity j 1 C is computed by ,18,"where W z ? R C2 d hid is the weight matrix , and b z ? R 2 d hid C is the bias .","The predicted probability of the i - th aspect with sentiment polarity j ? [ 1 , C ] is computed by :",method
natural_language_inference,58,Existing multi-turn models have a pre-de ned number of hops or iterations in their inference without regard to the complexity of each individual query or document .,introduction,introduction,0,28,19,19,0,introduction : introduction,0.08358208955223881,0.4871794871794872,0.4871794871794872,Existing multi turn models have a pre de ned number of hops or iterations in their inference without regard to the complexity of each individual query or document ,29,"By repeatedly processing the document and the question after digesting intermediate information , multi-turn reasoning can generally produce a better answer and these existing works have demonstrated its superior performance consistently .","However , when human read a document with a question in mind , we often decide whether we want to stop reading if we believe the observed information is adequate already to answer the question , or continue reading after digesting intermediate information until we can answer the question with con dence .",introduction
negation_scope_resolution,0,"The team from FBK ( Chowdhury 2012 ) trained CRF classifiers , trained on only features provided by the dataset .",approach,Conditional Random Field Approaches,0,123,46,13,1,approach : Conditional Random Field Approaches,0.5347826086956522,0.5054945054945055,0.65,The team from FBK Chowdhury 2012 trained CRF classifiers trained on only features provided by the dataset ,18,They expanded the set of features given to the CRF .,different set of features was considered for the CRF which exploited phrasal and contextual clues along with token specific features .,method
text_summarization,10,One potential reason for this lower readability score is that our entailment generation auxiliary task encourages our summarization model to rewrite more and to be more abstractive than - see abstractiveness results in .,evaluation,Human Evaluation,0,166,4,4,0,evaluation : Human Evaluation,0.6311787072243346,0.3076923076923077,0.6666666666666666,One potential reason for this lower readability score is that our entailment generation auxiliary task encourages our summarization model to rewrite more and to be more abstractive than see abstractiveness results in ,33,readability scores ( and is higher in terms of total aggregate scores ) .,"We also show human evaluation results on the Gigaword dataset in ( again based on pairwise comparisons for 100 samples ) , where we see that our MTL model is better than our state - of - theart baseline on both relevance and readability .",result
sentiment_analysis,1,Our model is biologically supported and captures both local and global inter-channel relations .,introduction,introduction,0,64,51,51,0,introduction : introduction,0.16161616161616166,0.8947368421052632,0.8947368421052632,Our model is biologically supported and captures both local and global inter channel relations ,15,) We propose a regularized graph neural network ( RGNN ) model to recognize emotions based on EEG signals .,") We propose two regularizers : a node - wise domain adversarial training ( NodeDAT ) and an emotionaware distribution learning ( EmotionDL ) , which aim to improve the robustness of our model against cross - subject variations and noisy labels , respectively .",introduction
natural_language_inference,55,"Likewise the function g (. ) which maps the answer into the same embedding space R k as the questions , is given by g ( a ) = W ? ( a ) .",system description,Embedding Questions and Answers,0,75,50,12,0,system description : Embedding Questions and Answers,0.5102040816326531,0.7575757575757576,0.8571428571428571,Likewise the function g which maps the answer into the same embedding space R k as the questions is given by g a W a ,26,"The function f ( . ) , which maps the questions into the embedding space R k is defined as f ( q ) = W ? ( q ) , where ?( q ) ? N N , is a sparse vector indicating the number of times each word appears in the question q ( usually 0 or 1 ) .","Here ? ( a ) ? N N is a sparse vector representation of the answer a , which we now detail .",method
sentiment_analysis,10,The annotations are available every 0.2 seconds in the original data base .,dataset,Datasets Used,0,145,4,4,0,dataset : Datasets Used,0.5642023346303502,0.8,0.8,The annotations are available every 0 2 seconds in the original data base ,14,We partition both datasets into train and test sets with roughly 80 20 ratio such that the partitions do not share any speaker . ) .,"However , in order to adapt the annotations to our need of utterance - level annotation , we averaged the attributes over the span of an utterance .",experiment
machine-translation,1,The ByteNet used in the experiments has 30 residual blocks in the encoder and 30 residual blocks in the decoder .,model,model,1,174,30,30,0,model : model,0.8656716417910447,0.6122448979591837,0.6122448979591837,The ByteNet used in the experiments has 30 residual blocks in the encoder and 30 residual blocks in the decoder ,21,We keep 323 characters in the German vocabulary and 296 in the English vocabulary .,"As in the ByteNet Decoder , the residual blocks are arranged in sets of five with corresponding dilation rates of 1 , 2 , 4 , 8 and 16 .",method
natural_language_inference,39,"This similarity focus layer is directly incorporated into our end - to - end model and is placed on top of the pairwise word interaction model , as in .",system description,Similarity Focus Layer,0,88,25,4,0,system description : Similarity Focus Layer,0.4271844660194175,0.373134328358209,0.14285714285714285,This similarity focus layer is directly incorporated into our end to end model and is placed on top of the pairwise word interaction model as in ,27,We therefore develop a similarity focus layer which can identify important word interactions and increase their model weights correspondingly .,shows one example where each cell of the matrix represents a pairwise word interaction .,method
natural_language_inference,52,"With this end - to - end approach , the model learns to select justifications that allow it to correctly answer questions .",approach,Approach,0,80,11,11,0,approach : Approach,0.3065134099616858,0.8461538461538461,0.8461538461538461,With this end to end approach the model learns to select justifications that allow it to correctly answer questions ,20,The system is trained using correct - incorrect answer pairs with a pairwise margin ranking loss objective function to enforce that the correct answer be ranked higher than any of the incorrect answers .,We hypothesize that this approach enables the model to indirectly learn to choose justifications that provide good explanations as to why the answer is correct .,method
natural_language_inference,20,"From the experiments , we can conclude that the model provides strong results on all of the three NLI datasets .",evaluation,Model Performance on the NLI task,0,123,41,3,0,evaluation : Model Performance on the NLI task,0.5061728395061729,0.4361702127659575,0.375,From the experiments we can conclude that the model provides strong results on all of the three NLI datasets ,20,"In this section , we discuss the performance of the proposed sentence - encoding approach in common natural language inference benchmarks .",It clearly outperforms the similar but non-hierarchical BiLSTM models reported in the literature and fares well in comparison to other state of the art architectures in the sentence encoding category .,result
named-entity-recognition,9,We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,method,Pre-training BioBERT,0,69,23,9,0,method : Pre-training BioBERT,0.3467336683417085,0.46,0.6428571428571429,We define BioBERT as a language representation model whose pre training corpora includes biomedical corpora e g BioBERT PubMed ,20,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .","For tokenization , BioBERT uses WordPiece tokenization , which mitigates the out - of - vocabulary issue .",method
natural_language_inference,69,"If we consider PPI chains across documents , we find examples like in .",system description,MEDHOP,0,148,23,12,0,system description : MEDHOP,0.4289855072463768,0.8518518518518519,0.75,If we consider PPI chains across documents we find examples like in ,13,"DDIs are caused by Protein - Protein Interaction ( PPI ) chains , forming biomedical pathways .","Here the first document states that the drug Leuprolide causes GnRH receptor - induced synaptic potentiations , which can be blocked by the protein Progonadoliberin - 1 .",method
natural_language_inference,44,of Venice and which other city ? The groundtruth answer text is in red text .,training,SQuAD-Adversarial,0,265,50,12,0,training : SQuAD-Adversarial,0.9265734265734266,0.7142857142857143,0.375,of Venice and which other city The groundtruth answer text is in red text ,15,"Smith also arranged for the publication of a series of etchings of Capricci in his vedette ideal , Canaletto is famous for his landscapes but the returns were not high enough , and in 1746 Canaletto moved to London , to be closer to his market .",of Venice and which other city ? The groundtruth answer text is in red text .,experiment
natural_language_inference,23,"Finally , a light - weight implementation for history - of - word , Fully - Aware Attention , is proposed .",system description,MACHINE COMPREHENSION & FULLY-AWARE ATTENTION,0,55,5,5,0,system description : MACHINE COMPREHENSION & FULLY-AWARE ATTENTION,0.10721247563352826,0.2272727272727273,1.0,Finally a light weight implementation for history of word Fully Aware Attention is proposed ,15,History - of - word can capture different levels of contextual information to fully understand the text ., ,method
natural_language_inference,80,Previous works typically utilize trivial ensemble strategies by either using majority votes or averaging the probability distributions over the same model with different initialization seeds .,experiment,experiment,0,140,14,14,0,experiment : experiment,0.4827586206896552,0.32558139534883723,0.32558139534883723,Previous works typically utilize trivial ensemble strategies by either using majority votes or averaging the probability distributions over the same model with different initialization seeds ,26,Ensemble methods use multiple models to obtain better predictive performance .,"By contrast , we use weighted averaging of the probability distributions where the weight of each model is learned through its performance on the SNLI development set .",experiment
text_summarization,2,We observe that salient source relations also play a critical role in predicting the next word .,approach,2-Way Combination (+Relation),0,127,76,2,0,approach : 2-Way Combination (+Relation),0.4568345323741007,0.5801526717557252,0.07692307692307693,We observe that salient source relations also play a critical role in predicting the next word ,17, ,"For example , if a dependency edge ( "" father "" nsubj ??? "" had "" ) is salient and "" father "" is selected to be included in the summary , it is likely that "" had "" will be selected next such that a salient source relation ( "" nsubj "" ) is preserved in the summary .",method
sentiment_analysis,41,"In order to better take advantage of aspect information , we append the input aspect embedding into each word input vector .",system description,Attention-based LSTM with Aspect Embedding (ATAE-LSTM),0,112,74,3,0,system description : Attention-based LSTM with Aspect Embedding (ATAE-LSTM),0.5022421524663677,0.9610389610389608,0.5,In order to better take advantage of aspect information we append the input aspect embedding into each word input vector ,21,The way of using aspect information in AE - LSTM is letting aspect embedding play a role in computing the attention weight .,The structure of this model is illustrated in 3 .,method
relation-classification,8,"On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .",training,Results on SemEval 2018 Task 7,1,120,17,7,0,training : Results on SemEval 2018 Task 7,0.8759124087591241,0.9444444444444444,1.0,On the other hand compared to the top singlemodel result which makes use of additional word and entity embeddings pretrained on in domain data our methods demonstrate clear advantage as a single model ,34,"Note that the system ( Rotsztejn et al. , 2018 ) integrates many techniques like feature - engineering , model combination , pretraining embeddings on in - domain data , and artificial data generation , while our model is almost a direct adaption from the ACE architecture .", ,experiment
natural_language_inference,70,"In addition , results are also reported in Average Recall ( AvgR ) , We merged the official Train1 , Train2 and Dev sets .",result,Submission and Results,0,131,9,9,0,result : Submission and Results,0.7485714285714286,0.16981132075471694,0.9,In addition results are also reported in Average Recall AvgR We merged the official Train1 Train2 and Dev sets ,20,The MAP@10 was the official metric .,"Mean Reciprocal Rank ( MRR ) , Precision ( P ) , Recall ( R ) , F 1 , and Accuracy ( Acc ) .",result
relation_extraction,11,We shall explore this point further in Section 7.3 .,system description,Relation Alias Side Information,0,160,93,11,0,system description : Relation Alias Side Information,0.6451612903225806,0.7265625,0.5238095238095238,We shall explore this point further in Section 7 3 ,11,"We note that even for cases when aliases for relations are not available , providing only the names of relations give competitive performance .","For matching P with the PPDB expanded relation alias set R , we project both in a d-dimensional space using GloVe embeddings .",method
natural_language_inference,2,The use of multiple factors helps to fine - tune answer inference by synthesizing information distributed across multiple sentences .,result,Results,0,206,15,15,0,result : Results,0.7686567164179104,0.3409090909090909,0.6818181818181818,The use of multiple factors helps to fine tune answer inference by synthesizing information distributed across multiple sentences ,19,One of the key contributions of this paper is multi-factor attentive encoding which aggregates information from the relevant passage words by using a tensor - based attention mechanism .,The number of factors is the granularity to which the model is allowed to refine the evidence .,result
text-classification,8,"Specifically , d represents the dimension of hidden units or the number of filters in LSTM or CNN , respectively .",system description,Simple Word-Embedding Model,0,107,60,42,0,system description : Simple Word-Embedding Model,0.3977695167286245,0.9090909090909092,0.875,Specifically d represents the dimension of hidden units or the number of filters in LSTM or CNN respectively ,19,We defined as the dimension of the final sequence representation .,We first examine the number of compositional parameters for each model .,method
sentiment_analysis,3,BERT BASE achieves very competitive performance on all datasets except EC due to its strong representational power via bi-directional context modelling using the Transformer .,baseline,Comparison with Baselines,1,240,8,8,0,baseline : Comparison with Baselines,0.821917808219178,0.4705882352941176,0.4705882352941176,BERT BASE achieves very competitive performance on all datasets except EC due to its strong representational power via bi directional context modelling using the Transformer ,26,"In contrast , when the utterance - level LSTM in c LSTM is replaced by features extracted by CNN , i.e. , the CNN + c LSTM , the model performs significantly better than c LSTM on long conversations , which further validates that modelling long conversations using only RNN models may not be sufficient .","Note that BERT BASE has considerably more parameters than other baselines and our model ( 110 M for BERT BASE versus 4 M for our model ) , which can be a dis advantage when deployed to devices with limited computing power and memory .",result
named-entity-recognition,2,"where ? t is a local factor , ? p is a pairwise factor that scores consecutive tags , and Z x is the partition function .",model,model,0,57,12,12,0,model : model,0.2676056338028169,0.2033898305084746,0.5217391304347826,where t is a local factor p is a pairwise factor that scores consecutive tags and Z x is the partition function ,23,We also consider a linear - chain CRF model that couples all of y together :,"where ? t is a local factor , ? p is a pairwise factor that scores consecutive tags , and Z x is the partition function .",method
natural_language_inference,55,"When this entity is not given , plain string matching is used to perform entity resolution .",system description,Task Definition,0,29,4,4,0,system description : Task Definition,0.19727891156462585,0.060606060606060615,0.5,When this entity is not given plain string matching is used to perform entity resolution ,16,We suppose that all potential answers are entities in the KB and that questions are sequences of words that include one identified KB entity .,Smarter methods could be used but this is not our focus .,method
named-entity-recognition,8,"As shown in , we denote input embedding as E , the final hidden vector of the special [ CLS ] token as C ? R H , and the final hidden vector for the i th input token as",model,model,0,101,23,23,0,model : model,0.26098191214470284,0.3108108108108108,0.3709677419354839,As shown in we denote input embedding as E the final hidden vector of the special CLS token as C R H and the final hidden vector for the i th input token as,34,"Second , we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B .","For a given token , its input representation is constructed by summing the corresponding token , segment , and position embeddings .",method
sentiment_analysis,15,The vectors are used for classifying each phrase using the same softmax classifier as in Eq.,model,MV-RNN: Matrix-Vector RNN,0,138,49,14,0,model : MV-RNN: Matrix-Vector RNN,0.5111111111111111,0.4579439252336449,0.9333333333333332,The vectors are used for classifying each phrase using the same softmax classifier as in Eq ,17,"Similarly , the second parent node is computed using the previously computed ( vector , matrix ) pair ( p 1 , P 1 ) as well as ( a , A ) .", ,method
machine-translation,1,"The strings are usually sentences of the respective languages ; the tokens are words or , as in the our case , characters .",model,Neural Translation Model,0,56,6,6,0,model : Neural Translation Model,0.27860696517412936,0.09836065573770493,0.5454545454545454,The strings are usually sentences of the respective languages the tokens are words or as in the our case characters ,21,Each conditional factor expresses complex and long - range dependencies among the source and target tokens .,The network that models p ( t | s ) is composed of two parts : a source network ( the encoder ) that processes the source string into a representation and a target network ( the decoder ) that uses the source representation to generate the target string .,method
question_answering,1,Our BIDAF model 1 outperforms all previous approaches on the highly - competitive Stanford Question Answering Dataset ( SQuAD ) test set leaderboard at the time of submission .,introduction,introduction,0,30,22,22,0,introduction : introduction,0.0946372239747634,0.9166666666666666,0.9166666666666666,Our BIDAF model 1 outperforms all previous approaches on the highly competitive Stanford Question Answering Dataset SQuAD test set leaderboard at the time of submission ,26,"Third , we use attention mechanisms in both directions , query - to - context and context - to - query , which provide complimentary information to each other .","With a modification to only the output layer , BIDAF achieves the state - of - the - art results on the CNN / DailyMail cloze test .",introduction
natural_language_inference,54,Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers thatare syntactically coherent over the baseline methods .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.03056768558951965,1.0,1.0,Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers thatare syntactically coherent over the baseline methods ,26,We evaluate our approach using a state - of - the - art neural attention model on the SQuAD dataset ., ,abstract
natural_language_inference,69,"When traversing the graph starting at s , several end points will be visited , though generally not all ; those visited define the candidate set C q .",dataset,Dataset Assembly,0,64,14,14,0,dataset : Dataset Assembly,0.1855072463768116,0.6086956521739131,0.6086956521739131,When traversing the graph starting at s several end points will be visited though generally not all those visited define the candidate set C q ,26,This ensures that precisely one of the end points corresponds to a correct answer to q .,"If however the correct answer a * is not among them we discard the ( q , a * ) pair .",experiment
machine-translation,7,The probability works out to be :,APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,231,9,9,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.6193029490616622,0.05960264900662252,0.5,The probability works out to be ,7,"To compute P ( x , i ) , we note that the G ( x ) i is nonzero if and only if H ( x ) i is greater than the k th - greatest element of H ( x ) excluding itself .","Where kth_excluding ( v , k , i ) means the kth highest component of v , excluding component i .",others
sentiment_analysis,33,The LLC ratios with respect to different phrase lengths are shown in .,analysis,Analysis,0,96,7,7,0,analysis : Analysis,0.768,0.3888888888888889,0.3888888888888889,The LLC ratios with respect to different phrase lengths are shown in ,13,We define the local label consistency ( LLC ) ratio as the ratio of m-grams that share the same sentiment labels as the original sentences .,Longer phrases are more likely to convey the same sentiments as the original sentences .,result
text-classification,8,We evaluate this method on the two documentlevel sentiment analysis tasks and the results are shown in the last row of .,model,model,0,188,58,58,0,model : model,0.6988847583643123,0.8055555555555556,0.8055555555555556,We evaluate this method on the two documentlevel sentiment analysis tasks and the results are shown in the last row of ,22,"Inspired by this observation , we propose using another simple pooling operation termed as hierarchical ( SWEM - hier ) , as detailed in Section 3.3 .","SWEM - hier greatly outperforms the other three SWEM variants , and the corresponding accuracies are comparable to the results of CNN or LSTM ) .",method
natural_language_inference,79,The difference between word vectors also carry meaning .,system description,Learning Vector Representation of Words,0,76,30,27,0,system description : Learning Vector Representation of Words,0.2835820895522388,0.8823529411764706,0.9,The difference between word vectors also carry meaning ,9,"For example , "" powerful "" and "" strong "" are close to each other , whereas "" powerful "" and "" Paris "" are more distant .","For example , the word vectors can be used to answer analogy questions using simple vector algebra : "" King "" - "" man "" + "" woman "" = "" Queen "" .",method
sentence_compression,0,"Although the method above has explicitly incorporated some syntactic information into the bi - LSTM model , the syntactic information is used in a soft manner through the learned model weights .",model,Global Inference through ILP,0,104,47,2,0,model : Global Inference through ILP,0.3727598566308244,0.5402298850574713,0.2857142857142857,Although the method above has explicitly incorporated some syntactic information into the bi LSTM model the syntactic information is used in a soft manner through the learned model weights ,30, ,We hypothesize that there are also hard constraints we can impose on the compressed sentences .,method
question_generation,0,"Inspired by ; , the BiGRU encoder not only reads the sentence words , but also handcrafted features , to produce a sequence of word - and - feature vectors .",approach,Feature-Rich Encoder,0,32,7,4,0,approach : Feature-Rich Encoder,0.17582417582417584,0.1891891891891892,0.2222222222222222,Inspired by the BiGRU encoder not only reads the sentence words but also handcrafted features to produce a sequence of word and feature vectors ,25,"To capture more context information , we use bidirectional GRU ( BiGRU ) to read the inputs in both forward and backward orders .","We concatenate the word vector , lexical feature embedding vectors and answer position indicator embedding vector as the input of BiGRU encoder .",method
sentiment_analysis,35,3 ) Position information is crucial for aspect - level sentiment analysis .,analysis,analysis,0,222,10,10,0,analysis : analysis,0.8951612903225806,0.3225806451612903,0.3225806451612903,3 Position information is crucial for aspect level sentiment analysis ,11,This is because that the C2F can effectively reduce the aspect granularity gap between tasks such that more useful knowledge can be distilled to facilitate the target task .,"The MGAN w / o PI , which does not utilize the position information , performs very poorly .",result
relation-classification,6,"Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks , such as information extraction , question answering and knowledge base population .",introduction,introduction,0,11,2,2,0,introduction : introduction,0.06077348066298342,0.1176470588235294,0.1176470588235294,Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks such as information extraction question answering and knowledge base population ,27, ,task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence .,introduction
sentiment_analysis,17,The classifier takes the hidden state h j at the node as input :,model,Tree-LSTM Classification,0,127,10,8,0,model : Tree-LSTM Classification,0.5644444444444444,0.3333333333333333,0.7272727272727273,The classifier takes the hidden state h j at the node as input ,14,"At each node j , we use a softmax classifier to predict the label ? j given the inputs {x } j observed at nodes in the subtree rooted at j .",The cost function is the negative log - likelihood of the true class labels y ( k ) at each labeled node :,method
natural_language_inference,44,"Given that the majority of examples are answerable with a single oracle sentence on SQuAD , we analyze the performance of an existing , competitive QA model when it is given the oracle sentence .",system description,Analyses on existing QA model,0,43,30,2,0,system description : Analyses on existing QA model,0.15034965034965034,0.7142857142857143,0.14285714285714285,Given that the majority of examples are answerable with a single oracle sentence on SQuAD we analyze the performance of an existing competitive QA model when it is given the oracle sentence ,33, ,"We train DCN + , one of the state - of - the - art models on SQuAD ( details in Section 3.1 ) , on the oracle sentence .",method
natural_language_inference,41,"We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of the original sentences and using a novel in - filling scheme , where spans of text are replaced with a single mask token .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.02723735408560312,0.5555555555555556,0.5555555555555556,We evaluate a number of noising approaches finding the best performance by both randomly shuffling the order of the original sentences and using a novel in filling scheme where spans of text are replaced with a single mask token ,40,"It uses a standard Tranformer - based neural machine translation architecture which , despite its simplicity , can be seen as generalizing BERT ( due to the bidirectional encoder ) , GPT ( with the left - to - right decoder ) , and many other more recent pretraining schemes .",BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks .,abstract
sentiment_analysis,45,"Especially , BERT has achieved excellent results in QA and NLI .",introduction,introduction,0,23,17,17,0,introduction : introduction,0.1597222222222222,0.6071428571428571,0.6071428571428571,Especially BERT has achieved excellent results in QA and NLI ,11,"More recently , the pre-trained language models , such as ELMo , OpenAI GPT , and BERT , have shown their effectiveness to alleviate the effort of feature engineering .","However , there is not much improvement in ( T ) ABSA task with the direct use of the pretrained BERT model ( see ) .",introduction
natural_language_inference,43,"Restricting the predictions to the subset for which candidate extraction succeeded , the F 1 of COMPQ - SUBSET is 48.5 , which is 3.4 F 1 points lower than WEBQA - SUBSET , which was trained on less data .",experiment,Experiments,1,122,22,22,0,experiment : Experiments,0.7870967741935484,0.6470588235294118,0.6470588235294118,Restricting the predictions to the subset for which candidate extraction succeeded the F 1 of COMPQ SUBSET is 48 5 which is 3 4 F 1 points lower than WEBQA SUBSET which was trained on less data ,38,"In this setup , COMPQ obtained 42.2 F 1 on the test set ( compared to 40.9 F 1 , when training on COM - PLEXQUESTIONS only , as we do ) .","Not using a KB , results in a considerable dis advantage for WEBQA .",experiment
sentiment_analysis,11,t u t ? hist ? and t ?,baseline,bc-LSTM:,0,269,15,10,0,baseline : bc-LSTM:,0.7819767441860465,0.46875,0.3703703703703704,t u t hist and t ,7,"Thus for utterance u i , both memories are created as M ? using {m t ? =",t u t ? hist ? and t ?,result
natural_language_inference,84,Embeddings are typically either trained from scratch or pretrained .,system description,ON THE FLY EMBEDDINGS,0,70,3,3,0,system description : ON THE FLY EMBEDDINGS,0.3043478260869565,0.027027027027027032,0.0967741935483871,Embeddings are typically either trained from scratch or pretrained ,10,"In general , a neural network processes a language input by replacing its elements x i , most often words , with the respective vectors e ( x i ) , often called embeddings .","When embeddings are trained from scratch , a restricted vocabulary V train = {w 1 , . . . , w n } is defined , usually based on training set frequency .",method
question-answering,1,"Indeed , in ARC - II if we choose ( by turning off some parameters in W ( , ) ) to keep the representations of the two sentences separated until the final MLP , ARC - II can actually act fully like ARC - I , as illustrated in .",model,model,0,108,3,3,0,model : model,0.5595854922279793,0.25,0.25,Indeed in ARC II if we choose by turning off some parameters in W to keep the representations of the two sentences separated until the final MLP ARC II can actually act fully like ARC I as illustrated in ,40,It is not hard to show that ARC - II actually subsumes ARC - I as a special case .,"More specifically , if we let the feature maps in the first convolution layer to be either devoted to S X or devoted to S Y ( instead of taking both as in general case ) , the output of each segment - pair is naturally divided into two corresponding groups .",method
relation_extraction,1,The predicate dis ambiguation task is to identify the correct meaning of a predicate in a given context .,model,Model,0,35,9,9,0,model : Model,0.39325842696629215,0.36,0.36,The predicate dis ambiguation task is to identify the correct meaning of a predicate in a given context ,19,Predicate sense dis ambiguation .,"As an example , for the sentence "" Barack Obama went to Paris "" , the predicate went has sense "" motion "" and has sense label 01 .",method
sentiment_analysis,6,"In our experiments , we obtained best results with 32 feature maps ( f m ) with the filter - size of",method,Context-Dependent Feature Extraction,0,105,47,4,0,method : Context-Dependent Feature Extraction,0.3633217993079585,0.7121212121212122,1.0,In our experiments we obtained best results with 32 feature maps f m with the filter size of,18,The pooling will be applied only to the last three dimensions of the array convout ., ,method
sentiment_analysis,13,"And BERT severely lacks two kinds of prior knowledge : ( 1 ) large - scale domain knowledge ( e.g. , about a specific product category ) , and ( 2 ) task - awareness knowledge ( MRC / RRC in this case ) .",system description,Review Reading Comprehension (RRC),0,104,28,14,0,system description : Review Reading Comprehension (RRC),0.3741007194244605,0.28,0.9333333333333332,And BERT severely lacks two kinds of prior knowledge 1 large scale domain knowledge e g about a specific product category and 2 task awareness knowledge MRC RRC in this case ,32,RRC may suffer from the prohibitive cost of annotating large - scale training data covering a wide range of domains .,We detail the technique of jointly incorporating these two types of knowledge in Sec. 4 .,method
named-entity-recognition,2,Our ID - CNN is not only a better token encoder than the Bi - LSTM but it is also faster .,baseline,Baselines,0,175,15,15,0,baseline : Baselines,0.8215962441314554,0.4166666666666667,0.6,Our ID CNN is not only a better token encoder than the Bi LSTM but it is also faster ,20,"When paired with Viterbi decoding , our ID - CNN performs on par with the Bi - LSTM , showing that the ID - CNN is also an effective token encoder for structured inference .","lists relative decoding times on the CoNLL development set , compared to the Bi - LSTM - CRF .",result
sentiment_analysis,4,These memories incorporate dynamic influences from each of the K utterances spoken in the history .,methodology,Multi-hop Memory,0,177,72,4,0,methodology : Multi-hop Memory,0.5429447852760736,0.7346938775510204,0.13333333333333333,These memories incorporate dynamic influences from each of the K utterances spoken in the history ,16,"The over all operation of the GRU g produces a sequence of memories M = [ s 1 , ... , s K ] ? R dem K .",They serve as a contextual memory bank from which selective person - specific information can be incorporated into test utterance u t to get discriminative features .,method
natural_language_inference,73,"In RSS , the elements of z are sampled in parallel according to probabilities computed by a learned attention mechanism .",model,Reinforced Sequence Sampling (RSS),0,90,8,5,0,model : Reinforced Sequence Sampling (RSS),0.3435114503816794,0.13333333333333333,0.2631578947368421,In RSS the elements of z are sampled in parallel according to probabilities computed by a learned attention mechanism ,20,"Given an input sequence x = [ x 1 , . . . , x n ] , RSS generates an equal - length sequence of binary random variables z = [ z 1 , . . . , z n ] where z i = 1 implies that xi is selected where as z i = 0 indicates that xi is discarded .",This is more efficient than using MCMC with iterative sampling .,method
natural_language_inference,41,PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions .,system description,BART Summary,0,247,23,23,0,system description : BART Summary,0.9610894941634242,0.696969696969697,0.696969696969697,PG E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions ,18,Eliud Kipchoge has run a marathon in less than two hours .,The aim is to reduce the risk of wildfires .,method
text_generation,0,"However , when applying MLE to generative models , there is a discrepancy between training and generating , which motivates our work .",implementation,Model Implementations,0,285,16,16,0,implementation : Model Implementations,0.8796296296296297,0.32653061224489793,0.4324324324324325,However when applying MLE to generative models there is a discrepancy between training and generating which motivates our work ,20,"The standard way of training an RNN G ? is the maximum likelihood estimation ( MLE ) , which involves minimizing the negative log - likelihood ? T t=1 log G ? ( y t = x t | {x 1 , . . . , x t?1 }) for a generated sequence ( y 1 , . . . , y T ) given input ( x 1 , . . . , x T ) .","The Discriminative Model for Sequences Deep discriminative models such as deep neural network ( DNN ) , convolutional neural network ( CNN ) ) and recurrent convolutional neural network ( RCNN ) have shown a high performance in complicated sequence classification tasks .",experiment
natural_language_inference,17,"Therefore , recent works build multi-round alignment architectures by stacking several identical aligning layers .",architecture,Alignment Architecture for MRC,0,64,21,21,0,architecture : Alignment Architecture for MRC,0.2461538461538461,0.15328467153284672,0.16153846153846155,Therefore recent works build multi round alignment architectures by stacking several identical aligning layers ,15,"Such architecture , however , is limited in its capability to capture complex interactions among question and context .","More specifically , let Vt = {v ti } n i =1 and Ut = {u t j } m j=1 denote the hidden representations of question and context in t-th layer , and",method
natural_language_inference,83,We use Expectation - Maximization for training .,model,Hidden Coherence Model,0,137,27,27,0,model : Hidden Coherence Model,0.4477124183006536,0.6,0.6,We use Expectation Maximization for training ,7,"The model parameters , w z and ? z , are learned during the training process by maximizing the log - likelihood of the data .",During the E - step we compute the expectations for latent variable assignments using parameter values from the previous iteration as :,method
part-of-speech_tagging,5,"successful recipe is to first create an initial context insensitive word representation , which usually has three main parts :",introduction,introduction,0,13,3,3,0,introduction : introduction,0.06435643564356436,0.15,0.15,successful recipe is to first create an initial context insensitive word representation which usually has three main parts ,19,Morphosyntactic tagging accuracy has seen dramatic improvements through the adoption of recurrent neural networks - specifically BiLSTMs to create sentence - level context sensitive encodings of words .,") A dynamically trained word embedding ; 2 ) a fixed pre-trained word - embedding , induced from a large corpus ; and 3 ) a sub-word character model , which itself is usually the final state of a recurrent model that ingests one character at a time .",introduction
text_summarization,10,"Note that even though our question generation task is generating one question at a time 2 , our multi-task framework ( see Sec. 4 ) is setup in such away that the sentence - level knowledge from this auxiliary task can help the documentlevel primary ( summarization ) task to generate multiple salient facts - by sharing high - level semantic layer representations .",model,Question Generation,0,79,34,7,0,model : Question Generation,0.3003802281368821,0.3655913978494624,0.3684210526315789,Note that even though our question generation task is generating one question at a time 2 our multi task framework see Sec 4 is setup in such away that the sentence level knowledge from this auxiliary task can help the documentlevel primary summarization task to generate multiple salient facts by sharing high level semantic layer representations ,57,"Next , we use the same sequenceto - sequence model architecture as our summarization model .",See Sec. 7 and Table 10 for a quantitative evaluation showing that the multi-task model can find multiple ( and more ) salient phrases in the source document .,method
natural_language_inference,2,f is the vector concatenation of the representations of the first wh-word and its following word from the sequencelevel question encoding Q .,architecture,Question-focused Attentional Pointing,0,131,86,14,0,architecture : Question-focused Attentional Pointing,0.4888059701492538,0.7889908256880734,0.4666666666666667,f is the vector concatenation of the representations of the first wh word and its following word from the sequencelevel question encoding Q ,24,"Intuitively , q ma aggregates the most relevant parts of the question with respect to all the words in the passage .","The set of wh-words we used is { what , who , how , when , which , where , why } .",method
named-entity-recognition,2,The greedy Bi - LSTM out - performs the lex -,baseline,OntoNotes 5.0 English NER,0,196,36,3,0,baseline : OntoNotes 5.0 English NER,0.92018779342723,1.0,1.0,The greedy Bi LSTM out performs the lex ,9,We observe similar patterns on OntoNotes as we do on CoNLL. lists over all F 1 scores of our models compared to those in the existing literature ., ,result
part-of-speech_tagging,4,"It is thus straightforward to see that the label embedding matrix x l corresponds to the weight matrix Wis Eq 1 , and the distribution ? corresponds toy in Eq 1 .",training,BiLSTM-LAN and BiLSTM-softmax,0,137,15,6,0,training : BiLSTM-LAN and BiLSTM-softmax,0.5879828326180258,0.9375,0.8571428571428571,It is thus straightforward to see that the label embedding matrix x l corresponds to the weight matrix Wis Eq 1 and the distribution corresponds toy in Eq 1 ,30,naive attention model over X has :,"It is thus straightforward to see that the label embedding matrix x l corresponds to the weight matrix Wis Eq 1 , and the distribution ? corresponds toy in Eq 1 .",experiment
natural_language_inference,36,"For both of the tasks , we also report the results by using pre-trained BERT as word representation in our baseline models",evaluation,Evaluation,0,165,6,6,0,evaluation : Evaluation,0.7857142857142857,0.2222222222222222,0.75,For both of the tasks we also report the results by using pre trained BERT as word representation in our baseline models,22,"In our experiments , we basically follow the same hyper - parameters for each model as the original settings from their corresponding literatures except those specified ( e.g. SRL embedding dimension ) .","The hyperparameters were selected using the Dev set , and the reported Dev and Test scores are averaged over 5 random seeds using those hyper - parameters .",result
natural_language_inference,68,"We use a standard one - directional LSTM 2 to process the passage and the question separately , as shown below :",method,LSTM Preprocessing Layer,0,97,47,4,0,method : LSTM Preprocessing Layer,0.3895582329317269,0.412280701754386,0.5,We use a standard one directional LSTM 2 to process the passage and the question separately as shown below ,20,The purpose for the LSTM preprocessing layer is to incorporate contextual information into the representation of each token in the passage and the question .,"The resulting matrices H p ? R lP and H q ? R lQ are hidden representations of the passage and the question , where l is the dimensionality of the hidden vectors .",method
named-entity-recognition,5,We empirically compare S - LSTMs and BiLSTMs on different classification and sequence labelling tasks .,experiment,Experiments,0,147,2,2,0,experiment : Experiments,0.7033492822966507,0.10526315789473684,0.6666666666666666,We empirically compare S LSTMs and BiLSTMs on different classification and sequence labelling tasks ,15, ,All experiments are conducted using a GeForce GTX 1080 GPU with 8 GB memory .,experiment
semantic_parsing,0,"This way , the exact same target queries ( with similar paraphrases ) in the test appear in training set as well .",system description,Related Work and Existing Datasets,0,60,17,17,0,system description : Related Work and Existing Datasets,0.2158273381294964,0.1619047619047619,0.5,This way the exact same target queries with similar paraphrases in the test appear in training set as well ,20,Most previous studies follow the standard question - based train and test split .,"Utilizing this assumption , existing models can achieve decent performances even on complex programs by memorizing data base - specific SQL templates .",method
sentiment_analysis,16,"Instead of directly following the common linear combination as shown in Eq. 3 , we use a non-linear projection ( tanh ) as the replacement to calculate the aspect - specific sentiment score .",approach,approach,0,128,6,6,0,approach : approach,0.4025157232704403,0.07317073170731707,0.07317073170731707,Instead of directly following the common linear combination as shown in Eq 3 we use a non linear projection tanh as the replacement to calculate the aspect specific sentiment score ,31,This is the first approach that utilizes a non-linear projection to capture the interplay between an aspect and its context .,"As shown in Eq. 4 , by applying a non-linear projection over attention - weighted c i and v t , the context and aspect information are coupled in a way that the final sentiment score can not be obtained by simply summing their individual contributions ( compared with Eq. 3 ) .",method
natural_language_inference,2,and the attention weights k which are used for max-attentional question aggregation .,architecture,Visualization,0,150,105,3,0,architecture : Visualization,0.5597014925373134,0.963302752293578,0.42857142857142855,and the attention weights k which are used for max attentional question aggregation ,14,"To understand how the proposed model works , for the example given in , we visualize the normalized multifactor attention weights","In , a small portion of F has been shown , in which the answer words Robert and Park are both assigned higher weights when paired with the context word Korean - American .",method
named-entity-recognition,4,During training the gradients are rescaled if their 2 norm exceeds 5.0 and parameters updated using Adam with constant learning rate of 0.001 .,model,model,0,261,40,40,0,model : model,0.9595588235294118,0.7843137254901961,0.7843137254901961,During training the gradients are rescaled if their 2 norm exceeds 5 0 and parameters updated using Adam with constant learning rate of 0 001 ,26,Variational dropout is added to the input of both biLSTM layers .,The pre-trained Senna embeddings are fine tuned during training .,method
semantic_role_labeling,1,"To incorporate syntax , one self - attention head is trained to attend to each token 's syntactic parent , allowing the model to use this attention head as an oracle for syntactic dependencies .",model,Model,0,40,7,7,0,model : Model,0.1834862385321101,0.07954545454545454,0.35,To incorporate syntax one self attention head is trained to attend to each token s syntactic parent allowing the model to use this attention head as an oracle for syntactic dependencies ,32,The basis for our model is the Transformer encoder introduced by : we transform word embeddings into contextually - encoded token representations using stacked multi-head self - attention and feedforward layers ( 2.1 ) .,We introduce this syntactically - informed self - attention ( ) in more detail in 2.2 .,method
part-of-speech_tagging,1,Part - of - speech tagging .,dataset,dataset,0,169,9,9,0,dataset : dataset,0.6759999999999999,0.6428571428571429,0.6428571428571429,Part of speech tagging ,5,"Although POS tags were made available for these datasets , we do not leverage those as additional information which sets our approach apart from that of transfer learning .",The Wall Street Journal ( WSJ ) portion of Penn Treebank ( PTB ) contains 25 sections and categorizes each word into one out of 45 POS tags .,experiment
semantic_role_labeling,1,"Unlike typical multi-task models , ours maintains the ability to leverage external syntactic information .",model,Syntactically-informed self-attention,0,101,68,21,0,model : Syntactically-informed self-attention,0.4633027522935779,0.7727272727272727,1.0,Unlike typical multi task models ours maintains the ability to leverage external syntactic information ,15,"In this way , our model can benefit from improved , external parsing models without re-training .", ,method
natural_language_inference,61,"In these models , LSTM ( Long Short - Term Memory networks ) , its variants GRU ( Gated Recurrent Units ) and Bi - LSTM , are usually utilized to encode the sentences since they were capable of learning long - term dependencies inside sentences .",introduction,introduction,0,28,21,21,0,introduction : introduction,0.1794871794871795,0.4117647058823529,0.4117647058823529,In these models LSTM Long Short Term Memory networks its variants GRU Gated Recurrent Units and Bi LSTM are usually utilized to encode the sentences since they were capable of learning long term dependencies inside sentences ,37,"Sentence encoding models ( their main architecture is shown in . a ) independently encode a pair of sentences , a premise and a hypothesis using pre-trained word embedding vectors , then learn semantic relation between two sentences with a multi - layer perceptron ( MLP ) .","scheme and compared several sentence encoding architectures : LSTM or GRU , Bi - LSTM with mean / max pooling , selfa ention network and hierarchical convolutional networks .",introduction
text-classification,1,"That is , an LSTM can be used to embed text regions of variable ( and possibly large ) sizes .",introduction,introduction,0,36,25,25,0,introduction : introduction,0.140625,0.7142857142857143,0.7142857142857143,That is an LSTM can be used to embed text regions of variable and possibly large sizes ,18,It is designed to enable learning of dependencies over larger time lags than feasible with traditional recurrent networks .,"We pursue the best use of LSTM for our purpose , and then compare the resulting model with the previous best methods including one - hot CNN and previous LSTM .",introduction
machine-translation,8,"where g is a nonlinear , potentially multi-layered , function that outputs the probability of y t , and st is the hidden state of the RNN .",system description,RNN ENCODER-DECODER,0,52,22,12,0,system description : RNN ENCODER-DECODER,0.15709969788519634,0.3055555555555556,0.9230769230769232,where g is a nonlinear potentially multi layered function that outputs the probability of y t and st is the hidden state of the RNN ,26,"With an RNN , each conditional probability is modeled as",It should be noted that other architectures such as a hybrid of an RNN and a de-convolutional neural network can be used .,method
sarcasm_detection,1,"Described as a combination of multiple characteristics , personality detection helps in identifying behavior , thought patterns of an individual .",method,Personality features,0,134,65,3,0,method : Personality features,0.4011976047904192,0.4304635761589404,0.07894736842105263,Described as a combination of multiple characteristics personality detection helps in identifying behavior thought patterns of an individual ,19,"Discovering personality from text has numerous NLP applications such as product recognition , mental health diagnosis , etc . .","To model the dependencies of users ' personality with their sarcastic nature , we include personality features in the user embeddings .",method
natural_language_inference,40,"At each time step t , the model can perform a "" soft "" lookup over all previous outputs through a weighted average t?1 i=1 ? i hi .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.08303249097472924,0.3333333333333333,0.3333333333333333,At each time step t the model can perform a soft lookup over all previous outputs through a weighted average t 1 i 1 i hi ,27,This led to the introduction of the attention mechanism which adapts the sequence model with a more explicit form of long term memory .,"At each time step t , the model can perform a "" soft "" lookup over all previous outputs through a weighted average t?1 i=1 ? i hi .",introduction
sentiment_analysis,43,"More importantly , in order to make use of the valuable aspect - level interaction information , we design an aspect alignment loss in the objective function to enhance the difference of the attention weights towards the aspects which have the same context and different sentiment polarities .",introduction,introduction,1,32,21,21,0,introduction : introduction,0.13333333333333333,0.875,0.875,More importantly in order to make use of the valuable aspect level interaction information we design an aspect alignment loss in the objective function to enhance the difference of the attention weights towards the aspects which have the same context and different sentiment polarities ,45,"In addition , we utilize the bidirectional coarsegrained attention ( i.e. C- Aspect2Context and C - Context2Aspect ) and combine them with finegrained attention vectors to compose the multigrained attention network for the final sentiment polarity prediction , which can leverage the advantages of them .","As far as we know , we are the first to explore the interactions among the aspects with the same context .",introduction
natural_language_inference,65,"Formally , the j - th elements of the vectors r q and r a are compute as follows :",training,Scoring and Training Procedure,0,86,5,5,0,training : Scoring and Training Procedure,0.3739130434782609,0.35714285714285715,0.35714285714285715,Formally the j th elements of the vectors r q and r a are compute as follows ,18,"Given the matrices Q and A , we compute the vector representations r q ? R c and r a ? R c by applying a column - wise max - pooling over Q and A , followed by a non-linearity .","The last layer in QA - CNN and QA - biLSTM scores the input pair ( q , a ) by computing the cosine similarity between the two representations :",experiment
text_generation,1,"Then , the average ranking score will be used to approximate the expected future reward for the current partial sequence .",training,Training,0,136,24,24,0,training : Training,0.4963503649635037,0.5333333333333333,0.5333333333333333,Then the average ranking score will be used to approximate the expected future reward for the current partial sequence ,20,We keep sampling n different paths with the corresponding ranking scores .,"With the feasible intermediate rewards , we can finalize the objective function for complete sentences .",experiment
natural_language_inference,19,"In our model , the inference layer is simply composed of 1 layer of 300D in order to focus on the training of sentence encoder .",result,MultiNLI Results,0,193,21,9,0,result : MultiNLI Results,0.7568627450980392,0.2530120481927711,0.8181818181818182,In our model the inference layer is simply composed of 1 layer of 300D in order to focus on the training of sentence encoder ,25,"However , it is a very deep structured LSTM model with 140.2 m parameters .","Both in and models , the inference layer was set very complex in order to improve the MultiNLI accuracy .",result
sentence_classification,0,where w is a parameter served as the query vector for dot - product attention .,model,model,0,43,16,16,0,model : model,0.16104868913857678,0.32,0.8421052631578947,where w is a parameter served as the query vector for dot product attention ,15,We then use an attention mechanism to get a single vector representing the whole input sequence :,So far we have obtained the citation representation as a vector z .,method
question_answering,2,"To allow the model to capture the context between timesteps based on the question , we introduce temporal focal pooling to connect neighboring time hidden states if they are related to the question .",architecture,Intra-sequence temporal dependency,0,138,59,18,0,architecture : Intra-sequence temporal dependency,0.4981949458483754,0.5412844036697247,0.6206896551724138,To allow the model to capture the context between timesteps based on the question we introduce temporal focal pooling to connect neighboring time hidden states if they are related to the question ,33,"The temporal correlation matrix captures the temporal dependency of question , image and text sequence .","For example , it can capture the relevance between the moment "" dinner "" and the moment later , "" Went dancing "" , given the question "" What did we do after the dinner on Ben 's birthday ? "" .",method
relation-classification,5,"Following , we encode the gold entity boundaries in the BILOU scheme .",experiment,Experimental setup,0,66,9,8,0,experiment : Experimental setup,0.584070796460177,0.39130434782608703,0.3636363636363637,Following we encode the gold entity boundaries in the BILOU scheme ,12,Thus the NER task which identifies both entity boundaries and classes reduces to the entity classification ( EC ) task .,"Then we represent each B , I , O , L or U boundary tag as a vector embedding .",experiment
named-entity-recognition,0,The center or its nearest neighbour are selected as exemplars .,system description,Classifiers,0,32,10,10,0,system description : Classifiers,0.11808118081180813,0.05617977528089888,0.2564102564102564,The center or its nearest neighbour are selected as exemplars ,11,Another category assumes that the samples are distributed around centers .,"Perhaps , Kmeans and Kmedoids are the most typical methods ( Kmedoids is a variant of Kmeans ) .",method
sentence_compression,0,"Intuitively , we would like to set y i to 1 if ? i is large .",model,The Objective Function,0,114,57,5,0,model : The Objective Function,0.4086021505376344,0.6551724137931034,0.3333333333333333,Intuitively we would like to set y i to 1 if i is large ,15,Let us use ? i to denote the probability of y i = 1 as estimated by the bi -LSTM model .,"Intuitively , we would like to set y i to 1 if ? i is large .",method
machine-translation,7,"batchwise ( X , T , m ) = | X | j=1 n i=1 ( M threshold ( x , T ) i ? M batchwise ( X , m ) j, i ) ( X j , i ? Ti ) ( 20 )",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,364,142,13,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9758713136729222,0.9403973509933776,0.9285714285714286,batchwise X T m X j 1 n i 1 M threshold x T i M batchwise X m j i X j i Ti 20 ,27,"To learn the threshold values , we apply an additional loss at training time which is minimized when the batchwise mask and the threshold mask are identical .","batchwise ( X , T , m ) = | X | j=1 n i=1 ( M threshold ( x , T ) i ? M batchwise ( X , m ) j, i ) ( X j , i ? Ti ) ( 20 )",others
natural_language_inference,37,"Pipelined approaches select a single paragraph * Work completed while interning at the Allen Institute for Artificial Intelligence from the input documents , which is then passed to the paragraph model to extract an answer .",introduction,introduction,0,17,8,8,0,introduction : introduction,0.06614785992217899,0.27586206896551724,0.27586206896551724,Pipelined approaches select a single paragraph Work completed while interning at the Allen Institute for Artificial Intelligence from the input documents which is then passed to the paragraph model to extract an answer ,34,There are two basic approaches to this task .,Confidence based methods apply the model to multiple paragraphs and returns the answer with the highest confidence .,introduction
text_generation,1,Experimental results clearly demonstrate that our proposed method outperforms the state - of - the - art methods .,introduction,introduction,0,37,27,27,0,introduction : introduction,0.13503649635036494,1.0,1.0,Experimental results clearly demonstrate that our proposed method outperforms the state of the art methods ,16,Our method is suitable for language learning in comparison to conventional GANs ., ,introduction
text-classification,8,"Consider a text sequence represented as X ( either a sentence or a document ) , composed of a sequence of words : {w 1 , w 2 , .... , w L } , where L is the number of tokens , i.e. , the sentence / document length .",system description,Models & training,0,49,2,2,0,system description : Models & training,0.1821561338289963,0.030303030303030307,0.3333333333333333,Consider a text sequence represented as X either a sentence or a document composed of a sequence of words w 1 w 2 w L where L is the number of tokens i e the sentence document length ,39, ,"Let {v 1 , v 2 , .... , v L } denote the respective word embeddings for each token , where v l ? R K .",method
topic_models,0,We used the same early stopping mechanism to train the classifiers .,baseline,baseline,0,297,14,14,0,baseline : baseline,0.7208737864077671,0.5384615384615384,0.5384615384615384,We used the same early stopping mechanism to train the classifiers ,12,"Note that we can not use GLCU here , because SMM yields only point - estimates of embeddings .","The experimental analysis in Section VII - C shows that Bayesian SMM is more robust to over-fitting when compared to SMM and NVDM , and does not require an early stopping mechanism .",result
natural_language_inference,11,We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.018115942028985508,0.3333333333333333,0.3333333333333333,We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models ,17,"Common- sense and background knowledge is required to understand natural language , but in most neural natural language understanding ( NLU ) systems , this knowledge must be acquired from training corpora during learning , and then it is static at test time .",general - purpose reading module reads background knowledge in the form of freetext statements ( together with task - specific text inputs ) and yields refined word representations to a task - specific NLU architecture that reprocesses the task inputs with these representations .,abstract
natural_language_inference,58,It is worth noting that this SQuAD leaderboard is highly active and competitive .,system description,Small Graph Large Graph,0,252,19,19,0,system description : Small Graph Large Graph,0.7522388059701492,0.1919191919191919,0.7916666666666666,It is worth noting that this SQuAD leaderboard is highly active and competitive ,14,"Speci cally , BiDAF model could be viewed as a special case of ReasoNet with T max = 1 .",The test set is hidden to all models and all the results on the leaderboard are produced and reported by the organizer ; thus all the results here are reproducible .,method
natural_language_inference,51,"The program memory stores the weights of the MANN 's controller network , which are retrieved quickly via a key - value attention mechanism across timesteps yet updated slowly via backpropagation .",introduction,introduction,1,23,15,15,0,introduction : introduction,0.08679245283018867,0.6521739130434783,0.6521739130434783,The program memory stores the weights of the MANN s controller network which are retrieved quickly via a key value attention mechanism across timesteps yet updated slowly via backpropagation ,30,"The program memory co-exists with the data memory in the MANN , providing more flexibility , reuseability and modularity in learning complicated tasks .","By introducing a meta network to moderate the operations of the program memory , our model , henceforth referred to as Neural Stored - program Memory ( NSM ) , can learn to switch the programs / weights in the controller network appropriately , adapting to different functionalities aligning with different parts of a sequential task , or different tasks in continual and few - shot learning .",introduction
relation_extraction,12,Results on Sentence - level Relation Extraction,model,Results on Sentence-level Relation Extraction,0,217,30,1,0,model : Results on Sentence-level Relation Extraction,0.6595744680851063,0.6521739130434783,0.3333333333333333,Results on Sentence level Relation Extraction,6, , ,method
natural_language_inference,88,The attention at the ( k + 1 ) th layer is computed as :,system description,Long Short-Term Memory-Network,0,113,41,31,0,system description : Long Short-Term Memory-Network,0.4574898785425101,0.6212121212121212,0.96875,The attention at the k 1 th layer is computed as ,12,This can be achieved by feeding the output h kt of the lower layer k as input to the upper layer ( k + 1 ) .,Skip - connections can be applied to feed x t to upper layers as well .,method
natural_language_inference,14,"Under such a data collection scheme , we may need to increase the reward for fooling the model in cross-validation compared to that for fooling the current model ( whereas , these two rewards were equal in CODAH ) , in order to disincentivize adversarial attacks that manipulate the current model to make it easy to fool on subsequent questions .",dataset,dataset,0,165,13,13,0,dataset : dataset,0.9649122807017544,1.0,1.0,Under such a data collection scheme we may need to increase the reward for fooling the model in cross validation compared to that for fooling the current model whereas these two rewards were equal in CODAH in order to disincentivize adversarial attacks that manipulate the current model to make it easy to fool on subsequent questions ,57,"Secondly , if we wish to use our adversarial collection approach to grow CODAH to tens of thousands of examples , we should update our system as new data arrives , so that contributors are able to tune their questions to remain difficult for the strongest , most up - to - date version of the system .", ,experiment
named-entity-recognition,7,"The system 's state is defined as [ S , i , A ] which denotes stack , buffer front index and action history respectively .",model,Shift-Reduce System,0,52,9,4,0,model : Shift-Reduce System,0.3291139240506329,0.1323529411764706,0.16,The system s state is defined as S i A which denotes stack buffer front index and action history respectively ,21,"Generally , our system employs a stack to store ( partially ) processed nested elements .",In each step .,method
natural_language_inference,70,The best SVM regularization parameter estimated during the tuning stage is C = 5 .,result,Subtask C Model:,0,152,30,9,0,result : Subtask C Model:,0.8685714285714285,0.5660377358490566,0.28125,The best SVM regularization parameter estimated during the tuning stage is C 5 ,14,"LK B is a linear kernel that oper - ates on feature vectors including : ( i ) the similarity metrics between o and q , and between o and the entire answer thread of q , as described in Section 3.1 ; ( ii ) ranking features discussed in Section 4.1 ; ( iii ) the features derived from the Subtask A scores ( see Section 4.4 ) .","We made two additional submissions in which the model has minor variations : in the Contrastive 1 ( KC1 ) , we substituted All SPTK with SPTK + whereas in the contrastive 2 ( KC2 ) we do not include the features derived from the Subtask A scores .",result
sentiment_analysis,14,"For sentiment tasks , accuracy score is reported .",evaluation,Evaluation,0,109,11,11,0,evaluation : Evaluation,0.7171052631578947,0.8461538461538461,0.8461538461538461,For sentiment tasks accuracy score is reported ,8,We release Emo2 Vec trained on all datasets .,"For other tasks , if it is binary task , we report f 1 score for the positive class .",result
temporal_information_extraction,1,"On the TD - Test dataset , all systems other than ClearTK had better F 1 scores compared to their performances on TE3 - PT .",baseline,Comparison with CAEVO,0,247,51,9,0,baseline : Comparison with CAEVO,0.9610894941634242,0.9272727272727272,0.6923076923076923,On the TD Test dataset all systems other than ClearTK had better F 1 scores compared to their performances on TE3 PT ,23,"Therefore , we have chosen CAEVO as the baseline system to evaluate the significance of the proposed ones .","This notable difference ( i.e. , 48.53 vs 40.3 ) indicates the better quality of the dense annotation scheme that was used to create TD .",result
text_summarization,12,"Readout state rt is a 2 d - dimensional vector , and the maxout layer ( Equation 16 ) picks the max value for every two numbers in rt and produces a d-dimensional vector mt .",model,Summary Decoder,0,122,43,11,0,model : Summary Decoder,0.5350877192982456,0.8775510204081632,1.0,Readout state rt is a 2 d dimensional vector and the maxout layer Equation 16 picks the max value for every two numbers in rt and produces a d dimensional vector mt ,33,"where W a , U a , W r , Ur , V rand W o are weight matrices .", ,method
text-classification,6,Accuracy is reported for all evaluations except STS Bench where we report the Pearson correlation of the similarity scores with human judgments .,model,Combined Transfer Models,0,96,9,9,0,model : Combined Transfer Models,0.6486486486486487,0.75,0.75,Accuracy is reported for all evaluations except STS Bench where we report the Pearson correlation of the similarity scores with human judgments ,23,"Models tagged with w2 v w.e. make use of pre-training word2vec skip - gram embeddings for the transfer task model , while models tagged with lrn w.e. use randomly initialized word embeddings that are learned only on the transfer task data .",Pairwise similarity scores are computed directly using the sentence embeddings from the universal sentence encoder as in Eq. ( 1 ) .,method
natural_language_inference,88,The results in show that both 1 - and 2 - layer LSTMNs outperform the LSTM baselines while achieving numbers comparable to state of the art .,model,Models,1,204,19,19,0,model : Models,0.8259109311740891,0.3584905660377358,0.8636363636363636,The results in show that both 1 and 2 layer LSTMNs outperform the LSTM baselines while achieving numbers comparable to state of the art ,25,"For comparison , we also report the performance of the paragraph vector model ( PV ; ; see , second block ) which neither operates on trees nor sequences but learns distributed document representations parameterized directly .",The number of layers for our models was set to be comparable to previously published results .,method
text-to-speech_synthesis,1,The optimizer and other hyper - parameters for FastSpeech are the same as the autoregressive Transformer TTS model .,training,Training and Inference,0,137,9,9,0,training : Training and Inference,0.6255707762557078,0.8181818181818182,0.8181818181818182,The optimizer and other hyper parameters for FastSpeech are the same as the autoregressive Transformer TTS model ,18,We train the FastSpeech model together with the duration predictor .,The FastSpeech model training takes about 80 k stepson 4 NVIDIA V100 GPUs .,experiment
question-answering,1,"As suggested by the order - preserving property and the generality of ARC - II , this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence , despite the fact that it is built on the interaction between two sentences .",model,model,0,114,9,9,0,model : model,0.5906735751295337,0.75,0.75,As suggested by the order preserving property and the generality of ARC II this architecture offers not only the capability but also the inductive bias for the individual development of internal abstraction on each sentence despite the fact that it is built on the interaction between two sentences ,49,"If we further limit the parameters in the second convolution units ( more specifically w ( 2 , f ) ) to those for S X and S Y , we can ensure the individual development of different levels of abstraction on each side , and fully recover the functionality of ARC - I .","As a result , ARC - II can naturally blend two seemingly diverging processes :",method
temporal_information_extraction,0,"The TimeBank 1.2 corpus contains 183 documents coming from a variety of news report , specifically from the ACE program and PropBank , while the AQUAINT corpus contains 73 news report documents and often referred to as the Opinion corpus .",dataset,dataset,0,132,3,3,0,dataset : dataset,0.6984126984126984,0.1875,0.1875,The TimeBank 1 2 corpus contains 183 documents coming from a variety of news report specifically from the ACE program and PropBank while the AQUAINT corpus contains 73 news report documents and often referred to as the Opinion corpus ,40,"For the evaluation of the temporal relation extraction module following TempEval - 3 , we use the same training and test data released for the shared task , 8 i.e. TBAQ - cleaned ( cleaned and improved version of the TimeBank 1.2 and the AQUAINT corpora ) and TempEval - 3 - platinum , respectively .","The TempEval - 3 - platinum corpus , containing 20 news articles , was annotated / reviewed by the TempEval - 3 organizers .",experiment
sentiment_analysis,41,is a vector consisting of attention weights and r is a weighted representation of sentence with given aspect .,system description,Attention-based LSTM (AT-LSTM),0,97,59,11,0,system description : Attention-based LSTM (AT-LSTM),0.4349775784753363,0.7662337662337663,0.4782608695652174,is a vector consisting of attention weights and r is a weighted representation of sentence with given aspect ,19,The attention mechanism will produce an attention weight vector ? and a weighted hidden representation r.,is a vector consisting of attention weights and r is a weighted representation of sentence with given aspect .,method
sentiment_analysis,2,Positive : Samples of SemEval 2014 Dataset .,dataset,Datasets,0,133,2,2,0,dataset : Datasets,0.5859030837004405,0.06896551724137931,0.06896551724137931,Positive Samples of SemEval 2014 Dataset ,7, ,LSTM : LSTM takes the sentence as input so as to get the hidden representation of each word .,experiment
named-entity-recognition,7,Words are represented by concatenating three vectors :,model,Representation of Words,0,88,45,2,0,model : Representation of Words,0.5569620253164557,0.6617647058823529,0.4,Words are represented by concatenating three vectors ,8, ,where e w i and e pi denote the embeddings for i - th word and it s POS tag respectively .,method
natural_language_inference,63,"However , most of these models suffer in reasoning overlong documents .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.03333333333333333,0.5714285714285714,0.5714285714285714,However most of these models suffer in reasoning overlong documents ,11,"In recent years , several end - to - end neural network models have been proposed to solve RC tasks .","In this work , we propose a novel Memory Augmented Machine Comprehension Network ( MAMCN ) to address long - range dependencies present in machine reading comprehension .",abstract
natural_language_inference,71,"From this perspective , learning to forget can be difficult with unitary / orthogonal RNNs and they can clog up the memory with useless information .",introduction,introduction,0,26,17,17,0,introduction : introduction,0.12093023255813952,0.6538461538461539,0.6538461538461539,From this perspective learning to forget can be difficult with unitary orthogonal RNNs and they can clog up the memory with useless information ,24,"The importance of forgetting for those networks is mainly due to that unitary / orthogonal RNNs can backpropagate the gradients without vanishing through time , and it is very easy for them to just have an output that depends on equal amounts of all the elements of the whole input sequence .","However , most real - world applications and natural tasks require the model to filter out irrelevant or redundant information from the input sequence .",introduction
natural_language_inference,53,"We experiment with two ways of combining the varying number of {h t } t to form a fixed - size vector , either by selecting the maximum value over each dimension of the hidden units ( max pooling ) or by considering the average of the representations ( mean pooling ) .",architecture,BiLSTM with mean/max pooling For a sequence of T words {w,0,69,11,4,0,architecture : BiLSTM with mean/max pooling For a sequence of T words {w,0.3317307692307692,0.3928571428571429,0.6666666666666666,We experiment with two ways of combining the varying number of h t t to form a fixed size vector either by selecting the maximum value over each dimension of the hidden units max pooling or by considering the average of the representations mean pooling ,46,is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions :,The movie was great,method
text-classification,5,"We propose Universal Language Model Finetuning ( ULMFiT ) , which pretrains a language model ( LM ) on a large general - domain corpus and fine - tunes it on the target task using novel techniques .",system description,Universal Language Model Fine-tuning,0,69,18,18,0,system description : Universal Language Model Fine-tuning,0.27380952380952384,0.18947368421052632,0.75,We propose Universal Language Model Finetuning ULMFiT which pretrains a language model LM on a large general domain corpus and fine tunes it on the target task using novel techniques ,31,"Formally , language modeling induces a hypothesis space H that should be useful for many other NLP tasks .",The method is universal in the sense that it meets these practical criteria :,method
natural_language_inference,26,"However , these approaches for learning word vectors only involve a single , context independent representation for each word with litter consideration of contextual encoding in sentence level .",model,Language Modeling for NLU,0,40,6,6,0,model : Language Modeling for NLU,0.18867924528301888,0.1,0.5,However these approaches for learning word vectors only involve a single context independent representation for each word with litter consideration of contextual encoding in sentence level ,27,Distributed representations have been widely used as a standard part of NLP models due to the ability to capture the local co-occurence of words from large scale unlabeled text ) .,"Thus recently introduced contextual language models including ELMo , GPT , BERT and XLNet fill the gap by strengthening the contextual sentence modeling for better representation , among which BERT uses a different pre-training objective , masked language model , which allows capturing both sides of context , left and right .",method
text-to-speech_synthesis,0,"Second , we train a variety of models ( CNN , RNN and Transformer ) for ensemble to get higher accuracy , and transfer the knowledge of the ensemble models to a light - weight model that is suitable for online deployment , again by knowledge distillation .",introduction,introduction,1,25,14,14,0,introduction : introduction,0.15432098765432098,0.6363636363636364,0.6363636363636364,Second we train a variety of models CNN RNN and Transformer for ensemble to get higher accuracy and transfer the knowledge of the ensemble models to a light weight model that is suitable for online deployment again by knowledge distillation ,41,"Specifically , we train a teacher model to generate the phoneme sequence as well as its probability distribution given unlabeled grapheme sequence , and regard the unlabeled grapheme sequence and the generated phoneme sequence as pseudo labeled data , and add them into the original training data .","Besides , we adopt Transformer instead of RNN or CNN as the basic encoder - decoder model structure , since it demonstrates advantages in a variety of sequence to sequence tasks , such as neural machine translation , text summarization , automatic speech recognition .",introduction
relation_extraction,13,The results displayed in show the development set performance when tuning on a random subset of the task specific training data .,experiment,Few-shot Relation Matching,0,201,23,18,0,experiment : Few-shot Relation Matching,0.943661971830986,0.8846153846153846,0.9,The results displayed in show the development set performance when tuning on a random subset of the task specific training data ,22,We also analyzed the performance of our two models while reducing the amount of supervised task specific tuning data .,"For all tasks , we see that MTB based training is even more effective for low - resource cases , where there is a larger gap in performance between our BERT EM and BERT EM + MTB based classifiers .",experiment
natural_language_inference,94,the letter is understood to contain a denunciation of saltram s most immoral acts .,training,training,0,330,65,65,0,training : training,0.8616187989556136,0.5508474576271186,0.5508474576271186,the letter is understood to contain a denunciation of saltram s most immoral acts ,15,"finally , the unnamed narrator is given a sealed letter and asked to give it to anvoy .",the narrator must decide whether to blight saltram s prospects by delivering the letter .,experiment
named-entity-recognition,2,"In , we also list the F1 of our ID - CNN model and the Bi - LSTM - CRF model trained on entire document context .",model,Model,0,206,10,10,0,model : Model,0.9671361502347418,0.7692307692307693,0.7692307692307693,In we also list the F1 of our ID CNN model and the Bi LSTM CRF model trained on entire document context ,23,"Incorporating greater context significantly boosts the score of our greedy model on OntoNotes , whereas the Bi - LSTM - CRF performs more poorly .","For the first time , we see the score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase .",method
sentiment_analysis,2,"Traditional machine learning approaches mainly involve text representation and feature extraction , such as bag - of - words models and sentiment lexicons features , then training a sentiment classifier .",system description,Machine Learning for Sentiment Analysis,0,205,2,2,0,system description : Machine Learning for Sentiment Analysis,0.9030837004405288,0.10526315789473684,0.4,Traditional machine learning approaches mainly involve text representation and feature extraction such as bag of words models and sentiment lexicons features then training a sentiment classifier ,27, ,demonstrated the utility of graph - based semi-supervised learning framework for building sentiment lexicons .,method
sentiment_analysis,41,We propose attention - based Long Short - Term memory for aspect - level sentiment classification .,introduction,introduction,0,30,19,19,0,introduction : introduction,0.13452914798206278,0.76,0.76,We propose attention based Long Short Term memory for aspect level sentiment classification ,14,The main contributions of our work can be summarized as follows :,The models are able to attend different parts of a sentence when different aspects are concerned .,introduction
machine-translation,1,An encoder and a decoder network that process sequences of different lengths can not be directly connected due to the different sizes of the computed representations .,model,Desiderata,0,83,33,22,0,model : Desiderata,0.4129353233830846,0.5409836065573771,0.44,An encoder and a decoder network that process sequences of different lengths can not be directly connected due to the different sizes of the computed representations ,27,This is in contrast to models that compress the source representation into a fixed - size vector or that pool over the source representation with a mechanism such as attentional pooling .,"We circumvent this issue via a mechanism which we call dynamic unfolding , which works as follows .",method
semantic_parsing,2,Sketches for SQL queries are simply the ( sorted ) sequences of condition operators cond op in WHERE clauses .,training,Natural Language to SQL,0,179,70,9,0,training : Natural Language to SQL,0.6151202749140894,0.5932203389830508,0.36,Sketches for SQL queries are simply the sorted sequences of condition operators cond op in WHERE clauses ,18,"WHERE can have zero or multiple conditions , which means that column cond col must satisfy the constraints expressed by the operator cond op 3 and the condition value cond .","For example , in , sketch "" WHERE > AND ="" has two condition operators , namely "" > "" and "" = "" .",experiment
natural_language_inference,79,"The second advantage of the paragraph vectors is that they take into consideration the word order , at least in a small context , in the same way that an n-gram model with a large n would do .",model,Advantages of paragraph vectors:,0,120,40,6,0,model : Advantages of paragraph vectors:,0.4477611940298508,0.7272727272727273,0.75,The second advantage of the paragraph vectors is that they take into consideration the word order at least in a small context in the same way that an n gram model with a large n would do ,38,"In this space , "" powerful "" is closer to "" strong "" than to "" Paris . ""","This is important , because the n-gram model preserves a lot of information of the paragraph , including the word order .",method
sentence_compression,0,We also compare our method with a traditional ILP - based method using syntactic structures of sentences but not based on neural networks .,introduction,introduction,0,42,35,35,0,introduction : introduction,0.15053763440860216,0.9722222222222222,0.9722222222222222,We also compare our method with a traditional ILP based method using syntactic structures of sentences but not based on neural networks ,23,"In the cross - domain setting , our proposed method can clearly outperform the original method .",We find that our method can outperform this baseline for both in - domain and out - of - domain data .,introduction
paraphrase_generation,1,"Similarly paraphrasing finds applications in information retrieval by generating query variants , and in machine translation or summarization by generating variants for automatic evaluation .",introduction,introduction,0,16,5,5,0,introduction : introduction,0.07239819004524888,0.16666666666666666,0.16666666666666666,Similarly paraphrasing finds applications in information retrieval by generating query variants and in machine translation or summarization by generating variants for automatic evaluation ,24,"In an open QA system pipeline , question analysis and paraphrasing is a critical first step , in which a given question is reformulated by expanding it with its various paraphrases with the intention of improvement in recall , an important metric in the early stage of the pipeline .","Copyright c 2018 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",introduction
natural_language_inference,58,It takes 7 hours per epoch to train on the Daily Mail dataset and 3 hours per epoch to train on the CNN dataset .,training,CNN and Daily Mail Datasets,0,180,49,45,0,training : CNN and Daily Mail Datasets,0.5373134328358209,0.6282051282051282,0.8035714285714286,It takes 7 hours per epoch to train on the Daily Mail dataset and 3 hours per epoch to train on the CNN dataset ,25,Models are trained on GTX TitanX 12 GB .,The models are usually converged within 6 epochs on both CNN and Daily Mail datasets .,experiment
question-answering,3,Sentence Similarity Learning by Lexical Decomposition and Composition,title,title,1,2,1,1,0,title : title,0.007874015748031496,1.0,1.0,Sentence Similarity Learning by Lexical Decomposition and Composition,8, , ,title
natural_language_inference,88,"We set the number of layers to 3 in this experiment , mainly to agree with the language modeling experiments of .",model,Language Modeling,0,163,18,18,0,model : Language Modeling,0.6599190283400811,0.5454545454545454,0.5454545454545454,We set the number of layers to 3 in this experiment mainly to agree with the language modeling experiments of ,21,"In general , both g LSTM and d LSTM are able to capture long - term dependencies to some degree , but they do not explicitly keep past memories .",Also note that that there are no single - layer variants for g LSTM and d LSTM ; they have to be implemented as multi - layer systems .,method
sentiment_analysis,25,where v a is the embedding vector of the given aspect category in ACSA or computed by another CNN over aspect terms in ATSA .,system description,Gated Convolutional Network with Aspect Embedding,0,107,63,40,0,system description : Gated Convolutional Network with Aspect Embedding,0.4819819819819821,0.7875,0.8333333333333334,where v a is the embedding vector of the given aspect category in ACSA or computed by another CNN over aspect terms in ATSA ,25,"Specifically , we compute the features c i as","The two convolutions in Equation 2 and 3 are the same as the convolution in the vanilla CNN , but the convolutional features a i receives additional aspect information v a with ReLU activation function .",method
natural_language_inference,78,"We conduct experiments on three popular benchmarks , SNLI , MultiNLI and SciTail , achieving competitive performance on all .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.028985507246376805,0.6666666666666666,0.6666666666666666,We conduct experiments on three popular benchmarks SNLI MultiNLI and SciTail achieving competitive performance on all ,17,"The design of our approach is aimed to be conceptually simple , compact and yet powerful .","lightweight parameterization of our model also enjoys a ? 3 times reduction in parameter size compared to the existing state - of - the - art models , e.g. , ESIM and DIIN , while maintaining competitive performance .",abstract
passage_re-ranking,1,"However , there is an important difference .",training,training,0,59,3,3,0,training : training,0.8082191780821918,0.2727272727272727,0.2727272727272727,However there is an important difference ,7,We follow the same procedure described for the MS MARCO dataset to fine - tune our models on TREC - CAR .,The official pre-trained BERT 21.8 19.8 - Conv-KNRM,experiment
sentiment_analysis,42,Effects of Location Attention,method,Effects of Location Attention,0,190,6,1,0,method : Effects of Location Attention,0.7539682539682541,0.2727272727272727,0.05882352941176471,Effects of Location Attention,4, , ,method
text_generation,0,"Then in order to increase the chance of passing Turing Test , we actually need to minimize the exact opposite average negative log -likelihood ? E x? q log p human ( x ) ( Huszr 2015 ) , with the role of p and q exchanged .",evaluation,Evaluation Metric,0,179,6,6,1,evaluation : Evaluation Metric,0.5524691358024691,0.6,0.6,Then in order to increase the chance of passing Turing Test we actually need to minimize the exact opposite average negative log likelihood E x q log p human x Huszr 2015 with the role of p and q exchanged ,41,We assume that the human observer has learned an accurate model of the natural distribution p human ( x ) .,"In our synthetic data experiments , we can consider the oracle to be the human observer for real - world problems , thus a perfect evaluation metric should be",result
natural_language_inference,56,"simple strategy works by noting that if a certain Sudoku cell is given as a "" 7 "" , one can safely remove "" 7 "" as an option from other cells in the same row , column and box .",system description,Recurrent Relational Networks,0,44,3,3,0,system description : Recurrent Relational Networks,0.13095238095238096,0.07894736842105263,0.07894736842105263,simple strategy works by noting that if a certain Sudoku cell is given as a 7 one can safely remove 7 as an option from other cells in the same row column and box ,35,"We ground the discussion of a recurrent relational network in something familiar , solving a Sudoku puzzle .","In a message passing framework , that cell needs to send a message to each other cell in the same row , column , and box , broadcasting it 's value as "" 7 "" , and informing those cells not to take the value "" 7 "" .",method
question_answering,2,sis the mapping defined in .,architecture,Cross Sequence Interaction,0,163,84,14,0,architecture : Cross Sequence Interaction,0.5884476534296029,0.7706422018348624,0.35897435897435903,sis the mapping defined in ,6,"where ? is a function to compute the correlation between question and context , w s ? R 4d 1 is the learned weights and b sis the bias term .","As explained for Eq. ( 4 ) , we use such similarity representations since they capture both the cosine similarity and Euclidean distance information .",method
sentiment_analysis,4,"At each time step t ? [ 1 , T ] of video U , our model is provided with the utterance spoken at that time , i.e. u t , and tasked to predict its emotion .",system description,Problem Setting,0,98,14,14,0,system description : Problem Setting,0.3006134969325153,0.6666666666666666,0.6666666666666666,At each time step t 1 T of video U our model is provided with the utterance spoken at that time i e u t and tasked to predict its emotion ,32,Our aim is to identify the emotions of utterances in conversational videos .,"At each time step t ? [ 1 , T ] of video U , our model is provided with the utterance spoken at that time , i.e. u t , and tasked to predict its emotion .",method
natural_language_inference,80,DR- BiLSTM : Dependent Reading Bidirectional LSTM for Natural Language Inference,title,title,1,2,1,1,0,title : title,0.006896551724137931,1.0,1.0,DR BiLSTM Dependent Reading Bidirectional LSTM for Natural Language Inference,10, , ,title
paraphrase_generation,0,Here EDD - LG - S refers to our EDD - LG shared model and others are the different variations of our model described in section 4.1.3 and the models on the right are the different variations proposed in .,model,Model,0,175,7,7,0,model : Model,0.7383966244725738,0.7,0.7777777777777778,Here EDD LG S refers to our EDD LG shared model and others are the different variations of our model described in section 4 1 3 and the models on the right are the different variations proposed in ,39,The error rates of other methods are reported in Figure 3 : The mean rank of all the models on the basis of BLEU score are plotted on the x - axis .,Also the colored lines between the two models represents that these models are not significantly different from each other .,method
sentiment_analysis,29,"In our error analysis , there are cases that our model can not handle efficiently .",analysis,Error Analysis,0,170,6,6,0,analysis : Error Analysis,0.9770114942528736,0.6,0.6,In our error analysis there are cases that our model can not handle efficiently ,15,Our case study also shows that our model learns the important parts in the sentence as well as in the target effectively .,One is the complex sentiment expression .,result
sentiment_analysis,37,Aspect - Aware Sentence Representation,model,Overview,0,70,10,6,0,model : Overview,0.2766798418972332,0.13513513513513514,0.75,Aspect Aware Sentence Representation,4,"For multi-worded aspect - terms , we take the mean of constituent word embeddings as aspect representation .","Following , all the words in a sentence are concatenated with the given aspect representation .",method
text_summarization,12,South Korean President Kim Young - Sam left here Wednesday on a week - long state visit to Russia and Uzbekistan for talks on North Korea 's nuclear confrontation and ways to strengthen bilateral ties .,system description,Input:,0,77,11,2,0,system description : Input:,0.33771929824561403,0.8461538461538461,0.5,South Korean President Kim Young Sam left here Wednesday on a week long state visit to Russia and Uzbekistan for talks on North Korea s nuclear confrontation and ways to strengthen bilateral ties ,34, ,Output : Kim leaves for Russia for talks on NKorea nuclear standoff :,method
semantic_role_labeling,2,"What is the model good at and what kinds of mistakes does it make ? How well do LSTMs model global structural consistency , despite conditionally independent tagging decisions ? Is our model implicitly learning syntax , and could explicitly modeling syntax still help ? All the analysis in this section is done on the CoNLL 2005 development set with gold predicates , unless otherwise stated .",analysis,Analysis,0,120,3,3,0,analysis : Analysis,0.5357142857142857,0.034883720930232565,0.375,What is the model good at and what kinds of mistakes does it make How well do LSTMs model global structural consistency despite conditionally independent tagging decisions Is our model implicitly learning syntax and could explicitly modeling syntax still help All the analysis in this section is done on the CoNLL 2005 development set with gold predicates unless otherwise stated ,61,"To better understand our deep SRL model and its relation to previous work , we address the following questions with a suite of empirical analyses :","What is the model good at and what kinds of mistakes does it make ? How well do LSTMs model global structural consistency , despite conditionally independent tagging decisions ? Is our model implicitly learning syntax , and could explicitly modeling syntax still help ? All the analysis in this section is done on the CoNLL 2005 development set with gold predicates , unless otherwise stated .",result
natural_language_inference,72,"Complete remission with reversal of pulmonary damage was achieved , as reported by CT scan , pulmonary function tests and functional status .",Training details and hyper-parameter optimization,Bridging inference passage,0,293,33,6,0,Training details and hyper-parameter optimization : Bridging inference passage,0.9391025641025642,0.6346153846153846,0.24,Complete remission with reversal of pulmonary damage was achieved as reported by CT scan pulmonary function tests and functional status ,21,"As lung injury was the main concern , treatment consisted of prednisolone and cyclophosphamide .","query Therefore , in severe cases an aggressive treatment , combining and glucocorticoids as used in systemic vasculitis , is suggested .",experiment
relation_extraction,6,"As an additional baseline , we re-implement a sentence - level model based on convolutional neural networks ( CNNs ) described in .",training,Held-out evaluation,0,106,9,2,0,training : Held-out evaluation,0.8153846153846154,0.3333333333333333,0.1,As an additional baseline we re implement a sentence level model based on convolutional neural networks CNNs described in ,20, ,This is a state - of - the - art model for fine - grained relation extraction that was previously tested on the single - relation dataset from .,experiment
sentiment_analysis,8,We use PyTorch to implement the LSTM classifiers described earlier .,implementation,implementation,1,166,5,5,0,implementation : implementation,0.7094017094017094,0.14285714285714285,0.14285714285714285,We use PyTorch to implement the LSTM classifiers described earlier ,11,"We use scikit - learn and xgboost [ 25 ] , the machine learning libraries for Python , to implement all the ML classifiers ( RF , XGB , SVM , MNB , and LR ) and the MLP .","In order to regularize the hidden space of the LSTM classifiers , we use a shut - off mechanism , called dropout , where a fraction of neurons are not used for final prediction .",experiment
natural_language_inference,9,QRN outperforms previous work by a large margin ( 2.0 + % ) in every comparison .,model,RESULTS.,1,225,45,11,0,model : RESULTS.,0.6818181818181818,0.569620253164557,0.24444444444444444,QRN outperforms previous work by a large margin 2 0 in every comparison ,14,Match ' is the extension to the model which additionally takes as input whether each answer candidate matches with context ( more details on Appendix ) .,"We test four types of ablations ( also discussed in Section 2.2 ) : number of layers ( 1 , 2 , 3 , or 6 ) , reset gate ( r ) , and gate vectorization ( v ) and the dimension of the hidden vector ( 50 , 100 ) .",method
sentiment_analysis,41,Three sentiment polarity although given different aspects .,model,Models,0,181,2,2,0,model : Models,0.8116591928251121,0.16666666666666666,0.16666666666666666,Three sentiment polarity although given different aspects ,8, ,"Since it can not take advantage of the aspect information , not surprisingly the model has worst performance .",method
natural_language_inference,9,"Our experiments show that QRN produces the state - of - the - art results in bAbI QA and dialog tasks , and in are al goal - oriented dialog dataset .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.021212121212121213,0.8333333333333334,0.8333333333333334,Our experiments show that QRN produces the state of the art results in bAbI QA and dialog tasks and in are al goal oriented dialog dataset ,27,"QRN considers the context sentences as a sequence of state - changing triggers , and reduces the original query to a more informed query as it observes each trigger ( context sentence ) through time .","In addition , QRN formulation allows parallelization on RNN 's time axis , saving an order of magnitude in time complexity for training and inference .",abstract
sentiment_analysis,8,"However , neither of them are able to beat the lighter E1 model ( Ensemble of RF , XGB and MLP ) which was trained on the eightdimensional audio feature vectors .",result,RESULTS,0,204,8,8,0,result : RESULTS,0.8717948717948718,0.26666666666666666,0.3478260869565217,However neither of them are able to beat the lighter E1 model Ensemble of RF XGB and MLP which was trained on the eightdimensional audio feature vectors ,28,Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight - dimensional features achieves very low accuracy as compared to the end - to - end trained ARE .,"look at the confusion matrix ) reveals that detecting "" neutral "" or distinguishing between "" angry "" , "" happy "" and "" sad "" is the most difficult for the model .",result
natural_language_inference,69,"When answering 100 questions , the annotator knew the answer prior to reading the documents for 9 % , and produced the correct answer after reading the document sets for 74 % of the cases .",analysis,36%,0,176,23,13,0,analysis : 36%,0.5101449275362319,0.4259259259259259,0.65,When answering 100 questions the annotator knew the answer prior to reading the documents for 9 and produced the correct answer after reading the document sets for 74 of the cases ,32,"These cases can either be due to conflicting information between WIKIDATA and WIKIPEDIA ( 8 % ) , e.g. when the date of birth for a person differs between WIKI - DATA and what is stated in the WIKIPEDIA article , or because the answer is consistent but can not be inferred from the support documents ( 12 % ) .","On 100 questions of a validated portion of the Dev set ( see Section 5.3 ) , 85 % accuracy was reached .",result
natural_language_inference,18,"Hence empirically , ' how ' questions are harder to ' understand and answer ' .",experiment,Q1 how old was sue lyon when she made lolita,0,206,80,15,0,experiment : Q1 how old was sue lyon when she made lolita,0.7545787545787546,1.0,1.0,Hence empirically how questions are harder to understand and answer ,11,The MAP of ' how ' questions is 0.524 which is the lowest among the five groups ., ,experiment
natural_language_inference,45,We then use an attention mechanism over I ij to output hidden states hi as follows,system description,DOCUMENT-QUERY FINE-GRAINED GATING,0,128,60,15,0,system description : DOCUMENT-QUERY FINE-GRAINED GATING,0.6432160804020101,0.9375,0.7894736842105263,We then use an attention mechanism over I ij to output hidden states hi as follows,16,where q j can be viewed as agate to filter the information in pi .,"where uh is ad v - dimensional model parameter , b h 1 and b h2 are scalar model parameters , w i and w j are one - hot encodings for pi and q j respectively .",method
sentiment_analysis,17,"While numerous LSTM variants have been described , here we describe the version used by .",system description,Overview,0,51,12,11,0,system description : Overview,0.22666666666666666,0.15384615384615385,0.5238095238095238,While numerous LSTM variants have been described here we describe the version used by ,15,The LSTM architecture addresses this problem of learning long - term dependencies by introducing a memory cell that is able to preserve state overlong periods of time .,"We define the LSTM unit at each time step t to be a collection of vectors in Rd : an input gate it , a forget gate ft , an output gate o t , a memory cell ct and a hidden state ht .",method
text_summarization,5,"Due to the strong rewriting ability of the seq2seq framework , in this paper , we propose to combine the seq2seq and template based summarization approaches .",introduction,introduction,1,30,21,21,0,introduction : introduction,0.11904761904761905,0.5833333333333334,0.5833333333333334,Due to the strong rewriting ability of the seq2seq framework in this paper we propose to combine the seq2seq and template based summarization approaches ,25,We call these existing summaries soft templates since no actual rules are nee-ded to build new summaries from them .,"We call our summarization system Re 3 Sum , which consists of three modules : Retrieve , Rerank and Rewrite .",introduction
sentiment_analysis,11,We compare CMN with the following baselines :,baseline,Baselines,0,256,2,2,0,baseline : Baselines,0.7441860465116279,0.0625,0.4,We compare CMN with the following baselines ,8, ,strong context - free benchmark model which uses similar multimodal approach on an ensemble of trees .,result
natural_language_inference,44,Who was the Jin dynasty defector who betrayed the location of the Jin army ? :,training,SQuAD-Adversarial,0,271,56,18,0,training : SQuAD-Adversarial,0.9475524475524476,0.8,0.5625,Who was the Jin dynasty defector who betrayed the location of the Jin army ,15,"In On the Abrogation of the Private Mass , he condemned as idolatry the idea that the mass is a sacrifice , asserting instead that it is a gift , to be received with thanksgiving by the whole congregation .","Examples on SQuAD , which MINIMAL predicts the wrong answer .",experiment
sarcasm_detection,1,This particular type of sarcasm is tough to detect .,introduction,introduction,0,20,11,11,0,introduction : introduction,0.05988023952095808,0.3548387096774194,0.3548387096774194,This particular type of sarcasm is tough to detect ,10,The usage of slangs and informal language also diminishes the reliance on lexical cues .,Contextual dependencies for sarcasm can take many forms .,introduction
relation_extraction,7,The following equation defines the operation mathematically .,methodology,BGRU over STP,0,109,48,9,0,methodology : BGRU over STP,0.4208494208494208,0.42105263157894735,0.8181818181818182,The following equation defines the operation mathematically ,8,shows the processing of BGRU over STP .,"In above equation , the t th word output hit ? R m of BGRU is the element - wise addition of the t th hidden states of forward GRU and backward one .",method
natural_language_inference,80,"man is waiting inline Contradiction for the bus. a P , Premise . b H , Hypothesis ..",introduction,introduction,0,20,11,11,0,introduction : introduction,0.06896551724137931,0.3793103448275862,0.3793103448275862,man is waiting inline Contradiction for the bus a P Premise b H Hypothesis ,15,man is looking to order Neutral a grilled cheese sandwich .,Various deep learning models have been proposed that achieve successful results for this task .,introduction
sentiment_analysis,16,We first expand the sentiment score calculation from Eq. 2 to its individual terms :,system description,Problem of the above Model for Target-Sensitive Sentiment,0,83,31,5,0,system description : Problem of the above Model for Target-Sensitive Sentiment,0.2610062893081761,0.4428571428571429,0.2631578947368421,We first expand the sentiment score calculation from Eq 2 to its individual terms ,15,The analysis can be generalized to many existing MNs as long as their improvements are on attention ? only .,"where "" + "" denotes element - wise summation .",method
question_generation,1,"where N is length of the sequence , and qt is the t th word of the question .",method,Method,0,90,10,10,0,method : Method,0.2295918367346939,0.10989010989010987,0.5263157894736842,where N is length of the sequence and qt is the t th word of the question ,18,"For a particular question , the above term is obtained as :",We have removed ? for simplicity .,method
natural_language_inference,44,"Our over all system achieves significant reductions in training ( up to 15 times ) and inference times ( up to 13 times ) , with accuracy comparable to or better than the state - of - the - art on SQuAD , News QA , Trivia QA and SQuAD - Open .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.03146853146853147,0.875,0.875,Our over all system achieves significant reductions in training up to 15 times and inference times up to 13 times with accuracy comparable to or better than the state of the art on SQuAD News QA Trivia QA and SQuAD Open ,42,"Inspired by this observation , we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model .","Furthermore , our experimental results and analyses show that our approach is more robust to adversarial inputs .",abstract
text_generation,0,"Then we use it to generate 10,000 sequences of length 20 as the training set S for the generative models .",training,Training Setting,0,186,3,3,0,training : Training Setting,0.5740740740740741,0.1875,0.1875,Then we use it to generate 10 000 sequences of length 20 as the training set S for the generative models ,22,"To setup the synthetic data experiments , we first initialize the parameters of an LSTM network following the normal distribution N ( 0 , 1 ) as the oracle describing the real data distribution G oracle ( x t |x 1 , . . . , x t?1 ) .","In SeqGAN algorithm , the training set for the discriminator is comprised by the generated examples with the label 0 and the instances from S with the label",experiment
sentiment_analysis,48,And the decoder combines the outputs from the classifier and the encoder to reconstruct the input sentence .,method,Method Description,0,74,12,12,0,method : Method Description,0.3135593220338983,0.18461538461538465,0.7058823529411765,And the decoder combines the outputs from the classifier and the encoder to reconstruct the input sentence ,18,The encoder transform the data into a latent space that is independent of the label y .,"For the labeled data , the classifier and the autoencoder are trained with the given label y .",method
natural_language_inference,37,On this dataset it is more important to train the model to produce well calibrated confidence scores .,result,Results,0,191,19,19,0,result : Results,0.7431906614785992,0.35185185185185186,0.35185185185185186,On this dataset it is more important to train the model to produce well calibrated confidence scores ,18,We show the same graph as before for this dataset in .,"Note the base model starts to lose performance as more paragraphs are used , showing that errors are being caused by the model being overly confident in incorrect extractions . :",result
natural_language_inference,58,These Glove vectors are xed during the model training .,dataset,SQuAD Dataset,0,220,11,11,0,dataset : SQuAD Dataset,0.6567164179104478,0.4583333333333333,0.4583333333333333,These Glove vectors are xed during the model training ,10,Embedding Layer : We use the 100 - dimensional pretrained Glove vectors as word embeddings .,"To alleviate the out - of - vocabulary issue , we adopt one layer 100 - dimensional convolutional neural network on character - level with a width size of 5 and each character encoded as an 8 - dimensional vector following the work .",experiment
relation-classification,1,"Thus it leads to the prediction of more single E and less ( E1 , E2 ) pairs .",analysis,Error Analysis,0,211,14,13,0,analysis : Error Analysis,0.8577235772357723,0.3333333333333333,0.9285714285714286,Thus it leads to the prediction of more single E and less E1 E2 pairs ,16,"They only obtain E1 and do not find its corresponding E2 , or obtain E2 and do not find its corresponding E1 .","Therefore , entity pair ( E1 , E2 ) has higher precision and lower recall than single E. Besides , the predicted results of ( E1 , E2 ) in have about 3 % improvement when compared predicted results in Table 1 , which means that 3 % of the test data is pre-dicted to be wrong because the relation type is predicted to be wrong .",result
natural_language_inference,50,"Next , we draw the readers attention to the time cost of AI - CNN .",analysis,Results and Analysis,0,233,10,10,0,analysis : Results and Analysis,0.7350157728706624,0.12658227848101267,0.38461538461538464,Next we draw the readers attention to the time cost of AI CNN ,14,"As such , comparing AI - CNN ( w/ o features ) with Hyper QA shows that our proposed model is a superior neural ranking model .",The training time per epoch is ? 3250s per epoch which is about 300 times longer than our model .,result
relation-classification,3,"The baseline model , described in detail in , is illustrated in .",model,Joint learning as head selection,1,36,3,2,0,model : Joint learning as head selection,0.26277372262773724,0.06521739130434782,0.0625,The baseline model described in detail in is illustrated in ,11, ,It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .,method
text-to-speech_synthesis,2,"The scores for the VCTK model tend to be higher than those for LibriSpeech , reflecting the cleaner nature of the dataset .",experiment,experiment,1,135,44,44,0,experiment : experiment,0.5510204081632653,0.4230769230769231,0.4230769230769231,The scores for the VCTK model tend to be higher than those for LibriSpeech reflecting the cleaner nature of the dataset ,22,Results are shown in .,This is also evident in the higher ground truth baselines on VCTK .,experiment
natural_language_inference,87,"RACE contains 27,933 passages and 97,687 questions in total , which is recognized as one of the largest and most difficult datasets in multi-choice MRC .",dataset,Dataset and Setup,0,123,10,10,0,dataset : Dataset and Setup,0.6988636363636364,0.9090909090909092,0.9090909090909092,RACE contains 27 933 passages and 97 687 questions in total which is recognized as one of the largest and most difficult datasets in multi choice MRC ,28,"Our multi - choice MRC is evaluated on Large - scale ReAding Comprehension Dataset From Examinations ( RACE ) dataset , which consists of two subsets : RACE - M and RACE - H corresponding to middle school and high school difficulty levels .",The official evaluation metric is accuracy .,experiment
natural_language_inference,15,We used the samples of the SNLI dev set in .,Visualization on the comparable models,Visualization on the comparable models,0,210,3,3,0,Visualization on the comparable models : Visualization on the comparable models,0.9292035398230089,0.15789473684210525,0.15789473684210525,We used the samples of the SNLI dev set in ,11,We study how the attentive weights flow as layers get deeper in each model using the dense or residual connection .,"and 5 show the attention map on each layer of the models of DRCN , Table 6 ( 8 ) , and Table 6 ( 9 ) .",others
sentence_classification,2,convolution operation involves applying a filter matrix W ? R hd to a window of h words and producing a new feature vector c i using the equation,system description,Problem: Translated Sentences as Context,0,63,11,10,0,system description : Problem: Translated Sentences as Context,0.25,0.3333333333333333,0.3125,convolution operation involves applying a filter matrix W R hd to a window of h words and producing a new feature vector c i using the equation,27,Let xi ? Rd be the d-dimensional word vector of the i - th word in a sentence of length n .,convolution operation involves applying a filter matrix W ? R hd to a window of h words and producing a new feature vector c i using the equation,method
natural_language_inference,81,"Located in the French Riviera , on the southeast coast of France on the Mediterranean Sea , at the foot of the Alps , Nice is the second - largest French city on the Mediterranean coast and the second - largest city in the Provence - Alpes - Cte dAzur region after Marseille .",APPENDIX,ATTENTION MAPS,0,254,46,31,0,APPENDIX : ATTENTION MAPS,0.6033254156769596,0.215962441314554,0.5166666666666667,Located in the French Riviera on the southeast coast of France on the Mediterranean Sea at the foot of the Alps Nice is the second largest French city on the Mediterranean coast and the second largest city in the Provence Alpes Cte dAzur region after Marseille ,47,"The urban are a of Nice extends beyond the administrative city limits , with a population of about 1 million on an are a of .","Nice is about 13 kilometres ( 8 miles ) from the principality of Monaco , and its airport is a gateway to the principality as well .",others
text_summarization,4,Selective dis ambiguation of entities,result,Results,0,214,20,20,0,result : Results,0.8294573643410853,0.3448275862068966,0.3448275862068966,Selective dis ambiguation of entities,5,We posit that this is due to the fact that the article title does not correspond to the summary of the first sentence .,We show the effectiveness of the selective dis ambiguation gate din selecting which entities to dis ambiguate or not .,result
sentiment_analysis,4,"Finally , the representation of the test utterance is updated by consolidating itself with the weighted memory m as :",methodology,Multi-hop Memory,0,193,88,20,0,methodology : Multi-hop Memory,0.5920245398773006,0.8979591836734694,0.6666666666666666,Finally the representation of the test utterance is updated by consolidating itself with the weighted memory m as ,19,This vector denotes the summary of the context that is person - specific and based on the test utterance .,"After the read operation at each hop , memories are updated for the next hop .",method
natural_language_inference,64,"In this paper , we study the use of Transformer - based models for AS2 and provide effective solutions to tackle the data scarceness problem for AS2 and the instability of the finetuning step .",system description,Related Work,1,31,12,12,0,system description : Related Work,0.12350597609561753,0.3636363636363637,0.3636363636363637,In this paper we study the use of Transformer based models for AS2 and provide effective solutions to tackle the data scarceness problem for AS2 and the instability of the finetuning step ,33,"Such problem also affects QA , and , in particular , AS2 since no large and and accurate dataset has been developed for it .","In detail , the contributions of our papers are :",method
sentence_classification,2,"During training , we use mini-batch size of 50 .",experiment,Experimental Setting,1,158,20,19,0,experiment : Experimental Setting,0.6269841269841271,0.7142857142857143,0.7037037037037037,During training we use mini batch size of 50 ,10,We use FastText pre-trained vectors 8 for all our data sets and their corresponding additional context .,Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule .,experiment
text_summarization,2,"Because summary words tend to follow the word order of the original , we assume selecting a source word and including it in the summary has an impact on its subsequent source words , but not the reverse .",approach,2-Way Combination (+Relation),0,130,79,5,0,approach : 2-Way Combination (+Relation),0.4676258992805755,0.6030534351145038,0.19230769230769232,Because summary words tend to follow the word order of the original we assume selecting a source word and including it in the summary has an impact on its subsequent source words but not the reverse ,37,"For example , if a dependency edge ( "" father "" nsubj ??? "" had "" ) is salient and "" father "" is selected to be included in the summary , it is likely that "" had "" will be selected next such that a salient source relation ( "" nsubj "" ) is preserved in the summary .","In this formulation we use ? t , i to capture the saliency of the dependency edge pointing to the i - th source word .",method
machine-translation,0,"Since the proposed RNN Encoder - Decoder also projects to and maps back from a sequence of words into a continuous space vector , we expect to see a similar property with the proposed model as well .",analysis,Word and Phrase Representations,0,196,46,4,0,analysis : Word and Phrase Representations,0.8949771689497716,0.8214285714285714,0.2857142857142857,Since the proposed RNN Encoder Decoder also projects to and maps back from a sequence of words into a continuous space vector we expect to see a similar property with the proposed model as well ,36,"It has been known for sometime that continuous space language models using neural networks are able to learn semantically meaningful embeddings ( See , e.g. , ) .",The left plot in shows the 2 - D embedding of the words using the word embedding matrix learned by the RNN Encoder - Decoder .,result
natural_language_inference,7,We use fixed pretrained embeddings to represent question and passage words .,model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,85,34,6,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.4775280898876405,0.6415094339622641,0.2727272727272727,We use fixed pretrained embeddings to represent question and passage words ,12,"To incorporate these question representations , we simply concatenate them with the passage word embeddings ( Question - focused passage word embedding in ) .","Therefore , in the following discussion , notation for the words are interchangeable with their embedding representations .",method
sentiment_analysis,12,All hyper - parameters were tuned on 20 % randomly held - out training data .,experiment,Experiments,1,156,19,19,0,experiment : Experiments,0.6964285714285714,0.59375,0.59375,All hyper parameters were tuned on 20 randomly held out training data ,13,"When implementing our approach , we empirically set the maximum iteration number K as 5 , ? in Equation 3 as 0.1 on LAPTOP data set , 0.5 on REST data set and 0.1 on TWITTER data set , respectively .","Finally , we used F1 - Macro and accuracy as our evaluation Model LAPTOP REST TWITTER Macro - F1 Accuracy Macro - F1 Accuracy Macro - F1 Accuracy MN 62 . * * and * means significant at p < 0.01 and p < 0.05 over the baselines ( MN , TNet ) on each test set , respectively .",experiment
relation_extraction,12,We can observe that AGGCN outperforms GCN by 1.1 F1 points .,model,Model F1,0,223,36,4,0,model : Model F1,0.6778115501519757,0.7826086956521741,0.2857142857142857,We can observe that AGGCN outperforms GCN by 1 1 F1 points ,13,Full tree ) 68.2 C - AGGCN ( K=2 ) 67.5 C - AGGCN ( K=1 ) 67.9 C - AGGCN ( K=0 ) 67.0 model with them .,We speculate that the limited improvement is due to the lack of contextual information about word order or dis ambiguation .,method
named-entity-recognition,8,Dev accuracy after finetuning from a checkpoint that has been pre-trained fork steps .,ablation,ablation,0,364,3,3,0,ablation : ablation,0.9405684754521964,0.1153846153846154,0.1153846153846154,Dev accuracy after finetuning from a checkpoint that has been pre trained fork steps ,15,Effect of Number of Training Steps presents MNLI,This allows us to answer the following questions :,result
relation_extraction,12,Relation extraction aims to detect relations among entities in the text .,introduction,introduction,0,17,2,2,0,introduction : introduction,0.05167173252279635,0.05,0.05,Relation extraction aims to detect relations among entities in the text ,12, ,"It plays a significant role in a variety of natural language processing applications including biomedical knowledge discovery , knowledge base population and question answering .",introduction
natural_language_inference,95,"At last , comparing our method with the baseline , we achieve an improvement of nearly 3 points without the yes / no classification .",ablation,Ablation Study,1,189,12,12,0,ablation : Ablation Study,0.8076923076923077,0.375,0.9230769230769232,At last comparing our method with the baseline we achieve an improvement of nearly 3 points without the yes no classification ,22,"Another discovery is that jointly training the three models can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .",This significant improvement proves the effectiveness of our approach .,result
text_summarization,1,The generation stage uses a standard encoder - decoder model to generate a target sequence given each selected content from the source ( one - to - one mapping ) .,introduction,introduction,1,36,26,26,0,introduction : introduction,0.15,0.7428571428571429,0.7428571428571429,The generation stage uses a standard encoder decoder model to generate a target sequence given each selected content from the source one to one mapping ,26,"The diversification stage leverages content selection to map the source to multiple sequences , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .",We present a generic module called SELECTOR that is specialized for diversification .,introduction
natural_language_inference,97,This computation uses the cosine similarity as before :,model,Sentential,0,127,45,3,0,model : Sentential,0.4349315068493151,0.4017857142857143,0.2727272727272727,This computation uses the cosine similarity as before ,9,"Inspired by the work of in paraphrase detection , we compute matches between hypotheses and text sentences at the word level .","ca kn = cos (t k , n ) .",method
part-of-speech_tagging,4,"The conditional probabilities of label distribution sequences y = {y 1 , , y n } is",baseline,CRF.,0,80,19,3,0,baseline : CRF.,0.3433476394849785,0.7037037037037037,0.2727272727272727,The conditional probabilities of label distribution sequences y y 1 y n is,13,CRF layer is used on top of the hidden vectors H w .,"Here y represents an arbitrary label distribution sequence , W l i CRF is a model parameter specific to l i , and b",result
natural_language_inference,12,"Tuning decisions for word embedding training strategy , the hyperparameters of dimension and number of layers for biLSTM , and the activation type and number of layers for MLP , are all explained in Section 4 .",model,Parameter Settings,0,57,35,7,0,model : Parameter Settings,0.6477272727272727,1.0,1.0,Tuning decisions for word embedding training strategy the hyperparameters of dimension and number of layers for biLSTM and the activation type and number of layers for MLP are all explained in Section 4 ,34,We used pre-trained 300D Glove 840B vectors to initialize the word embeddings ., ,method
natural_language_inference,97,"If so , the Parallel - Hierarchical model is a good start on the former .",analysis,analysis,0,283,27,27,0,analysis : analysis,0.9691780821917808,0.9642857142857144,0.9642857142857144,If so the Parallel Hierarchical model is a good start on the former ,14,"However , it maybe that human language processing can be factored into separate processes of comprehension and reasoning .","Indeed , if we train the method exclusively on single questions then its results become even more impressive : we can achieve a test accuracy of 79.1 % on MCTest - 500 .",result
natural_language_inference,52,"We then use these rankings to make a set of four reciprocal rank features , IR ++ 0 , ... , IR ++",model,Semi-Lexicalized Discourse features (lexDisc):,0,126,44,19,0,model : Semi-Lexicalized Discourse features (lexDisc):,0.4827586206896552,0.7857142857142857,0.9047619047619048,We then use these rankings to make a set of four reciprocal rank features IR 0 IR ,18,"We repeat this process using an unboosted query as well , for a total of four rankings of the answer candidates .",", for each answer candidate ( i.e. , IR ++ 0 = 1.0 for the top - ranked candidate in the first ranking , IR ++",method
natural_language_inference,30,"The prefix L : , resp.",evaluation,Full Ranking,0,195,13,4,0,evaluation : Full Ranking,0.7558139534883721,0.9285714285714286,0.8,The prefix L resp ,5,Examples of nearest neighboring entities and relationships from REVERB for some words from our vocabulary .,": , indicates the embedding of an entity when appearing in left - hand side , resp. right - hand side , of triples .",result
sentiment_analysis,14,Best parameter settings are tuned on the validation set .,system description,Pre-training Emo2Vec,0,90,4,4,0,system description : Pre-training Emo2Vec,0.5921052631578947,0.3333333333333333,0.5714285714285714,Best parameter settings are tuned on the validation set ,10,Parameters of T and CNN are randomly initialized and Adam is used for optimization .,"For the best model , we use the batch size of 16 , embedding size of 100 , 1024 filters and filter sizes are 1 , 3 ,5 and 7 respectively .",method
natural_language_inference,86,"To initialize the word embeddings in the word representation layer , we use the 300 - dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus .",experiment,Experiment Settings,1,104,7,6,0,experiment : Experiment Settings,0.6081871345029239,0.3181818181818182,0.5,To initialize the word embeddings in the word representation layer we use the 300 dimensional GloVe word vectors pre trained from the 840B Common Crawl corpus ,27,"To evaluate the experimental results , we employ two metrics : Exact Match ( EM ) and F1 score .","For the out - of - vocabulary ( OOV ) words , we initialize the word embeddings randomly .",experiment
machine-translation,6,We outperform the baselines for 1.06/0.71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English - German : BLEU scores on test set on WMT2014 English - German and IWSLT German - English tasks .,model,Machine Translation,1,223,10,3,0,model : Machine Translation,0.7663230240549829,0.5555555555555556,0.5,We outperform the baselines for 1 06 0 71 in the term of BLEU in transformer_base and transformer_big settings in WMT14 English German BLEU scores on test set on WMT2014 English German and IWSLT German English tasks ,38,The results of neural machine translation on WMT14 English - German and IWSLT14 German - English tasks are shown in .,The model learned from adversarial training also outperforms original one in IWSLT14 German - English task by 0.85 .,method
temporal_information_extraction,1,"Since system 2 is simply a transitive closure of system 1 , they would have the same evaluation scores .",system description,Quality of A Temporal Graph,0,74,20,10,0,system description : Quality of A Temporal Graph,0.28793774319066145,0.9523809523809524,0.9090909090909092,Since system 2 is simply a transitive closure of system 1 they would have the same evaluation scores ,19,"For example , if system 1 produces ripping is before hurt and hurt is before monitor , and system 2 adds ripping is before monitor on top of system",Note that vague relations are usually considered as non-existing TLINKs and are not counted during evaluation .,method
text-classification,3,"For the topic categorization task we used the BBC news dataset 5 , 20 News , Reuters 6 and The code for this CNN implementation is the same as in , which is available at https://github.com/pilehvar/sensecnn",experiment,Experimental setup,0,71,10,10,0,experiment : Experimental setup,0.5772357723577236,0.19607843137254904,0.7692307692307693,For the topic categorization task we used the BBC news dataset 5 20 News Reuters 6 and The code for this CNN implementation is the same as in which is available at https github com pilehvar sensecnn,37,The embedding layer was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the 3B - word UMBC WebBase corpus with standard hyperparameters,Context window of 5 words and hierarchical softmax .,experiment
sentiment_analysis,39,"Model should predict "" Positive "" for location1 and "" Negative "" for location 2",baseline,Long Short-Term Memory (LSTM),0,186,34,10,0,baseline : Long Short-Term Memory (LSTM),0.7591836734693878,1.0,1.0,Model should predict Positive for location1 and Negative for location 2,11,"In this figure , LSTM is trained to identify the sentiment of aspect "" price "" .", ,result
named-entity-recognition,3,Pre-trained language models .,experiment,experiment,0,90,21,21,0,experiment : experiment,0.4864864864864865,0.4883720930232558,0.4883720930232558,Pre trained language models ,5,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .","The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark , a publicly available benchmark for largescale language modeling .",experiment
semantic_role_labeling,1,Additional details on optimization and hyperparameters are included in Appendix A.,training,Training,0,131,10,10,0,training : Training,0.6009174311926605,1.0,1.0,Additional details on optimization and hyperparameters are included in Appendix A ,12,We use gradient clipping to avoid exploding gradients ., ,experiment
part-of-speech_tagging,3,This is consistent with results reported by previous work .,system description,Word Embeddings,0,155,20,10,0,system description : Word Embeddings,0.7635467980295566,0.625,0.5263157894736842,This is consistent with results reported by previous work ,10,more heavily on pretrained embeddings than POS tagging .,"For different pretrained embeddings , Stanford 's Glo Ve 100 dimensional embeddings achieve best results on both tasks , about 0.1 % better on POS accuracy and 0.9 % better on NER F1 score than the Senna 50 dimensional one .",method
text_summarization,4,"# -# # # ## # , in @guadalajara @country club , the @lorena ochoa foundation said in a statement on wednesday .",result,Results,0,236,42,42,0,result : Results,0.9147286821705426,0.7241379310344828,0.7241379310344828, in guadalajara country club the lorena ochoa foundation said in a statement on wednesday ,16,"The firm model produced the more relevant summary , focusing on the po-Gigaword Dataset Example Original western mexico @state @jalisco will host the first edition of the @ UNK dollar @lorena ochoa invitation @golf tournament on nov.","Baseline netanyahu says he is a country of "" UNK cheating "" and that it is a country of "" UNK cheating "" netanyahu says he is a country of "" UNK cheating "" and that "" is a very bad deal "" he says he says he says the plan is a country of "" UNK cheating "" and that it is a country of "" UNK cheating "" he says the u.s. is a country of "" UNK cheating "" and that is a country of "" UNK cheating "" Soft benjamin netanyahu : "" i think there 's a third alternative , and that is standing firm , "" netanyahu tells cnn .",result
question-answering,0,"As expected , string matching greatly improves results , both in precision and recall , and also significantly reduces evaluation time .",result,Word,0,231,35,14,0,result : Word,0.8953488372093024,0.8536585365853658,0.7,As expected string matching greatly improves results both in precision and recall and also significantly reduces evaluation time ,19,L:x - ray.e L:gamma - ray .,"The final F1 obtained by our fine - tuned model is even better then the result of paralex in reranking , which is pretty remarkable , because this time , this setting advantages it quite a lot .",result
natural_language_inference,58,"When ReasoNet is set with T max = 1 in CNN and Daily Mail , it directly applies s 0 to make predictions on the entity candidates , without performing attention on the memory module .",training,CNN and Daily Mail Datasets,0,182,51,47,0,training : CNN and Daily Mail Datasets,0.5432835820895522,0.6538461538461539,0.8392857142857143,When ReasoNet is set with T max 1 in CNN and Daily Mail it directly applies s 0 to make predictions on the entity candidates without performing attention on the memory module ,33,The models are usually converged within 6 epochs on both CNN and Daily Mail datasets .,The prediction module in ReasoNets is the same as in AS Reader .,experiment
natural_language_inference,53,"Our BiLSTM - max trained on SNLI performs much better than released SkipThought vectors on MR , CR , MPQA , SST , MRPC - accuracy , SICK - R , SICK - E and STS14 ( see ) .",model,Task transfer,1,174,33,13,0,model : Task transfer,0.8365384615384616,0.5409836065573771,0.3170731707317073,Our BiLSTM max trained on SNLI performs much better than released SkipThought vectors on MR CR MPQA SST MRPC accuracy SICK R SICK E and STS14 see ,28,We train our model in less than a day on a single GPU compared to the best SkipThought - LN network trained for a month .,"Except for the SUBJ dataset , it also performs better than SkipThought - LN on MR , CR and MPQA .",method
natural_language_inference,96,"When comparing machine results to human results , we see there exists a lot of headroom .",result,Results,0,241,8,8,0,result : Results,0.617948717948718,0.8,0.8,When comparing machine results to human results we see there exists a lot of headroom ,16,"The best results come from pairwise NLI models : when fully trained on Swag , ESIM + ELMo obtains 59.2 % accuracy .","Though there likely is some noise in the task , our results suggest that humans ( even untrained ) converge to a consensus .",result
sentiment_analysis,3,"In terms of the evaluation metric , for EC and DailyDialog , we follow to use the micro-averaged F1 excluding the majority class ( neutral ) , due to their extremely unbalanced labels ( the percentage of the majority class in the test set is over 80 % ) .",dataset,Datasets and Evaluations,0,195,6,6,0,dataset : Datasets and Evaluations,0.6678082191780822,0.8571428571428571,0.8571428571428571,In terms of the evaluation metric for EC and DailyDialog we follow to use the micro averaged F1 excluding the majority class neutral due to their extremely unbalanced labels the percentage of the majority class in the test set is over 80 ,43,"The emotion labels include neutral , happiness , sadness , anger , frustrated , and excited .","For the rest relatively balanced datasets , we follow to use the weighted macro -F1 .",experiment
named-entity-recognition,8,"Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent .",SQuAD v1.1,SQuAD v1.1,0,355,11,11,0,SQuAD v1.1 : SQuAD v1.1,0.9173126614987079,0.6470588235294118,0.6470588235294118,Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources with human annotations for whether the sentences in the pair are semantically equivalent ,28,They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning .,"Recognizing Textual Entailment is a binary entailment task similar to MNLI , but with much less training data ) .",others
text-classification,0,"The fields we used include question title , question content and best answer .",method,Large-scale Datasets and Results,0,179,77,37,0,method : Large-scale Datasets and Results,0.7885462555066078,0.6470588235294118,0.8222222222222222,The fields we used include question title question content and best answer ,13,"Each class contains 140,000 training samples and 5,000 testing samples .","We obtained an Amazon review dataset from the Stanford Network Analysis Project ( SNAP ) , which spans 18 years with 34,686,770 reviews from 6,643,669 users on 2,441,053 products .",method
natural_language_inference,17,End - to - end Architecture,architecture,Alignment Architecture for MRC,0,126,83,83,0,architecture : Alignment Architecture for MRC,0.4846153846153846,0.6058394160583942,0.6384615384615384,End to end Architecture,4,where ? a and ? bare trainable parameters .,"Based on previous innovations , we introduce an end - to - end architecture called Reinforced Mnemonic Reader , which is shown in .",method
natural_language_inference,95,"It should be noted that the pointer network is applied to the concatenation of all passages , which is denoted as P so that the probabilities are comparable across passages .",model,Answer Boundary Prediction,0,90,23,7,0,model : Answer Boundary Prediction,0.38461538461538464,0.46,0.7777777777777778,It should be noted that the pointer network is applied to the concatenation of all passages which is denoted as P so that the probabilities are comparable across passages ,30,"By utilizing the attention weights , the probability of the k th word in the passage to be the start and end position of the answer is obtained as ? 1 k and ? 2 k .",This boundary model can be trained by minimizing the negative log probabilities of the true start and end indices :,method
natural_language_inference,27,"All models share the same hyperparameters 6 and are trained with different numbers of steps , 2 M for English - French and 340 K for English - German .",model,DATA AUGMENTATION BY BACKTRANSLATION,0,139,95,12,0,model : DATA AUGMENTATION BY BACKTRANSLATION,0.4112426035502959,0.7142857142857143,0.8571428571428571,All models share the same hyperparameters 6 and are trained with different numbers of steps 2 M for English French and 340 K for English German ,27,All data have been tokenized and split into subword units as described in .,Our English - French systems achieve 36.7 BLEU on newstest2014 for translating into French and 35.9 BLEU for the reverse direction .,method
text-classification,5,"where T is the number of training iterations 4 , cut f rac is the fraction of iterations we increase 3 An unrelated method of the same name exists for deep Boltzmann machines",system description,Target task LM fine-tuning,0,111,60,30,0,system description : Target task LM fine-tuning,0.4404761904761905,0.631578947368421,0.8108108108108109,where T is the number of training iterations 4 cut f rac is the fraction of iterations we increase 3 An unrelated method of the same name exists for deep Boltzmann machines,32,"Instead , we propose slanted triangular learning rates ( STLR ) , which first linearly increases the learning rate and then linearly decays it according to the following update schedule , which can be seen in :","In other words , the number of epochs times the number of updates per epoch .",method
natural_language_inference,37,We use the weight averages at test time .,implementation,Implementation,0,172,10,10,0,implementation : Implementation,0.669260700389105,1.0,1.0,We use the weight averages at test time ,9,"During training , we maintain an exponential moving average of the weights with a decay rate of 0.999 .","First , we do an ablation study on Trivia QA web to show the effects of our proposed methods for our pipeline model .",experiment
relation_extraction,5,Note that the GCN model presented above uses the same parameters for all edges in the dependency graph .,model,Graph Convolutional Networks over Dependency Trees,0,56,21,19,0,model : Graph Convolutional Networks over Dependency Trees,0.2129277566539924,0.31343283582089554,0.8260869565217391,Note that the GCN model presented above uses the same parameters for all edges in the dependency graph ,19,"layers gives us a deep GCN network , where we set h Moreover , the propagation of information between tokens occurs in parallel , and the runtime does not depend on the depth of the dependency tree .","We also experimented with : ( 1 ) using different transformation matrices W for topdown , bottom - up , and self - loop edges ; and ( 2 ) adding dependency relation - specific parameters for edge - wise gating , similar to .",method
text_generation,2,"Generative Adversarial Nets ( GAN ) , which is firstly proposed for continous data ( image generation etc. ) , is then extended to discrete , sequential data to alleviate the above problem and has shown promising results ) .",introduction,introduction,0,21,9,9,0,introduction : introduction,0.06,0.2368421052631579,0.2368421052631579,Generative Adversarial Nets GAN which is firstly proposed for continous data image generation etc is then extended to discrete sequential data to alleviate the above problem and has shown promising results ,32,"scheduled sampling approach is proposed to addressed this problem , but is proved to be fundamentally inconsistent .","Due to the discrete nature of text samples , text generation is modeled as a sequential decision making process , where the state is previously generated words , the action is the next word to be generated , and the generative net G is a stochastic policy that maps current state to a distribution over the action space .",introduction
sentiment_analysis,47,The results verify the usage of target attention is rewarding in our model .,performance,The Effect of Two-side Target Representation,0,178,26,9,0,performance : The Effect of Two-side Target Representation,0.827906976744186,0.4482758620689655,0.5294117647058824,The results verify the usage of target attention is rewarding in our model ,14,It can be seen that No - Target - Attention model performs a little worse than LCR - Rot .,"By comparing LCR - Rot and No - Target - Learned , we find that removing the support of multi-word target phrase will cause a more rapid decline .",result
sentiment_analysis,50,We derived two document - level datasets from Yelp2014 and the Amazon Electronics dataset respectively .,dataset,Datasets and Experimental Settings,0,82,6,6,0,dataset : Datasets and Experimental Settings,0.5061728395061729,0.2727272727272727,0.2727272727272727,We derived two document level datasets from Yelp2014 and the Amazon Electronics dataset respectively ,15,Statistics of the resulting datasets are presented in .,The original reviews were rated on a 5 point scale .,experiment
machine-translation,5,"As a result , the Estonian data consist of the the following factors : word part , position , lemma , and morpho-syntactic tag .",system,Data Pre-processing,0,76,53,13,0,system : Data Pre-processing,0.5314685314685315,0.7162162162162162,0.9285714285714286,As a result the Estonian data consist of the the following factors word part position lemma and morpho syntactic tag ,21,"Finally , data for the constrained systems are factored using an averaged perceptron - based morpho-syntactic tagger for Estonian and the lexicalized probabilistic parser , we introduce also a factor indicating a word part 's position in a word ( beginning , middle , end , or the word part represents the whole word - B , I , E , or O ) .","The English data consist of the following factors : word part , position , lemma , part - of - speech tag , and syntactic function .",method
question_answering,1,"Prediction : "" A piece of paper "" , Answer : "" his last statement """,analysis,Paraphrase problems 14,0,267,5,3,0,analysis : Paraphrase problems 14,0.8422712933753943,0.09090909090909093,0.05660377358490566,Prediction A piece of paper Answer his last statement ,10,"Question : "" What was later discovered written by Luther ? ""","Context : "" Generally , education in Australia follows the threetier model which includes primary education ( primary schools ) , followed by secondary education ( secondary schools / high schools ) and tertiary education ( universities and / or TAFE colleges ) . """,result
named-entity-recognition,4,Other recent work has also focused on learning context - dependent representations .,introduction,introduction,0,32,25,25,0,introduction : introduction,0.1176470588235294,0.6410256410256411,0.6410256410256411,Other recent work has also focused on learning context dependent representations ,12,"Our approach also benefits from subword units through the use of character convolutions , and we seamlessly incorporate multi-sense information into downstream tasks without explicitly training to predict predefined sense classes .","Melamud et al. , 2016 ) uses a bidirectional Long Short Term Memory ( LSTM ; Hochreiter and Schmidhuber , 1997 ) to encode the context around a pivot word .",introduction
natural_language_inference,45,The word embeddings are updated during training .,performance,CLOZE-STYLE QUESTIONS,0,162,10,7,0,performance : CLOZE-STYLE QUESTIONS,0.8140703517587939,0.3225806451612903,0.4117647058823529,The word embeddings are updated during training ,8,"For the fine - grained gating approach , we use the same hyper - parameters as in except that we use a character - level GRU with 100 units to be of the same size as the word lookup table .","In addition to different ways of combining word - level and character - level representations , we also compare two different ways of integrating documents and queries : GA refers to the gated attention reader and FG refers to our fine - grained gating described in Section 3.3 .",result
relation_extraction,13,"To do this , we define the following binary classifier",system description,Learning Setup,0,138,20,9,0,system description : Learning Setup,0.647887323943662,0.3333333333333333,0.8181818181818182,To do this we define the following binary classifier,9,We aim to learn a relation statement encoder f ? that we can use to determine whether or not two relation statements encode the same relation .,"to assign a probability to the case that rand r encode the same relation ( l = 1 ) , or not ( l = 0 ) .",method
natural_language_inference,97,These distinct perspectives naturally form a hierarchy that we depict in .,introduction,introduction,0,35,24,24,0,introduction : introduction,0.11986301369863013,0.6486486486486487,0.6486486486486487,These distinct perspectives naturally form a hierarchy that we depict in ,12,Words are represented throughout by embedding vectors .,"Language is hierarchical , so it makes sense that comprehension relies on hierarchical levels of understanding .",introduction
sentiment_analysis,27,"Finally , the output of the last attention layer is used to infer the polarity of the aspect .",method,Comparative methods,0,200,7,7,0,method : Comparative methods,0.7246376811594203,0.3043478260869565,0.3043478260869565,Finally the output of the last attention layer is used to infer the polarity of the aspect ,18,Mem Net uses a deep memory network on the context word embeddings for sentence representation to capture the relevance between each context word and the aspect .,IAN generates the representations for aspect terms and contexts with two attention - based LSTM network separately .,method
text_summarization,5,"However , according to Max and Rerank , we find the Rerank performance of Re 3 Sum is far from perfect .",evaluation,Informativeness Evaluation,0,178,17,17,0,evaluation : Informativeness Evaluation,0.7063492063492064,0.2698412698412698,0.85,However according to Max and Rerank we find the Rerank performance of Re 3 Sum is far from perfect ,20,"Rerank largely outperforms First , which verifies the effectiveness of the Rerank module .","Likewise , comparing Max and First , we observe that the improving capacity of the Retrieve module is high .",result
text_summarization,14,"Finally , we apply importance sampling by the weight exp{ ( s ( x , y , y * ) + d ) /? } and perform normalization , where the proposal distribution is Hamming distance sampling 2 .",model,Optimizing by Entailment-based Sampling,0,111,65,11,0,model : Optimizing by Entailment-based Sampling,0.4868421052631579,0.9420289855072465,0.7333333333333333,Finally we apply importance sampling by the weight exp s x y y d and perform normalization where the proposal distribution is Hamming distance sampling 2 ,27,"Then , we weight the counts by exp{?d/? } and perform normalization .","We define entailment reward s ( x , y , y * ) as follows :",method
sentiment_analysis,49,"We observe similar performance in the MOSEI dataset , where we obtain 79.02 % ( a ) Softmax attention weights N1 & N2 for MMMU - ( c ) Softmax attention weights N1 & N2 for MMMU - BATA .",analysis,Analysis of Attention Mechanism,0,181,16,16,0,analysis : Analysis of Attention Mechanism,0.7154150197628458,0.2285714285714285,0.4848484848484849,We observe similar performance in the MOSEI dataset where we obtain 79 02 a Softmax attention weights N1 N2 for MMMU c Softmax attention weights N1 N2 for MMMU BATA ,31,"In MOSI for tri-modal inputs , the MMMU - BA architecture reports a reduced accuracy of 80 . 89 % without attention framework as compared to 82.31 % with attention .",d ) MU - SAtext matrix .,result
semantic_parsing,1,"Generalization ability TRANX employs ASTs as a general - purpose intermediate meaning representation , and the task - dependent grammar is provided to the system as external knowledge to guide the parsing process , therefore decoupling the semantic parsing procedure with specificities of grammars .",introduction,introduction,1,20,13,13,0,introduction : introduction,0.13986013986013987,0.7222222222222222,0.7222222222222222,Generalization ability TRANX employs ASTs as a general purpose intermediate meaning representation and the task dependent grammar is provided to the system as external knowledge to guide the parsing process therefore decoupling the semantic parsing procedure with specificities of grammars ,41,TRANX is designed with the following principles in mind :,Extensibility TRANX uses a simple transition system to parse NL utterances into tree -,introduction
text_generation,0,The fourth one is the Policy Gradient with BLEU ( PG - BLEU ) .,training,Training Setting,1,195,12,12,0,training : Training Setting,0.6018518518518519,0.75,0.75,The fourth one is the Policy Gradient with BLEU PG BLEU ,12,The third one is scheduled sampling .,"In the scheduled sampling , the training process gradually changes from a fully guided scheme feeding the true previous tokens into LSTM , towards a less guided scheme which mostly feeds the LSTM with its generated tokens .",experiment
text_summarization,8,The LEAD - 3 baseline is a commonly used baseline in news summarization that extracts the first three sentences from an article .,analysis,Analysis and Discussion,0,232,5,5,0,analysis : Analysis and Discussion,0.8111888111888111,0.14705882352941174,0.14705882352941174,The LEAD 3 baseline is a commonly used baseline in news summarization that extracts the first three sentences from an article ,22,shows experiments comparing content selection to extractive baselines .,Top - 3 shows the performance when we extract the top three sentences by average copy probability from the selector .,result
question_answering,5,Remark : Note that neither of the above methods require any training .,method,Measuring Strength by Count,0,78,27,8,0,method : Measuring Strength by Count,0.2826086956521739,0.29347826086956524,0.8,Remark Note that neither of the above methods require any training ,12,"In the re-ranking scenario , it is not necessary to exhaustively consider all the probabilities of all the spans in the passages , as there maybe a large number of different answer spans and most of them are irrelevant to the ground - truth answer .",Both just take the candidate predictions from the baseline QA system and perform counting or probability calculations .,method
phrase_grounding,0,We further study the use of ELMo for text embedding or the commonly used approach of training a Bi - LSTM .,ablation,Ablation Study,0,206,11,11,0,ablation : Ablation Study,0.9155555555555556,0.6470588235294118,0.6470588235294118,We further study the use of ELMo for text embedding or the commonly used approach of training a Bi LSTM ,21,"We can also see that non-linear mapping seems more important on the visual side , but best results are obtained with both text and visual non-linear mappings .","Specifically , we simply replaced the pre-trained BiLSTMs of ELMo model with a trainable BiLSTM ( on top of word embeddings of ELMo ) , and directly feed the BiLSTM outputs to the attention model .",result
sentiment_analysis,51,"While transfer learning ( pretraining and finetuning ) has become the de-facto standard in computer vision , NLP is yet to utilize this concept fully .",introduction,introduction,0,19,10,10,0,introduction : introduction,0.12666666666666668,0.5,0.5,While transfer learning pretraining and finetuning has become the de facto standard in computer vision NLP is yet to utilize this concept fully ,24,"Recently , there has been an explosion of developments in NLP as well as other deep learning architectures .","However , neural language models such as word vectors , paragraph vectors , and Glo Ve have started the transfer learning revolution in NLP .",introduction
natural_language_inference,55,"Our approach is competitive with the current state - of - the - art on the recent benchmark We-bQuestions without using any lexicon , rules or additional system for partof - speech tagging , syntactic or dependency parsing during training as most other systems do .",introduction,introduction,0,25,19,19,0,introduction : introduction,0.17006802721088435,1.0,1.0,Our approach is competitive with the current state of the art on the recent benchmark We bQuestions without using any lexicon rules or additional system for partof speech tagging syntactic or dependency parsing during training as most other systems do ,41,The main contributions of the paper are : ( 1 ) a more sophisticated inference procedure that is both efficient and can consider longer paths ( considered only answers directly connected to the question in the graph ) ; and ( 2 ) a richer representation of the answers which encodes the question - answer path and surrounding subgraph of the KB ., ,introduction
text_summarization,0,"3 ) CGU : propose to use the convolutional gated unit to refine the source representation , which achieves the state - of - the - art performance on social media text summarization dataset .",method,method,1,226,6,6,0,method : method,0.7508305647840532,0.6666666666666666,0.6666666666666666,3 CGU propose to use the convolutional gated unit to refine the source representation which achieves the state of the art performance on social media text summarization dataset ,29,"2 ) S2SR : We simply add the reader attention on attention distribution ? t , in each decoding step .","4 ) LEAD1 : LEAD1 is a commonly used baseline , which selects the first sentence of document as the summary .",method
part-of-speech_tagging,5,"Critically , however , all the information fed to this representation comes from the word itself , and not a wider sentence - level context .",model,Word-based Character Model,0,78,28,5,0,model : Word-based Character Model,0.38613861386138615,0.6666666666666666,1.0,Critically however all the information fed to this representation comes from the word itself and not a wider sentence level context ,22,We refer the reader to those works for more details ., ,method
sentiment_analysis,11,This effect is evidenced in the figure where higher hops lead to a dip in performance .,result,Results,0,303,17,17,0,result : Results,0.8808139534883721,0.3207547169811321,1.0,This effect is evidenced in the figure where higher hops lead to a dip in performance ,17,"However , each added hop contributes a new set of parameters for memory representation , leading to an increase in total parameters of the model and making it susceptible to overfitting .", ,result
natural_language_inference,84,This is inline with our expectations that when definitions are available they should be helpful for handling rare words .,model,LANGUAGE MODELLING,0,210,32,32,0,model : LANGUAGE MODELLING,0.9130434782608696,1.0,1.0,This is inline with our expectations that when definitions are available they should be helpful for handling rare words ,20,"For example , on the 1 % version of the dataset , in the restricted setting , adding definitions to the spelling helps to bridge half of the 6 points PPL after OOV gap between spelling and Glo Ve .", ,method
natural_language_inference,34,"At the second step , mentions of the same entity "" IRA "" are detected by Mask2 , serving as abridge for propagating information across two paragraphs .",result,Case Study,0,288,68,6,0,result : Case Study,0.9762711864406779,0.9066666666666666,0.4615384615384616,At the second step mentions of the same entity IRA are detected by Mask2 serving as abridge for propagating information across two paragraphs ,24,Information of two start entities is then passed to their neighbors on the entity graph .,"Finally , two reasoning chains are linked together by the bridge entity "" IRA "" , which is exactly the answer .",result
natural_language_inference,2,max - attentional question aggregation ( q ma ),architecture,Question-focused Attentional Pointing,0,122,77,5,0,architecture : Question-focused Attentional Pointing,0.4552238805970149,0.7064220183486238,0.16666666666666666,max attentional question aggregation q ma ,7,We formulate a question representation based on two parts :,question type representation ( q f ) q ma is formulated by using the attention matrix A and the sequence - level question encoding Q .,method
natural_language_inference,69,"This not only removes answer frequency cues , The superdocument has a larger number of tokens compared to e.g. SQuAD , thus the additional memory requirements .",model,Lexical Abstraction: Candidate Masking,0,248,37,8,0,model : Lexical Abstraction: Candidate Masking,0.7188405797101449,0.7708333333333334,1.0,This not only removes answer frequency cues The superdocument has a larger number of tokens compared to e g SQuAD thus the additional memory requirements ,26,"Masking is consistent within one sample , but generally different for the same expression across samples .", ,method
semantic_role_labeling,2,"For example , if the model is unsure whether two consecutive words should belong to an ARG0 or ARG1 , it might generate inconsistent BIO sequences such as B ARG0 , I ARG1 when decoding at the token level .",analysis,BIO Violations,0,169,52,14,0,analysis : BIO Violations,0.7544642857142857,0.6046511627906976,0.2916666666666667,For example if the model is unsure whether two consecutive words should belong to an ARG0 or ARG1 it might generate inconsistent BIO sequences such as B ARG0 I ARG1 when decoding at the token level ,37,This suggests that BIO inconsistencies occur when there is some ambiguity .,Using BIO - constrained decoding can resolve this ambiguity and result in a structurally consistent solution .,result
natural_language_inference,74,"For DMP task , we use stochastic gradient descent with initial learning rate as 0.1 , and we anneal by half each time the validation accuracy is lower than the previous epoch .",implementation,Implementation Details,1,162,11,11,0,implementation : Implementation Details,0.7232142857142857,0.8461538461538461,0.8461538461538461,For DMP task we use stochastic gradient descent with initial learning rate as 0 1 and we anneal by half each time the validation accuracy is lower than the previous epoch ,32,The parameter ? in the objective function is set to be 0.2 .,"The number of epochs is set to be 10 , and the feedforward dropout rate is 0.2 .",experiment
sentiment_analysis,26,"This shows that the sentiment embeddings are most useful when they are of high quality and domain - specific training data is scarce , although a modest amount of training data is still needed for the model to be able to adapt to the target domain .",analysis,Results and Analysis,0,211,65,65,0,analysis : Results and Analysis,0.8612244897959184,0.8333333333333334,1.0,This shows that the sentiment embeddings are most useful when they are of high quality and domain specific training data is scarce although a modest amount of training data is still needed for the model to be able to adapt to the target domain ,45,"In , we additionally observe that the gains are over all much more remarkable on smaller training sets .", ,result
sentiment_analysis,18,"The results show that attention - based LSTM can be substantially improved by incorporating our two proposed methods , and that the resulting model outperforms all baseline methods on aspect - level sentiment classification .",introduction,introduction,0,52,38,38,0,introduction : introduction,0.2175732217573222,1.0,1.0,The results show that attention based LSTM can be substantially improved by incorporating our two proposed methods and that the resulting model outperforms all baseline methods on aspect level sentiment classification ,32,"We conducted experiments on attention - based LSTM models using the SemEval 2014 , 2015 , and 2016 datasets .", ,introduction
natural_language_inference,29,"In these experimental results , we see that PAAG achieves a 111 % , 8 % and 62.73 % increment over the stateof - the - art baseline SNet in terms of BLEU , embedding greedy and consistency score , respectively .",implementation,implementation,1,294,12,12,0,implementation : implementation,0.8054794520547945,0.3428571428571429,0.3428571428571429,In these experimental results we see that PAAG achieves a 111 8 and 62 73 increment over the stateof the art baseline SNet in terms of BLEU embedding greedy and consistency score respectively ,34,Significant differences are with respect to SNet ( row with shaded background ) .,"In , we see that our PAAG outperforms all the baseline significantly in semantic distance with respect to the ground truth .",experiment
text_generation,2,We remove the words with frequency lower than 10 as well as the sentence containing them .,experiment,Middle Text Generation: COCO Image Captions,0,204,27,7,0,experiment : Middle Text Generation: COCO Image Captions,0.5828571428571429,0.7297297297297297,0.6363636363636364,We remove the words with frequency lower than 10 as well as the sentence containing them ,17,"The COCO Image Captions training dataset consists of 20,734 words and 417,126 sentences .","After the preprocessing , the dataset includes 4,980 words .",experiment
sentiment_analysis,18,Our second approach exploits syntactic information to construct a syntax - based attention model .,introduction,introduction,1,46,32,32,0,introduction : introduction,0.19246861924686195,0.8421052631578947,0.8421052631578947,Our second approach exploits syntactic information to construct a syntax based attention model ,14,"For example , embeddings of the words service , servers , staff , and courteous appear close to the same aspect embedding , which we interpret to represent the aspect service .",The attention models used in previous works give equal importance to all context words .,introduction
sentiment_analysis,39,We also experimented with adding tri-grams but it did not have a positive effect on the over all scores .,training,training,0,214,18,18,0,training : training,0.8734693877551021,0.6923076923076923,0.6923076923076923,We also experimented with adding tri grams but it did not have a positive effect on the over all scores ,21,"Also , by adding POS information , we gain an increase in the performance .","Separating the left and the right context ( LR - Left - Right ) for BoW representation , does not improve the performance .",experiment
text-classification,5,"We propose discriminative fine - tuning and slanted triangular learning rates for fine - tuning the LM , which we introduce in the following .",system description,Target task LM fine-tuning,0,86,35,5,0,system description : Target task LM fine-tuning,0.3412698412698413,0.3684210526315789,0.13513513513513514,We propose discriminative fine tuning and slanted triangular learning rates for fine tuning the LM which we introduce in the following ,22,"Given a pretrained general - domain LM , this stage converges faster as it only needs to adapt to the idiosyncrasies of the target data , and it allows us to train a robust LM even for small datasets .","As different layers capture different types of information , they should be fine - tuned to different extents .",method
natural_language_inference,78,"Subsequently , we pass each concatenated word vector into a two layer highway network in order to learn a k-dimensional representation .",model,Input Encoding Layer,0,84,9,6,0,model : Input Encoding Layer,0.3043478260869565,0.10344827586206896,0.35294117647058826,Subsequently we pass each concatenated word vector into a two layer highway network in order to learn a k dimensional representation ,22,Character representations are learned using a convolutional encoder with max pooling function and is commonly used in many relevant literature .,Highway networks are gated projection layers which learn adaptively control how much information is being carried to the next layer .,method
text_summarization,3,"The hyper - parameter settings were ? = 0.99 , ? = 0.1 , ? = 2.92 on DUC - 2004 and ? = 1.68 on Gigaword .",system description,Distant Supervision (DS) for Model Adaption,0,172,117,44,0,system description : Distant Supervision (DS) for Model Adaption,0.7107438016528925,0.8931297709923665,0.7586206896551724,The hyper parameter settings were 0 99 0 1 2 92 on DUC 2004 and 1 68 on Gigaword ,20,"At the time of decoding , the summaries were produced through a beam search of size","We trained our concept pointer generator for 450 k iterations yielded the best performance , then took the optimization using RL rewards for RG - L at 95 K iterations on DUC - 2004 and at 50 K iterations on Gigaword .",method
text_summarization,9,We chose hyper - parameters based on a grid search and picked the one which gave the best perplexity on the validation set .,system description,Architectural Choices,1,100,6,6,0,system description : Architectural Choices,0.6535947712418301,0.5454545454545454,0.5454545454545454,We chose hyper parameters based on a grid search and picked the one which gave the best perplexity on the validation set ,23,For the decoder we experimented with both the Elman RNN and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .,"In particular we searched over the number of hidden units H of the recurrent layer , the learning rate ? , the learning rate annealing schedule ? ( the factor by which to decrease ? if the validation perplexity increases ) , and the gradient clipping threshold ?.",method
sentiment_analysis,13,BERT - DK post - trains BERT 's weights only on domain knowledge ( reviews ) and fine - tunes on the 3 end tasks .,method,method,0,233,17,17,0,method : method,0.8381294964028777,0.5483870967741935,0.5483870967741935,BERT DK post trains BERT s weights only on domain knowledge reviews and fine tunes on the 3 end tasks ,21,We use this baseline to answer RQ2 and show that BERT 's pre-trained weights alone have limited performance gains on review - based tasks .,We use BERT - DK and the following BERT - MRC to answer RQ3 .,method
natural_language_inference,90,"Similarly , one can conclude that It is sunny outside contradicts the first sentence , by aligning thunder and lightning with sunny and recognizing that these are most likely incompatible .",introduction,introduction,0,22,15,15,0,introduction : introduction,0.14666666666666667,0.625,0.625,Similarly one can conclude that It is sunny outside contradicts the first sentence by aligning thunder and lightning with sunny and recognizing that these are most likely incompatible ,29,"However , it is fairly easy to conclude that the second sentence follows from the first one , by simply aligning Bob with Bob and can not sleep with awake and recognizing that these are synonyms .","We leverage this intuition to build a simpler and more lightweight approach to NLI within a neural framework ; with considerably fewer parameters , our model outperforms more complex existing neural architectures .",introduction
machine-translation,3,"The full vocabulary of the German corpus is larger , so we select more German words to build the target vocabulary .",data set,Data sets,0,198,11,11,0,data set : Data sets,0.6325878594249201,0.8461538461538461,0.8461538461538461,The full vocabulary of the German corpus is larger so we select more German words to build the target vocabulary ,21,For the target language we select the most frequent 80 K French words and the most frequent 160K German words as the output vocabulary .,Out - of - vocabulary words are replaced with the unknown symbol unk .,experiment
text_generation,5,"In order to make the model more robust , we modify the KL term by :",system description,Semi-supervised VAE,0,143,75,30,0,system description : Semi-supervised VAE,0.4847457627118644,0.986842105263158,0.967741935483871,In order to make the model more robust we modify the KL term by ,15,We find that the model can easily get stuck in two local optimum : the KL term is very small and q ( y|x ) is close to uniform distribution or the KL term is very large and all samples collapse to one class .,"That is , we only minimize the KL term when it is large enough .",method
sentiment_analysis,5,"In sections IV and V , we discuss the paper and conclude it .",introduction,introduction,0,61,52,52,0,introduction : introduction,0.23018867924528305,1.0,1.0,In sections IV and V we discuss the paper and conclude it ,13,"In section III , we conduct extensive experiments to evaluate the proposed method for EEG emotion recognition .", ,introduction
natural_language_inference,20,"But before , we first give some more details about the implementation of the model and the training procedures we use .",model,Fig. 1. Overall NLI Architecture,0,68,33,26,0,model : Fig. 1. Overall NLI Architecture,0.2798353909465021,0.9705882352941176,0.9629629629629628,But before we first give some more details about the implementation of the model and the training procedures we use ,21,"Using these initial findings , we will now look at a more detailed analyses of the performance of HBMP on various datasets and tasks .","Note , that the same specifications also apply to the experiments that we already discussed above .",method
natural_language_inference,75,"The best memory representation for directly reading documents uses "" Window - level + Center Encoding + Title "" ( W = 7 and H = 2 ) ; see for a comparison of results for different representation types .",model,WikiMovies,0,172,123,12,0,model : WikiMovies,0.8472906403940886,0.7987012987012987,0.4615384615384616,The best memory representation for directly reading documents uses Window level Center Encoding Title W 7 and H 2 see for a comparison of results for different representation types ,30,"Reading from Wikipedia documents directly ( Doc ) outperforms an IE - based KB ( IE ) , which is an encouraging result towards automated machine reading though a gap to a humanannotated KB still remains ( 93.9 vs. 76.2 ) .","Both center encoding and title features help the windowlevel representation , while sentence - level is inferior .",method
sentiment_analysis,39,"Notice that since each sentence can contain one or more opinions , the total number of opinions ( 5920 ) in the dataset is higher than the number of sentences .",dataset,Dataset,1,134,8,8,0,dataset : Dataset,0.5469387755102041,0.8,0.8,Notice that since each sentence can contain one or more opinions the total number of opinions 5920 in the dataset is higher than the number of sentences ,28,The general aspect is the most frequent aspect with over 2000 sentences while aspect touristy has occurred in less than 100 sentences .,"Location entity names are masked by location1 and location 2 in the whole dataset , so the task does not involve identification and segmentation of the named entities .",experiment
natural_language_inference,44,"Here , ' ; ' denotes the concatenation of two vectors , and h is a hyperparameter of the hidden dimension .",model,Sentence Selector,0,75,16,13,0,model : Sentence Selector,0.26223776223776224,0.5161290322580645,0.4642857142857143,Here denotes the concatenation of two vectors and h is a hyperparameter of the hidden dimension ,17,"Here , Di ? Rh d is the hidden state of sentence embedding for the i th word and","Next , the decoder is a task - specific module which computes the score for the sentence by calculating bilinear similarities between sentence encodings and question encodings as follows .",method
natural_language_inference,68,END- TO- END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,system description,END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0,234,1,1,0,system description : END-TO-END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,0.9397590361445785,0.16666666666666666,0.16666666666666666,END TO END NEURAL NETWORK MODELS FOR MACHINE COMPREHENSION,9, , ,method
named-entity-recognition,2,"Today when many practitioners run basic NLP on the entire web and large - volume traffic , faster methods are paramount to saving time and energy costs .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.018779342723004692,0.25,0.25,Today when many practitioners run basic NLP on the entire web and large volume traffic faster methods are paramount to saving time and energy costs ,26, ,Recent advances in GPU hardware have led to the emergence of bi-directional LSTMs as a standard method for obtaining pertoken vector representations serving as input to labeling tasks such as NER ( often followed by prediction in a linear - chain CRF ) .,abstract
natural_language_inference,26,"Our model is evaluated on 11 benchmark datasets involving natural language inference , question answering , semantic similarity and text classification .",introduction,introduction,0,31,21,21,0,introduction : introduction,0.14622641509433962,0.9130434782608696,0.9130434782608696,Our model is evaluated on 11 benchmark datasets involving natural language inference question answering semantic similarity and text classification ,20,The proposed SemBERT will be directly applied to typical NLU tasks .,Sem - BERT obtains new state - of - the - art on SNLI and also obtains significant gains on the GLUE benchmark and SQuAD 2.0 .,introduction
natural_language_inference,75,"The key should be designed with features to help match it to the question , while the value should be designed with features to help match it to the response ( answer ) .",system description,Key-Value Memory Networks,0,48,6,6,0,system description : Key-Value Memory Networks,0.2364532019704433,0.8571428571428571,0.8571428571428571,The key should be designed with features to help match it to the question while the value should be designed with features to help match it to the response answer ,31,This gives both ( i ) greater flexibility for the practitioner to encode prior knowledge about their task ; and ( ii ) more effective power in the model via nontrivial transforms between key and value .,An important property of the model is that the entire model can be trained with key - value transforms while still using standard backpropagation via stochastic gradient descent .,method
relation_extraction,5,The graph convolutional network is an adaptation of the convolutional neural network for encoding graphs .,model,Graph Convolutional Networks over Dependency Trees,0,39,4,2,0,model : Graph Convolutional Networks over Dependency Trees,0.1482889733840304,0.05970149253731344,0.08695652173913042,The graph convolutional network is an adaptation of the convolutional neural network for encoding graphs ,16, ,"Given a graph with n nodes , we can represent the graph structure with an n n adjacency matrix A where A ij = 1 if there is an edge going from node i to node j.",method
natural_language_inference,51,"Nevertheless , they do not support modularity .",model,Text Question Answering,0,237,34,31,0,model : Text Question Answering,0.8943396226415095,0.8095238095238095,0.7948717948717948,Nevertheless they do not support modularity ,7,These methods attempt to make the working weight of RNNs dependent on the input to enable quick adaption through time .,"In particular , Hypernetwork generates scaling factors for the single weight of the main RNN .",method
machine-translation,6,Sample a minibatchV = V pop ? V rare from V .,experiment,4:,0,163,7,2,0,experiment : 4:,0.5601374570446735,0.1346153846153846,0.6666666666666666,Sample a minibatchV V pop V rare from V ,10, ,Sample a minibatchV = V pop ? V rare from V .,experiment
text_summarization,1,"In particular , we assume that the mask is the ground truth overlap between input and target sequences at test time .",result,Diversity vs. Number of Mixtures,0,224,18,11,0,result : Diversity vs. Number of Mixtures,0.9333333333333332,0.6923076923076923,0.5789473684210527,In particular we assume that the mask is the ground truth overlap between input and target sequences at test time ,21,show the upper bound performance of SELECTOR by feeding focus guide to generator during test time .,The gap between the oracle metric ( top -k accuracy ) and the upper bound is very small for question generation .,result
natural_language_inference,75,"As an alternative to directly reading documents , we explore leveraging information extraction techniques to transform documents into a KB format .",model,Doc,0,142,93,13,0,model : Doc,0.6995073891625616,0.6038961038961039,0.65,As an alternative to directly reading documents we explore leveraging information extraction techniques to transform documents into a KB format ,21,We finally only retain KB triples where the entities also appear in the Wikipedia articles 4 to try to guarantee that all QA pairs will be equally answerable by either the KB or Wikipedia document sources .,An IE - KB representation has attractive properties such as more precise and compact expressions of facts and logical key - value pairings based on subjectverb - object groupings .,method
text-classification,1,"In its simplest form , onehot CNN works as follows .",introduction,introduction,0,18,7,7,0,introduction : introduction,0.0703125,0.2,0.2,In its simplest form onehot CNN works as follows ,10,"The embedding function is shared among all the locations , so that useful features can be detected irrespective of their locations .","document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .",introduction
sentiment_analysis,10,"Each GRU cell computes a hidden state defined ash t = GRU * ( h t?1 , x t ) , where x t is the current input and h t? 1 is the previous GRU state .",model,Our Model,0,72,12,12,0,model : Our Model,0.2801556420233463,0.2033898305084746,0.21428571428571427,Each GRU cell computes a hidden state defined ash t GRU h t 1 x t where x t is the current input and h t 1 is the previous GRU state ,33,We use GRU cells to update the states and representations .,ht also serves as the current GRU output .,method
natural_language_inference,44,"We do not use handcraft word features such as POS and NER tagging , which is different from Document Reader in DrQA .",training,training,0,220,5,5,0,training : training,0.7692307692307693,0.07142857142857142,0.14285714285714285,We do not use handcraft word features such as POS and NER tagging which is different from Document Reader in DrQA ,22,"We obtain the embeddings of the document and the question by concatenating 300 - dimensional Glove embeddings pretrained on the 840B Common Crawl corpus , 100 - dimensional character ngram embeddings by , and 300 - dimensional contextualized embeddings pretrained on .","Hence , the dimension of the embedding ( d h ) is 600 .",experiment
natural_language_inference,82,An entity in the summary ( Robert Downey Jr. ) is replaced by the placeholder [ X ] to form a query .,introduction,introduction,0,16,9,9,0,introduction : introduction,0.14035087719298245,0.4736842105263158,0.4736842105263158,An entity in the summary Robert Downey Jr is replaced by the placeholder X to form a query ,19,document - query - answer triple constructed from a news article and its bullet point summary .,All entities are anonymized to exclude world knowledge and focus on reading comprehension .,introduction
relation_extraction,8,It means that we can not capture more useful information by simply increasing the network parameter .,evaluation,Manual Evaluation,0,258,22,22,0,evaluation : Manual Evaluation,0.9591078066914498,0.7857142857142857,0.7857142857142857,It means that we can not capture more useful information by simply increasing the network parameter ,17,"Since the parameters for all the model are determined by grid search , we can observe that CNNs can not achieve competitive results compared to PCNNs when increasing the size of the hidden layer of convolutional neural networks .",These results demonstrate that the proposed piecewise max pooling technique is beneficial and :,result
sentiment_analysis,30,Update of the memory of an entity is only triggered when the gate is activated .,methodology,Methodology,0,54,27,27,0,methodology : Methodology,0.421875,0.5192307692307693,0.5192307692307693,Update of the memory of an entity is only triggered when the gate is activated ,16,"Essentially , gate ? ? g j i determines how much the j - th memory should be updated , factoring in three elements : ( 1 ) how similar the current input w i is to the entity being tracked ( k j ) ; ( 2 ) how related the current input w i is to the state of the j - th entity ( ? ? h j i?1 ) ; and how past activation should influence the current one .","Following the update , the model performs a normalis ation step , allowing the memory to forget :",method
sentiment_analysis,9,"Similar to the named entity recognition ( NER ) task , the ATE task is a sequence labeling task , which aims to extract aspects from the reviews or tweet .",introduction,introduction,0,28,14,14,0,introduction : introduction,0.10108303249097472,0.3684210526315789,0.3684210526315789,Similar to the named entity recognition NER task the ATE task is a sequence labeling task which aims to extract aspects from the reviews or tweet ,27,"It is obvious that the sentiment polarity classified based on aspects can better mine the fine - grained emotional tendency in reviews or tweets , thus providing a more accurate reference for decision - makers .","In most researches ; , the ATE task is studied independently , away from the APC task .",introduction
text-classification,7,"This dataset consists of 10,788 documents from the Reuters financial newswire service , where each document contains either multiple labels or a single label .",ablation,Single-Label to Multi-Label Text Classification,0,166,16,11,0,ablation : Single-Label to Multi-Label Text Classification,0.6831275720164609,0.2857142857142857,0.4583333333333333,This dataset consists of 10 788 documents from the Reuters financial newswire service where each document contains either multiple labels or a single label ,25,The evaluation is carried on the Reuters - 21578 dataset .,We reprocess the corpus to evaluate the capability of capsule networks of transferring from single - label to multi-label text classification .,result
sentiment_analysis,1,"Investigations on the neuronal activities reveal that pre-frontal , parietal and occipital regions maybe the most informative regions for emotion recognition , which is consistent with relevant prior studies .",abstract,abstract,0,12,10,10,0,abstract : abstract,0.030303030303030307,0.9090909090909092,0.9090909090909092,Investigations on the neuronal activities reveal that pre frontal parietal and occipital regions maybe the most informative regions for emotion recognition which is consistent with relevant prior studies ,29,Our model analysis demonstrates that the proposed biologically supported adjacency matrix and two regularizers contribute consistent and significant gain to the performance .,"In addition , experimental results suggest that global inter-channel relations between the left and right hemispheres are important for emotion recognition and local inter-channel relations between ( FP1 , AF3 ) , ( F6 , F8 ) and ( FP2 , AF4 ) may also provide useful information .",abstract
relation_extraction,13,"Finally , we report BERT EM + MTB 's performance on all of FewRel 's fully supervised tasks in Table 3 .",experiment,Few-shot Relation Matching,0,196,18,13,0,experiment : Few-shot Relation Matching,0.92018779342723,0.6923076923076923,0.65,Finally we report BERT EM MTB s performance on all of FewRel s fully supervised tasks in Table 3 ,20,The results in show that MTB training could be used to significantly reduce effort in implementing an exemplar based relation extraction system .,"We see that it outperforms the human upper bound reported by , and it significantly outperforms all other submissions to the FewRel leaderboard , published or unpublished .",experiment
sentiment_analysis,50,"I was highly dis appointed in the [ food ] neg .""",analysis,Analysis,0,134,13,13,0,analysis : Analysis,0.8271604938271605,0.3611111111111111,0.3611111111111111,I was highly dis appointed in the food neg ,10,Below are two examples where the target is enclosed in [ ] with it s true sentiment indicated in the subscript :,". "" This particular location certainly uses substandard [ meats ] neg .""",result
named-entity-recognition,4,"Unlike previous approaches for learning contextualized word vectors , ELMo representations are deep , in the sense that they are a function of all of the internal layers of the biLM .",introduction,introduction,0,16,9,9,0,introduction : introduction,0.05882352941176471,0.2307692307692308,0.2307692307692308,Unlike previous approaches for learning contextualized word vectors ELMo representations are deep in the sense that they are a function of all of the internal layers of the biLM ,30,"For this reason , we call them ELMo ( Embeddings from Language Models ) representations .","More specifically , we learn a linear combination of the vectors stacked above each input word for each end task , which markedly improves performance over just using the top LSTM layer .",introduction
natural_language_inference,11,"are incrementally refined by reading the input text and textual renderings of relevant background knowledge before computing the representations used by the task model ( in this figure , RTE ) .",system description,Refining Word Embeddings by Reading,0,63,29,18,0,system description : Refining Word Embeddings by Reading,0.2282608695652173,0.5178571428571429,0.9473684210526316,are incrementally refined by reading the input text and textual renderings of relevant background knowledge before computing the representations used by the task model in this figure RTE ,29,The reading architecture constructs refinements of word representations incrementally ( conceptually represented as columns in a series of embedding matrices ),our model by n and a fully - connected layer by,method
text_summarization,8,"Following the pointergenerator model of , we reuse the attention p ( a j | x , y 1:j?1 ) distribution as copy distribution , i.e. the copy probability of a token in the source w through the copy attention is computed as the sum of attention towards all occurrences of w .",system description,Masked Source Summary,0,89,19,8,0,system description : Masked Source Summary,0.3111888111888112,0.2087912087912088,0.8888888888888888,Following the pointergenerator model of we reuse the attention p a j x y 1 j 1 distribution as copy distribution i e the copy probability of a token in the source w through the copy attention is computed as the sum of attention towards all occurrences of w ,50,"The copy distribution is a probability distribution over the source text , and the joint distribution is computed as a convex combination of the two parts of the model , where the two parts represent copy and generation distribution respectively .","During training , we maximize marginal likelihood with the latent switch variable .",method
natural_language_inference,72,The patient was then subjected to a thoracic CT scan that also showed significant radiological improvement .,introduction,introduction,0,18,10,10,0,introduction : introduction,0.0576923076923077,0.2083333333333333,0.2083333333333333,The patient was then subjected to a thoracic CT scan that also showed significant radiological improvement ,17,gradual improvement in clinical and laboratory status was achieved within 20 days of antituberculous treatment .,"Thereafter , tapering of corticosteroids was initiated with no clinical relapse .",introduction
sentiment_analysis,37,"In our experiments , we tried different hop counts of the memory network .",analysis,Hop-Performance Relation,0,241,8,2,0,analysis : Hop-Performance Relation,0.9525691699604744,0.5714285714285714,0.25,In our experiments we tried different hop counts of the memory network ,13, ,"We observed that the network performs best with three hops for restaurant domain and ten hops for laptop domain , which is shown in the hop count - performance plot in .",result
question_generation,1,Tag is language based context .,ablation,How are exemplars improving Embedding,0,343,93,31,0,ablation : How are exemplars improving Embedding,0.875,0.6549295774647887,0.5081967213114754,Tag is language based context ,6,Analysis of Network E.1 Analysis of Tag Context,"These tags are extracted from caption , except question - tags which is fixed as the 7 ' Wh words '",result
natural_language_inference,75,This explains why using a satisfactory KB - typically only available in closed domains - is preferred over raw text .,introduction,introduction,0,19,9,9,0,introduction : introduction,0.09359605911330048,0.45,0.45,This explains why using a satisfactory KB typically only available in closed domains is preferred over raw text ,19,"Retrieving answers directly from text is harder than from KBs because information is far less structured , is indirectly and ambiguously expressed , and is usually scattered across multiple documents .","We postulate that before trying to provide answers thatare not in KBs , document - based QA systems should first reach KB - based systems ' performance in such closed domains , where clear comparison and evaluation is possible .",introduction
machine-translation,8,"This is likely due to the fact that the RNNsearch does not require encoding along sentence into a fixed - length vector perfectly , but only accurately encoding the parts of the input sentence that surround a particular word .",analysis,LONG SENTENCES,0,161,16,3,0,analysis : LONG SENTENCES,0.48640483383685795,0.5,0.3333333333333333,This is likely due to the fact that the RNNsearch does not require encoding along sentence into a fixed length vector perfectly but only accurately encoding the parts of the input sentence that surround a particular word ,38,As clearly visible from the proposed model ( RNNsearch ) is much better than the conventional model ( RNNencdec ) at translating long sentences .,"As an example , consider this source sentence from the test set :",result
sentiment_analysis,38,This approach outperformed training the same model from random initialization and achieved state of the art on several text classification datasets .,introduction,introduction,0,39,28,28,0,introduction : introduction,0.23214285714285715,0.6511627906976745,0.6511627906976745,This approach outperformed training the same model from random initialization and achieved state of the art on several text classification datasets ,22,"In contrast to learning a generic representation on one large dataset and then evaluating on other tasks / datasets , proposed using similar unsupervised objectives such as sequence autoencoding and language modeling to first pretrain a model on a dataset and then finetune it for a given task .",Combining language modelling with topic modelling and fitting a small supervised feature extractor on top has also achieved strong results on in - domain document level sentiment analysis .,introduction
query_wellformedness,0,We augment their system by training a discriminative reranker with the model score of the question generation model and the wellformedness probability of our classifier as features to optimize BLEU score between the selected question from the 10 - best list and the reference question on the development set .,experiment,Improving Question Generation,0,81,13,8,0,experiment : Improving Question Generation,0.84375,0.6842105263157895,0.5714285714285714,We augment their system by training a discriminative reranker with the model score of the question generation model and the wellformedness probability of our classifier as features to optimize BLEU score between the selected question from the 10 best list and the reference question on the development set ,49,Their current best model selects the top ranked question from the n-best list produced by the decoder as the output .,We then use this reranker to select the best question from the 10 - best list of the test set .,experiment
natural_language_inference,95,"In this way , we define the loss function as the averaged cross entropy :",model,Answer Content Modeling,0,103,36,11,0,model : Answer Content Modeling,0.44017094017094016,0.72,0.8461538461538461,In this way we define the loss function as the averaged cross entropy ,14,"We transform the boundary labels into a continuous segment , which means the words within the answer span will be labeled as 1 and other words will be labeled as 0 .",The content probabilities provide another view to measure the quality of the answer in addition to the boundary .,method
natural_language_inference,88,"Although encouraging , this result should be interpreted with caution since our model has substantially more parameters compared to related systems .",model,Natural Language Inference,0,236,51,29,0,model : Natural Language Inference,0.9554655870445344,0.9622641509433962,0.935483870967742,Although encouraging this result should be interpreted with caution since our model has substantially more parameters compared to related systems ,21,"With standard training , our deep fusion yields the state - of - the - art performance in this task .",We could compare different models using the same number of total parameters .,method
natural_language_inference,61,We then draw conclusions in Section 4 .,introduction,introduction,0,58,51,51,0,introduction : introduction,0.3717948717948718,1.0,1.0,We then draw conclusions in Section 4 ,8,"We describe the datasets and the experiment se ings , and analyze our experimental results in Section 3 .", ,introduction
natural_language_inference,34,Text - based question answering ( TBQA ) has been studied extensively in recent years .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.013559322033898305,0.2222222222222222,0.2222222222222222,Text based question answering TBQA has been studied extensively in recent years ,13, ,Most existing approaches focus on finding the answer to a question within a single paragraph .,abstract
part-of-speech_tagging,3,"While there is clearly inconsistency among the precision ( 91.5 % ) , recall ( 91.4 % ) and F1 scores ( 91.2 % ) , it is unclear in which way they are incorrect .",system description,Word Embeddings,0,153,18,8,0,system description : Word Embeddings,0.7536945812807881,0.5625,0.42105263157894735,While there is clearly inconsistency among the precision 91 5 recall 91 4 and F1 scores 91 2 it is unclear in which way they are incorrect ,28,Numbers are taken from the of the original paper .,more heavily on pretrained embeddings than POS tagging .,method
question_answering,5,We formulate the above evidence aggregation as an answer re-ranking problem .,introduction,introduction,1,35,21,21,0,introduction : introduction,0.12681159420289856,0.5675675675675675,0.5675675675675675,We formulate the above evidence aggregation as an answer re ranking problem ,13,"To provide more accurate answers for open - domain QA , we hope to make better use of multiple passages for the same question by aggregating both the strengthened and the complementary evidence from all the passages .","Re-ranking has been commonly used in NLP problems , such as in parsing and translation , in order to make use of high - order or global features thatare too expensive for decoding algorithms .",introduction
sentiment_analysis,9,"The optimal performance is in bold . "" * "" indicates the real performance is almost up to 100 % .",analysis,Effectiveness of Multi-task Learning,0,259,25,10,0,analysis : Effectiveness of Multi-task Learning,0.9350180505415162,0.7142857142857143,1.0,The optimal performance is in bold indicates the real performance is almost up to 100 ,16,"The "" - "" indicates that the statistics are not important during single - task learning optimization and not listed in the table .", ,result
question-answering,3,"In this section , we propose a sentence similarity learning model to tackle all three challenges ( mentioned in Section 1 ) .",model,Model Overview,0,55,2,2,0,model : Model Overview,0.21653543307086606,0.02,0.2,In this section we propose a sentence similarity learning model to tackle all three challenges mentioned in Section 1 ,20, ,"To deal with the first challenge , we represent each word as a distributed vector , so that we can calculate similarities for formally different but semantically related words .",method
relation-classification,1,large amount of training data can be obtained by means of distant supervision methods without manually labeling .,dataset,dataset,0,140,3,3,0,dataset : dataset,0.5691056910569106,0.5,0.5,large amount of training data can be obtained by means of distant supervision methods without manually labeling ,18,"To evaluate the performance of our methods , we use the public dataset NYT 2 which is produced by distant supervision method .",While the test set is manually labeled to ensure its quality .,experiment
text_summarization,9,Summarization systems can be broadly classified into two categories .,introduction,introduction,0,12,4,4,0,introduction : introduction,0.0784313725490196,0.2,0.2,Summarization systems can be broadly classified into two categories ,10,Tackling this task is an important step towards natural language understanding .,Extractive models generate summaries by cropping important segments from the original text and putting them together to form a coherent summary .,introduction
text_summarization,10,"In order to verify that our improvements were from the auxiliary tasks ' specific character / capabilities and not just due to adding more data , we separately trained word embeddings on each auxiliary dataset ( i.e. , SNLI and SQuAD ) and incorporated them into the summarization model .",result,Multi-Task with Question Generation,0,159,18,7,0,result : Multi-Task with Question Generation,0.6045627376425855,0.8571428571428571,0.7,In order to verify that our improvements were from the auxiliary tasks specific character capabilities and not just due to adding more data we separately trained word embeddings on each auxiliary dataset i e SNLI and SQuAD and incorporated them into the summarization model ,45,Stat. significance is computed via bootstrap test with 100K samples .,"We found that both our 2 - way multi-task models perform sig - Multi - Task with Entailment and Question Generation Finally , we perform multi-task learning with all three tasks together , achieving the best of both worlds ( inference skills and saliency ) .",result
relation_extraction,5,"In our experiments we found that the output vector h sent tends to have large magnitude , and : Aggregated 5 - run difference compared to PA - LSTM on the TACRED dev set .",experiment,Training,0,247,23,7,0,experiment : Training,0.9391634980988594,0.5897435897435898,0.4666666666666667,In our experiments we found that the output vector h sent tends to have large magnitude and Aggregated 5 run difference compared to PA LSTM on the TACRED dev set ,31,"Due to the small size of the SemEval dataset , we train all models for 150 epochs , and use an initial learning rate of 0.5 with a decay rate of 0.95 .","For each example , if X out of 5 GCN models predicted its label correctly and Y PA - LSTM models did , it is aggregated in the bar labeled X ? Y .",experiment
natural_language_inference,60,"With our method , embeddings from different domains can be combined , optionally while taking into account contextual information .",introduction,introduction,0,28,21,21,0,introduction : introduction,0.14507772020725387,0.9130434782608696,0.9130434782608696,With our method embeddings from different domains can be combined optionally while taking into account contextual information ,18,"are often trained on a single domain , such as Wikipedia or newswire .","Multi-modality Multi-modal information has proven useful in many tasks , yet the question of multi-modal fusion remains an open problem .",introduction
sentiment_analysis,27,The model makes embeddings of aspects to participate in computing attention weights .,system description,Aspect-level sentiment classification,0,66,11,11,0,system description : Aspect-level sentiment classification,0.2391304347826087,0.5,0.6875,The model makes embeddings of aspects to participate in computing attention weights ,13,Wang et al. propose ATAE - LSTM that combines LSTM and attention mechanism .,RAM is proposed by Chen et al . which adopts multiple - attention mechanism on the memory built with bidirectional LSTM .,method
text-classification,3,Two different settings were studied : ( 1 ) word embedding 's training corpus and the evaluation dataset were preprocessed in a similar manner ( Section 3.2 ) ; and ( 2 ) the two were preprocessed differently ( Section 3.3 ) .,system description,Multiword grouping,0,60,28,9,0,system description : Multiword grouping,0.4878048780487805,0.9655172413793104,0.9,Two different settings were studied 1 word embedding s training corpus and the evaluation dataset were preprocessed in a similar manner Section 3 2 and 2 the two were preprocessed differently Section 3 3 ,35,"We considered two tasks for our experiments : topic categorization , i.e. assigning a topic to a given document from a pre-defined set of topics , and polarity detection , i.e. detecting if the sentiment of a given piece of text is positive or negative .",In what follows we describe the common experimental setting as well as the datasets and preprocessing used for the evaluation .,method
natural_language_inference,80,"It is noteworthy that recent deep learning models surpass the human performance in the NLI task. , previous deep learning models ( rows 2 - 19 ) can be divided into three categories :",experiment,experiment,0,165,39,39,0,experiment : experiment,0.5689655172413793,0.9069767441860463,0.9069767441860463,It is noteworthy that recent deep learning models surpass the human performance in the NLI task previous deep learning models rows 2 19 can be divided into three categories ,30,"We also report the estimated human performance on the SNLI dataset , which is the average accuracy of five annotators in comparison to the gold labels .",") sentence encoding based models ( rows 2 - 7 ) , 2 ) single inter-sentence attention - based models ( rows 8 - 16 ) , and 3 ) ensemble inter-sentence attention - based models ( rows 17 - 19 ) .",experiment
sentiment_analysis,1,We notice that fear is the only emotion that performs better in subject - independent classification than in subject - dependent classification .,evaluation,Confusion Matrix,0,342,26,7,0,evaluation : Confusion Matrix,0.8636363636363636,0.9629629629629628,0.875,We notice that fear is the only emotion that performs better in subject independent classification than in subject dependent classification ,21,"For SEED - IV , our model performs significantly better on sad emotion than all other emotions in both classification settings .",This finding indicates that participants watching horror movies may generate similar EEG patterns .,result
named-entity-recognition,8,"Wikipedia containing the answer , the task is to predict the answer text span in the passage .",experiment,GLUE,0,185,33,31,0,experiment : GLUE,0.4780361757105943,0.4647887323943662,0.4492753623188406,Wikipedia containing the answer the task is to predict the answer text span in the passage ,17,10 https://gluebenchmark.com/leaderboard,"As shown in , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding .",experiment
natural_language_inference,60,"are often trained on a single domain , such as Wikipedia or newswire .",introduction,introduction,0,27,20,20,0,introduction : introduction,0.13989637305699482,0.8695652173913043,0.8695652173913043,are often trained on a single domain such as Wikipedia or newswire ,13,One of the main problems with NLP systems is dealing with out - ofvocabulary words : our method increases lexical coverage by allowing systems to take the union over different embeddings .,"With our method , embeddings from different domains can be combined , optionally while taking into account contextual information .",introduction
machine-translation,0,Details of the architecture used in the experiments are explained in more depth in the supplementary material .,system description,RNN Encoder-Decoder,0,47,12,12,0,system description : RNN Encoder-Decoder,0.2146118721461187,0.14634146341463414,1.0,Details of the architecture used in the experiments are explained in more depth in the supplementary material ,18,The model was trained for approximately three days ., ,method
sentence_classification,0,Citations play a unique role in scientific discourse and are crucial for understanding and analyzing scientific work .,introduction,introduction,0,10,2,2,0,introduction : introduction,0.03745318352059925,0.25,0.25,Citations play a unique role in scientific discourse and are crucial for understanding and analyzing scientific work ,18, ,"They are also typically used as the main measure for assessing impact of scientific publications , venues , and researchers .",introduction
natural_language_inference,86,"where is the elementwise multiplication , and W k is the k - th row of W , which controls the k - th perspective and assigns different weights to different dimensions of the d-dimensional space .",system description,Context Representation Layer,0,83,49,18,0,system description : Context Representation Layer,0.4853801169590643,0.7777777777777778,0.5625,where is the elementwise multiplication and W k is the k th row of W which controls the k th perspective and assigns different weights to different dimensions of the d dimensional space ,34,"Each element m k ? m is a matching value from the k - th perspective , and it is calculated by the cosine similarity between two weighted vectors","Second , on the orthogonal direction off m , we define three matching strategies to compare each contextual embedding of the passage with the question :",method
sentiment_analysis,11,"At a particular hop r , the output memory of the previous hop M ? .",system description,Multiple Layers,0,206,62,6,0,system description : Multiple Layers,0.5988372093023255,0.6262626262626263,0.25,At a particular hop r the output memory of the previous hop M ,14,This is done by stacking the single hop layers ( Section 4.2.1 ) as follows :,This constraint of sharing parameters adjacently between layers is added for reduction in total parameters and ease of training .,method
sentiment_analysis,15,Set 1 : Negating Positive Sentences .,experiment,Model Analysis: High Level Negation,0,241,45,4,0,experiment : Model Analysis: High Level Negation,0.8925925925925926,0.6521739130434783,0.17391304347826084,Set 1 Negating Positive Sentences ,6,"For each type , we use a separate dataset for evaluation .",The first set contains positive sentences and their negation .,experiment
text_generation,1,"The lower the NLL score is , the higher probability the generated sentence will pass the Turing test .",experiment,Simulation on synthetic data,0,182,18,15,0,experiment : Simulation on synthetic data,0.6642335766423357,0.17475728155339804,0.42857142857142855,The lower the NLL score is the higher probability the generated sentence will pass the Turing test ,18,"Following this , we take the sentences generated by RankGAN as the input of the oracle model , and estimate the average negative loglikelihood ( NLL ) .","We compare our approach with the state - of - the - art methods including maximum likelihood estimation ( MLE ) , policy gradient with BLEU ( PG - BLEU ) , and SeqGAN .",experiment
text_generation,1,"The script is splited into 2 , 500 training sentences and 565 test sentences .",experiment,Results on Shakespeare's plays,0,263,99,4,0,experiment : Results on Shakespeare's plays,0.95985401459854,0.9611650485436892,0.5,The script is splited into 2 500 training sentences and 565 test sentences ,14,"In this experiment , we train our model on the Romeo and Juliet play to further validate the proposed method .","To learn the rare words in the script , we adjust the threshold of UNK from 5 to 2 .",experiment
sentiment_analysis,36,It is a CNN - based model implemented by us which directly concatenates target representation to each word embedding ; TD - LSTM :,experiment,Experimental Setup,1,155,14,13,0,experiment : Experimental Setup,0.62,0.4,0.38235294117647056,It is a CNN based model implemented by us which directly concatenates target representation to each word embedding TD LSTM ,21,"SVM : It is a traditional support vector machine based model with extensive feature engineering ; AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ; AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ; IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ; CNN - ASP :",It is a CNN - based model implemented by us which directly concatenates target representation to each word embedding ; TD - LSTM :,experiment
sentiment_analysis,28,There are already some studies use attention to generate targetspecific sentence representations or to transform sentence representations according to target words .,introduction,introduction,0,18,8,8,0,introduction : introduction,0.1,0.27586206896551724,0.27586206896551724,There are already some studies use attention to generate targetspecific sentence representations or to transform sentence representations according to target words ,22,"Attention mechanism , which has been successfully used in machine translation , is incorporated to enforce the model to pay more attention to context words with closer semantic relations with the target .","However , these studies depend on complex recurrent neural networks ( RNNs ) as sequence encoder to compute hidden semantics of texts .",introduction
natural_language_inference,64,We thus apply a second fine - tuning step to adapt the classifier to the target AS2 domain .,system description,TANDA,0,112,41,6,0,system description : TANDA,0.4462151394422311,0.6307692307692307,0.6,We thus apply a second fine tuning step to adapt the classifier to the target AS2 domain ,18,The resulting model will not perform optimally on the data of the target domain due to the specificity of the latter .,"For example , in the transfer step , we may have general questions such as , What is the average heart rate of a healthy person while , in the adapt step , the target domain , e.g. , sport news , may contain specific questions such as : When did the Philadelphia eagles play the fog bowl ? Using different training stepson the target data to improve performance is a rather intuitive approach .",method
natural_language_inference,22,"However , BIDAF and some similar models miss some questions because they do n't have the capacity to reflect on problematic candidate answers and revise their decisions .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.09090909090909093,0.4166666666666667,0.4166666666666667,However BIDAF and some similar models miss some questions because they do n t have the capacity to reflect on problematic candidate answers and revise their decisions ,28,BIDAF uses a bi-directional attention matrix which calculates the correlations between each word pair in context and query to build query - aware context representation .,"When humans are reading a text with the goal of answering a question , they tend to read it multiple times to get a better understanding of the context and question , and to give a better response .",introduction
text-to-speech_synthesis,2,"There has been significant interest in end - to - end training of TTS models , which are trained directly from text - audio pairs , without depending on hand crafted intermediate representations .",introduction,introduction,0,24,16,16,0,introduction : introduction,0.09795918367346937,0.42105263157894735,0.42105263157894735,There has been significant interest in end to end training of TTS models which are trained directly from text audio pairs without depending on hand crafted intermediate representations ,29,"We train the synthesis network on 1.2K speakers and show that training the encoder on a much larger set of 18K speakers improves adaptation quality , and further enables synthesis of completely novel speakers by sampling from the embedding prior .","Tacotron 2 used WaveNet as a vocoder to invert spectrograms generated by an encoderdecoder architecture with attention , obtaining naturalness approaching that of human speech by combining Tacotron 's prosody with WaveNet 's audio quality .",introduction
sentiment_analysis,41,"We compare our model with several baselines , including LSTM , TD - LSTM , and TC - LSTM .",baseline,Comparison with baseline methods,0,175,2,2,0,baseline : Comparison with baseline methods,0.7847533632286996,0.3333333333333333,0.3333333333333333,We compare our model with several baselines including LSTM TD LSTM and TC LSTM ,15, ,"LSTM : Standard LSTM can not capture any aspect information in sentence , so it must get the same ( a ) the aspect of this sentence : service ( b ) the aspect of this sentence : food : Attention Visualizations .",result
natural_language_inference,19,Masked Multi - Head Attention,architecture,Masked Multi-Head Attention,0,92,17,1,0,architecture : Masked Multi-Head Attention,0.3607843137254902,0.20987654320987653,0.043478260869565216,Masked Multi Head Attention,4, , ,method
sentiment_analysis,31,"The goal is to classify whether the sentiment towards the aspect in the sentence is positive , negative , or neutral .",system description,Problem Definition,0,50,6,3,0,system description : Problem Definition,0.31446540880503143,0.10714285714285714,1.0,The goal is to classify whether the sentiment towards the aspect in the sentence is positive negative or neutral ,20,"In aspect level sentiment classification , we are given a sentence s = [ w 1 , w 2 , ... , w i , ... , w n ] and an aspect target t = [ w i , w i + 1 , ... , w i +m ?1 ] .", ,method
semantic_role_labeling,3,This indicates that our model has some advantages on such difficult adjunct distinction .,model,Detailed Scores,0,234,41,17,0,model : Detailed Scores,0.8863636363636364,0.6119402985074627,1.0,This indicates that our model has some advantages on such difficult adjunct distinction ,14,"Compared with the previous work , our model still confuses ARG2 with AM - DIR , AM - LOC and AM - MNR , but to a lesser extent .", ,method
natural_language_inference,15,"Here , the operations +, ? and | | are performed elementwise to infer the relationship between two sentences .",method,Interaction and Prediction Layer,0,103,45,5,0,method : Interaction and Prediction Layer,0.4557522123893805,0.9,0.5,Here the operations and are performed elementwise to infer the relationship between two sentences ,15,"Then , we aggregate these representations p and q for the two sentences P and Q in various ways in the interaction layer and the final feature vector v for semantic sentence matching is obtained as follows :",The element - wise subtraction p ? q is an asymmetric operator for one - way type tasks such as natural language inference or answer sentence selection .,method
natural_language_inference,65,"The input in each round is two pairs ( q , a + ) and ( q , a ? ) , where a + is aground truth answer for q , and a ? is an incorrect answer .",training,Scoring and Training Procedure,0,89,8,8,0,training : Scoring and Training Procedure,0.3869565217391304,0.5714285714285714,0.5714285714285714,The input in each round is two pairs q a and q a where a is aground truth answer for q and a is an incorrect answer ,28,Both networks are trained by minimizing a pairwise ranking loss function over the training set D .,"The input in each round is two pairs ( q , a + ) and ( q , a ? ) , where a + is aground truth answer for q , and a ? is an incorrect answer .",experiment
natural_language_inference,0,Question Evidence Common Word Feature ( qecomm ) : recently proposed a simple token level indicator feature which significantly boosts reading comprehension performance in some cases .,model,Further Enhancements,0,103,42,9,0,model : Further Enhancements,0.5228426395939086,0.875,0.6,Question Evidence Common Word Feature qecomm recently proposed a simple token level indicator feature which significantly boosts reading comprehension performance in some cases ,24,"The embedding C ( w ) is generated by taking the final outputs z f nc and z b nc of a Bi - GRU applied to embeddings from a lookup table of characters in the token , and applying a linear transformation :","For each token in the document we construct a one - hot vector f i ? { 0 , 1 } 2 indicating its presence in the query .",method
natural_language_inference,53,The first group corresponds to models trained with unsupervised unordered sentences .,model,Task transfer,0,165,24,4,0,model : Task transfer,0.7932692307692307,0.3934426229508197,0.0975609756097561,The first group corresponds to models trained with unsupervised unordered sentences ,12,We group models by the nature of the data on which they were trained .,"This includes bagof - words models such as word2 vec - SkipGram , the Unigram - TFIDF model , the Paragraph Vector model , the Sequential Denoising Auto - Encoder ( SDAE ) and the SIF model , all trained on the Toronto book corpus .",method
natural_language_inference,97,"Moreover , linearization of the dependency graph , because it relies on an eigendecomposition , is not differentiable .",model,Dependency Sliding Window,0,152,70,6,0,model : Dependency Sliding Window,0.5205479452054794,0.625,0.24,Moreover linearization of the dependency graph because it relies on an eigendecomposition is not differentiable ,16,"Thus , the dependency graph can be considered a fixed feature .","However , we handle the linearization in data preprocessing so that the model sees only reordered word - vector inputs .",method
natural_language_inference,61,It promoted almost 0.5 percent accuracy and outperformed the baselines on MultiNLI .,experiment,Experiment results,1,139,10,10,0,experiment : Experiment results,0.8910256410256411,0.7692307692307693,0.9090909090909092,It promoted almost 0 5 percent accuracy and outperformed the baselines on MultiNLI ,14,"According to the results in , a ESIM model achieved 88.1 % on SNLI corpus , elevating 0.8 percent higher than ESIM model .","It also achieved 88.01 % on ora. erefore , we concluded that a ESIM with further word a ention and word orientation operation was superior to ESIM model .",experiment
natural_language_inference,28,"In detail , the RNN is fed into a sequence of characters , e.g. "" a1s2d3f4 g5 ? ? d "" .",experiment,ASSOCIATIVE RECALL TASK,0,169,30,5,0,experiment : ASSOCIATIVE RECALL TASK,0.6236162361623616,0.5882352941176471,0.3333333333333333,In detail the RNN is fed into a sequence of characters e g a1s2d3f4 g5 d ,17,We follow the same setting as in and and modify the original task so that it can test for longer sequences .,"The RNN is supposed to output the character based on the "" key "" which is located at the end of the sequence .",experiment
natural_language_inference,34,"Ent and dynamic graph attention , we realize a reasoning step at the entity level .",system description,Reasoning with the Fusion Block,0,179,76,46,0,system description : Reasoning with the Fusion Block,0.6067796610169491,0.7524752475247525,0.8363636363636363,Ent and dynamic graph attention we realize a reasoning step at the entity level ,15,Graph to Document Flow .,"However , the unrestricted answer still can not be backtraced .",method
sentiment_analysis,28,"After we obtain the introspective context representation h c and the context - perceptive target representation ht , we employ another MHA to obtain the target - specific context representation h tsc = {h tsc 1 , h tsc 2 , ... , h tsc m } by :",methodology,GloVe Embedding,0,99,49,36,0,methodology : GloVe Embedding,0.55,0.875,0.8372093023255814,After we obtain the introspective context representation h c and the context perceptive target representation ht we employ another MHA to obtain the target specific context representation h tsc h tsc 1 h tsc 2 h tsc m by ,40,Target - specific Attention Layer,The multi-head attention function here also has its independent parameters .,method
natural_language_inference,63,"In the memory controller , we use 100 x 36 size memory initialized with zeros , 4 read heads and 1 write head .",implementation,Implementation Details,1,139,4,4,0,implementation : Implementation Details,0.7722222222222223,0.5714285714285714,0.5714285714285714,In the memory controller we use 100 x 36 size memory initialized with zeros 4 read heads and 1 write head ,22,"For the word - level embedding , we tokenize the documents using NLTK toolkit and substitute words with GloVe 6B 43.16 46.90 49.28 55.83 BiDAF 40.32 45.91 44.86 50.71 hidden size is set to 200 for QUASAR - T and Triv - iaQA , and 100 for SQuAD .","The optimizer is AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 0.5 .",experiment
phrase_grounding,0,"Other methods have used the Canonical Correlation Analysis ( CCA ) , which finds linear projections that maximize the correlation between projected vectors from the two views of heterogeneous data .",system description,Mapping to common space,0,52,19,4,0,system description : Mapping to common space,0.2311111111111111,0.475,0.5714285714285714,Other methods have used the Canonical Correlation Analysis CCA which finds linear projections that maximize the correlation between projected vectors from the two views of heterogeneous data ,28,"Current works usually apply a multi - layer perceptron ( MLP ) , element - wise multiplication , or cosine similarity to combine representations from different modalities .",introduced the Multimodal Compact Bilinear ( MCB ) pooling method that uses a compressed feature from the outer product of two vectors of visual and language features to fuse them .,method
part-of-speech_tagging,0,"To prevent this issue , we normalize word / character embeddings so that they have mean 0 and variance 1 for every entry , as in .",training,Adversarial Training,0,77,20,20,0,training : Adversarial Training,0.3130081300813008,0.5128205128205128,0.5128205128205128,To prevent this issue we normalize word character embeddings so that they have mean 0 and variance 1 for every entry as in ,24,"Note that the perturbation ? is generated in the direction that significantly increases the loss L. We find such ? against the current model parameterized by ? , at each training step , and construct an adversarial example by s adv = s + ? However , if we do not restrict the norm of word / character embeddings , the model could trivially learn embeddings of large norms to make the perturbations insignificant .",The normalization is performed every time we feed input embeddings into the LSTMs and generate adversarial examples .,experiment
natural_language_inference,60,"The downside of concatenating embeddings and giving that as input to an RNN encoder , however , is that the network then quickly becomes inefficient as we combine more and more embeddings .",baseline,Naive baseline,0,71,4,4,0,baseline : Naive baseline,0.36787564766839376,0.1111111111111111,0.17391304347826084,The downside of concatenating embeddings and giving that as input to an RNN encoder however is that the network then quickly becomes inefficient as we combine more and more embeddings ,31,"Concatenation is a sensible strategy for combining different embedding sets , because it provides the sentence encoder with all of the information in the individual embeddings :","For dynamic meta-embeddings , we project the embeddings into a common ddimensional space by learned linear functions",result
question-answering,1,"It is essentially the Siamese architecture introduced in , which has been applied to different tasks as a nonlinear similarity function .",model,Architecture-I (ARC-I),0,78,19,4,0,model : Architecture-I (ARC-I),0.4041450777202073,0.4130434782608696,0.5714285714285714,It is essentially the Siamese architecture introduced in which has been applied to different tasks as a nonlinear similarity function ,21,"It first finds the representation of each sentence , and then compares the representation for the two sentences with a multi - layer perceptron ( MLP ) .","Although ARC - I enjoys the flexibility brought by the convolutional sentence model , it suffers from a drawback inherited from the Siamese architecture : it defers the interaction between two sentences ( in the final MLP ) to until their individual representation matures ( in the convolution model ) , therefore runs at the risk of losing details ( e.g. , a city name ) important for the matching task in representing the sentences .",method
relation-classification,6,The mathematical formulation is the follows :,model,Entity Features with Latent Type,0,119,73,11,0,model : Entity Features with Latent Type,0.6574585635359116,0.9480519480519479,0.7333333333333333,The mathematical formulation is the follows ,7,The LET constructs the type representations by weighting K latent type vectors based on attention mechanisms .,where c i is the i - th latent type vector and K is the number of latent entity types .,method
natural_language_inference,81,"For example , for the query country of citizenship jamie burnett , the model correctly attends to the documents about Jamie Burnett being born in South Larnarkshire and about Lanarkshire being in Scotland .",analysis,ERROR ANALYSIS,0,175,22,5,0,analysis : ERROR ANALYSIS,0.4156769596199525,0.7096774193548387,0.35714285714285715,For example for the query country of citizenship jamie burnett the model correctly attends to the documents about Jamie Burnett being born in South Larnarkshire and about Lanarkshire being in Scotland ,32,The first type ( 42 % of errors ) results from the model aggregating the wrong reference .,"However it wrongly focuses on the word "" england "" in the latter document instead of the answer "" scotland "" .",result
natural_language_inference,69,KB inference methods include Inductive Logic Programming and probabilistic relaxations to logic like Markov Logic .,system description,Compositional Knowledge Base Inference,0,299,4,4,0,system description : Compositional Knowledge Base Inference,0.8666666666666667,0.16666666666666666,0.2,KB inference methods include Inductive Logic Programming and probabilistic relaxations to logic like Markov Logic ,16,Combining multiple facts is common for structured knowledge resources which formulate facts using first - order logic .,"These approaches suffer from limited coverage and inefficient inference , though efforts to circumvent sparsity have been undertaken .",method
sentiment_analysis,40,"Each comment is annotated by at least two annotators , and only if they agree with each other , the comment will be added into our dataset .",experiment,Experimental Setting,0,141,8,7,0,experiment : Experimental Setting,0.6322869955156951,0.6153846153846154,0.5833333333333334,Each comment is annotated by at least two annotators and only if they agree with each other the comment will be added into our dataset ,26,"We purposely add more negation , contrastive , and question comments to make it more challenging .","Moreover , we replace each opinion target ( i.e. word / phrase of pronoun or person name ) with a placeholder , as did in .",experiment
natural_language_inference,23,All other settings remain unchanged .,DETAILED CONFIGURATIONS IN THE ABLATION STUDY,APPLICATION TO NATURAL LANGUAGE INFERENCE,0,383,84,35,0,DETAILED CONFIGURATIONS IN THE ABLATION STUDY : APPLICATION TO NATURAL LANGUAGE INFERENCE,0.7465886939571149,0.8936170212765957,0.7777777777777778,All other settings remain unchanged ,6,This is as simple as replacing,"To incorporate fully - aware multi-level fusion into ESIM , we change the input for inference BiLSTM from",result
sentiment_analysis,41,The rest of our paper is structured as follows :,introduction,introduction,0,35,24,24,0,introduction : introduction,0.15695067264573992,0.96,0.96,The rest of our paper is structured as follows ,10,"Experimental results indicate that our approach can improve the performance compared with several baselines , and further examples demonstrate the attention mechanism works well for aspect - level sentiment classification .","Section 2 discusses related works , Section 3 gives a detailed description of our attention - based proposals , Section 4 presents extensive experiments to justify the effectiveness of our proposals , and Section 5 summarizes this work and the future direction .",introduction
relation-classification,7,The RE results of each model are shown in .,experiment,Experimental results,1,141,24,7,0,experiment : Experimental results,0.7085427135678392,0.75,0.4666666666666667,The RE results of each model are shown in ,10,"The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .","BERT achieved better performance than the state - of - the - art model on the CHEMPROT dataset , which demonstrates its effectiveness in RE .",experiment
natural_language_inference,36,The predicted semantic role Labels are defined in PropBank augmented with B - I - O tag set to represent argument spans .,model,Downstream Model,0,131,31,31,0,model : Downstream Model,0.6238095238095238,1.0,1.0,The predicted semantic role Labels are defined in PropBank augmented with B I O tag set to represent argument spans ,21,The encoded representation is then projected using a final dense layer followed by a softmax activation to form a distribution over all possible tags ., ,method
natural_language_inference,23,"Context : Imperialism has played an important role in the histories of Japan , Korea , the Assyrian Empire , the Chinese Empire , the Roman Empire , Greece , the Byzantine Empire , the Persian Empire , the Ottoman Empire , Ancient Egypt , the British Empire , India , and many other empires .",model,FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0,463,70,47,0,model : FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0.9025341130604289,0.5833333333333334,0.5529411764705883,Context Imperialism has played an important role in the histories of Japan Korea the Assyrian Empire the Chinese Empire the Roman Empire Greece the Byzantine Empire the Persian Empire the Ottoman Empire Ancient Egypt the British Empire India and many other empires ,43,ID : 573092088ab72b1400f9c598-high-conf-turk2,"Imperi-alism was a basic component to the conquests of Genghis Khan during the Mongol Empire , and of other war - lords .",method
natural_language_inference,51,"In our experiment , we let the models see the training data from the while freezing others , we force "" hard "" attention over the programs by replacing the softmax function in Eq. 5 with the Gumbel - softmax .",result,Few-shot Learning,0,185,52,5,0,result : Few-shot Learning,0.6981132075471698,0.7428571428571429,0.21739130434782608,In our experiment we let the models see the training data from the while freezing others we force hard attention over the programs by replacing the softmax function in Eq 5 with the Gumbel softmax ,36,We design an experiment similar to the Split MNIST to investigate whether NSM can improve NTM 's performance .,"Also , to ignore catastrophic forgetting in the state network , we use Feedforward controllers in the two baselines .",result
sentiment_analysis,12,"More specifically , (? ( * ) , ? ( * ) ; ? ) is an Euclidean Distance style loss that penalizes the dis agreement between ?( * ) and ? ( * ) .",approach,Model Training with Attention Supervision Information,0,129,64,3,0,approach : Model Training with Attention Supervision Information,0.5758928571428571,0.8888888888888888,0.2727272727272727,More specifically is an Euclidean Distance style loss that penalizes the dis agreement between and ,16,"To exploit the above extracted context words to refine the training of attention mechanisms for ASC models , we propose a soft attention regularizer (? ( s a ( x ) ? s m ( x ) ) , ? ( s a ( x ) ? s m ( x ) ) ; ? ) to jointly minimize the standard training objective , where ?( * ) and ? ( * ) denotes the model - induced and expected attention weight distributions of words in s a ( x ) ?s m ( x ) , respectively .","As previously analyzed , we expect to equally continue focusing on the context words of s a ( x ) during the final model training .",method
sentiment_analysis,18,"After incorporating the two proposed approaches into the attention - based LSTM , our final model is illustrated in .",model,Overall Architecture and Training Objective,0,128,62,2,0,model : Overall Architecture and Training Objective,0.5355648535564853,0.8378378378378378,0.14285714285714285,After incorporating the two proposed approaches into the attention based LSTM our final model is illustrated in ,18, ,The attention - based LSTM component is associated with the categorical cross entropy loss of sentiment classification .,method
semantic_parsing,2,"In the following , we will explain how p ( a|x ) and p ( y|x , a ) are estimated .",system description,Problem Formulation,0,72,17,17,0,system description : Problem Formulation,0.24742268041237114,0.3148148148148148,1.0,In the following we will explain how p a x and p y x a are estimated ,18,"where a <t = a 1 a t?1 , and y <t = y 1 y t?1 .", ,method
text_generation,5,"Further , we conduct an in - depth investigation of the use of VAE ( with our new decoding architecture ) for semi-supervised and unsupervised labeling tasks , demonstrating gains over several strong baselines .",abstract,abstract,0,11,9,9,0,abstract : abstract,0.03728813559322034,1.0,1.0,Further we conduct an in depth investigation of the use of VAE with our new decoding architecture for semi supervised and unsupervised labeling tasks demonstrating gains over several strong baselines ,31,"We demonstrate perplexity gains on two datasets , representing the first positive language modeling result with VAE .", ,abstract
natural_language_inference,52,"For IR , these were the highest - scoring retrieved documents , and for our system , these were 11 Another difference between our system and that of the DAN baseline is our usage of a text justification .",performance,Justification Performance,0,201,31,5,0,performance : Justification Performance,0.7701149425287356,0.6326530612244898,0.35714285714285715,For IR these were the highest scoring retrieved documents and for our system these were 11 Another difference between our system and that of the DAN baseline is our usage of a text justification ,35,"For each question , we assessed the quality of each of the top five justifications .","However , we suspect this difference is not the source of the performance difference : see , where a variant of the DAN baseline that included an averaged representation of a justification alongside the averaged representations of the question and answer failed to show a performance increase .",result
text_summarization,6,"For the attention based sequence decoding process , RAS - Elman selects Elman RNN as decoder , and RAS - LSTM selects Long Short - Term Memory architecture .",method,Comparative Methods,0,203,16,16,0,method : Comparative Methods,0.7748091603053435,0.8,0.8,For the attention based sequence decoding process RAS Elman selects Elman RNN as decoder and RAS LSTM selects Long Short Term Memory architecture ,24,RAS - LSTM and RAS - Elman both consider words and word positions as input and use convolutional encoders to handle the source information .,LenEmb uses a mechanism to control the summary length by considering the length embedding vector as the input .,method
natural_language_inference,23,Note that our proposed symmetric form with nonlinearity should be used to guarantee the boost .,DETAILED CONFIGURATIONS IN THE ABLATION STUDY,DETAILED CONFIGURATIONS IN THE ABLATION STUDY,0,313,14,14,0,DETAILED CONFIGURATIONS IN THE ABLATION STUDY : DETAILED CONFIGURATIONS IN THE ABLATION STUDY,0.6101364522417154,0.14893617021276595,0.42424242424242425,Note that our proposed symmetric form with nonlinearity should be used to guarantee the boost ,16,The performance of FA High - Level can already outperform many state - of - the - art models in the literature .,FA All - Level .,result
question_answering,5,The human performance is evaluated in a similar way to the Quasar - T dataset .,dataset,dataset,0,157,9,9,0,dataset : dataset,0.5688405797101449,0.9,0.9,The human performance is evaluated in a similar way to the Quasar T dataset ,15,questions and uses Google to collect about 50 web page snippets as passages for each question .,TriviaQA ( Open-Domain Setting ) 7,experiment
natural_language_inference,77,"It is motivated on the one hand by the intuition that question tokens which rarely appear in the context are more likely to be important for answering the question , and on the other hand by the fact that question words might occur as morphological variants , synonyms or related words in the context .",system description,Context Matching,0,63,31,9,0,system description : Context Matching,0.2307692307692308,0.4246575342465753,0.6923076923076923,It is motivated on the one hand by the intuition that question tokens which rarely appear in the context are more likely to be important for answering the question and on the other hand by the fact that question words might occur as morphological variants synonyms or related words in the context ,53,"The wiq w j feature for context word x j is defined in Eq. 3 , where Eq. 2 defines a basic similarity score between q i and x j based on their word - embeddings .",The latter can be captured ( softly ) by using word embeddings instead of the words themselves whereas the former is captured by the application of the softmax operation in Eq. 3 which ensures that infrequent occurrences of words are weighted more heavily .,method
relation_extraction,4,The resulting representation is then fed into a fully - connected layer followed by a softmax layer for relation classification .,model,model,0,107,12,12,0,model : model,0.5169082125603864,0.5217391304347826,0.5217391304347826,The resulting representation is then fed into a fully connected layer followed by a softmax layer for relation classification ,20,"This model learns a representation of the input sentence , by first running a series of convolutional operations on the sentence with various filters , and then feeding the output into a max - pooling layer to reduce the dimension .","As an extension , positional embeddings are also introduced into this model to better capture the relative position of each word to the subject and object entities and were shown to achieve improved results .",method
semantic_parsing,0,Models need to select them and fill them into the right slots in their predicted SQL .,evaluation,Exact Matching,0,200,24,10,0,evaluation : Exact Matching,0.7194244604316546,0.6857142857142857,0.6666666666666666,Models need to select them and fill them into the right slots in their predicted SQL ,17,"Instead of generating these values , a list of gold values for each question is given .",We exclude value prediction in Component and Exact Matching evaluations and do not provide Execution Accuracy in the current version .,result
natural_language_inference,42,"However , we introduce a layer that processes embeddings and encodings separately before summing them up .",system description,Encoder,0,115,75,13,0,system description : Encoder,0.3979238754325259,0.42857142857142855,0.8666666666666667,However we introduce a layer that processes embeddings and encodings separately before summing them up ,16,"Indeed , in , the final vectorial representation of apiece of text is defined by the sum of the embeddings ? with the position encoding E , which would require d model = d input .","Because we also use this layer to reduce the embedding size from d input to d model , we named it "" reduction layer "" .",method
natural_language_inference,81,This section includes attention maps produced by the CFC on the development split of WikiHop .,APPENDIX,ATTENTION MAPS,0,225,17,2,0,APPENDIX : ATTENTION MAPS,0.5344418052256532,0.07981220657276995,0.03333333333333333,This section includes attention maps produced by the CFC on the development split of WikiHop ,16, ,"We include the fine - grain mention self - attention and coattention , the coarse - grain summary selfattention , and the document self - attention and coattention for the top scoring supporing documents , ranked by the summary self - attention score .",others
question_answering,5,"In our implementation an "" aspect "" of the question is a hidden state of a bi-directional LSTM .",method,EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0,88,37,8,0,method : EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0.3188405797101449,0.4021739130434783,0.12698412698412698,In our implementation an aspect of the question is a hidden state of a bi directional LSTM ,18,"As in the examples shown in ( b ) , we hope the textual entailment model will reflect ( i ) how each aspect of the question is matched by the union of multiple passages ; and ( ii ) whether all the aspects of the question can be matched by the union of multiple passages .",The match - LSTM model is one way to achieve the above effect in entailment .,method
natural_language_inference,43,This shows the importance of using the redundancy of the web for our QA system .,experiment,Experiments,0,134,34,34,0,experiment : Experiments,0.8645161290322579,1.0,1.0,This shows the importance of using the redundancy of the web for our QA system ,16,"Note that TF - IDF is by far the most impactful feature , leading to a large drop of 12 points in performance .", ,experiment
relation_extraction,12,"Interestingly , while deeper GCNs can capture richer neighborhood information of a graph , empirically it has been observed that the best performance is achieved with a 2 - layer model .",introduction,introduction,0,44,29,29,0,introduction : introduction,0.1337386018237082,0.725,0.725,Interestingly while deeper GCNs can capture richer neighborhood information of a graph empirically it has been observed that the best performance is achieved with a 2 layer model ,29,shallow GCN model may not be able to capture non-local interactions of large graphs .,"With the help of dense connections , we are able to train the AGGCN model with a large depth , allowing rich local and non-local dependency information to be captured .",introduction
phrase_grounding,0,"In their attention mechanism , they add a dummy location in attention map when no region or word the model should attend along with a softmax .",system description,Attention mechanisms,0,60,27,5,0,system description : Attention mechanisms,0.26666666666666666,0.675,0.7142857142857143,In their attention mechanism they add a dummy location in attention map when no region or word the model should attend along with a softmax ,26,dense co-attention mechanism is explored in to solve the Visual Question Answering task by using a fully symmetric architecture between visual and language representations .,"In AttnGAN , a deep attention multimodal similarity model is proposed to compute a fine - grained image - text matching loss .",method
named-entity-recognition,9,"For all the datasets , we used the same dataset splits used in previous works ) for a fair evaluation ; however , the splits of LINAAEUS and Species - 800 could not be found from and maybe different .",dataset,Datasets,0,112,15,15,0,dataset : Datasets,0.5628140703517588,0.75,0.75,For all the datasets we used the same dataset splits used in previous works for a fair evaluation however the splits of LINAAEUS and Species 800 could not be found from and maybe different ,35,We have made the pre-processed BioASQ datasets publicly available .,"Like previous work , we reported the performance of 10 - fold cross-validation on datasets that do not have separate test sets ( e.g. GAD , EU - ADR ) .",experiment
machine-translation,5,"In order to aid the NMT model in translating such tokens better , we extracted named entity and non-translatable token dictionaries from the parallel corpora .",system,Automatic Post-editing of Named Entities,0,107,10,4,0,system : Automatic Post-editing of Named Entities,0.7482517482517482,0.3448275862068966,0.3636363636363637,In order to aid the NMT model in translating such tokens better we extracted named entity and non translatable token dictionaries from the parallel corpora ,26,"Named entities and non-translatable entities ( various product names , identifiers , etc. ) are often rare or unknown .","This was done by performing word alignment of the parallel corpora using fast align and searching ( in a language - agnostic manner ) for transliterated source - target word pairs using a similarity metric based on Levenshtein distance , which start with upper-case letters .",method
question_answering,1,"Hence we obtain a matrix M ? R 2 d T , which is passed onto the output layer to predict the answer .",model,Attention Flow,0,86,54,45,0,model : Attention Flow,0.2712933753943217,0.5346534653465347,0.6521739130434783,Hence we obtain a matrix M R 2 d T which is passed onto the output layer to predict the answer ,22,"We use two layers of bi-directional LSTM , with the output size of d for each direction .","Hence we obtain a matrix M ? R 2 d T , which is passed onto the output layer to predict the answer .",method
sentiment_analysis,0,"Second , the newly proposed model , MDRE , shows a substantial performance gain .",evaluation,Performance evaluation,1,147,10,10,0,evaluation : Performance evaluation,0.8258426966292135,0.4761904761904762,0.4761904761904762,Second the newly proposed model MDRE shows a substantial performance gain ,12,"From this result , we note that textual data are informative in emotion prediction tasks , and the recurrent encoder model is effective in understanding these types of sequential data .",It thus achieves the state - of - the - art performance with a WAP value of 0.718 .,result
natural_language_inference,95,The BiDAF and Match - LSTM models are provided as two baseline systems .,result,Results on DuReader,0,163,3,3,0,result : Results on DuReader,0.6965811965811965,0.1875,0.1875,The BiDAF and Match LSTM models are provided as two baseline systems ,13,The results of our model and several baseline systems on the test set of DuReader are shown in .,"Based on BiDAF , as is described in Section 3.2 , we tried a new paragraph selection strategy by employing a paragraph ranking ( PR ) model .",result
sentiment_analysis,0,"On the other hand , the TRE model shows higher performance gain compared to the ARE .",evaluation,Performance evaluation,1,145,8,8,0,evaluation : Performance evaluation,0.8146067415730337,0.3809523809523809,0.3809523809523809,On the other hand the TRE model shows higher performance gain compared to the ARE ,16,"First , our ARE model shows the baseline performance because we use minimal audio features , such as the MFCC and prosodic features with simple architectures .","From this result , we note that textual data are informative in emotion prediction tasks , and the recurrent encoder model is effective in understanding these types of sequential data .",result
machine-translation,6,"The proposed learning framework includes a task - specific predictor and a discriminator , whose function is to classify rare and popular words .",model,Loss,0,89,5,4,0,model : Loss,0.30584192439862545,0.3333333333333333,0.2857142857142857,The proposed learning framework includes a task specific predictor and a discriminator whose function is to classify rare and popular words ,22,Loss predict predict :,Both modules use word embeddings as the input .,method
sentiment_analysis,2,"For aspect term menu , the sentiment polarity is positive , but for aspect term server , the polarity is negative while for specials , the polarity is neutral .",introduction,introduction,0,22,6,6,0,introduction : introduction,0.09691629955947137,0.18181818181818185,0.18181818181818185,For aspect term menu the sentiment polarity is positive but for aspect term server the polarity is negative while for specials the polarity is neutral ,26,"For instance , given the mentioned aspect terms { menu , server , specials } , and the sentence is "" The menu looked good , except for offering the Chilean Sea Bass , but the server does not offer up the specials that were written on the board outside . "" .",One important challenge in aspect - level sentiment analysis is how to model the semantic relationship between aspect terms and sentences .,introduction
sentiment_analysis,3,"However , not all related concepts are equal in detecting emotions given the conversational context .",model,Dynamic Context-Aware Affective Graph Attention,0,120,55,14,0,model : Dynamic Context-Aware Affective Graph Attention,0.410958904109589,0.4508196721311475,0.3043478260869565,However not all related concepts are equal in detecting emotions given the conversational context ,15,standard graph attention mechanism computes wk by feeding t and ck into a single - layer feedforward neural network .,"In our model , we make the assumption that important concepts are those that relate to the conversational context and have strong emotion intensity .",method
sentiment_analysis,42,"In a similar way , we stack multiple hops and run these steps multiple times , so that more abstractive evidences could be selected from the external memory m .",system description,An Overview of the Approach,0,79,27,14,0,system description : An Overview of the Approach,0.3134920634920635,0.4354838709677419,0.8235294117647058,In a similar way we stack multiple hops and run these steps multiple times so that more abstractive evidences could be selected from the external memory m ,28,The output of attention layer and the linear transformation of aspect vector 2 are summed and the result is considered as the input of next layer ( hop 2 ) .,"The output vector in last hop is considered as the representation of sentence with regard to the aspect , and is further used as the feature for aspect level sentiment classification .",method
part-of-speech_tagging,0,"For example , in Romanian , the baseline model starts to increase development loss after 1,000 iterations even with dropout , whereas the AT model keeps improving until 2,500 iterations , achieving notably lower development loss ( 0.4 down ) .",result,Results,0,149,12,12,0,result : Results,0.6056910569105691,0.8571428571428571,0.8571428571428571,For example in Romanian the baseline model starts to increase development loss after 1 000 iterations even with dropout whereas the AT model keeps improving until 2 500 iterations achieving notably lower development loss 0 4 down ,38,"For all the three languages , we can observe that the AT model ( red solid line ) prevents overfitting better than the baseline ( black dotted line ) , and this advantage is more significant in low resource languages .",These results illustrate that AT can prevent overfitting especially well on small datasets and can augment the regularization power beyond dropout .,result
semantic_role_labeling,3,We observe a slightly performance drop when using constrained decoding .,model,model,0,214,21,21,0,model : model,0.8106060606060606,0.31343283582089554,0.9130434782608696,We observe a slightly performance drop when using constrained decoding ,11,show the effects of constrained decoding ) on top of DEEPATT with FFN sub - layers .,"Moreover , adding constrained decoding slowdown the decoding speed significantly .",method
relation-classification,1,"To evaluate the performance of our methods , we use the public dataset NYT 2 which is produced by distant supervision method .",dataset,dataset,0,139,2,2,0,dataset : dataset,0.5650406504065041,0.3333333333333333,0.3333333333333333,To evaluate the performance of our methods we use the public dataset NYT 2 which is produced by distant supervision method ,22, ,large amount of training data can be obtained by means of distant supervision methods without manually labeling .,experiment
natural_language_inference,8,The TRAIN - ALL training set-significantly larger than any of the other parts of the data - is noisy as answers were labelled automatically using pattern matching .,dataset,dataset,0,144,8,8,0,dataset : dataset,0.6824644549763034,0.6666666666666666,0.6666666666666666,The TRAIN ALL training set significantly larger than any of the other parts of the data is noisy as answers were labelled automatically using pattern matching ,27,"summarises the answer selection dataset , and describes the train / dev / test split of the data .","The task is to rank the candidate answers based on their relatedness to the question , and is thus measured in Mean Average Precision ( MAP ) and Mean Reciprocal Rank ( MRR ) , which are standard metrics in Information Retrieval and Question Answering .",experiment
natural_language_inference,70,"Longest common subsequence measure ( Allison and Dix , 1986 ) drops the contiguity requirement of the previous measure and allows to detect similarity in case of word insertions / deletions .",system description,Intra-pair similarities,0,58,29,12,1,system description : Intra-pair similarities,0.3314285714285714,0.3118279569892473,0.631578947368421,Longest common subsequence measure Allison and Dix 1986 drops the contiguity requirement of the previous measure and allows to detect similarity in case of word insertions deletions ,28,Longest common substring measure determines the length of the longest contiguous sequence of characters shared by two text segments .,Greedy String Tiling provides a similarity between two sentences by counting the number of shuffles in their subparts .,method
semantic_parsing,2,"Configuration Model hyperparameters were cross - validated on the training set for GEO , and were validated on the development split for the other datasets .",experiment,Experimental Setup,0,236,9,6,0,experiment : Experimental Setup,0.8109965635738832,0.42857142857142855,0.3333333333333333,Configuration Model hyperparameters were cross validated on the training set for GEO and were validated on the development split for the other datasets ,24,"WIK - ISQL was preprocessed by the script provided by , where inputs were lowercased and tokenized by Stanford CoreNLP .","Dimensions of hidden vectors and word embeddings were selected from { 250 , 300 } and { 150 , 200 , 250 , 300 } , respectively .",experiment
semantic_role_labeling,1,"For fair comparison we use GloVe embeddings , provide predicate indicator embeddings on the input and reencode the sequence relative to each gold predicate .",experiment,Semantic role labeling,0,173,24,4,0,experiment : Semantic role labeling,0.7935779816513762,0.6,0.3076923076923077,For fair comparison we use GloVe embeddings provide predicate indicator embeddings on the input and reencode the sequence relative to each gold predicate ,24,To compare to more prior work we also evaluate our models in the artificial setting where gold predicates are provided at test time .,"Here LISA still excels : with D&M parses , LISA out - performs the previous state - of - the - art by more than 2 F1 on both WSJ and Brown .",experiment
natural_language_inference,33,We also observe that comparing sentences using For MRPC and STSB we consider only the F 1 score and Spearman correlations respectively and we also multiply the SICK - R scores by 100 to have all differences in the same scale .,evaluation,EXPERIMENTAL RESULTS & DISCUSSION,0,153,29,19,0,evaluation : EXPERIMENTAL RESULTS & DISCUSSION,0.5862068965517241,0.2116788321167883,0.6129032258064516,We also observe that comparing sentences using For MRPC and STSB we consider only the F 1 score and Spearman correlations respectively and we also multiply the SICK R scores by 100 to have all differences in the same scale ,41,"In Appendix , we note that our sentence representations outperform skip - thoughts and are on par with Infersent for image - caption retrieval .",Bold numbers indicate the best performing transfer model on a given task .,result
question_generation,1,"During inference , we sample a question word q i from the softmax distribution and continue sampling until the end token or maximum length for the question is reached .",method,Method,0,97,17,17,0,method : Method,0.2474489795918368,0.1868131868131868,0.8947368421052632,During inference we sample a question word q i from the softmax distribution and continue sampling until the end token or maximum length for the question is reached ,29,"Our model contains three main modules , ( a ) Representation Module that extracts multimodal features ( b ) Mixture Module that fuses the multimodal representation and ( c ) Decoder that generates question using an LSTM - based language model .",We experimented with both sampling and argmax and found out that argmax works better .,method
natural_language_inference,79,Paragraph Vector without word ordering :,model,Paragraph Vector without word ordering: Distributed bag of words,0,123,43,1,0,model : Paragraph Vector without word ordering: Distributed bag of words,0.45895522388059706,0.7818181818181819,0.07692307692307693,Paragraph Vector without word ordering ,6, , ,method
question_answering,1,The phrase is derived by predicting the start and the end indices of the phrase in the paragraph .,model,Attention Flow,0,95,63,54,0,model : Attention Flow,0.2996845425867508,0.6237623762376238,0.7826086956521741,The phrase is derived by predicting the start and the end indices of the phrase in the paragraph ,19,The QA task requires the model to find a sub-phrase of the paragraph to answer the query .,We obtain the probability distribution of the start index over the entire paragraph by,method
sentiment_analysis,16,Based on the above example snippets or phrases we have four corresponding inequalities :,system description,The Proposed Approaches,0,100,48,3,0,system description : The Proposed Approaches,0.31446540880503143,0.6857142857142857,0.12,Based on the above example snippets or phrases we have four corresponding inequalities ,14,indicates a positive sentiment and s < 0 indicates a negative sentiment .,"We can drop all ? terms here as they all equal to 1 , i.e. , they are the only context word in the snippets to attend to ( the target words are not contexts ) .",method
topic_models,0,This is the same as in our proposed Bayesian SMM ( 1 ) .,model,model,0,217,27,27,0,model : model,0.5266990291262136,0.7297297297297297,0.7297297297297297,This is the same as in our proposed Bayesian SMM 1 ,12,"The generative process for a document in CTM is same as in LDA , except for document vectors are now drawn from Gaussian , i.e. , In this formulation , the document embeddings ? are no longer in the ( K ? 1 ) simplex , rather they are dependent through the logistic normal .",The advantage is that the document vectors can model the correlations in topics .,method
natural_language_inference,23,"Meanwhile , we verify the generalization of Fusion - Net with two adversarial SQuAD datasets and it sets up the new state - of - the - art on both datasets : on AddSent , FusionNet increases the best F1 metric from 46.6 % to 51.4 % ; on AddOneSent , FusionNet boosts the best F1 metric from 56.0 % to 60.7 % .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.017543859649122806,0.5,0.5,Meanwhile we verify the generalization of Fusion Net with two adversarial SQuAD datasets and it sets up the new state of the art on both datasets on AddSent FusionNet increases the best F1 metric from 46 6 to 51 4 on AddOneSent FusionNet boosts the best F1 metric from 56 0 to 60 7 ,55,"We apply FusionNet to the Stanford Question Answering Dataset ( SQuAD ) and it achieves the first position for both single and ensemble model on the official SQuAD leaderboard at the time of writing ( Oct. 4th , 2017 ) .",broad - coverage challenge corpus for sentence understanding through inference .,abstract
relation_extraction,2,"It has also been applied to the SQuAD question answering problem , in which the objective is to find the starting point and ending point of an answer span .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.17037037037037034,0.5909090909090909,0.5909090909090909,It has also been applied to the SQuAD question answering problem in which the objective is to find the starting point and ending point of an answer span ,29,The tasks that BERT has been applied to are typically modeled as classification problems and sequence labeling problems .,"As far as we know , the pretrained BERT model has not been applied to relation classification , which relies not only on the information of the whole sentence but also on the information of the specific target entities .",introduction
named-entity-recognition,6,"For instance , "" dammstadt "" , which appears only 5 times in WiFiNE , and which refers to the Damm city in Germany , is most similar to / location / city and / location / railway .",method,Word,0,91,40,7,0,method : Word,0.429245283018868,0.8,0.6363636363636364,For instance dammstadt which appears only 5 times in WiFiNE and which refers to the Damm city in Germany is most similar to location city and location railway ,29,"Also , we observe that entity words thatare either not or rarely annotated in WiFiNE are still adequately associated with their right type .","Interestingly , this mention does not have its page in English Wikipedia .",method
machine-translation,9,The baseline embeddings can be a set of pre-trained vectors such as word2vec or GloVe embeddings .,introduction,introduction,0,49,36,36,0,introduction : introduction,0.17073170731707318,0.6792452830188679,0.6792452830188679,The baseline embeddings can be a set of pre trained vectors such as word2vec or GloVe embeddings ,18,where | V | is the vocabulary size .,"In Eq. 3 , the baseline embedding matrix ? is approximated by M codewords selected from M codebooks .",introduction
text_summarization,10,"We use the publicly - available pretrained model from 's github for these DUC transfer results , which produces the results in .",result,Entailment Generation,0,180,5,3,0,result : Entailment Generation,0.6844106463878327,0.3333333333333333,0.2307692307692308,We use the publicly available pretrained model from s github for these DUC transfer results which produces the results in ,21,"We use the same architecture as described in Sec. 3.1 with pointer mech - 7 Note that we did not have output files of any previous work 's model on Gigaword ; however , our baseline is already a strong state - of - the - art model as shown in .",All other comparisons and analysis in our paper are based on their higher results .,result
natural_language_inference,5,"However , most previous answer-selection studies have employed small datasets compared with the large datasets employed for other natural language processing ( NLP ) tasks .",introduction,introduction,0,17,9,9,0,introduction : introduction,0.11564625850340135,0.39130434782608703,0.39130434782608703,However most previous answer selection studies have employed small datasets compared with the large datasets employed for other natural language processing NLP tasks ,24,This task has been extensively investigated by researchers because it is a fundamental task that can be applied to other QA - related tasks .,"Therefore , * Work conducted while the author was an intern at Adobe Research .",introduction
question_generation,0,"Concretely , the encoder reads not only the input sentence , but also the answer position indicator and lexical features .",introduction,introduction,1,18,10,10,0,introduction : introduction,0.0989010989010989,0.5882352941176471,0.5882352941176471,Concretely the encoder reads not only the input sentence but also the answer position indicator and lexical features ,19,The Neural Question Generation framework extends the sequence - to - sequence models by enriching the encoder with answer and lexical features to generate answer focused questions .,"The answer position feature denotes the answer span in the input sentence , which is essential to generate answer relevant questions .",introduction
relation-classification,4,"have proven to be very effective in relation extraction , because they capture long - range syntactic relations that are obscure from the surface form alone ( e.g. , when long clauses or complex scoping are present ) .",introduction,introduction,1,18,7,7,0,introduction : introduction,0.06870229007633588,0.2916666666666667,0.2916666666666667,have proven to be very effective in relation extraction because they capture long range syntactic relations that are obscure from the surface form alone e g when long clauses or complex scoping are present ,35,"Note that negation ( "" not "" ) is off the dependency path .",Traditional feature - based models are able to represent dependency information by featurizing dependency trees as overlapping paths along the trees .,introduction
natural_language_inference,55,illustrates our model .,system description,Representing Candidate Answers,0,91,66,10,0,system description : Representing Candidate Answers,0.6190476190476191,1.0,1.0,illustrates our model ,4,We thus adopt the subgraph approach ., ,method
question-answering,6,"On the other hand , vector gates in bAbI story - based QA 10 k dataset sometimes help .",model,RESULTS.,1,234,54,20,0,model : RESULTS.,0.7090909090909091,0.6835443037974683,0.4444444444444444,On the other hand vector gates in bAbI story based QA 10 k dataset sometimes help ,17,"c ) Including vector gates hurts in 1 k datasets , as the model either overfits to the training data or converges to local minima .","d ) Increasing the dimension of the hidden state to 100 in the dialog 's Task 6 ( DSTC2 ) helps , while there is not much improvement in the dialog 's Task 1 - 5 .",method
natural_language_inference,54,"Whose role is to design the works , prepare the specifications and produce construction drawings , administer the contract , tender the works , and manage the works from inception to completion ? On the other hand , a dependency tree is constructed based on the dependency structure of a sentence .",introduction,introduction,0,20,13,13,0,introduction : introduction,0.08733624454148471,0.5909090909090909,0.5909090909090909,Whose role is to design the works prepare the specifications and produce construction drawings administer the contract tender the works and manage the works from inception to completion On the other hand a dependency tree is constructed based on the dependency structure of a sentence ,46,"Utilizing the knowledge of a constituency relations , we can reduce the size of the candidate space and help the algorithm to identify the correct answer .","Whose role is to design the works , prepare the specifications and produce construction drawings , administer the contract , tender the works , and manage the works from inception to completion ? On the other hand , a dependency tree is constructed based on the dependency structure of a sentence .",introduction
text_summarization,10,We teach these skills to the abstractive summarization model via multi-task training with two related auxiliary tasks : question generation task and entailment generation .,model,Two Auxiliary Tasks,0,72,27,3,0,model : Two Auxiliary Tasks,0.2737642585551331,0.2903225806451613,1.0,We teach these skills to the abstractive summarization model via multi task training with two related auxiliary tasks question generation task and entailment generation ,25,"Despite the strengths of the strong model described above with attention , pointer , and coverage , a good summary should also contain maximal salient information and be a directed logical entailment of the source document .", ,method
natural_language_inference,49,The result is competitive among other sentence - encoding based model .,ablation,ablation,0,198,13,13,0,ablation : ablation,0.7795275590551181,0.4814814814814815,0.4814814814814815,The result is competitive among other sentence encoding based model ,11,We obtain 73.2 for matched score and 73.6 on mismatched data .,We further study how encoding layer contribute in enriching the feature space in interaction tensor .,result
natural_language_inference,28,"By feeding inputs to RUM , ? adapts to encode rotations , which align the hidden states in desired locations in RN h , without changing the norm of h .",architecture,THE RUM ARCHITECTURE,0,137,39,39,0,architecture : THE RUM ARCHITECTURE,0.5055350553505535,0.951219512195122,0.951219512195122,By feeding inputs to RUM adapts to encode rotations which align the hidden states in desired locations in RN h without changing the norm of h ,27,Instead we suggest utilizing additional memory via the target vector ? .,"By feeding inputs to RUM , ? adapts to encode rotations , which align the hidden states in desired locations in RN h , without changing the norm of h .",method
sentiment_analysis,32,"In IAN , we need to optimize all the parameters notated as ? which are from LSTM networks :",training,Model Training,0,98,2,2,0,training : Model Training,0.4260869565217391,0.125,0.125,In IAN we need to optimize all the parameters notated as which are from LSTM networks ,17, ,"In IAN , we need to optimize all the parameters notated as ? which are from LSTM networks :",experiment
relation_extraction,3,"dz is the dimension of BERT embedding at each token position , and l is the number of relation labels .",approach,Structured Prediction with BERT for MRE,0,43,16,11,0,approach : Structured Prediction with BERT for MRE,0.3138686131386861,0.4571428571428571,1.0,dz is the dimension of BERT embedding at each token position and l is the number of relation labels ,20,where W L ? R 2 dzl ., ,method
text_summarization,10,"The task of question generation is to generate a question from a given input sentence , which in turn is related to the skill of being able to find the important salient information to ask questions about .",model,Question Generation,0,74,29,2,0,model : Question Generation,0.2813688212927757,0.3118279569892473,0.10526315789473684,The task of question generation is to generate a question from a given input sentence which in turn is related to the skill of being able to find the important salient information to ask questions about ,37, ,"First the model has to identify the important information present in the given sentence , then it has to frame ( generate ) a question based on this salient information , such that , given the sentence and the question , one has to be able to predict the correct answer ( salient information in this case ) .",method
natural_language_inference,31,"The input of the n - th block x ( n ) ( n ? 2 ) , is the concatenation of the input of the first block x ( 1 ) and the summation of the output of previous two blocks ( denoted by rectangles with diagonal stripes in ) :",approach,Augmented Residual Connections,0,66,30,8,0,approach : Augmented Residual Connections,0.2391304347826087,0.46875,0.7272727272727273,The input of the n th block x n n 2 is the concatenation of the input of the first block x 1 and the summation of the output of previous two blocks denoted by rectangles with diagonal stripes in ,41,"The input of the first block x ( 1 ) , as mentioned before , is the output of the embedding layer ( denoted by blank rectangles in ) .",where [ ; ] denotes the concatenation operation .,method
natural_language_inference,70,"We derived 250 dimensional vectors for 37,000 words by applying the settings min-count = 50 , window = 5 , iter = 10 and negative = 10 .",system description,Intra-pair similarities,0,61,32,15,0,system description : Intra-pair similarities,0.3485714285714286,0.3440860215053764,0.7894736842105263,We derived 250 dimensional vectors for 37 000 words by applying the settings min count 50 window 5 iter 10 and negative 10 ,24,Cosine similarity between additive representations of word embeddings generated by applying word2vec to the entire Qatar Living corpus from SemEval 2015 3 .,"Five features are derived considering ( i ) only nouns , ( ii ) only adjectives , ( iii ) only verbs , ( iv ) only adverbs and ( v ) all the above words .",method
sentiment_analysis,24,"where T denotes the maximum number of iterations in the message passing mechanism , Na denotes the total number of aspect - level training instances , n i denotes the number of tokens contained in the ith training instance , and y ae i , j ( y as i , j ) denotes the one - hot encoding of the gold label for AE ( AS ) .",method,Learning,0,133,78,5,0,method : Learning,0.4276527331189711,0.582089552238806,0.08196721311475409,where T denotes the maximum number of iterations in the message passing mechanism Na denotes the total number of aspect level training instances n i denotes the number of tokens contained in the ith training instance and y ae i j y as i j denotes the one hot encoding of the gold label for AE AS ,58,"When trained on aspect - level instances , the loss function is as follows :",is the cross - entropy loss applied to each token .,method
relation_extraction,13,C e 1 worked at Bell Labs alongside e 3 creators Ken Thompson and Dennis Ritchie .,system description,rB,0,144,26,4,0,system description : rB,0.676056338028169,0.43333333333333335,0.5,C e 1 worked at Bell Labs alongside e 3 creators Ken Thompson and Dennis Ritchie ,17,1's Ratfor was eventually put in the public domain .,"Mentions e 1 = Brian Kernighan , e 2 = Software Tools , e3 = Unix ( 1 ) ? e 1 ,e 1 ? e 2 ,e 2 log p ( l = 1 |r , r ) + ( 1 ? ? e 1 ,e 1 ? e 2 ,e 2 ) log ( 1 ? p ( l = 1|r , r ) )",method
relation_extraction,4,"For the example in , the shortest dependency path between the two entities is :",model,model,0,113,18,18,0,model : model,0.5458937198067633,0.7826086956521741,0.7826086956521741,For the example in the shortest dependency path between the two entities is ,14,The intuition is to eliminate tokens that are potentially less relevant to the classification of the relation .,We follow the SDP - LSTM model proposed by .,method
text_summarization,5,"Template based summarization ( e.g. , ) is a traditional approach to abstractive summarization .",introduction,introduction,0,22,13,13,0,introduction : introduction,0.0873015873015873,0.3611111111111111,0.3611111111111111,Template based summarization e g is a traditional approach to abstractive summarization ,13,"Therefore , we argue that , the free generation based on the source sentence is not enough for a seq2seq model .","In general , a template is an incomplete sentence which can be filled with the input text using the manually defined rules .",introduction
text_summarization,10,caley thistle went onto win the game 3 - 2 after extra - time and denied rory delia ? men the chance to secure a domestic treble this season .,abstract,abstract,0,227,8,8,0,abstract : abstract,0.8631178707224335,0.1951219512195122,0.1951219512195122,caley thistle went onto win the game 3 2 after extra time and denied rory delia men the chance to secure a domestic treble this season ,27,the hoops were left outraged by referee steven mclean ? failure to award a penalty or red card for a clear handball in the box by josh meekings to deny leigh griffith ? goal - bound shot during the first - half .,caley thistle went onto win the game 3 - 2 after extra - time and denied rory delia ? men the chance to secure a domestic treble this season .,abstract
question_generation,1,Are captions necessary for our method ? This is not actually necessary .,ablation,How are exemplars improving Embedding,0,326,76,14,0,ablation : How are exemplars improving Embedding,0.8316326530612245,0.5352112676056338,0.2295081967213115,Are captions necessary for our method This is not actually necessary ,12,"This suggests that with random exemplar , the model learns to ignore the cue .",Are captions necessary for our method ? This is not actually necessary .,result
text-classification,0,"Lg "" stands for "" large "" and "" Sm "" stands for "" small "" .",method,Large-scale Datasets and Results,0,160,58,18,0,method : Large-scale Datasets and Results,0.7048458149779736,0.4873949579831933,0.4,Lg stands for large and Sm stands for small ,10,Numbers are in percentage .,"w2 v "" is an abbreviation for "" word2vec "" , and "" Lk "" for "" lookup DBPedia ontology dataset .",method
natural_language_inference,19,visualizes the maximum value of this final output vector .,result,SNLI Mix,0,232,60,34,0,result : SNLI Mix,0.9098039215686274,0.7228915662650602,0.5964912280701754,visualizes the maximum value of this final output vector ,10,"That is , the final output of position - wise ffn for input x is the d e - dimensional vector of LayerNorm ( x + FFN ( x ) ) .","In , keywords with a high deactivation ratio is shown in panel ( a ) and a high final max value in panel ( b ) .",result
relation_extraction,9,"Instead of regular pooling , we rely on an attention - based pooling strategy to determine the importance of individual windows in R * , as encoded by the convolutional kernel .",model,Convolutional Max-Pooling with Secondary Attention,0,126,79,8,0,model : Convolutional Max-Pooling with Secondary Attention,0.6028708133971292,0.8977272727272727,0.4705882352941176,Instead of regular pooling we rely on an attention based pooling strategy to determine the importance of individual windows in R as encoded by the convolutional kernel ,28,Attention - Based Pooling .,Some of these windows could represent meaningful n-grams in the input .,method
question_generation,1,"The triplet network consists of three sub-parts : target , supporting , and contrasting networks .",method,Representation Module,0,113,33,4,0,method : Representation Module,0.2882653061224489,0.3626373626373626,0.5,The triplet network consists of three sub parts target supporting and contrasting networks ,14,"We refereed a similar kind of work done in ( Patro and Namboodiri , 2018 ) for building our triplet network .",All three networks share the same parameters .,method
relation_extraction,9,The choices generated by this process are given in . approaches .,experiment,Experimental Setup,0,152,15,14,0,experiment : Experimental Setup,0.7272727272727273,0.5555555555555556,0.56,The choices generated by this process are given in approaches ,11,We apply a cross-validation procedure on the training data to select suitable hyperparameters .,We observe that our novel attentionbased architecture achieves new state - of - the - art results on this relation classification dataset .,experiment
natural_language_inference,17,"The computation is based on the fact that two words should share similar semantics if their attentions about same texts are highly overlapped , and be less similar vice versa .",introduction,introduction,1,28,19,19,0,introduction : introduction,0.1076923076923077,0.6333333333333333,0.6333333333333333,The computation is based on the fact that two words should share similar semantics if their attentions about same texts are highly overlapped and be less similar vice versa ,30,"To address the first problem , we present a reattention mechanism that temporally memorizes past attentions and uses them to refine current attentions in a multi-round alignment architecture .","Therefore , the reattention can be more concentrated if past attentions focus on same parts of the input , or be relatively more distracted so as to focus on new regions if past attentions are not overlapped at all .",introduction
natural_language_inference,16,? is element - wise maximum .,model,Multi-perspective Matching Operation,0,108,54,18,0,model : Multi-perspective Matching Operation,0.5070422535211268,0.7826086956521741,0.5454545454545454, is element wise maximum ,6,2 ) Maxpooling - Matching .,? is element - wise maximum .,method
relation_extraction,1,"As an example , for the sentence "" Barack Obama went to Paris "" , the predicate went has sense "" motion "" and has sense label 01 .",model,Model,0,36,10,10,0,model : Model,0.4044943820224719,0.4,0.4,As an example for the sentence Barack Obama went to Paris the predicate went has sense motion and has sense label 01 ,23,The predicate dis ambiguation task is to identify the correct meaning of a predicate in a given context .,We formulate this task as sequence labeling .,method
text_summarization,2,": red cross negotiators from rivals north korea and south korea held talks wednesday on emergency food shipments to starving north koreans and agreed to meet again thursday T : koreas meet in beijing to discuss food aid from south eds B : north korea , south korea agree to meet again I : north korea , south korea meet again H : north korea , south korea meet on emergency food shipments W : north korea , south korea hold talks on food shipments R : north korea , south korea hold talks on emergency food shipments :",experiment,Experimental Setup,0,209,12,12,0,experiment : Experimental Setup,0.7517985611510791,0.8,0.8, red cross negotiators from rivals north korea and south korea held talks wednesday on emergency food shipments to starving north koreans and agreed to meet again thursday T koreas meet in beijing to discuss food aid from south eds B north korea south korea agree to meet again I north korea south korea meet again H north korea south korea meet on emergency food shipments W north korea south korea hold talks on food shipments R north korea south korea hold talks on emergency food shipments ,88,"2 Way + Relation "" is able to preserve important source relations in the summary , e.g. , "" government nsubj ? ?? ? files , "" "" files dobj ??? round , "" and "" round nmod ? ?? ? charges . """,Example system summaries .,experiment
machine-translation,4,"In the first stage , we train the proposed model with denoising auto - encoding , backtranslation and the local GANs , until no improvement is achieved on the development set .",training,Unsupervised Training,0,139,42,42,0,training : Unsupervised Training,0.5815899581589958,0.9545454545454546,0.9545454545454546,In the first stage we train the proposed model with denoising auto encoding backtranslation and the local GANs until no improvement is achieved on the development set ,28,There are two stages in the proposed unsupervised training .,"Specifically , we perform one batch of denoising autoencoding for the source and target languages , one batch of back - translation for the two languages , and another batch of local GAN for the two languages .",experiment
natural_language_inference,9,Output Module for story - based QA .,model,MODEL DETAILS,0,190,10,10,0,model : MODEL DETAILS,0.5757575757575758,0.12658227848101267,0.29411764705882354,Output Module for story based QA ,7,The same encoder with the same embedding matrix is also used to obtain the question vector q from q.,"In the output module , we are given the vector representation of the predicted answer ? and we want to obtain the natural language form of the answer , ?.",method
natural_language_inference,33,One could also consider controllable text generation by directly manipulating the sentence representations and realizing it by decoding with a conditional language model .,evaluation,CONCLUSION & FUTURE WORK,0,175,51,10,0,evaluation : CONCLUSION & FUTURE WORK,0.6704980842911877,0.3722627737226277,1.0,One could also consider controllable text generation by directly manipulating the sentence representations and realizing it by decoding with a conditional language model ,24,"Having a rich , continuous sentence representation space could allow the application of state - of - the - art generative models of images such as that of to language .", ,result
text_summarization,2,Notice that the scores on the valid - 2000 dataset are generally higher than those of test - 1951 .,result,Results,0,229,17,17,0,result : Results,0.8237410071942446,0.5151515151515151,0.5151515151515151,Notice that the scores on the valid 2000 dataset are generally higher than those of test 1951 ,18,"Overall , our proposed approach with structureinfused pointer networks perform strongly , yielding ROUGE scores thatare on - par with or surpassing state - of - the - art published systems .","This is because the ( source , summary ) pairs in the Gigaword test set are not pruned ( see 4.1 ) .",result
natural_language_inference,89,"Case1 : the question is answerable , the no-answer proba - bility is less than the threshold , and the answer is correct .",analysis,Error Analysis,0,215,3,3,0,analysis : Error Analysis,0.8269230769230769,0.14285714285714285,0.14285714285714285,Case1 the question is answerable the no answer proba bility is less than the threshold and the answer is correct ,21,"To perform error analysis , we first categorize all examples on the development set into 5 classes :","Case2 : the question is unanswerable , and the no-answer probability is larger than the threshold .",result
relation-classification,8,"BERT SP with entity indicators on input layer : it replaces our structured attention layer , and adds indicators of entities ( transformed to embeddings )",method,Methods,0,83,9,9,0,method : Methods,0.6058394160583942,0.3103448275862069,1.0,BERT SP with entity indicators on input layer it replaces our structured attention layer and adds indicators of entities transformed to embeddings ,23,"Then , for each entity pair , it takes the hidden states , adds the relative position embeddings corresponding to the target entities , and finally makes the relation prediction for this pair .", ,method
text_summarization,0,"In this section , we propose our reader - aware summary generator , abbreviated as RASG .",system description,The Proposed RASG Model Overview,0,97,10,2,0,system description : The Proposed RASG Model Overview,0.3222591362126246,0.2,0.25,In this section we propose our reader aware summary generator abbreviated as RASG ,14, ,The overview of RASG is shown in which can be split into four main parts :,method
natural_language_inference,35,"In short , given a 3 - tuple ( x q , {x pk } , s ) , the system predicts P ( y ) .",system description,Problem Formulation,0,37,4,4,0,system description : Problem Formulation,0.14068441064638784,0.6666666666666666,0.6666666666666666,In short given a 3 tuple x q x pk s the system predicts P y ,17,"and an answer style label s , an RC model outputs an answer y = {y 1 , . . . , y T } conditioned on the style .","The training data is a set of 6 - tuples : ( x q , {x pk } , s , y , a , {r pk } ) , where a and {r pk } are optional .",method
machine-translation,4,"Nevertheless , without any constraint , the AE quickly learns to merely copy every word one by one , without capturing any internal structure of the language involved .",training,Unsupervised Training,0,103,6,6,0,training : Unsupervised Training,0.4309623430962343,0.13636363636363635,0.13636363636363635,Nevertheless without any constraint the AE quickly learns to merely copy every word one by one without capturing any internal structure of the language involved ,26,"In this form , each encoder should learn to compose the embeddings of its corresponding language and each decoder is expected to learn to decompose this representation into its corresponding language .","To address this problem , we utilize the same strategy of denoising AE and add some noise to the input sentences .",experiment
part-of-speech_tagging,2,"Given a labeling rater , we randomly sample a ratio r of the sentences from the training set and discard the rest of the training data - e.g. , a labeling rate of 0.001 results in around 900 training tokens on PTB POS tagging ( Cf. ) .",performance,TRANSFER LEARNING PERFORMANCE,0,138,7,7,0,performance : TRANSFER LEARNING PERFORMANCE,0.7752808988764045,0.17073170731707318,0.28,Given a labeling rater we randomly sample a ratio r of the sentences from the training set and discard the rest of the training data e g a labeling rate of 0 001 results in around 900 training tokens on PTB POS tagging Cf ,45,"We vary the labeling rate of the target task at 0.001 , 0.01 , 0.1 and 1.0 .","The results on transfer learning are plotted in , where we compare the results with and without transfer learning under various labeling rates .",result
natural_language_inference,56,"In our experiments , since the messages in ( 1 ) are linear , this is similar to how log-probabilities are summed in belief propagation .",system description,Recurrent Relational Networks,0,62,21,21,0,system description : Recurrent Relational Networks,0.1845238095238096,0.5526315789473685,0.5526315789473685,In our experiments since the messages in 1 are linear this is similar to how log probabilities are summed in belief propagation ,23,"where N ( j ) are all the nodes that have an edge into node j. For Sudoku , N ( j ) contains the nodes in the same row , column and box as j .",Recurrent node updates .,method
natural_language_inference,54,"Our focus is to show adding structural embedding can improve baseline models , rather than directly compare to published SQuAD results .",introduction,introduction,0,28,21,21,0,introduction : introduction,0.1222707423580786,0.9545454545454546,0.9545454545454546,Our focus is to show adding structural embedding can improve baseline models rather than directly compare to published SQuAD results ,21,"Experimental results on SQuAD dataset illustrates that the syntactic information helps the model to choose the answers thatare both succinct and grammatically coherent , which boosted the performance on both qualitative studies and numerical results .","Although the methods proposed in the paper are demonstrated using syntactic trees , we note that similar approaches can be used to encode other types of tree structured information such as knowledge graphs and ontology relations .",introduction
sentiment_analysis,2,"For the later part , we can obtain the attention weights of the words in aspect term according to the position information , which is used forgetting the final representation of a sentence .",model,Word Representation,0,83,34,18,0,model : Word Representation,0.3656387665198238,0.576271186440678,0.4186046511627907,For the later part we can obtain the attention weights of the words in aspect term according to the position information which is used forgetting the final representation of a sentence ,32,"For the former part , we can obtain the different hidden contextual representation of a sentence according to different word in aspect term .",Details will be described in follwing sections .,method
natural_language_inference,93,"In , words in a passage with their corresponding attention - weighted question context are en - coded together to produce question - aware passage representation .",introduction,introduction,0,27,17,17,0,introduction : introduction,0.13043478260869565,0.6071428571428571,0.6071428571428571,In words in a passage with their corresponding attention weighted question context are en coded together to produce question aware passage representation ,23,"First , we propose a gated attention - based recurrent network , which adds an additional gate to the attention - based recurrent networks , to account for the fact that words in the passage are of different importance to answer a particular question for reading comprehension and question answering .","By introducing a gating mechanism , our gated attention - based recurrent network assigns different levels of importance to passage parts depending on their relevance to the question , masking out irrelevant passage parts and emphasizing the important ones .",introduction
text_summarization,3,"The challenge is that there are no explicit supervision labels to indicate whether the training set is close to the testing set , so a new training paradigm is needed .",system description,Distant Supervision (DS) for Model Adaption,0,132,77,4,0,system description : Distant Supervision (DS) for Model Adaption,0.5454545454545454,0.5877862595419847,0.06896551724137931,The challenge is that there are no explicit supervision labels to indicate whether the training set is close to the testing set so a new training paradigm is needed ,30,The result would be a training model that better fits the specific testing data .,"In answer to this need and also to provide end - to - end functionality in the model , we developed a simple approach for labeling summary - document pairs by calculating the Kullback - Leibler ( KL ) divergence between each training reference summary and a set of testing documents .",method
text-classification,1,Our code and experimental details are available at http://riejohnson.com/cnn download.html .,introduction,introduction,0,46,35,35,0,introduction : introduction,0.1796875,1.0,1.0,Our code and experimental details are available at http riejohnson com cnn download html ,15,We report performances exceeding the previous best results on four benchmark datasets ., ,introduction
semantic_role_labeling,1,Following we perform scaled dot -product attention :,model,Self-attention token encoder,0,72,39,19,0,model : Self-attention token encoder,0.3302752293577982,0.4431818181818182,0.7037037037037037,Following we perform scaled dot product attention ,8,We can then multiply Q between each pair of tokens in the sentence .,We scale the weights by the inverse square root of their embedding dimension and normalize with the softmax function to produce a distinct distribution for each token over all the tokens in the sentence :,method
sentiment_analysis,23,"The sample corresponds a sentence in the dataset , "" The stunning dreamlike visual will impress even those viewers who have little patience for Euro - film pretension . """,model,The Effect of Sentence Lengths,0,255,19,6,0,model : The Effect of Sentence Lengths,0.8732876712328768,0.38,0.375,The sample corresponds a sentence in the dataset The stunning dreamlike visual will impress even those viewers who have little patience for Euro film pretension ,26,Visualizing how features ( after convolution ) are related to the sentiment of a sentence .,The numbers in brackets denote the fraction of a node 's features thatare gathered by the max pooling layer ( also indicated by colors ) .,method
natural_language_inference,49,"By comparing the base model and the model the in experiment 6 , we show that the fuse gate not only well serves as a skip connection , but also makes good decision upon which information the fuse for both representation .",ablation,ablation,0,207,22,22,0,ablation : ablation,0.8149606299212598,0.8148148148148148,0.8148148148148148,By comparing the base model and the model the in experiment 6 we show that the fuse gate not only well serves as a skip connection but also makes good decision upon which information the fuse for both representation ,40,The comparison indicates self - attention layer makes the training harder to converge while a skip connection could ease the gradient flow for both highway layer and self - attention layer .,"To show that dense interaction tensor contains more semantic information , we replace the dense interaction tensor with dot product similarity matrix between the encoded representation of premise and hypothesis .",result
semantic_parsing,2,We argue that there are at least three advantages to the proposed approach .,introduction,introduction,0,19,11,11,0,introduction : introduction,0.06529209621993128,0.3793103448275862,0.3793103448275862,We argue that there are at least three advantages to the proposed approach ,14,"Specifically , the sketch constrains the generation process and is encoded into vectors to guide decoding .","Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the decoders to model meaning at different levels of granularity .",introduction
natural_language_inference,57,"Nearest non - query images in COCO train max ( "" man "" , "" cat "" ) max ( "" black dog "" , "" park "" ) :",SUPPLEMENTARY MATERIAL,SUPPLEMENTARY MATERIAL,0,170,2,2,0,SUPPLEMENTARY MATERIAL : SUPPLEMENTARY MATERIAL,0.9883720930232558,0.5,0.5,Nearest non query images in COCO train max man cat max black dog park ,15, ,Multimodal regularities found with embeddings learned for the caption - image retrieval task .,others
sentiment_analysis,24,Each task - specific component has it s own sets of latent and output variables .,method,Proposed Method,0,65,10,10,0,method : Proposed Method,0.2090032154340836,0.07462686567164177,0.5555555555555556,Each task specific component has it s own sets of latent and output variables ,15,"The sequence of shared latent vectors 3 {h s 1 , h s 2 , ... , h s n } is used as input to the different task - specific components .","The output variables correspond to a label sequence in a sequence tagging task ; in AE , we assign to each token a label indicating whether it belongs to any aspect or opinion 4 term , while in AS , we label each word with its sentiment .",method
natural_language_inference,78,The effectiveness of the factorization alignment over alternative baselines such as feed - forward neural networks is confirmed by early experiments .,introduction,introduction,0,43,32,32,0,introduction : introduction,0.15579710144927536,0.8205128205128205,0.8205128205128205,The effectiveness of the factorization alignment over alternative baselines such as feed forward neural networks is confirmed by early experiments ,21,"Moreover , factorization - based models are also known to be able to model low - rank structure and reduce risks of overfitting .",The major contributions of this work are summarized as follows :,introduction
semantic_role_labeling,4,Label Confusion Matrix shows a confusion matrix for labeling errors of the span - based model using ELMo.,performance,performance,0,204,11,11,0,performance : performance,0.68,0.22448979591836726,0.5,Label Confusion Matrix shows a confusion matrix for labeling errors of the span based model using ELMo ,18,"For A2 , the span - based models outperformed the CRF - based model by about 1.0 F1 on the both datasets .","10 Following , we only count predicted arguments that match the gold span boundaries .",result
text-classification,0,"For the normal bag - of - words , we use the counts of each word as the features .",method,Traditional Methods,0,108,6,6,0,method : Traditional Methods,0.4757709251101322,0.050420168067226885,0.3333333333333333,For the normal bag of words we use the counts of each word as the features ,17,"For each dataset , the bag - of - words model is constructed by selecting 50,000 most frequent words from the training subset .","For the TFIDF ( term-frequency inverse - document - frequency ) version , we use the counts as the term-frequency .",method
machine-translation,7,Details can be found in Appendix C.2 .,performance,BALANCING EXPERT UTILIZATION,0,167,66,24,0,performance : BALANCING EXPERT UTILIZATION,0.4477211796246649,0.5689655172413793,0.6153846153846154,Details can be found in Appendix C 2 ,9,"These models had larger LSTMs , and fewer but larger and experts .",Results of these three models form the bottom line of - right .,result
question-answering,7,"The MMA - SNE further slightly improved the result , indicating that reading the premise memory is helpful while encoding the hypothesis .",experiment,Natural Language Inference,0,156,34,22,0,experiment : Natural Language Inference,0.5672727272727273,0.4415584415584416,0.8148148148148148,The MMA SNE further slightly improved the result indicating that reading the premise memory is helpful while encoding the hypothesis ,21,NSE outperformed the previous sentence encoders on this task .,The last set of methods designs inter-sentence relation with parameterized soft attention .,experiment
natural_language_inference,19,The FFN function of the above equation 13 is applied to each position of the result of the fusion gate .,architecture,Position-wise Feed Forward Networks,0,141,66,5,0,architecture : Position-wise Feed Forward Networks,0.5529411764705883,0.8148148148148148,0.625,The FFN function of the above equation 13 is applied to each position of the result of the fusion gate ,21,"The position - wise feed forward network employs the same fully connected network to each position of sentence , in which the fully connected layer consists of two linear transformations , with the ReLU activation in between .",Note that position - wise feed forward network is combined with the residual connection as shown in .,method
natural_language_inference,71,We also show the big gains over EURNN by introducing the gates .,model,bAbI: Episodic Question Answering,0,181,22,20,0,model : bAbI: Episodic Question Answering,0.8418604651162791,0.4489795918367347,1.0,We also show the big gains over EURNN by introducing the gates ,13,We found that the GORU performs averagely better than GRU / LSTM and EURNN ., ,method
text_summarization,13,"Layers before the hard attention node receive backpropagated policy gradient ?L ?? = r ? log p (?|? ) ?? , where r is some reward and p ( ?|? ) is the attention distribution that we sample from .",model,model,0,126,57,57,0,model : model,0.4719101123595506,0.5757575757575758,0.5757575757575758,Layers before the hard attention node receive backpropagated policy gradient L r log p where r is some reward and p is the attention distribution that we sample from ,30,"Specifically , we use the REINFORCE algorithm , also formalized by in the stochastic computation graph framework .","Layers before the hard attention node receive backpropagated policy gradient ?L ?? = r ? log p (?|? ) ?? , where r is some reward and p ( ?|? ) is the attention distribution that we sample from .",method
part-of-speech_tagging,0,We also report the average tightness across all the clusters .,analysis,Effects on Representation Learning,0,201,24,9,0,analysis : Effects on Representation Learning,0.8170731707317073,0.3870967741935484,0.28125,We also report the average tightness across all the clusters ,11,"To measure the tightness of each cluster , we compute the cosine similarity for every pair of words within , and then take the average .",The evaluation results are summarized in Table 6 .,result
natural_language_inference,81,"For example , find that 90 % of the questions in the Stanford Question Answering Dataset are answerable given 1 sentence in a document .",introduction,introduction,0,44,4,4,0,introduction : introduction,0.10451306413301664,0.2352941176470588,0.2352941176470588,For example find that 90 of the questions in the Stanford Question Answering Dataset are answerable given 1 sentence in a document ,23,"Although existing datasets enabled the development of effective end - to - end neural question answering systems , they tend to focus on reasoning over localized sections of a single document .","In this work , we instead focus on multi-evidence QA , in which answering the question requires aggregating evidence from multiple documents .",introduction
natural_language_inference,60,"Alternatively , we can make the selfattention mechanism context - dependent , leading to contextualized DME ( CDME ) :",baseline,Naive baseline,0,82,15,15,0,baseline : Naive baseline,0.4248704663212435,0.4166666666666667,0.6521739130434783,Alternatively we can make the selfattention mechanism context dependent leading to contextualized DME CDME ,15,"We also experiment with an Unweighted variant of this approach , that just sums up the projections .","where h j ? R 2 m is the j th hidden state of a BiL - STM taking {w i , j } s j=1 as input , a ? R 2 m and b ? R .",result
topic_models,0,"After obtaining the model parameters from VB training , we can infer ( extract ) the posterior distribution of document embedding q ( w ) for any given document x .",training,training,0,152,45,45,0,training : training,0.36893203883495146,0.5421686746987951,0.5421686746987951,After obtaining the model parameters from VB training we can infer extract the posterior distribution of document embedding q w for any given document x ,26,Inferring embeddings for new documents,This is done by iteratively updating the parameters of q ( w ) that maximize L ( q ) from .,experiment
machine-translation,9,"However , setting M to a large number will result in longer hash codes , thus significantly increase the size of the embedding layer .",analysis,SENTIMENT ANALYSIS,0,200,23,23,0,analysis : SENTIMENT ANALYSIS,0.6968641114982579,0.26136363636363635,0.71875,However setting M to a large number will result in longer hash codes thus significantly increase the size of the embedding layer ,23,We can see that increasing either M or K can effectively decrease the reconstruction loss .,"Hence , it is important to choose correct numbers for M and K to balance the performance and model size .",result
question_answering,3,brief high - level overview to our proposed encoder is given as follows :,introduction,introduction,0,27,17,17,0,introduction : introduction,0.09642857142857143,0.4594594594594595,0.4594594594594595,brief high level overview to our proposed encoder is given as follows ,13,"The output of the dilated composition mechanism acts as gating functions , which are then used to learn compositional representations of the input sequence .","Firstly , sequences are chunked into blocks based on user - defined ( hyperparameter ) block sizes .",introduction
natural_language_inference,28,See section B for further details about the data .,experiment,ASSOCIATIVE RECALL TASK,0,173,34,9,0,experiment : ASSOCIATIVE RECALL TASK,0.6383763837638377,0.6666666666666666,0.6,See section B for further details about the data ,10,"In this example , the correct answer is "" 3 "" .","In this experiment , we compare RUM to an LSTM , , a Fast - weight RNN ) and a recent successful RNN WeiNet ) .",experiment
text_summarization,4,"Finally , distractionbased models were proposed to enable models to traverse the text content and grasp the over all meaning .",model,Extending from the base model,0,159,89,14,0,model : Extending from the base model,0.6162790697674418,0.9673913043478259,0.8235294117647058,Finally distractionbased models were proposed to enable models to traverse the text content and grasp the over all meaning ,20,"Hierarchical attention - based recurrent neural networks have also been applied to the task , owing to the idea that there are multiple sentences in a document .","The current state - of the - art performance is achieved by a graph - based attentional neural model , considering the key factors of document summarization such as saliency , fluency and novelty .",method
relation_extraction,7,"In our model , entitywise attention assigns the weight ? e it to focus on the target entity and removes noise further .",methodology,Entity-wise Attention,0,117,56,6,0,methodology : Entity-wise Attention,0.4517374517374517,0.4912280701754386,0.6,In our model entitywise attention assigns the weight e it to focus on the target entity and removes noise further ,21,Entity words are of great importance because they are significantly beneficial to relation extraction .,"In our model , entitywise attention assigns the weight ? e it to focus on the target entity and removes noise further .",method
sentiment_analysis,19,This defines the SuBiL - STM model .,introduction,introduction,0,44,28,28,0,introduction : introduction,0.2875816993464052,0.7777777777777778,0.7777777777777778,This defines the SuBiL STM model ,7,Here ; is the concatenation operator .,We also define another representation where the two LSTMs encoding the sequence in the same direction are the same or their weights are tied .,introduction
sentiment_analysis,16,The two sentiment effects are modeled as two terms :,approach,approach,0,180,58,58,0,approach : approach,0.5660377358490566,0.7073170731707317,0.7073170731707317,The two sentiment effects are modeled as two terms ,10,"The intuition is that , when we want to reduce the ( embedding ) parameters and still learn a joint representation , two different sentiment effects need to be separated in different vector spaces .",where the first term can be viewed as learning target - independent sentiment effect while the second term captures the TCS interaction .,method
natural_language_inference,38,"There are two different metrics to evaluate model accuracy : Exact Match ( EM ) and F1 Score , which measures the weighted average of the precision and recall rate at character level .",result,TriviaQA Results,0,125,7,5,0,result : TriviaQA Results,0.6613756613756614,0.5833333333333334,0.5555555555555556,There are two different metrics to evaluate model accuracy Exact Match EM and F1 Score which measures the weighted average of the precision and recall rate at character level ,30,"QA is the first dataset where questions are authored by trivia enthusiasts , independently of the evidence documents .","Because the evidence is gathered by an automated process , the documents are not guaranteed to contain all facts needed to answer the question .",result
text-classification,5,"Recent approaches that concatenate embeddings derived from other tasks with the input at different layers ) still train the main task model from scratch and treat pretrained embeddings as fixed parameters , limiting their usefulness .",introduction,introduction,0,19,11,11,0,introduction : introduction,0.07539682539682539,0.4074074074074074,0.4074074074074074,Recent approaches that concatenate embeddings derived from other tasks with the input at different layers still train the main task model from scratch and treat pretrained embeddings as fixed parameters limiting their usefulness ,34,"For inductive transfer , fine - tuning pretrained word embeddings , a simple transfer technique that only targets a model 's first layer , has had a large impact in practice and is used in most state - of - the - art models .","In light of the benefits of pretraining , we should be able to do better than randomly initializing the remaining parameters of our models .",introduction
natural_language_inference,21,"In contrast to RNN and CNN , the attention mechanism is trained to capture the dependencies that make significant contributions to the task , regardless of the distance between the elements in the sequence .",abstract,abstract,0,15,13,13,0,abstract : abstract,0.05172413793103448,0.2954545454545455,0.2954545454545455,In contrast to RNN and CNN the attention mechanism is trained to capture the dependencies that make significant contributions to the task regardless of the distance between the elements in the sequence ,33,"It allows RNN / CNN to maintain a variable - length memory , so that elements from the input sequence can be selected by their importance / relevance and merged into the output .",It can thus provide complementary information to the distance - aware dependencies modeled by RNN / CNN .,abstract
natural_language_inference,71,"Recently , using unitary and orthogonal matrices ( instead of general matrices ) in RNNs have attracted an increasing amount of attention in the machine learning community .",introduction,introduction,0,18,9,9,0,introduction : introduction,0.08372093023255814,0.3461538461538461,0.3461538461538461,Recently using unitary and orthogonal matrices instead of general matrices in RNNs have attracted an increasing amount of attention in the machine learning community ,25,Gated RNNs are also empirically shown to achieve better results for a wide variety of real - world tasks .,This trend was following the demonstration that these matrices can be effective in solving tasks involving long - term dependencies and gradients vanishing / exploding problem .,introduction
sentence_classification,0,"The baseline incorrectly classifies it as a BACK - GROUND , likely due to attending to another part of the sentence ( "" analyzed seprately "" ) .",analysis,Analysis,0,209,9,9,0,analysis : Analysis,0.7827715355805244,0.45,0.5,The baseline incorrectly classifies it as a BACK GROUND likely due to attending to another part of the sentence analyzed seprately ,22,In second example ( 3 b ) the true label is RESULTCOMPARISON .,Our model correctly classifies this instance by putting more attention weights on words that relate to comparison of the results .,result
named-entity-recognition,9,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",dataset,Datasets,0,104,7,7,0,dataset : Datasets,0.5226130653266332,0.35,0.35,We did not use alternate annotations for the BC2 GM dataset and all NER evaluations are based on entity level exact matches ,23,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,"Note that although there are several other recently introduced high quality biomedical NER datasets , we use datasets thatare frequently used by many biomedical NLP researchers , which makes it much easier to compare our work with theirs .",experiment
sentiment_analysis,4,"In particular , for each historical utterance u i<t ? H ? , an internal memory state h",methodology,SIM: Self-Influence Module,0,155,50,7,0,methodology : SIM: Self-Influence Module,0.4754601226993865,0.5102040816326531,0.5833333333333334,In particular for each historical utterance u i t H an internal memory state h,15,"For each ? ? { a , b} , GRU s ? attempts to model the emotional inertia of speaker P ? which represents the emotional dependency of a speaker with their own previous states .",Unit : GRUs are gated recurrent cells introduced by .,method
text-classification,8,"Notably , on SNLI dataset , we observe that SWEM - max performs the best among all SWEM variants , consistent with the findings in Nie and Bansal ( 2017 ) ; , that max - pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset .",model,model,1,153,23,23,1,model : model,0.5687732342007435,0.3194444444444444,0.3194444444444444,Notably on SNLI dataset we observe that SWEM max performs the best among all SWEM variants consistent with the findings in Nie and Bansal 2017 that max pooling over BiLSTM hidden units outperforms average pooling operation on SNLI dataset ,40,"Surprisingly , on most of the datasets considered ( except WikiQA ) , SWEM demonstrates the best results compared with those with CNN or the LSTM encoder .","As a result , with only 120K parameters , our SWEM - max achieves a test accuracy of 83.8 % , which is very competitive among state - of the - art sentence encoding - based models ( in terms of both performance and number of parameters )",method
natural_language_inference,96,Bottom : CDF for verbs in SNLI and Swag .,analysis,Swag versus existing NLI datasets,0,249,6,5,0,analysis : Swag versus existing NLI datasets,0.6384615384615384,0.375,1.0,Bottom CDF for verbs in SNLI and Swag ,9,"Continue "" is cutoff for SNLI ( it has frequency 6 10 ?5 ) .", ,result
sentiment_analysis,27,"Sentiment analysis , also known as opinion mining , is an important research topic in Natural Language Processing ( NLP ) .",system description,Aspect-level sentiment classification,0,57,2,2,0,system description : Aspect-level sentiment classification,0.2065217391304348,0.09090909090909093,0.125,Sentiment analysis also known as opinion mining is an important research topic in Natural Language Processing NLP ,18, ,Aspect - level sentiment classification is a fine - grained task in sentiment analysis .,method
natural_language_inference,39,"This shows that our model is able to recognize important fine - grained word - level information for better similarity measurement , suggesting the reason why our model performs well .",result,result,0,202,28,28,0,result : result,0.9805825242718448,1.0,1.0,This shows that our model is able to recognize important fine grained word level information for better similarity measurement suggesting the reason why our model performs well ,28,"From these visualizations , we see that our model is able to identify important word pairs ( in dark red ) and tag them with proper similarity values , which are significantly higher than the ones of their neighboring unimportant pairs .", ,result
natural_language_inference,97,We present a parallel - hierarchical approach to machine comprehension designed to work well in a data - limited setting .,introduction,introduction,1,21,10,10,0,introduction : introduction,0.07191780821917808,0.2702702702702703,0.2702702702702703,We present a parallel hierarchical approach to machine comprehension designed to work well in a data limited setting ,19,"Inference and reasoning are important human skills that apply broadly , beyond language .","There are many use-cases in which comprehension over limited data would be handy : for example , user manuals , internal documentation , legal contracts , and soon .",introduction
question_answering,4,Our model is implemented in Tensorflow .,experiment,Experimental Setup,1,178,2,2,0,experiment : Experimental Setup,0.6926070038910506,0.14285714285714285,0.14285714285714285,Our model is implemented in Tensorflow ,7, ,"The sequence lengths are capped at 800/700/1500/1100 for News QA , Search QA , Quasar - T and Narrative QA respectively .",experiment
sentiment_analysis,42,"Aspect level sentiment classification is a finegrained classification task in sentiment analysis , which aims at identifying the sentiment polarity of a sentence expressed towards an aspect .",system description,Aspect Level Sentiment Classification,0,138,2,2,0,system description : Aspect Level Sentiment Classification,0.5476190476190477,0.2,0.2,Aspect level sentiment classification is a finegrained classification task in sentiment analysis which aims at identifying the sentiment polarity of a sentence expressed towards an aspect ,27, ,"Most existing works use machine learning algorithms , and build sentiment classifier from sentences with manually annotated polarity labels .",method
named-entity-recognition,1,"Using higher rates negatively impacted our results , while smaller rates led to longer training time .",training,Training,0,163,8,8,0,training : Training,0.7874396135265701,0.6153846153846154,0.6153846153846154,Using higher rates negatively impacted our results while smaller rates led to longer training time ,16,We set the dropout rate to 0.5 .,The stack - LSTM model uses two layers each of dimension 100 for each stack .,experiment
text_summarization,4,The full entity encoding submodule is illustrated in .,model,Entity encoding submodule,0,125,55,33,0,model : Entity encoding submodule,0.4844961240310077,0.5978260869565217,0.9705882352941176,The full entity encoding submodule is illustrated in ,9,The final entity vector ? i is calculated as the linear transfor - mation of e i and e i :,"Ultimately , the submodule outputs the dis ambiguated entity vectors ? = {? 1 ,? 2 , ... , ? m }.",method
natural_language_inference,20,"The dataset is divided into training ( 23,596 pairs ) , development ( 1,304 pairs ) and test sets ( 2,126 pairs ) .",evaluation,SNLI:,0,107,25,17,0,evaluation : SNLI:,0.4403292181069959,0.26595744680851063,0.5666666666666667,The dataset is divided into training 23 596 pairs development 1 304 pairs and test sets 2 126 pairs ,20,Each question and the correct answer choice have been converted into an assertive statement to form the hypothesis .,"Unlike the SNLI and MultiNLI datasets , SciTail uses only two labels : entailment and neutral .",result
relation-classification,0,We report the primary micro F1 -scores as well as micro precision and recall on both entity and relation extraction to better explain model performance .,result,Data and Task Settings,0,140,7,6,0,result : Data and Task Settings,0.6194690265486725,0.3684210526315789,0.3333333333333333,We report the primary micro F1 scores as well as micro precision and recall on both entity and relation extraction to better explain model performance ,26,"We use the same data splits , preprocessing , and task settings as .",We treat an entity as correct when it s type and the region of its head are correct .,result
natural_language_inference,28,"Therefore , it seems that RUM utilizes the capacity of the hidden state almost completely .",analysis,VISUAL ANALYSIS,0,228,10,10,0,analysis : VISUAL ANALYSIS,0.8413284132841329,0.2222222222222222,0.625,Therefore it seems that RUM utilizes the capacity of the hidden state almost completely ,15,One way to interpret the importance of the diagonal contrast is that each neuron in the hidden state plays an important role for learning since each element on the diagonal activates a distinct neuron .,"For this reason , we might consider RUM as an architecture that is close to the theoretical optimum of the representational power of RNN models .",result
sentiment_analysis,20,"We add Gaussian noise with ? = 0.2 and dropout of 0.3 at the embedding layer , dropout of 0.5 at the LSTM layers and dropout of 0.25 at the recurrent connections of the LSTM .",training,Hyper-parameters,1,157,12,5,0,training : Hyper-parameters,0.8440860215053764,0.7058823529411765,0.5,We add Gaussian noise with 0 2 and dropout of 0 3 at the embedding layer dropout of 0 5 at the LSTM layers and dropout of 0 25 at the recurrent connections of the LSTM ,37,"The size of the embedding layer is 300 , and the LSTM layers 150 ( 300 for BiLSTM ) .","Finally , we add L 2 regularization of 0.0001 at the loss function .",experiment
text_generation,0,"As the generator gets progressed via training on g-steps updates , the discriminator needs to be retrained periodically to keeps a good pace with the generator .",system description,SeqGAN via Policy Gradient,0,138,76,61,0,system description : SeqGAN via Policy Gradient,0.4259259259259259,0.7102803738317757,0.953125,As the generator gets progressed via training on g steps updates the discriminator needs to be retrained periodically to keeps a good pace with the generator ,27,"After the pre-training , the generator and discriminator are trained alternatively .","When training the discriminator , positive examples are from the given dataset S , whereas negative examples are generated from our generator .",method
sentiment_analysis,41,TD - LSTM : TD - LSTM can improve the performance of sentiment classifier by treating an aspect as a target .,model,Models,1,183,4,4,0,model : Models,0.820627802690583,0.3333333333333333,0.3333333333333333,TD LSTM TD LSTM can improve the performance of sentiment classifier by treating an aspect as a target ,19,"Since it can not take advantage of the aspect information , not surprisingly the model has worst performance .","Since there is no attention mechanism in TD - LSTM , it can not "" know "" which words are important for a given aspect .",method
sentiment_analysis,26,"The first component of this objective seeks to ensure that sentiment embeddings of words accord with those of their connected words , in terms of the dot product .",approach,Sentiment Embedding Computation,0,54,33,32,0,approach : Sentiment Embedding Computation,0.22040816326530613,0.3928571428571429,0.8648648648648649,The first component of this objective seeks to ensure that sentiment embeddings of words accord with those of their connected words in terms of the dot product ,28,"Given this data , we aim to minimize",The second part ensures that the deviation from any available initial word vectors ? x is minimal ( for some very high constant C ) .,method
natural_language_inference,39,"Since not all words are created equal , important pairwise word interactions between the sentences ( Sec. 5 ) that can better contribute to the similarity measurement deserve more model focus .",system description,Similarity Focus Layer,0,86,23,2,0,system description : Similarity Focus Layer,0.4174757281553398,0.34328358208955223,0.07142857142857142,Since not all words are created equal important pairwise word interactions between the sentences Sec 5 that can better contribute to the similarity measurement deserve more model focus ,29, ,We therefore develop a similarity focus layer which can identify important word interactions and increase their model weights correspondingly .,method
natural_language_inference,9,"QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",introduction,introduction,1,25,17,17,0,introduction : introduction,0.07575757575757576,0.53125,0.53125,QRN considers the context sentences as a sequence of state changing triggers and transforms reduces the original query to a more informed query as it observes each trigger through time ,31,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .","For instance in , the original question , Where is the apple ? , can not be directly answered by any single sentence from the story .",introduction
question_answering,2,Experiments show that it outperforms existing attention methods .,introduction,introduction,0,45,35,35,0,introduction : introduction,0.1624548736462094,0.9210526315789472,0.9210526315789472,Experiments show that it outperforms existing attention methods ,9,We propose a novel attention kernel for VQA on visual - text data .,The proposed attention tensor can be used to localize evidential image and text snippets to explain the reasoning process .,introduction
natural_language_inference,6,"From Section 3.1 to 3.3 , we describe its architecture , our training strategy to scale to 93 languages , and the training data used for that purpose .",method,Proposed method,0,56,3,3,0,method : Proposed method,0.2258064516129032,0.1875,0.1875,From Section 3 1 to 3 3 we describe its architecture our training strategy to scale to 93 languages and the training data used for that purpose ,28,"We use a single , language agnostic BiLSTM encoder to build our sentence embeddings , which is coupled with an auxiliary decoder and trained on parallel corpora .","Architecture illustrates the architecture of the proposed system , which is based on Schwenk ( 2018 ) .",method
sentiment_analysis,17,Two commonly - used variants of the basic LSTM architecture are the Bidirectional LSTM and the Multilayer LSTM ( also known as the stacked or deep LSTM ) .,system description,Variants,0,63,24,2,0,system description : Variants,0.28,0.3076923076923077,0.2,Two commonly used variants of the basic LSTM architecture are the Bidirectional LSTM and the Multilayer LSTM also known as the stacked or deep LSTM ,26, ,Bidirectional LSTM consists of two LSTMs thatare run in parallel : one on the input sequence and the other on the reverse of the input sequence .,method
text_summarization,8,This result shows that the model is quite effective at finding important words ( ROUGE - 1 ) but less effective at chaining them together ( ROUGE - 2 ) .,analysis,Analysis and Discussion,0,238,11,11,0,analysis : Analysis and Discussion,0.8321678321678322,0.3235294117647059,0.3235294117647059,This result shows that the model is quite effective at finding important words ROUGE 1 but less effective at chaining them together ROUGE 2 ,25,"The oracle score represents the results if our model had a perfect accuracy , and shows that the content selector , while yielding competitive results , has room for further improvements in future work .","Similar to , we find that the decrease in ROUGE - 2 indicates alack of fluency and grammaticality of the generated summaries .",result
named-entity-recognition,4,"To add ELMo to the supervised model , we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN .",model,Using biLMs for supervised NLP tasks,0,79,32,9,0,model : Using biLMs for supervised NLP tasks,0.29044117647058826,0.5517241379310345,0.6,To add ELMo to the supervised model we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation x k ELMo task k into the task RNN ,43,"Then , the model forms a context - sensitive representation h k , typically using either bidirectional RNNs , CNNs , or feed forward networks .","For some tasks ( e.g. , SNLI , SQuAD ) , we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] .",method
sentiment_analysis,4,"In our work , we use a deep 3D - CNN to model spatiotemporal features of each utterance video .",methodology,Visual Features,0,136,31,3,0,methodology : Visual Features,0.4171779141104294,0.3163265306122449,0.2727272727272727,In our work we use a deep 3D CNN to model spatiotemporal features of each utterance video ,18,Visual indicators such as facial expressions are key to understand emotions .,3D - CNN helps to understand emotional concepts such as smiling or frowning thatare often spread across multiple frames of a video with no predefined spatial location .,method
natural_language_inference,36,"Different from the baseline model , we replace the Glo Ve embeddings with ELMo representations 3 due to the recent success of ELMo in NLP tasks .",model,Downstream Model,0,108,8,8,0,model : Downstream Model,0.5142857142857142,0.25806451612903225,0.25806451612903225,Different from the baseline model we replace the Glo Ve embeddings with ELMo representations 3 due to the recent success of ELMo in NLP tasks ,26,"Following , we model SRL as a span tagging problem 2 and use an 8 - layer deep BiL - STM with forward and backward directions interleaved .","In brief , the implementation of our SRL is a series of stacked interleaved LSTMs with highway connections .",method
sentiment_analysis,42,"We feed location vector vi to a sigmoid function ? , and calculate mi with element - wise multiplication :",model,model,0,130,4,4,0,model : model,0.5158730158730159,0.4,1.0,We feed location vector vi to a sigmoid function and calculate mi with element wise multiplication ,17,"Different from Model 3 , location representations are regarded as neural gates to control how many percent of word semantics is written into the memory .", ,method
named-entity-recognition,2,CoNLL - 2003 English NER 6.3.1 Sentence - level prediction lists F 1 scores of models predicting with sentence - level context on CoNLL - 2003 .,baseline,Baselines,1,169,9,9,0,baseline : Baselines,0.7934272300469484,0.25,0.36,CoNLL 2003 English NER 6 3 1 Sentence level prediction lists F 1 scores of models predicting with sentence level context on CoNLL 2003 ,25,"Since our task is sequence labeling , we desire a model that maintains the token - level resolution of the input , making dilated convolutions an elegant solution .","For models that we trained , we report F1 and standard deviation obtained by averaging over 10 random restarts .",result
natural_language_inference,81,"Archaeology , or archeology , is the study of human activity through the recovery and analysis of material culture .",APPENDIX,Answer town,0,310,102,22,0,APPENDIX : Answer town,0.7363420427553444,0.4788732394366197,0.16541353383458646,Archaeology or archeology is the study of human activity through the recovery and analysis of material culture ,18,Arctic seas contain seasonal sea ice in many places .,"The archaeological record consists of artifacts , architecture , biofacts or ecofacts , and cultural landscapes .",others
sentence_compression,0,"In our model , we combine the word embedding , POS embedding and dependency embedding into a single vector to be processed by the bi-LSTM model :",model,Incorporation of Syntactic Features,0,98,41,18,0,model : Incorporation of Syntactic Features,0.35125448028673834,0.4712643678160919,0.8181818181818182,In our model we combine the word embedding POS embedding and dependency embedding into a single vector to be processed by the bi LSTM model ,26,We can also concatenate w i with r i and feed the new vector to the bi -LSTM model .,"where ? represents concatenation of vectors , and ? ? ? and ? ? ? are parameters of the bi - LSTM model .",method
natural_language_inference,47,"This yields a most -frequent - class baseline accuracy of 66 % on SNLI , and 71 % on SICK .",model,Excitement Open Platform models,0,125,11,11,0,model : Excitement Open Platform models,0.5813953488372093,0.2037037037037037,0.7857142857142857,This yields a most frequent class baseline accuracy of 66 on SNLI and 71 on SICK ,17,"To convert 3 - class problems like SICK and SNLI to this setting , all instances of contradiction and unknown are converted to nonentailment .","This is intended primarily to demonstrate the difficulty of the task , rather than necessarily the performance of a state - of - the - art RTE system .",method
semantic_parsing,2,"If agg col is the k -th table column , it s probability is computed via :",training,SELECT Clause,0,200,91,5,0,training : SELECT Clause,0.6872852233676976,0.7711864406779662,0.15625,If agg col is the k th table column it s probability is computed via ,16,We feed the question vector ? into a softmax classifier to obtain the aggregation operator agg op .,"where M j=1 p ( agg col = j|x ) = 1 , ? ( ) is a scoring network , and W 4 ? R 2 nm , w 3 , b 4 ? R mare parameters .",experiment
natural_language_inference,77,This leads to a correct LAT for most questions .,system description,Type Matching,0,45,13,4,0,system description : Type Matching,0.16483516483516486,0.1780821917808219,0.3076923076923077,This leads to a correct LAT for most questions ,10,"For the BoW baseline , we extract the span in the question that refers to the expected , lexical answer type ( LAT ) by extracting either the question word ( s ) ( e.g. , who , when , why , how , how many , etc. ) or the first noun phrase of the question after the question words "" what "" or "" which "" ( e.g. , "" what year did ... "" ) .",We encode the LAT by concatenating the embedding of the first - and last word together with the average embedding of all words within the LAT .,method
sentiment_analysis,37,"It would be fair to assume that not all the words in a sentence carry sentimental information of a particular aspect ( e.g. , stop words have no impact ) .",model,Aspect-Aware Sentence Representation,0,110,50,2,0,model : Aspect-Aware Sentence Representation,0.43478260869565216,0.6756756756756757,0.16666666666666666,It would be fair to assume that not all the words in a sentence carry sentimental information of a particular aspect e g stop words have no impact ,29, ,This warrants a sentence representation that reflects the sentiment of the given aspect .,method
natural_language_inference,33,"We note that max - pooling works best on sentiment tasks such as MR , CR , SUBJ and MPQA , while the last hidden state works better on all other tasks .",evaluation,VOCABULARY EXPANSION & REPRESENTATION POOLING,0,193,69,5,0,evaluation : VOCABULARY EXPANSION & REPRESENTATION POOLING,0.7394636015325671,0.5036496350364964,0.8333333333333334,We note that max pooling works best on sentiment tasks such as MR CR SUBJ and MPQA while the last hidden state works better on all other tasks ,29,We consider both of these approaches and pick the one with better performance on the validation set .,We also employ vocabulary expansion on all tasks as in by training a linear regression to map from the space of pre-trained word embeddings ( GloVe ) to our model 's word embeddings .,result
natural_language_inference,68,"To train the model , we minimize the following loss function based on the training examples :",method,Answer Pointer Layer,0,157,107,55,0,method : Answer Pointer Layer,0.6305220883534136,0.9385964912280702,0.8870967741935484,To train the model we minimize the following loss function based on the training examples ,16,"We can then model the probability of generating the answer sequence as p ( a| H r ) = k p ( a k | a 1 , a 2 , . . . , a k?1 , H r ) , and p ( a k = j|a 1 , a 2 , . . . , a k?1 , H r ) = ? k , j .","log p ( a n | P n , Q n ) .",method
machine-translation,2,"We compute the dot products of the query with all keys , divide each by ? d k , and apply a softmax function to obtain the weights on the values .",model,Scaled Dot-Product Attention,0,68,27,4,0,model : Scaled Dot-Product Attention,0.3035714285714285,0.24770642201834864,0.26666666666666666,We compute the dot products of the query with all keys divide each by d k and apply a softmax function to obtain the weights on the values ,29,"The input consists of queries and keys of dimension d k , and values of dimension d v .","In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .",method
natural_language_inference,52,"For this reason , we assessed significance of our model combinations with respect to both the IR baseline as well as the IR ++ ( indicated by * and s , respectively ) .",performance,QA Performance,0,176,6,6,0,performance : QA Performance,0.6743295019157088,0.12244897959183673,0.2307692307692308,For this reason we assessed significance of our model combinations with respect to both the IR baseline as well as the IR indicated by and s respectively ,28,"The best performing baseline on the validation data was a model using only IR ++ features ( line 3 ) , but its performance dropped substantially when evaluated on test due to the failure of several random seed initializations to learn .","Our full model that combines IR ++ , lexical overlap , discourse , and embeddings - based features , has a P@1 of 53.3 % ( line 7 ) , an absolute gain of 6.3 % over the strong IR baseline despite using the same background knowledge .",result
text-to-speech_synthesis,1,"For each training sequence pair , we extract the decoder - to - encoder attention alignments from the trained teacher model .",system description,Duration Predictor,0,99,38,10,0,system description : Duration Predictor,0.4520547945205479,0.8444444444444444,0.5882352941176471,For each training sequence pair we extract the decoder to encoder attention alignments from the trained teacher model ,19,We first train an autoregressive encoder - attention - decoder based Transformer TTS model following .,"There are multiple attention alignments due to the multihead self - attention , and not all attention heads demonstrate the diagonal property ( the phoneme and mel-spectrogram sequence are monotonously aligned ) .",method
text_summarization,6,We directly download the prepared dataset used in .,experiment,Datesets,0,170,5,4,0,experiment : Datesets,0.648854961832061,0.2777777777777778,0.2352941176470588,We directly download the prepared dataset used in ,9,Gigawords is an English sentence summarization dataset prepared based on Annotated Gigawords 1 by extracting the first sentence from articles with the headline to form a sourcesummary pair .,"It roughly contains 3.8 M training pairs , 190K validation pairs , and 2,000 test pairs .",experiment
natural_language_inference,15,"We obtained accuracies of 90.15 % and 91.30 % in single and ensemble methods , respectively , surpassing the previous state - of - the - art model of DIIN .",experiment,Analysis,1,147,21,7,0,experiment : Analysis,0.6504424778761062,0.875,0.7,We obtained accuracies of 90 15 and 91 30 in single and ensemble methods respectively surpassing the previous state of the art model of DIIN ,26,BiMPM using the multiperspective matching technique between two sentences reports baseline performance of a L.D.C. network and basic multi-perspective models .,TrecQA and SelQA shows the performance of different models on TrecQA and SelQA datasets for answer sentence selection task that aims to select a set of candidate answer sentences given a question .,experiment
named-entity-recognition,5,Different model parameters are used in each stacked BiLSTM layer .,baseline,Baseline BiLSTM,0,88,17,17,0,baseline : Baseline BiLSTM,0.42105263157894735,0.2297297297297297,1.0,Different model parameters are used in each stacked BiLSTM layer ,11,"Multiple layers of BiLTMs can be stacked for increased representation power , where the hidden vectors of a lower layer are used as inputs for an upper layer .", ,result
part-of-speech_tagging,1,"The output size is 4 m h feature maps , like a bottleneck layer .",architecture,architecture,0,110,31,31,0,architecture : architecture,0.44,0.3875,0.3875,The output size is 4 m h feature maps like a bottleneck layer ,14,"Firstly , a N1 convolution transforms the input .",The next step consists of multiple 1 - D convolutions with kernels of different sizes .,method
semantic_role_labeling,1,"Though prior work re-encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL , we more efficiently encode each sentence only once , predict its predicates , part - of - speech tags and labeled syntactic parse , then predict the semantic roles for all predicates in the sentence in parallel .",introduction,introduction,1,23,11,11,0,introduction : introduction,0.10550458715596332,0.5238095238095238,0.5238095238095238,Though prior work re encodes each sentence to predict each desired task and again with respect to each predicate to perform SRL we more efficiently encode each sentence only once predict its predicates part of speech tags and labeled syntactic parse then predict the semantic roles for all predicates in the sentence in parallel ,55,"Whereas prior work typically requires separate models to provide linguistic analysis , including most syntaxfree neural models which still rely on external predicate detection , our model is truly end - to - end : earlier layers are trained to predict prerequisite parts - of - speech and predicates , the latter of which are supplied to later layers for scoring .","The model is trained such that , as syntactic parsing models improve , providing high - quality parses at test time will improve its performance , allowing the model to leverage updated parsing models without requiring re-training .",introduction
natural_language_inference,37,"We find 67.4 % of the questions to be contextindependent , 22.6 % to be document - dependent , and the remaining 10 % to be paragraphdependent .",result,Results,0,211,39,39,0,result : Results,0.8210116731517509,0.7222222222222222,0.7222222222222222,We find 67 4 of the questions to be contextindependent 22 6 to be document dependent and the remaining 10 to be paragraphdependent ,24,"While all our approaches had some benefit , the shared - norm model is the strongest , and is the only one to not lose performance as large numbers of paragraphs are used .","The many document - dependent questions stem from the fact that questions are frequently about the subject of the document , so the article 's title is often sufficient to resolve coreferences or ambiguities that appear in the question .",result
sentiment_analysis,12,"Adam ( Kingma and Ba , 2015 ) was adopted as the optimizer with the learning rate 0.001 .",experiment,Experiments,1,154,17,17,1,experiment : Experiments,0.6875,0.53125,0.53125,Adam Kingma and Ba 2015 was adopted as the optimizer with the learning rate 0 001 ,17,"To alleviate overfitting , we employed dropout strategy ( Hinton et al. , 2012 ) on the input word embeddings of the LSTM and the ultimate aspect - related sentence representation .","When implementing our approach , we empirically set the maximum iteration number K as 5 , ? in Equation 3 as 0.1 on LAPTOP data set , 0.5 on REST data set and 0.1 on TWITTER data set , respectively .",experiment
text-to-speech_synthesis,0,G2P conversion can be viewed as a sequence to sequence task and modeled by the encoder - decoder framework .,introduction,introduction,0,14,3,3,0,introduction : introduction,0.08641975308641975,0.13636363636363635,0.13636363636363635,G2P conversion can be viewed as a sequence to sequence task and modeled by the encoder decoder framework ,19,"Grapheme - to - phoneme ( G2P ) conversion aims to generate a sequence of pronunciation symbols ( phonemes ) given a sequence of letters ( graphemes ) , which is an important component in automatic speech recognition and text - to - speech systems to provide accurate pronunciations for the words not covered by the lexicon .",adopt LSTM for G2P conversion and achieve improvements than the previous joint n-gram model .,introduction
relation_extraction,12,We now report the results on the TACRED dataset for the sentence - level relation extraction task in Model PR F1,model,Results on Sentence-level Relation Extraction,0,218,31,2,0,model : Results on Sentence-level Relation Extraction,0.662613981762918,0.6739130434782609,0.6666666666666666,We now report the results on the TACRED dataset for the sentence level relation extraction task in Model PR F1,20, ,LR 73.5 49.9 59.4 SDP - LSTM Tree-LSTM ** 66.0 59.2 62.4 PA- LSTM 65.7 64.5 65.1 GCN 69.8 59.0 64.0 C-GCN 69.9 63.3 66.4 AGGCN ( ours ) 69.9 60.9 65.1 C - AGGCN ( ours ) 73.1 64.2 68.2,method
natural_language_inference,84,"In order to compare how helpful these sources are when they are available , we run additional set of experiments with "" restricted "" inputs .",model,LANGUAGE MODELLING,0,189,11,11,0,model : LANGUAGE MODELLING,0.8217391304347826,0.34375,0.34375,In order to compare how helpful these sources are when they are available we run additional set of experiments with restricted inputs ,23,"These sources of auxiliary information were available for 63.35 % , 97.43 % and 100 % of the rest of occurrences respectively .","Specifically , we only use auxiliary information for a word if it has both a Glo Ve embedding and a dictionary definition .",method
sentiment_analysis,17,"These architectures are in fact closely related ; since we consider only binarized constituency trees , the parameterizations of the two models are very similar .",system description,-ary Tree-LSTMs,0,116,77,20,0,system description : -ary Tree-LSTMs,0.5155555555555555,0.9871794871794872,0.9523809523809524,These architectures are in fact closely related since we consider only binarized constituency trees the parameterizations of the two models are very similar ,24,"In the remainder of this paper , we focus on the special cases of Dependency Tree - LSTMs and Constituency Tree - LSTMs .","The key difference is in the application of the compositional parameters : dependent vs. head for Dependency Tree - LSTMs , and left child vs. right child for Constituency Tree - LSTMs .",method
natural_language_inference,97,The identity initialization requires that the network weight matrices are square ( d = D ) .,training,Training and Model Details,0,230,16,16,0,training : Training and Model Details,0.7876712328767124,0.6153846153846154,0.6153846153846154,The identity initialization requires that the network weight matrices are square d D ,14,"Without this , training was not effective at all .","We found dropout to be particularly effective at improving generalization from the training to the test set , and used 0.5 as the dropout probability .",experiment
text-classification,8,"We hypothesize that incorporating information about the local word - order , i.e. , n-gram features , is likely to largely mitigate the limitations of the above three SWEM variants .",model,model,0,186,56,56,0,model : model,0.6914498141263941,0.7777777777777778,0.7777777777777778,We hypothesize that incorporating information about the local word order i e n gram features is likely to largely mitigate the limitations of the above three SWEM variants ,29,the input document .,"Inspired by this observation , we propose using another simple pooling operation termed as hierarchical ( SWEM - hier ) , as detailed in Section 3.3 .",method
sentiment_analysis,39,SentiHood is based on the text from a QA platform in the domain of neighbourhoods of a city .,introduction,introduction,0,54,40,40,0,introduction : introduction,0.22040816326530613,0.975609756097561,0.975609756097561,SentiHood is based on the text from a QA platform in the domain of neighbourhoods of a city ,19,To facilitate research on this task we introduce the SentiHood dataset .,shows examples of input sentences and annotations provided .,introduction
machine-translation,8,The context vector c i are recomputed at each step by the alignment model : :,model,DECODER,0,271,58,9,0,model : DECODER,0.8187311178247734,0.4915254237288136,0.3461538461538461,The context vector c i are recomputed at each step by the alignment model ,15,The initial hidden state s 0 is computed by,Learning statistics and relevant information .,method
machine-translation,7,"For each model , we report the test perplexity , the computational budget , the parameter counts , the value of DropP rob , and the computational efficiency .",APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,301,79,29,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8069705093833779,0.5231788079470199,0.6170212765957447,For each model we report the test perplexity the computational budget the parameter counts the value of DropP rob and the computational efficiency ,24,Results are reported in .,We implement several memory optimizations in order to fit up to 1 billion parameters per GPU .,others
text_generation,2,The visualized traces are plotted in and more cases are presented in the supplementary material .,model,Model Explanation,0,245,5,5,0,model : Model Explanation,0.7,0.25,0.25,The visualized traces are plotted in and more cases are presented in the supplementary material ,16,"Besides , we visualize the feature trace , i.e. the features ft of prefix st during the generation , for LeakGAN , SeqGAN and Rank GAN via a 2 - D principal component analysis ( PCA ) .","As we can see , during the generation process , in LeakGAN , the feature vector gradually approaches the real data feature vector region .",method
machine-translation,3,"On this subset , the SMT model achieves 37.5 , which is similar to its score 37.0 on the full test set .",analysis,Unknown words,0,285,14,7,0,analysis : Unknown words,0.9105431309904152,0.4827586206896552,0.6363636363636364,On this subset the SMT model achieves 37 5 which is similar to its score 37 0 on the full test set ,23,. We also list the results from SMT model the score 37.7 on the full test set .,This suggests that the difficulty on this subset is not much different from that on the full set .,result
paraphrase_generation,0,"In this paper , we propose a method for obtaining sentence - level embeddings .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.016877637130801686,0.16666666666666666,0.16666666666666666,In this paper we propose a method for obtaining sentence level embeddings ,13, ,"While the problem of securing word - level embeddings is very well studied , we propose a novel method for obtaining sentence - level embeddings .",abstract
relation_extraction,8,Using word embeddings that have been trained a priori has become common practice for enhancing many other NLP tasks .,methodology,Word Embeddings,0,101,17,4,0,methodology : Word Embeddings,0.3754646840148699,0.1650485436893204,0.4444444444444444,Using word embeddings that have been trained a priori has become common practice for enhancing many other NLP tasks ,20,"They have recently been shown to capture both semantic and syntactic information about words very well , setting performance records in several word similarity tasks .",common method of training a neural network is to randomly initialize all parameters and then optimize them using an optimization algorithm .,method
natural_language_inference,22,"In addition , we introduce an answer-question similarity loss to penalize overlap between question and predicted answer , a common feature in the errors of our base model .",introduction,introduction,1,26,16,16,0,introduction : introduction,0.11818181818181818,0.6666666666666666,0.6666666666666666,In addition we introduce an answer question similarity loss to penalize overlap between question and predicted answer a common feature in the errors of our base model ,28,"We observe a surprising phenomenon that when an LSTM layer in the context ruminate layer takes same input in each timestep , it can produce useful representation for the gates .","This allows us to achieve an F 1 score of 79.5 and Exact Match ( EM ) score of 70.6 on hidden test set , 1 an improvement of 2.2 F1 score and 2.9 EM on BIDAF .",introduction
natural_language_inference,82,correct answer found by max - pooling .,model,Models,0,96,7,7,0,model : Models,0.8421052631578947,0.5,0.5,correct answer found by max pooling ,7,"the actor showed the child two arms , one from @entity0 's movies and one for @entity 7 : are al , working robotic @entity 2 arm .!:",Attention to each entity occurrence shown on left .,method
natural_language_inference,4,Our encoder is shown in .,system description,DEEP RESIDUAL COATTENTION ENCODER,0,50,25,10,0,system description : DEEP RESIDUAL COATTENTION ENCODER,0.2525252525252525,0.25773195876288657,0.5555555555555556,Our encoder is shown in ,6,This reduces the length of signal paths .,Suppose we are given a document of m words and a question of n words .,method
question_answering,3,"Finally , the final variant is the DCU - LSTM which places a DCU encoder layer on top of a BiLSTM layer .",method,Our Methods,0,200,25,5,0,method : Our Methods,0.7142857142857143,0.7352941176470589,0.35714285714285715,Finally the final variant is the DCU LSTM which places a DCU encoder layer on top of a BiLSTM layer ,21,The model denoted by DCU ( without any prefix ) corresponds to the recurrent DCU model .,We report the dimensions of the encoder as well as training time ( per epoch ) for each variant .,method
text_generation,2,We further perform a t-test for the improvement of Leak GAN over the second highest performance and report the p-value .,training,Training Settings,0,177,19,19,0,training : Training Settings,0.5057142857142857,1.0,1.0,We further perform a t test for the improvement of Leak GAN over the second highest performance and report the p value ,23,"For real - world data experiments , BLEU statistics ) and human rating scores in the Turing test are reported .", ,experiment
relation_extraction,12,We can observe that adding either attention guided layers or densely connected layers improves the performance of the model .,analysis,Analysis and Discussion,1,238,5,5,0,analysis : Analysis and Discussion,0.7234042553191491,0.16129032258064516,0.16129032258064516,We can observe that adding either attention guided layers or densely connected layers improves the performance of the model ,20,shows the results .,"This suggests that both layers can assist GCNs to learn better information aggregations , producing better representations for graphs , where the attention - guided layer seems to be playing a more significant role .",result
text-classification,4,"one - layer architec - ture is utilized for both the CNN baseline and the ACNN model , since we did not observe significant performance gains with a multilayer architecture .",training,training,1,155,5,5,0,training : training,0.7013574660633484,0.35714285714285715,0.35714285714285715,one layer architec ture is utilized for both the CNN baseline and the ACNN model since we did not observe significant performance gains with a multilayer architecture ,28,"For direct comparison , we employ the same filter shape / size settings as in our basic CNN implementation .","The minibatch size is set as 128 , and a dropout rate of 0.2 is utilized on the embedding layer .",experiment
natural_language_inference,19,The NLI task can be solved through two different approaches : sentence encoding - based models and joint models .,abstract,abstract,0,12,10,10,0,abstract : abstract,0.047058823529411764,0.7692307692307693,0.7692307692307693,The NLI task can be solved through two different approaches sentence encoding based models and joint models ,18,"Additionally , we show that our model has a strength in long sentences or documents . *","The former separately encode each sentence , whereas the latter take into account the direct relationship between two sentences .",abstract
question_answering,5,"To demonstrate that R 3 serves as a strong baseline on the TriviaQA data , we generate the R 3 results following the leaderboard setting .",analysis,analysis,0,180,7,7,0,analysis : analysis,0.6521739130434783,0.5384615384615384,0.5384615384615384,To demonstrate that R 3 serves as a strong baseline on the TriviaQA data we generate the R 3 results following the leaderboard setting ,25,While we re-run model R 3 based on the authors ' source code and extend the model to the datasets of Search QA and TriviaQA datasets .,"The results showed that R 3 achieved F1 56.0 , EM 50.9 on Wiki domain and F1 68.5 , EM 63.0 on Web domain , which is competitive to the state - of - the - arts .",result
text_generation,4,The generated samples mixed with real sentences are chosen from a random pool and form the evaluation set given to such an audience who marks if the sentences are fake or not .,training,Proposed Oracle Experiment.,0,102,28,5,0,training : Proposed Oracle Experiment.,0.9357798165137616,0.8,0.7142857142857143,The generated samples mixed with real sentences are chosen from a random pool and form the evaluation set given to such an audience who marks if the sentences are fake or not ,33,This prevents them from being certain whether a sentence in question has or has not occurred in any work of the said author .,This data run through a scoring metric would determine a model 's performance .,experiment
named-entity-recognition,2,We also compare against a non-dilated CNN architecture with the same number of convolutional layers as our dilated network ( 4 - layer CNN ) and one with enough layers to incorporate an effective input width of the same size as that of the dilated network ( 5 - layer CNN ) to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions ( i.e. using fewer parameters ) .,baseline,Baselines,1,163,3,3,0,baseline : Baselines,0.7652582159624414,0.08333333333333333,0.12,We also compare against a non dilated CNN architecture with the same number of convolutional layers as our dilated network 4 layer CNN and one with enough layers to incorporate an effective input width of the same size as that of the dilated network 5 layer CNN to demonstrate that the dilated convolutions more effectively aggregate contextual information than simple convolutions i e using fewer parameters ,67,"We compare our ID - CNN against strong LSTM and CNN baselines : a Bi - LSTM with local decoding , and one with CRF decoding ( Bi - LSTM - CRF ) .","We also compare our document - level ID - CNNs to a baseline which does not share parameters between blocks ( noshare ) and one that computes loss only at the last block , rather than after every iterated block of dilated convolutions ( 1 - loss ) .",result
semantic_role_labeling,1,"reports precision , recall and F1 on the CoNLL - 2012 test set .",experiment,Semantic role labeling,0,175,26,6,0,experiment : Semantic role labeling,0.8027522935779816,0.65,0.4615384615384616,reports precision recall and F1 on the CoNLL 2012 test set ,12,"Here LISA still excels : with D&M parses , LISA out - performs the previous state - of - the - art by more than 2 F1 on both WSJ and Brown .",We observe performance similar to that observed on ConLL - 2005 :,experiment
text_summarization,10,An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.015209125475285171,0.3333333333333333,0.3333333333333333,An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document ,23, ,"We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation , where the former teaches the summarization model how to look for salient questioning - worthy details , and the latter teaches the model how to rewrite a summary which is a directed - logical subset of the input document .",abstract
text_summarization,10,these alternate sharing methods in all metrics with statistical significance ( p < 0.05 ) .,model,Models,0,48,3,3,0,model : Models,0.1825095057034221,0.03225806451612903,0.75,these alternate sharing methods in all metrics with statistical significance p 0 05 ,14,Average Entailment Probability Baseline 0.907 Multi- Task ( EG ) 0.912 : Entailment classification results of our baseline vs. EG - multi - task model ( p < 0.001 ) ., ,method
natural_language_inference,69,"The annotator was asked if the answer to the query "" follows "" , "" is likely "" , or "" does not follow "" , given the relevant documents .",analysis,36%,0,181,28,18,0,analysis : 36%,0.5246376811594203,0.5185185185185185,0.9,The annotator was asked if the answer to the query follows is likely or does not follow given the relevant documents ,22,"We opted to verify the dataset quality by providing only the subset of documents relevant to support the correct answer , i.e. , those traversed along the path reaching the answer .","68 % of the cases were considered as "" follows "" or as "" is likely "" .",result
natural_language_inference,7,The simplest alternative is to consider this task as binary classification for every word ( Membership prediction in ) .,model,Learning objective EM F1,0,144,19,7,0,model : Learning objective EM F1,0.8089887640449438,0.4130434782608696,0.2058823529411765,The simplest alternative is to consider this task as binary classification for every word Membership prediction in ,18,"In order to provide clean comparisons , we restrict the alternatives to objectives thatare trained and evaluated with exact decoding .","In this baseline , we optimize the logistic loss for binary labels indicating whether passage words belong to the correct answer span .",method
machine-translation,4,"We do not test the the importance of the auto - encoding , back - translation and the pre-trained embeddings because they have been widely tested in .",ablation,Ablation study,0,218,4,4,0,ablation : Ablation study,0.9121338912133892,0.3333333333333333,0.3333333333333333,We do not test the the importance of the auto encoding back translation and the pre trained embeddings because they have been widely tested in ,26,Results are reported in table 3 .,shows that the best performance is obtained with the simultaneous use of all the tested elements .,result
text-classification,1,"document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .",introduction,introduction,0,19,8,8,0,introduction : introduction,0.07421875,0.2285714285714285,0.2285714285714285,document is represented as a sequence of one hot vectors each of which indicates a word by the position of a 1 a convolution layer converts small regions of the document e g I love it to low dimensional vectors at every location embedding of text regions a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average and the top layer classifies a document vector with a linear model ,78,"In its simplest form , onehot CNN works as follows .",The onehot CNN and its semi-supervised extension were shown to be superior to a number of previous methods .,introduction
relation-classification,0,Stacking Sequence and Dependency Layers,model,Stacking Sequence and Dependency Layers,0,103,55,1,0,model : Stacking Sequence and Dependency Layers,0.4557522123893805,0.7142857142857143,0.25,Stacking Sequence and Dependency Layers,5, , ,method
natural_language_inference,54,We compared these models on both the development set and official test set and reported the results in .,performance,Predictive Performance,0,150,12,12,0,performance : Predictive Performance,0.6550218340611353,0.48,0.9230769230769232,We compared these models on both the development set and official test set and reported the results in ,19,The ensemble model choose the answer with the highest sum of confidence scores among the 5 single models for each question .,"We found that the models have higher performance on the test set than the development set , which coincides with the previous results on the same data set .",result
natural_language_inference,43,Our candidate extraction step finds the correct answer in the top - K candidates in 65.9 % of development examples and 62.7 % of test examples .,experiment,Experiments,1,118,18,18,0,experiment : Experiments,0.7612903225806451,0.5294117647058824,0.5294117647058824,Our candidate extraction step finds the correct answer in the top K candidates in 65 9 of development examples and 62 7 of test examples ,26,"WEBQA obtained 32.6 F 1 ( 33.5 p@1 , 42.4 MRR ) compared to 40.9 F 1 of COMPQ .","Thus , our test F 1 on examples for which candidate extraction succeeded ( WEBQA - SUBSET ) is 51.9 ( 53.4 p@1 , 67.5 MRR ) .",experiment
phrase_grounding,0,An overview of our multi-level multimodal attention mechanism for calculating attended visual feature can be seen in .,method,Multi-Level Multimodal Attention Mechanism,0,123,50,16,0,method : Multi-Level Multimodal Attention Mechanism,0.5466666666666666,0.6666666666666666,0.9411764705882352,An overview of our multi level multimodal attention mechanism for calculating attended visual feature can be seen in ,19,"In other words , a t ,l is a vector in the hyperplane spanned by a subset of visual representations in the common space , this subset being selected based on the heatmap tensor .","In the sequel , we describe how we use this attended feature to choose the most representative hyperplane , and calculate a multimodal loss to be minimized by weak supervision of image - sentence relevance labels .",method
semantic_role_labeling,3,Increas - Decoding F 1 Speed Argmax Decoding 83.1 50 K Constrained Decoding 83.0,model,model,0,197,4,4,0,model : model,0.7462121212121212,0.05970149253731344,0.17391304347826084,Increas Decoding F 1 Speed Argmax Decoding 83 1 50 K Constrained Decoding 83 0,15,"We increase the number of hidden units from 200 to 400 and 400 to 600 as listed in rows 1 , 6 and 7 of , and the corresponding hidden size hf of FFN sublayers is increased to 1600 and 2400 respectively .",17K : Comparison between argmax decoding and constrained decoding on top of our model .,method
natural_language_inference,29,"To generate the answer , we use the RNN - based decoder which fuses the facts extracted from reviews and attributes when generating words .",system description,PAAG MODEL 4.1 Overview,0,119,24,14,0,system description : PAAG MODEL 4.1 Overview,0.326027397260274,0.15,0.7777777777777778,To generate the answer we use the RNN based decoder which fuses the facts extracted from reviews and attributes when generating words ,23,3 ) Facts decoder :,See Section 4.5 ) Existing approaches easily generate a grammatically correct answer but conflicts to the facts .,method
natural_language_inference,89,"Here we use a heuristic function o = F ( x , y) proposed by , which demonstrates good performances compared to other options :",approach,Answer Verifier,0,136,70,33,0,approach : Answer Verifier,0.5230769230769231,0.7954545454545454,0.6470588235294118,Here we use a heuristic function o F x y proposed by which demonstrates good performances compared to other options ,21,"can have various forms , such as BiLSTM , multilayer perceptron , and soon .","where gelu is the Gaussian Error Linear Unit , is element - wise multiplication , and the bias term is omitted .",method
sentiment_analysis,16,They are linearly combined to determine the final sentiment score s .,system description,Problem of the above Model for Target-Sensitive Sentiment,0,86,34,8,0,system description : Problem of the above Model for Target-Sensitive Sentiment,0.27044025157232704,0.4857142857142857,0.42105263157894735,They are linearly combined to determine the final sentiment score s ,12,"In Eq. 3 , ? i W c i can be viewed as the individual sentiment logit for a context word and W v t is the sentiment logit of an aspect .",This can be problematic in ASC .,method
natural_language_inference,55,Our model learns low - dimensional embeddings of words and knowledge base constituents ; these representations are used to score natural language questions against candidate answers .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.034013605442176874,0.75,0.75,Our model learns low dimensional embeddings of words and knowledge base constituents these representations are used to score natural language questions against candidate answers ,25,This paper presents a system which learns to answer questions on a broad range of topics from a knowledge base using few handcrafted features .,"Training our system using pairs of questions and structured representations of their answers , and pairs of question paraphrases , yields competitive results on a recent benchmark of the literature .",abstract
sentiment_analysis,39,One example is the following ( are a names are highlighted in bold and aspect related terms are underlined ) :,introduction,introduction,0,35,21,21,0,introduction : introduction,0.14285714285714285,0.5121951219512195,0.5121951219512195,One example is the following are a names are highlighted in bold and aspect related terms are underlined ,19,"In general , these conversations are very comprehensible : they often contain specific information about several aspects of several neighbourhoods .","Other places to look at in South London are Streatham ( good range of shops and restaurants , maybe a bit far out of central London but you get more for your money ) Brixton ( good transport links , trendy , can be a bit edgy )",introduction
natural_language_inference,93,"To determine the importance of passage parts and attend to the ones relevant to the question , we add another gate to the input ( [u Pt , ct ] ) of RNN :",system description,Gated Attention-based Recurrent Networks,0,75,37,12,0,system description : Gated Attention-based Recurrent Networks,0.3623188405797101,0.5873015873015873,0.7058823529411765,To determine the importance of passage parts and attend to the ones relevant to the question we add another gate to the input u Pt ct of RNN ,29,Pt as an additional input into the recurrent network :,"Different from the gates in LSTM or GRU , the additional gate is based on the current passage word and its attention - pooling vector of the question , which focuses on the relation between the question and current passage word .",method
natural_language_inference,81,"Providing 64 calories in a typical serving of one tablespoon ( 15 ml ) equivalent to 1272 kj per 100 g , honey has no significant nutritional value .",APPENDIX,Answer town,0,365,157,77,0,APPENDIX : Answer town,0.8669833729216152,0.7370892018779343,0.5789473684210527,Providing 64 calories in a typical serving of one tablespoon 15 ml equivalent to 1272 kj per 100 g honey has no significant nutritional value ,26,"Although some evidence indicates honey maybe effective in treating diseases and other medical conditions , such as wounds and burns , the over all evidence for its use in therapy is not conclusive .","Honey is generally safe , but may have various , potential adverse effects or interactions with excessive consumption , existing disease conditions , or drugs .",others
semantic_parsing,0,"We did not create questions thatare ( 1 ) vague or too ambiguous , or ( 2 ) require knowledge outside the data base to answer .",system description,Question and SQL Annotation,0,114,71,14,0,system description : Question and SQL Annotation,0.4100719424460432,0.6761904761904762,0.4,We did not create questions thatare 1 vague or too ambiguous or 2 require knowledge outside the data base to answer ,22,) Question clarity .,"First , ambiguous questions refer to the questions that do not have enough clues to infer which columns to return and which conditions to consider .",method
machine-translation,2,We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.04464285714285714,0.4705882352941176,0.4705882352941176,We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data ,26,"On the WMT 2014 English - to - French translation task , our model establishes a new single - model state - of - the - art BLEU score of 41.8 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .",Listing order is random .,abstract
text_generation,2,The results on Chinese Poems indicate that LeakGAN successfully handles the short text generation tasks .,experiment,Short Text Generation: Chinese Poems,1,214,37,6,0,experiment : Short Text Generation: Chinese Poems,0.6114285714285714,1.0,1.0,The results on Chinese Poems indicate that LeakGAN successfully handles the short text generation tasks ,16,The experimental results are provided in ., ,experiment
text_summarization,4,"Ultimately , the submodule outputs the dis ambiguated entity vectors ? = {? 1 ,? 2 , ... , ? m }.",model,Entity encoding submodule,0,126,56,34,0,model : Entity encoding submodule,0.4883720930232558,0.6086956521739131,1.0,Ultimately the submodule outputs the dis ambiguated entity vectors 1 2 m ,13,The full entity encoding submodule is illustrated in ., ,method
sentiment_analysis,45,Construction of the auxiliary sentence,methodology,Construction of the auxiliary sentence,0,44,10,1,0,methodology : Construction of the auxiliary sentence,0.3055555555555556,0.1851851851851852,0.047619047619047616,Construction of the auxiliary sentence,5, , ,method
natural_language_inference,53,"In this section , we refer to "" micro "" and "" macro "" averages of development set ( dev ) results on transfer tasks whose metrics is accuracy : we compute a "" macro "" aggregated score that corresponds to the classical average of dev accuracies , and the "" micro "" score that is a sum of the dev accuracies , weighted by the number of dev samples .",result,Empirical results,0,140,2,2,0,result : Empirical results,0.6730769230769231,1.0,1.0,In this section we refer to micro and macro averages of development set dev results on transfer tasks whose metrics is accuracy we compute a macro aggregated score that corresponds to the classical average of dev accuracies and the micro score that is a sum of the dev accuracies weighted by the number of dev samples ,57, , ,result
natural_language_inference,68,"For example , "" when "" questions look for temporal expressions as answers , whereas "" where "" questions look for locations as answers .",result,FURTHER ANALYSES,0,212,27,10,0,result : FURTHER ANALYSES,0.8514056224899599,0.675,0.43478260869565216,For example when questions look for temporal expressions as answers whereas where questions look for locations as answers ,19,These different question words roughly refer to questions with different types of answers .,"According to the performance on the development data set , our models work the best for "" when "" questions .",result
named-entity-recognition,1,Our models rely on two sources of information about words : character - based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.028985507246376805,0.6666666666666666,0.6666666666666666,Our models rely on two sources of information about words character based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora ,28,"In this paper , we introduce two new neural architectures - one based on bidirectional LSTMs and conditional random fields , and the other that constructs and labels segments using a transition - based approach inspired by shift - reduce parsers .",Our models obtain state - of - the - art performance in NER in four languages without resorting to any language - specific knowledge or resources such as gazetteers .,abstract
sentiment_analysis,37,"Section 2 discusses previous works ; Section 3 delves into the method we present ; Section 4 mentions the dataset , baselines , and experimental settings ; Section 5 presents and analyzes the results ; finally , Section 6 concludes this paper .",introduction,introduction,0,38,30,30,0,introduction : introduction,0.15019762845849802,1.0,1.0,Section 2 discusses previous works Section 3 delves into the method we present Section 4 mentions the dataset baselines and experimental settings Section 5 presents and analyzes the results finally Section 6 concludes this paper ,36,The rest of the paper structured as follows :, ,introduction
natural_language_inference,37,Confidence based methods apply the model to multiple paragraphs and returns the answer with the highest confidence .,introduction,introduction,0,18,9,9,0,introduction : introduction,0.07003891050583658,0.3103448275862069,0.3103448275862069,Confidence based methods apply the model to multiple paragraphs and returns the answer with the highest confidence ,18,"Pipelined approaches select a single paragraph * Work completed while interning at the Allen Institute for Artificial Intelligence from the input documents , which is then passed to the paragraph model to extract an answer .","Confidence methods have the advantage of being robust to errors in the ( usually less sophisticated ) paragraph selection step , however they require a model that can produce accurate confidence scores for each paragraph .",introduction
natural_language_inference,71,We found GORU significantly outperforms all other models with same hidden size as shown in,model,Speech Spectrum Prediction,0,208,49,9,0,model : Speech Spectrum Prediction,0.9674418604651164,1.0,1.0,We found GORU significantly outperforms all other models with same hidden size as shown in,15,We trained all RNNs for with the same batch size 32 using Adam optimization with a learning rate of 0.001 ., ,method
part-of-speech_tagging,7,"We complement the NN - based and HMM - based tagger with a CRF tagger , using a freely available implementation ) based on crfsuite .",system description,Tagging with bi-LSTMs,0,63,35,35,0,system description : Tagging with bi-LSTMs,0.504,1.0,1.0,We complement the NN based and HMM based tagger with a CRF tagger using a freely available implementation based on crfsuite ,22,We use TNT as it was among the best performing taggers evaluated in ., ,method
text_summarization,6,"The summary "" The emergence of the ' cyber cold war "" ' matches with the structure of "" What "" , and the summary "" St. Louis ' public library computers hacked "" follows the structure of "" What - Happened "" .",introduction,introduction,0,20,11,11,0,introduction : introduction,0.07633587786259542,0.3928571428571429,0.3928571428571429,The summary The emergence of the cyber cold war matches with the structure of What and the summary St Louis public library computers hacked follows the structure of What Happened ,31,"Similarly , the summaries "" [ fixes ] [ botched @POTUS account transfer ] "" , "" [ to pay ] [ $ 20 million ] for misleading drivers "" , and "" [ Bipartis an bill ] aims to [ H - 1B vis a system ] "" also follow the structure of "" Who Action What "" .","Intuitively , if we can incorporate the latent structure information of summaries into the abstractive summarization model , it will improve the quality of the generated summaries .",introduction
passage_re-ranking,0,"On the recent MS MARCO dataset , our approach is competitive with the best results on the official leaderboard , and we report the best - known results on TREC CAR .",introduction,introduction,0,29,19,19,0,introduction : introduction,0.2377049180327869,0.7916666666666666,0.7916666666666666,On the recent MS MARCO dataset our approach is competitive with the best results on the official leaderboard and we report the best known results on TREC CAR ,29,This is the first successful application of document expansion using neural networks that we are aware of .,We further show that document expansion is more effective than query expansion on these two datasets .,introduction
natural_language_inference,87,"In contrast , existing studies have verified that human reads sentences efficiently by taking a sequence of fixation and saccades after a quick first glance .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.10795454545454546,0.4090909090909091,0.4090909090909091,In contrast existing studies have verified that human reads sentences efficiently by taking a sequence of fixation and saccades after a quick first glance ,25,"Generally , if the text is particularly lengthy and detailedriddled , it would be quite difficult for deep learning model to understand as it suffers from noise and pays vague attention on the text components , let alone accurately answering questions .","Besides , for passage involved reading comprehension , a input sequence always consists of multiple sentences .",introduction
named-entity-recognition,5,"Adding one additional sentence - level node as described in Section 3.2 does not lead to accuracy improvements , although the number of parameters and decoding time increase accordingly .",experiment,Development Experiments,0,153,8,5,0,experiment : Development Experiments,0.7320574162679426,0.42105263157894735,0.3125,Adding one additional sentence level node as described in Section 3 2 does not lead to accuracy improvements although the number of parameters and decoding time increase accordingly ,29,"Without the sentence - level node , the accuracy of S - LSTM drops to 81.76 % , demonstrating the necessity of global information exchange .","As a result , we use only 1 sentence - level node for the remaining experiments .",experiment
paraphrase_generation,0,"The labels are classified into 5 sentiment classes , namely { Very Negative , Negative , Neutral , Positive , Very Positive } .",dataset,Dataset,0,126,4,4,0,dataset : Dataset,0.5316455696202531,0.8,0.8,The labels are classified into 5 sentiment classes namely Very Negative Negative Neutral Positive Very Positive ,17,extended this by parsing the reviews to subphrases and then fine - graining the sentiment labels for all the phrases of movies reviews using Amazon Mechanical Turk .,"This dataset contains a total 126 k phrases for training set , 30 k phrases for validation set and 66 k phrases for test set .",experiment
text_generation,2,"think you should really really leave for because we had n't been busy , where it goes to one , "" he wrote .",SeqGAN,SeqGAN,0,336,18,18,0,SeqGAN : SeqGAN,0.96,0.5625,0.5625,think you should really really leave for because we had n t been busy where it goes to one he wrote ,22,"You only certainly might not rush it down for those circumstances where we are when they were the heads , and when she 's name . ""","All the study knew was that they are , so they continue to provide support service and it does n't exist . '",others
natural_language_inference,40,"Similarly , we can obtain the output of the backward subgraph H b , and concatenate with Hf such that elements of the original sequence lineup .",method,MAGE-GRUs,0,121,44,20,0,method : MAGE-GRUs,0.4368231046931408,0.676923076923077,0.6896551724137931,Similarly we can obtain the output of the backward subgraph H b and concatenate with Hf such that elements of the original sequence lineup ,25,Output for the forward subgraph is given by,The collection of all previous output representations Mt = [ h 0 ; h 1 ; . . . ; h t?1 ] can be viewed as the memory available to the recurrent model at time - step t.,method
text-classification,6,The DAN encoder is trained similarly to the Transformer based encoder .,model,Deep Averaging Network (DAN),0,57,29,4,0,model : Deep Averaging Network (DAN),0.3851351351351352,0.90625,0.5714285714285714,The DAN encoder is trained similarly to the Transformer based encoder ,12,"Similar to the Transformer encoder , the DAN encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional sentence embedding .",We make use of mul-titask learning whereby a single DAN encoder is used to supply sentence embeddings for multiple downstream tasks .,method
machine-translation,8,This drawback is not severe with the task of translation in which most of input and output sentences are only 15 - 40 words .,system description,LEARNING TO ALIGN,0,187,9,9,0,system description : LEARNING TO ALIGN,0.5649546827794562,0.4736842105263158,0.9,This drawback is not severe with the task of translation in which most of input and output sentences are only 15 40 words ,24,"Our approach , on the other hand , requires computing the annotation weight of every word in the source sentence for each word in the translation .","However , this may limit the applicability of the proposed scheme to other tasks .",method
relation-classification,1,We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.028455284552845524,0.8333333333333334,0.8333333333333334,We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods ,35,"Then , based on our tagging scheme , we study different end - toend models to extract entities and their relations directly , without identifying entities and relations separately .","What 's more , the end - to - end model proposed in this paper , achieves the best results on the public dataset .",abstract
named-entity-recognition,1,"Although RNNs can , in theory , learn long dependencies , in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence .",model,LSTM,0,33,7,4,0,model : LSTM,0.15942028985507245,0.3043478260869565,0.21052631578947367,Although RNNs can in theory learn long dependencies in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence ,29,"They take as input a sequence of vectors ( x 1 , x 2 , . . . , x n ) and return another sequence ( h 1 , h 2 , . . . , h n ) that represents some information about the sequence at every step in the input .",Long Short - term Memory Networks ( LSTMs ) have been designed to combat this issue by incorporating a memory - cell and have been shown to capture long - range dependencies .,method
question_similarity,0,"In this experiments set , we use the same structure described in Section 2.3 while changing the RNN cell type only .",experiment,Effect of RNN Cell Type,0,94,14,2,0,experiment : Effect of RNN Cell Type,0.6811594202898551,0.3684210526315789,0.25,In this experiments set we use the same structure described in Section 2 3 while changing the RNN cell type only ,22, ,"We use all 45,514 examples from the augmented dataset in the training process .",experiment
natural_language_inference,3,Our proposed architecture can deal with different sentence matching tasks .,training,Training,0,138,2,2,0,training : Training,0.6634615384615384,0.18181818181818185,0.2857142857142857,Our proposed architecture can deal with different sentence matching tasks ,11, ,The loss functions varies with different tasks .,experiment
natural_language_inference,78,We observe that the Sub and Concat compositions were more important than the Mul composition .,ablation,Ablation Study,1,248,7,7,0,ablation : Ablation Study,0.8985507246376812,0.7777777777777778,0.7777777777777778,We observe that the Sub and Concat compositions were more important than the Mul composition ,16,"Finally , in ( 7 - 9 ) , we remove the alignment features based on their composition type .","However , removing any of the three will result in some performance degradation .",result
natural_language_inference,96,Our methodology could potentially be extended to construct datasets free of ( possibly intersectional ) gender or racial bias .,dataset,dataset,0,323,9,9,0,dataset : dataset,0.8282051282051283,0.6,0.6,Our methodology could potentially be extended to construct datasets free of possibly intersectional gender or racial bias ,18,"However , it is not perfect in this regard , particularly due to biases in movies .","Prior work has studied learning grounded knowledge about objects and verbs : from knowledge bases , syntax parses ( Forbes and Choi , 2017 ) , word embeddings , and images and dictionary definitions ) .",experiment
text-classification,2,"In this work , we explore ways to scale these baselines to very large corpus with a large output space , in the context of text classification .",introduction,introduction,0,14,8,8,0,introduction : introduction,0.15053763440860216,0.8,0.8,In this work we explore ways to scale these baselines to very large corpus with a large output space in the context of text classification ,26,They also have the potential to scale to very large corpus .,"Inspired by the recent work in efficient word representation learning , we show that linear models with a rank constraint and a fast loss approximation can train on a billion words within ten minutes , while achieving performance on par with the state - of - the - art .",introduction
question_answering,5,"In addition , we see that our coverage - based re-ranker achieves consistently good performance on the three datasets , even though its performance is marginally lower than the strength - based re-ranker on the Search QA dataset .",result,result,1,192,6,6,0,result : result,0.6956521739130435,1.0,1.0,In addition we see that our coverage based re ranker achieves consistently good performance on the three datasets even though its performance is marginally lower than the strength based re ranker on the Search QA dataset ,37,"Moreover , our model is much better than the human performance on the Search QA dataset .", ,result
natural_language_inference,97,Comprehension tests pose questions based on short text passages to evaluate such understanding .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.017123287671232876,0.3333333333333333,0.3333333333333333,Comprehension tests pose questions based on short text passages to evaluate such understanding ,14,Understanding unstructured text is a major goal within natural language processing .,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .",abstract
natural_language_inference,88,The The FBI FBI is is chasing chasing a criminal on the run .,abstract,abstract,0,14,12,12,0,abstract : abstract,0.05668016194331984,0.7058823529411765,0.7058823529411765,The The FBI FBI is is chasing chasing a criminal on the run ,14,The The FBI FBI is is chasing a criminal on the run .,The The FBI FBI is is chasing chasing a a criminal on the run .,abstract
question_answering,0,"To summarize , the main contributions of our work are :",introduction,introduction,0,39,30,30,0,introduction : introduction,0.13220338983050847,0.8571428571428571,0.8571428571428571,To summarize the main contributions of our work are ,10,"Throughout the experiments , we use the Wikidata open - domain KB to construct semantic parses and retrieve the answers .",i ) Our analysis shows that the current solutions for KB QA do not perform well on complex questions ; ( ii ) We apply Gated Graph Neural Networks on directed graphs with labeled edges and adapt them to handle a large set of possible entities and relations types from the KB .,introduction
natural_language_inference,22,"Prediction During prediction , we use a local search strategy that for token indices a and a , we maximize p s a p ea , where 0 ? a ? a ? 15 . Dynamic programming is applied during search , resulting in O ( C ) time complexity .",training,Answer-Question Similarity Loss,0,130,10,8,0,training : Answer-Question Similarity Loss,0.5909090909090909,0.9090909090909092,0.8888888888888888,Prediction During prediction we use a local search strategy that for token indices a and a we maximize p s a p ea where 0 a a 15 Dynamic programming is applied during search resulting in O C time complexity ,41,"where s refers to the start index of answer span , e refers to the end index of the answer span , q BoW is the bag of words representation of query encoding , cos ( a , b) is the cosine similarity between a and b , C sand Ce are the s-th and e-th vector representation of context encoding .","Prediction During prediction , we use a local search strategy that for token indices a and a , we maximize p s a p ea , where 0 ? a ? a ? 15 . Dynamic programming is applied during search , resulting in O ( C ) time complexity .",experiment
natural_language_inference,94,"To ensure that the external knowledge is indeed helpful to the task , and also to explicitly link 2nd degree neighbor concepts within the context , we finish the process by grounding it again into context by connecting c 4 to c 5 ? C via r 4 , e.g. , child ? their .",method,method,0,149,81,81,0,method : method,0.3890339425587467,0.6377952755905512,0.6377952755905512,To ensure that the external knowledge is indeed helpful to the task and also to explicitly link 2nd degree neighbor concepts within the context we finish the process by grounding it again into context by connecting c 4 to c 5 C via r 4 e g child their ,50,"This emulates the gathering of useful external information to complete paths within the context , e.g. , house ? child , daughter ? child .","To ensure that the external knowledge is indeed helpful to the task , and also to explicitly link 2nd degree neighbor concepts within the context , we finish the process by grounding it again into context by connecting c 4 to c 5 ? C via r 4 , e.g. , child ? their .",method
natural_language_inference,50,The adoption of the pairwise hinge loss is motivated by the good empirical results demonstrated in Rao et al ..,approach,Optimization and Learning,0,155,63,9,1,approach : Optimization and Learning,0.4889589905362776,0.84,0.42857142857142855,The adoption of the pairwise hinge loss is motivated by the good empirical results demonstrated in Rao et al ,20,"where ? q is the set of all QA pairs for question q , s ( q , a ) is the score between q and a , and ? is the margin which controls the extent of discrimination between positive QA pairs and corrupted QA pairs .","Additionally , we also adopt the mix sampling strategy for sampling negative samples as described in their work .",method
sentiment_analysis,18,"target = "" food "" , aspect = food#quality , polarity = Pos ) ( target = "" food "" , aspect = food#prices , polarity = Neg )",dataset,Datasets,0,149,8,8,0,dataset : Datasets,0.6234309623430963,0.3809523809523809,0.3809523809523809,target food aspect food quality polarity Pos target food aspect food prices polarity Neg ,15,The food was delicious but expensive .,"Since our model only takes a sentence and an opinion target as input , without using the aspect information , we remove a sample in both training and test sets if the opinion target has different polarities as the example above .",experiment
natural_language_inference,17,"Next , a new question summarys is updated by fusing context information of the start position , which is computed as l = R 3 p 1 , into the old question summary : s = fusion ( s , l ) .",architecture,Model,0,179,136,6,0,architecture : Model,0.6884615384615385,0.9927007299270072,0.8571428571428571,Next a new question summarys is updated by fusing context information of the start position which is computed as l R 3 p 1 into the old question summary s fusion s l ,34,Then we compute the start probability p 1 ( i ) by heuristically attending the context representation R 3 with the question summary s as,Finally the end probability p 2 ( j| i ) is computed as,method
sentiment_analysis,27,"Despite these advances , the studies above still remain problems .",introduction,introduction,0,32,21,21,0,introduction : introduction,0.11594202898550725,0.5,0.5,Despite these advances the studies above still remain problems ,10,These attention - based models have proven to be successful and effective in learning aspect - specific representations .,"They all build models with each aspect individually ignoring the sentiment dependencies information between mul - tiple aspects , which will lose some additional valuable information .",introduction
natural_language_inference,55,We suppose that all potential answers are entities in the KB and that questions are sequences of words that include one identified KB entity .,system description,Task Definition,0,28,3,3,0,system description : Task Definition,0.19047619047619047,0.045454545454545456,0.375,We suppose that all potential answers are entities in the KB and that questions are sequences of words that include one identified KB entity ,25,Our main motivation is to provide a system for open QA able to be trained as long as it has access to : ( 1 ) a training set of questions paired with answers and ( 2 ) a KB providing a structure among answers .,"When this entity is not given , plain string matching is used to perform entity resolution .",method
natural_language_inference,14,The full dataset is available at https://github.com/Websail-NU /CODAH .,dataset,dataset,1,51,6,6,0,dataset : dataset,0.2982456140350877,0.2307692307692308,0.2307692307692308,The full dataset is available at https github com Websail NU CODAH ,13,"This task definition allows for easy evaluation by many state - of the - art models , such as BERT and GPT - 1 , and enables us to utilize the large SWAG dataset for pre-training .",We collected questions via a Web - based system .,experiment
temporal_information_extraction,0,"We use LIBLINEAR ( Fan et al. , 2008 ) L2 - loss linear SVM ( default parameters ) , and one - vs - rest strategy for multi-class classification .",system,Temporal Supervised Classifiers,0,73,46,3,1,system : Temporal Supervised Classifiers,0.3862433862433862,0.7076923076923077,0.375,We use LIBLINEAR Fan et al 2008 L2 loss linear SVM default parameters and one vs rest strategy for multi class classification ,23,"We build three supervised classification models , one for event - DCT ( E - D ) , one for event - timex ( E - T ) and one for event - event ( E -E ) pairs .","Tools and Resources Several external tools and resources are used to extract features from each temporal entity pair , including :",method
sentiment_analysis,27,"Firstly , the former module is used to get new representations of aspects based on the context .",methodology,Bidirectional attention mechanism,0,114,37,4,0,methodology : Bidirectional attention mechanism,0.4130434782608696,0.4352941176470588,0.8,Firstly the former module is used to get new representations of aspects based on the context ,17,This mechanism consists of two modules : context to aspect attention module and aspect to context attention module .,"Secondly , based on the new representations , the later module is employed to obtain the aspect - specific context representations which will be fed into the downstream GCN .",method
natural_language_inference,81,These two strategies of reasoning are respectively modeled by the coarse - grain and fine - grain modules of the CFC .,introduction,introduction,0,50,10,10,0,introduction : introduction,0.1187648456057007,0.5882352941176471,0.5882352941176471,These two strategies of reasoning are respectively modeled by the coarse grain and fine grain modules of the CFC ,20,"In fine - grain reasoning , the model matches specific finegrain contexts in which the candidate is mentioned with the query in order to gauge the relevance of the candidate .",Each module employs a novel hierarchical attention - a hierarchy of coattention and self - attention - to combine information from the support documents conditioned on the query and candidates .,introduction
natural_language_inference,20,Model Performance on the NLI task,evaluation,Model Performance on the NLI task,0,121,39,1,0,evaluation : Model Performance on the NLI task,0.4979423868312757,0.4148936170212766,0.125,Model Performance on the NLI task,6, , ,result
text_summarization,4,"In this new vector , important scores of non -top k entities are ??.",model,Pooling submodule,0,141,71,15,0,model : Pooling submodule,0.5465116279069767,0.7717391304347826,0.7894736842105263,In this new vector important scores of non top k entities are ,13,The sparse vector P is added to the original importance score vector G to create a new importance score vector .,"When softmax is applied , this gives very small , negligible , and closeto - zero values to non -top k entities .",method
natural_language_inference,6,Tatoeba 13 is an open collection of English sentences and high - quality translations into more than 300 languages .,training,Tatoeba: dataset,0,223,35,2,0,training : Tatoeba: dataset,0.8991935483870968,0.5833333333333334,0.2222222222222222,Tatoeba 13 is an open collection of English sentences and high quality translations into more than 300 languages ,19, ,The number of available translations is updated every Saturday .,experiment
natural_language_inference,7,"Second , we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN .",model,Learning objective EM F1,1,162,37,25,0,model : Learning objective EM F1,0.9101123595505618,0.8043478260869565,0.7352941176470589,Second we observe the importance of allowing interactions between the endpoints using the spanlevel FFNN ,16,"On the other hand , in RASOR , the semantics of an answer span is naturally encoded by the set of labels .","RASOR outperforms the endpoint prediction model by 1.1 in exact match , The interaction between endpoints enables RASOR to enforce consistency across its two substructures .",method
natural_language_inference,39,"Will fine - grained wordlevel information , which is crucial for similarity measurement , get lost in the coarse - grained sentence representations ? Is it really effective to "" cram "" whole sentence meanings into fixed - length vectors ?",introduction,introduction,0,14,7,7,0,introduction : introduction,0.06796116504854369,0.4117647058823529,0.4117647058823529,Will fine grained wordlevel information which is crucial for similarity measurement get lost in the coarse grained sentence representations Is it really effective to cram whole sentence meanings into fixed length vectors ,33,"Despite its conceptual simplicity , researchers have raised concerns about this approach :","Will fine - grained wordlevel information , which is crucial for similarity measurement , get lost in the coarse - grained sentence representations ? Is it really effective to "" cram "" whole sentence meanings into fixed - length vectors ?",introduction
sentiment_analysis,46,"Tree-LSTM : Memory cells was introduced by Tree - Structured Long Short - Term Memory and gates into tree - structured neural network , which is beneficial to capture semantic relatedness by parsing syntax trees .",baseline,Baselines,1,92,5,5,0,baseline : Baselines,0.7540983606557377,0.5,0.5,Tree LSTM Memory cells was introduced by Tree Structured Long Short Term Memory and gates into tree structured neural network which is beneficial to capture semantic relatedness by parsing syntax trees ,32,LSTM / Bi-LSTM : Cho et al. ( 2014 ) employs Long Short - Term Memory and the bidirectional variant to capture sequential information .,CNN : Convolutional Neural Networks ) is applied to generate task - specific sentence representation .,result
named-entity-recognition,8,The advantage of these approaches is that few parameters need to be learned from scratch .,system description,Unsupervised Fine-tuning Approaches,0,56,17,4,0,system description : Unsupervised Fine-tuning Approaches,0.14470284237726094,0.4358974358974359,0.2857142857142857,The advantage of these approaches is that few parameters need to be learned from scratch ,16,"More recently , sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine - tuned for a supervised downstream task .","At least partly due to this advantage , OpenAI achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark .",method
natural_language_inference,58,"Meanwhile , the analysis in also illustrates the huge variations in the di culty level with respect to questions in the CNN / Daily Mail datasets .",introduction,introduction,0,31,22,22,0,introduction : introduction,0.09253731343283582,0.5641025641025641,0.5641025641025641,Meanwhile the analysis in also illustrates the huge variations in the di culty level with respect to questions in the CNN Daily Mail datasets ,25,This behavior generally varies from document to document or question to question because it is related to the sophistication of the document or the di culty of the question .,"For a signi ca nt part of the datasets , this analysis shows that the problem can not be solved without appropriate reasoning on both its query and document .",introduction
sentiment_analysis,1,"We analyze the performance of our model across varying L1 sparsity coefficient ? ( see ) and noise coefficient in Emotion DL ( see and ) , as illustrated in .",analysis,Sensitivity Analysis,0,361,2,2,0,analysis : Sensitivity Analysis,0.9116161616161615,0.08333333333333333,0.16666666666666666,We analyze the performance of our model across varying L1 sparsity coefficient see and noise coefficient in Emotion DL see and as illustrated in ,25, ,"We analyze the performance of our model across varying L1 sparsity coefficient ? ( see ) and noise coefficient in Emotion DL ( see and ) , as illustrated in .",result
natural_language_inference,41,The new encoder can use a separate vocabulary from the original BART model .,architecture,Machine Translation,0,84,46,7,0,architecture : Machine Translation,0.32684824902723736,0.5168539325842697,0.7,The new encoder can use a separate vocabulary from the original BART model ,14,"The model is trained end - to - end , which trains the new encoder to map foreign words into an input that BART can de - noise to English .","We train the source encoder in two steps , in both cases backpropagating the cross - entropy loss from the output of the BART model .",method
sentiment_analysis,3,This phenomenon has been observed in a prior study .,analysis,Model Analysis,0,258,9,9,0,analysis : Model Analysis,0.8835616438356164,0.2571428571428571,0.3461538461538461,This phenomenon has been observed in a prior study ,10,"However , adding more context is contributing diminishing performance gain or even making negative impact in some datasets .","One possible explanation is that incorporating long contextual information may introduce additional noises , e.g. , polysemes expressing different meanings in different utterances of the same context .",result
text-classification,7,Single - Label to Multi - Label Text Classification,ablation,Single-Label to Multi-Label Text Classification,0,156,6,1,0,ablation : Single-Label to Multi-Label Text Classification,0.6419753086419753,0.10714285714285714,0.04166666666666666,Single Label to Multi Label Text Classification,7, , ,result
sentiment_analysis,14,"To make the final classification , the vector f m i,1:J is projected to the target label space by adding another fully connected layer ( i.e. parameterized by W and b ) , with a softmax activation .",model,CNN model,0,55,13,13,0,model : CNN model,0.3618421052631579,0.5416666666666666,1.0,To make the final classification the vector f m i 1 J is projected to the target label space by adding another fully connected layer i e parameterized by W and b with a softmax activation ,37,"All f m j for j ? [ 1 , J ] are then concatenated together to produce a vector representation f m 1:J of the whole input sentence .", ,method
natural_language_inference,3,"According to the phases of interaction between two sentences , previous models can be classified into three categories .",introduction,introduction,0,14,5,5,0,introduction : introduction,0.0673076923076923,0.35714285714285715,0.35714285714285715,According to the phases of interaction between two sentences previous models can be classified into three categories ,18,"Recently , deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses .","Models Some early works focus on sentence level interactions , such as ARC - I , and soon .",introduction
sentiment_analysis,38,Any trailing whitespace is removed and replaced with a space to simulate an end token .,experiment,Experimental Setup and Results,0,85,9,9,0,experiment : Experimental Setup and Results,0.5059523809523809,0.4090909090909091,0.4090909090909091,Any trailing whitespace is removed and replaced with a space to simulate an end token ,16,Any leading whitespace is removed and replaced with a newline + space to simulate a start token .,The text is encoded as a UTF - 8 byte sequence .,experiment
text_summarization,1,Question generation is the task of generating a question from a passage - answer pair .,experiment,Question Generation,0,145,6,2,0,experiment : Question Generation,0.6041666666666666,0.8571428571428571,0.6666666666666666,Question generation is the task of generating a question from a passage answer pair ,15, ,Answers are given as a span in the passage ( See ) .,experiment
sentiment_analysis,9,He and McAuley ( 2016 ) .,model,Model,0,230,9,9,1,model : Model,0.8303249097472925,0.6923076923076923,0.6923076923076923,He and McAuley 2016 ,5,We adopted the method proposed in to obtain the domain - adapted pre-trained BERT model based on the corpus of Yelp Dataset Challenge reviews 7 and the amazon Laptops review dataset,shows that the performance of APC task significantly improved by domain - adapted BERT model .,method
sarcasm_detection,1,"Also , in the case of user embeddings , CASCADE Main Pol user dis - balanced imbalanced cca concat .",analysis,Case Studies,0,324,27,16,0,analysis : Case Studies,0.9700598802395208,0.84375,0.9411764705882352,Also in the case of user embeddings CASCADE Main Pol user dis balanced imbalanced cca concat ,17,"Thus , sequential discourse modeling using RNNs maybe better suited for such cases .",course Acc. F1 Acc.,result
natural_language_inference,5,"First , we explore the effect of additional information by adopting a pretrained language model to compute the vector representation of the input text and by applying transfer learning from a large - scale corpus .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.034013605442176874,0.5,0.5,First we explore the effect of additional information by adopting a pretrained language model to compute the vector representation of the input text and by applying transfer learning from a large scale corpus ,34,"In this paper , we propose a novel method for a sentence - level answer- selection task that is a fundamental problem in natural language processing .","Second , we enhance the compare - aggregate model by proposing a novel latent clustering method to compute additional information within the target corpus and by changing the objective function from listwise to pointwise .",abstract
semantic_parsing,2,"For the missing details , we use the hidden vector ht to compute p ( y t |y <t , x , a ) , analogously to Equations ( 7 ) - ( 10 ) .",system description,Meaning Representation Generation,0,109,54,18,0,system description : Meaning Representation Generation,0.3745704467353952,1.0,1.0,For the missing details we use the hidden vector ht to compute p y t y t x a analogously to Equations 7 10 ,25,"In , sketch token "" NUMBER "" specifies that a numeric token should be emitted .", ,method
natural_language_inference,29,"h , b hare all trainable parameters .",system description,Consistency discriminator,0,219,124,29,0,system description : Consistency discriminator,0.6,0.775,0.6590909090909091,h b hare all trainable parameters ,7,So we have :,Here we apply the Vanilla generative adversarial network ( GAN ) with a sigmoid function on the D ( d * t ) to produce the classification probability and tries to minimize the Jensen - Shannon divergence between real and generated data distribution .,method
natural_language_inference,44,"First , we use the full document ( FULL ) .",system description,SQuAD and NewsQA,0,106,13,3,0,system description : SQuAD and NewsQA,0.3706293706293706,0.14285714285714285,0.037037037037037035,First we use the full document FULL ,8,"For each QA model , we experiment with three types of inputs .","Next , we give the model the oracle sentence containing the groundtruth answer span ( ORACLE ) .",method
text_summarization,0,There is also a discriminator which uses convolutional neural network to extract features and then distinguishes how similar is decoder focused aspect to reader focused aspect .,system description,The Proposed RASG Model Overview,0,102,15,7,0,system description : The Proposed RASG Model Overview,0.3388704318936877,0.3,0.875,There is also a discriminator which uses convolutional neural network to extract features and then distinguishes how similar is decoder focused aspect to reader focused aspect ,27,Supervisor measures the semantic gap between decoder focused aspect and reader focused aspect .,"Goal tracker utilizes the semantic gap learned by supervisor and the features extracted learned by the discriminator to set a goal , which is further utilized as a more specific guidance for summary generator to produce better summary .",method
sentiment_analysis,22,"From , we can observe that those words are paid much attention as we expect .",performance,performance,0,203,50,50,0,performance : performance,0.8864628820960698,0.9433962264150944,0.9433962264150944,From we can observe that those words are paid much attention as we expect ,15,"Obviously , the words "" freshest "" and "" most delicious "" play an important role in judging the sentiment polarity of "" array of sushi "" .","And it is worth noting that the word "" freshest "" obtains as much attention as "" delicious "" , although "" freshest "" is much farther from the target than "" delicious "" .",result
temporal_information_extraction,1,"De-spite the existence of denser annotation schemes ( e.g. , ) , the TLINK annotation task is quadratic in the number of nodes , and it is practically infeasible to annotate complete graphs .",training,Missing Annotations,0,155,80,4,0,training : Missing Annotations,0.603112840466926,0.7547169811320755,0.13333333333333333,De spite the existence of denser annotation schemes e g the TLINK annotation task is quadratic in the number of nodes and it is practically infeasible to annotate complete graphs ,31,"While some of these missing TLINKs can be inferred from existing ones , the vast majority still remain unknown as shown in .","Therefore , the problem of identifying these unknown relations in training and test is a major issue that dramatically hurts existing methods .",experiment
text_generation,3,"We also report the diversity of the generated responses by calculating the number of distinct unigrams , bigrams , and trigrams .",result,Results,0,102,9,9,0,result : Results,0.7234042553191491,0.2432432432432433,0.2432432432432433,We also report the diversity of the generated responses by calculating the number of distinct unigrams bigrams and trigrams ,20,"It is much more obvious than the improvement from the Seq2Seq model to the Seq2Seq + Attention , which is 0.29 BLEU - 4 point .",The results are shown in .,result
paraphrase_generation,1,This VAE - S model can bethought of as a variation of the proposed model where we remove the encoder LSTM related to the paraphrase sentence from the encoder side .,baseline,Baselines,0,125,8,8,0,baseline : Baselines,0.5656108597285068,0.8888888888888888,0.8888888888888888,This VAE S model can bethought of as a variation of the proposed model where we remove the encoder LSTM related to the paraphrase sentence from the encoder side ,30,"In the unsupervised version , the VAE generator reconstructs multiple variants of the input sentence using the VAE generative model trained only using the original sentence ( without their paraphrases ) ; in VAE - S , the VAE generator generates the paraphrase conditioned on the original sentence , just like in the proposed model .","Alternatively , it is akin to a variation of VAE where decoder is made supervised by making it to generate "" paraphrases "" ( instead of the reconstructing original sentence as in VAE ) by conditioning the decoder on the input sentence .",result
text_summarization,9,where bk j is the j - th column of the matrix Bk .,architecture,Attentive Encoder,0,71,34,10,0,architecture : Attentive Encoder,0.4640522875816994,0.7906976744186046,0.5263157894736842,where bk j is the j th column of the matrix Bk ,13,The output of convolution is given by :,"Thus the d dimensional aggregate embedding vector z i is defined as z i = [ z i 1 , . . . , z id ] .",method
text_summarization,1,show that pairwise similarity increases ( diversity ?) when the number of mixtures increases for Mixture Decoder .,result,Diversity vs. Number of Mixtures,1,216,10,3,0,result : Diversity vs. Number of Mixtures,0.9,0.38461538461538464,0.15789473684210525,show that pairwise similarity increases diversity when the number of mixtures increases for Mixture Decoder ,16,Here we compare the effect of number of mixtures in our SELECTOR and Mixture Decoder .,"While we observe a similar trend for SELECTOR in the question generation task , shows the opposite , and the rest are from our experiments using PG as a generator .",result
named-entity-recognition,1,Character - based models of words,training,Character-based models of words,0,127,59,1,0,training : Character-based models of words,0.6135265700483091,0.6941176470588235,0.0625,Character based models of words,5, , ,experiment
natural_language_inference,31,The input and output of the encoder are concatenated and then fed into an alignment layer to model the alignment and interaction between the two sequences .,approach,Our Approach,0,47,11,11,0,approach : Our Approach,0.17028985507246375,0.171875,0.5,The input and output of the encoder are concatenated and then fed into an alignment layer to model the alignment and interaction between the two sequences ,27,"Inside each block , a sequence encoder first computes contextual features of the sequence ( solid rectangles in ) .",fusion layer fuses the input and output of the alignment layer .,method
natural_language_inference,72,"We apply several baselines and state - of - the - art neural readers to the dataset , and observe a considerable gap in performance ( 20 % F1 ) between the best human and machine readers .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.019230769230769232,0.6666666666666666,0.6666666666666666,We apply several baselines and state of the art neural readers to the dataset and observe a considerable gap in performance 20 F1 between the best human and machine readers ,31,"Our dataset uses clinical case reports with around 100,000 gap - filling queries about these cases .",We analyze the skills required for successful answering and show how reader performance varies depending on the applicable skills .,abstract
natural_language_inference,37,We then sample two different paragraphs from this set each epoch .,system description,Sampling,0,155,13,5,0,system description : Sampling,0.603112840466926,0.65,0.4166666666666667,We then sample two different paragraphs from this set each epoch ,12,QA web we take the top four paragraphs ranked by TF - IDF score for each question - document pair .,"Since we observe that the higher - ranked paragraphs are much more likely to contain the context needed to answer the question , we sample the highest ranked paragraph that contains an answer twice as often as the others .",method
machine-translation,6,We tested these methods in machine translation and found the performance is not good .,experiment,Settings,0,202,46,28,0,experiment : Settings,0.6941580756013745,0.8846153846153846,0.8235294117647058,We tested these methods in machine translation and found the performance is not good ,15,"To improve the training for imbalanced labeled data , a common method is to adjust loss function by reweighting the training samples ; To regularize the parameter space , a common method is to use l 2 regularization .",Detailed analysis is provided in the supplementary material ( part B ) .,experiment
text_summarization,12,"The backward GRU reads the input sentence embeddings reversely , from right to left , and results in another sequence of hidden states , ( h 1 , h 2 , . . . , h n ) :",model,Sentence Encoder,0,92,13,7,0,model : Sentence Encoder,0.4035087719298245,0.2653061224489796,0.7777777777777778,The backward GRU reads the input sentence embeddings reversely from right to left and results in another sequence of hidden states h 1 h 2 h n ,28,"The forward GRU reads the input sentence word embeddings from left to right and gets a sequence of hidden states , ( h 1 , h 2 , . . . , h n ) .","The initial states of the BiGRU are set to zero vectors , i.e. , h 1 = 0 and h n =",method
sentiment_analysis,25,The performance of SVM depends on the availability of the features it can use .,analysis,ACSA,0,183,17,16,0,analysis : ACSA,0.8243243243243243,0.4722222222222222,0.6666666666666666,The performance of SVM depends on the availability of the features it can use ,15,"Convolutional neural networks CNN and GCN are not designed for aspect based sentiment analysis , but their performance exceeds that of ATAE - LSTM .","Without the large amount of sentiment lexicons , SVM perform worse than neural methods .",result
relation-classification,2,We formulate the problem as a multi-head selection problem extending previous work as described in Section 2.3 .,model,Joint model,0,109,4,4,0,model : Joint model,0.3694915254237288,0.061538461538461535,0.3076923076923077,We formulate the problem as a multi head selection problem extending previous work as described in Section 2 3 ,20,"The model is able to simultaneously identify the entities ( i.e. , types and boundaries ) and all the possible relations between them at once .","By multi-head , we mean that any particular entity maybe the CoNLL04 dataset is presented .",method
relation-classification,0,"When the predicted labels are inconsistent , we select the positive and more confident label , similar to .",model,Relation Classification,0,125,77,19,0,model : Relation Classification,0.5530973451327433,1.0,1.0,When the predicted labels are inconsistent we select the positive and more confident label similar to ,17,"Also , we assign two labels to each word pair in prediction since we consider both left - to - right and right - to - left directions .", ,method
natural_language_inference,98,"Illustrated by the case of hidden states of premise hp , or",method,Composition Layer,0,56,29,4,0,method : Composition Layer,0.4745762711864407,0.6304347826086957,0.2857142857142857,Illustrated by the case of hidden states of premise hp or,11,We propose intra-sentence gated - attention to obtain a fixed - length vector .,"Illustrated by the case of hidden states of premise hp , or",method
natural_language_inference,15,The word tight clothing in the hypothesis can be inferred from spandex in the premise .,ablation,ablation,0,183,32,32,0,ablation : ablation,0.8097345132743363,0.7804878048780488,0.7804878048780488,The word tight clothing in the hypothesis can be inferred from spandex in the premise ,16,"In the figure , we can see that tight , competing and bicycle are more important words than others in classifying the label .",And competing is also inferred from race .,result
natural_language_inference,4,We compute the second coattention layer in a similar fashion .,system description,MIXED OBJECTIVE USING SELF-CRITICAL POLICY LEARNING,0,66,41,8,0,system description : MIXED OBJECTIVE USING SELF-CRITICAL POLICY LEARNING,0.3333333333333333,0.42268041237113396,0.125,We compute the second coattention layer in a similar fashion ,11,Equation to equation 5 describe a single coattention layer .,"Namely , let coattn denote a multi-valued mapping whose inputs are the two input sequences E D 1 and E Q 1 .",method
text_summarization,4,"Due to its recent success , neural network models have been used with competitive results on abstractive summarization .",model,Extending from the base model,0,151,81,6,0,model : Extending from the base model,0.5852713178294574,0.8804347826086957,0.35294117647058826,Due to its recent success neural network models have been used with competitive results on abstractive summarization ,18,The concatenated vector is finally used to create the output vector :,"neural attention model was first applied to the task , easily achieving stateof - the - art performance on multiple datasets .",method
named-entity-recognition,7,"Our model turns out to be around 3 - 5 times faster than theirs , showing its scalability .",result,Decoding Speed,0,145,15,4,0,result : Decoding Speed,0.9177215189873418,0.9375,0.8,Our model turns out to be around 3 5 times faster than theirs showing its scalability ,17,"To compare the decoding speed , we re-implemented their model with the same platform ( PyTorch ) and run them on the same machine ( CPU : Intel i5 2.7 GHz ) .",We also additionally tried using embeddings trained on PubMed for GENIA but the performance was comparable .,result
question_answering,0,"Overall , we can conclude that the explicit modeling of the structure of semantic graphs with GGNN results in a better performance both in - domain on We bQSP - WD and out - of - domain on QALD - 7 that was only used for evaluation .",experiment,QALD-7,0,162,23,7,0,experiment : QALD-7,0.5491525423728814,1.0,1.0,Overall we can conclude that the explicit modeling of the structure of semantic graphs with GGNN results in a better performance both in domain on We bQSP WD and out of domain on QALD 7 that was only used for evaluation ,42,"QALD - 7 is much smaller , but also more complex on average ( cf. the average number of edges needed to find the correct answer in ) .", ,experiment
named-entity-recognition,2,OntoNotes 5.0 English NER,baseline,OntoNotes 5.0 English NER,1,194,34,1,0,baseline : OntoNotes 5.0 English NER,0.9107981220657276,0.9444444444444444,0.3333333333333333,OntoNotes 5 0 English NER,5, , ,result
sentiment_analysis,5,"Thus in this section , we investigate how the performance varies with relatively small numbers of electrodes .",The activity maps of the paired EEG electrodes,Electrodes reduction,0,240,16,3,0,The activity maps of the paired EEG electrodes : Electrodes reduction,0.9056603773584906,0.4444444444444444,0.3333333333333333,Thus in this section we investigate how the performance varies with relatively small numbers of electrodes ,17,"For the emotion recognition system in real - world applications , fewer electrodes will be preferred considering the feasibility and comfort .","Motivated by the results from , we select the paired electrodes on four brain are as referring to the locations of frontal and temporal lobes , denoted by Frontal ( 6 ) , Frontal ( 10 ) , Temporal and Temporal ( 9 ) 4 .",others
natural_language_inference,75,"The ability to encode prior knowledge in this way is an important component of KV - MemNNs , and we are free to define ? X , ? Y , ? K and ? V for the query , answer , keys and values respectively .",model,Key-Value Memories,0,81,32,3,0,model : Key-Value Memories,0.3990147783251232,0.2077922077922078,0.07317073170731707,The ability to encode prior knowledge in this way is an important component of KV MemNNs and we are free to define X Y K and V for the query answer keys and values respectively ,36,There are a variety of ways to employ key - value memories that can have important effects on over all performance .,"The ability to encode prior knowledge in this way is an important component of KV - MemNNs , and we are free to define ? X , ? Y , ? K and ? V for the query , answer , keys and values respectively .",method
temporal_information_extraction,1,"In this work , we follow the reduced set of temporal relation types used in CAEVO : before , after , includes , is included , equal , and vague .",system description,Temporal Relation Types,0,64,10,10,0,system description : Temporal Relation Types,0.2490272373540856,0.4761904761904762,1.0,In this work we follow the reduced set of temporal relation types used in CAEVO before after includes is included equal and vague ,24,"Due to the ambiguity in natural language , determining relations like before and immediately before can be a difficult task itself .", ,method
natural_language_inference,85,"As discussed above , it is intriguing to explore the effectiveness of syntax for natural language inference ; for example , whether it is useful even when incorporated into the best - performing models .",model,Input Encoding,0,68,23,15,0,model : Input Encoding,0.28936170212765955,0.22115384615384606,0.5357142857142857,As discussed above it is intriguing to explore the effectiveness of syntax for natural language inference for example whether it is useful even when incorporated into the best performing models ,31,We examined other recurrent memory blocks such as GRUs ( Gated Recurrent Units ) and they are inferior to LSTMs on the heldout set for our NLI task .,"To this end , we will also encode syntactic parse trees of a premise and hypothesis through tree - LSTM , which extends the chain LSTM to a recursive network .",method
text_generation,1,baked mother cake sits on a street with a rear of it .,experiment,SeqGAN (Baseline),0,224,60,2,0,experiment : SeqGAN (Baseline),0.8175182481751825,0.5825242718446602,0.3333333333333333,baked mother cake sits on a street with a rear of it ,13, ,tennis player who is in the ocean .,experiment
natural_language_inference,0,"As an example , human readers are able to keep the question in mind during multiple passes of reading , to successively mask away information irrelevant to the query .",system description,Gated-Attention Reader,0,55,7,7,0,system description : Gated-Attention Reader,0.27918781725888325,0.5384615384615384,0.5384615384615384,As an example human readers are able to keep the question in mind during multiple passes of reading to successively mask away information irrelevant to the query ,28,"In reading comprehension tasks , ideally , the semantic meanings carried by the contextual embeddings should be aware of the query across hops .","However , existing neural network readers are restricted to either attend to tokens or entire sentences , with the assumption that certain sub-parts of the document are more important than others .",method
text_generation,2,We thus call it a leakage of information from D.,introduction,introduction,0,39,27,27,0,introduction : introduction,0.11142857142857143,0.7105263157894737,0.7105263157894737,We thus call it a leakage of information from D ,11,As the information from D is internally - maintained and in an adversarial game it is not supposed to provide G with such information .,"Next , given the goal embedding produced by the MAN - AGER , the WORKER first encodes current generated words with another LSTM , then combines the output of the LSTM and the goal embedding to take a final action at current state .",introduction
natural_language_inference,62,typical category of general knowledge is inter-word semantic connections .,introduction,introduction,0,21,13,13,0,introduction : introduction,0.09375,0.7222222222222222,0.7222222222222222,typical category of general knowledge is inter word semantic connections ,11,"The reason for these phenomena , we believe , is that MRC models can only utilize the knowledge contained in each given passagequestion pair , but in addition to this , human beings can also utilize general knowledge .","As shown in , such general knowledge is essential to the reading comprehension ability of human beings .",introduction
semantic_role_labeling,3,"mt is set to 1 if the corresponding word is a predicate , or 0 if not .",system description,Pipeline,0,129,91,4,0,system description : Pipeline,0.4886363636363637,0.8348623853211009,0.18181818181818185,mt is set to 1 if the corresponding word is a predicate or 0 if not ,17,We take the very original utterances and the corresponding predicate masks m as the input features .,"Formally , in SRL task , we have a word vocabulary V and mask vocabulary C = { 0 , 1 }.",method
natural_language_inference,31,Vanilla multi-layer convolutional networks with same padding ) are adopted as the encoder .,approach,Our Approach,0,55,19,19,0,approach : Our Approach,0.19927536231884047,0.296875,0.8636363636363636,Vanilla multi layer convolutional networks with same padding are adopted as the encoder ,14,"We use only word embeddings in the embedding layer , without character embeddings or syntactic features .","Recurrent networks are slower and do not lead to further improvements , so they are not adopted here .",method
natural_language_inference,75,The KB is stored as triples ; see for examples .,model,Doc,0,138,89,9,0,model : Doc,0.6798029556650246,0.577922077922078,0.45,The KB is stored as triples see for examples ,10,"We built a KB using OMDb and MovieLens metadata with entries for each movie and nine different relation types : director , writer , actor , release year , language , genre , tags , IMDb rating and IMDb votes , with ? 10 k related actors , ? 6 k directors and ? 43 k entities in total .","IMDb ratings and votes are originally real - valued but are binned and converted to text ( "" unheard of "" , "" unknown "" , "" well known "" , "" highly watched "" , "" famous "" ) .",method
natural_language_inference,12,"Next , in , we show that fine - tuning the word embeddings also improves results , again for both the in - domain task and cross - domain tasks ( the ablation results are based on a smaller model with a 128 +256 2 - layer biLSTM ) .",ablation,Ablation Analysis Results,1,69,11,11,0,ablation : Ablation Analysis Results,0.7840909090909091,0.5,0.7857142857142857,Next in we show that fine tuning the word embeddings also improves results again for both the in domain task and cross domain tasks the ablation results are based on a smaller model with a 128 256 2 layer biLSTM ,41,This demonstrates that simply stacking the biLSTM layers is not sufficient to handle a complex task like Multi - NLI and it is significantly better to have the higher layer connected to both the output and the original input of all the previous layers ( note that results are based on multi-layered models with shortcut connections ) .,"Hence , all our models were trained with word embeddings being fine - tuned .",result
text-to-speech_synthesis,1,We compute the focus rate for each head and choose the head with the largest F as the attention alignments .,system description,Duration Predictor,0,104,43,15,0,system description : Duration Predictor,0.4748858447488584,0.9555555555555556,0.8823529411764706,We compute the focus rate for each head and choose the head with the largest F as the attention alignments ,21,"= 1 SS s=1 max 1?t ? T a s ,t , where Sand T are the lengths of the ground - truth spectrograms and phonemes , a s ,t donates the element in the s - th row and t- th column of the attention matrix .","Finally , we extract the phoneme duration sequence D = [ d 1 , d 2 , ... , d n ] according to the duration extractor",method
relation-classification,0,"After decoding the entire model structure , we update the parameters simultaneously via backpropagation through time ( BPTT ) .",model,Model,0,54,6,6,0,model : Model,0.23893805309734514,0.07792207792207792,0.8571428571428571,After decoding the entire model structure we update the parameters simultaneously via backpropagation through time BPTT ,17,"During decoding , we build greedy , left - to - right entity detection on the sequence layer and realize relation classification on the dependency layers , where each subtree based LSTM - RNN corresponds to a relation candidate between two detected entities .","The dependency layers are stacked on the sequence layer , so the embedding and sequence layers are shared by both entity detection and relation classification , and the shared parameters are affected by both entity and relation labels .",method
natural_language_inference,21,We apply positional masks to attention distribution since they can easily encode prior structure knowledge such as temporal order and dependency parsing .,abstract,abstract,1,33,31,31,0,abstract : abstract,0.11379310344827588,0.7045454545454546,0.7045454545454546,We apply positional masks to attention distribution since they can easily encode prior structure knowledge such as temporal order and dependency parsing ,23,"We compute feature - wise attention since each element in a sequence is usually represented by a vector , e.g. , word / character embedding , and attention on different features can contain different information about dependency , thus to handle the variation of contexts around the same word .","This design mitigates the weakness of attention in modeling order information , and takes full advantage of parallel computing .",abstract
relation_extraction,5,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .",model,Baseline Models,1,128,7,7,0,model : Baseline Models,0.4866920152091255,0.5,0.5,Tree LSTM which is a recursive model that generalizes the LSTM to arbitrary tree structures ,16,"2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .","We investigate the child - sum variant of Tree - LSTM , and apply it to the dependency tree ( or part of it ) .",method
text_summarization,1,"Each annotator is asked to choose either a better method ( resulting in "" win "" or "" lose "" ) or "" tie "" if their quality is indistinguishable .",evaluation,Human Evaluation Setup,0,198,7,7,0,evaluation : Human Evaluation Setup,0.825,0.875,0.875,Each annotator is asked to choose either a better method resulting in win or lose or tie if their quality is indistinguishable ,23,They are instructed to select a question / summary that is more coherent with the source passage / document .,"Diversity and accuracy evaluations are conducted separately , and every pair of methods are presented to 10 annotators 1 .",result
natural_language_inference,36,"SRL task is generally formulated as multi-step classification subtasks in pipeline systems , consisting of predicate identification , predicate dis ambiguation , argument identification and argument classification .",system description,Semantic Role Labeling,0,80,44,8,0,system description : Semantic Role Labeling,0.3809523809523809,0.6875,0.5,SRL task is generally formulated as multi step classification subtasks in pipeline systems consisting of predicate identification predicate dis ambiguation argument identification and argument classification ,26,"Recently , SRL has aroused much attention from researchers and has been applied in many NLP tasks .",Most previous SRL approaches adopt a pipeline framework to handle these subtasks one after another .,method
sentiment_analysis,17,"For instance , in a Tree - LSTM over a dependency tree , each node in the tree takes the vector corresponding to the headword as input , whereas in a Tree - LSTM over a constituency tree , the leaf nodes take the corresponding word vectors as input .",system description,Tree-Structured LSTMs,0,87,48,16,0,system description : Tree-Structured LSTMs,0.38666666666666666,0.6153846153846154,1.0,For instance in a Tree LSTM over a dependency tree each node in the tree takes the vector corresponding to the headword as input whereas in a Tree LSTM over a constituency tree the leaf nodes take the corresponding word vectors as input ,44,The input word at each node depends on the tree structure used for the network ., ,method
natural_language_inference,74,We first obtain a similarity matrix A ? R nm between the premise and hypothesis by,system description,Interaction Layer,0,86,24,3,0,system description : Interaction Layer,0.38392857142857145,0.5106382978723404,0.15,We first obtain a similarity matrix A R nm between the premise and hypothesis by,15,"In this section , we feed the results of the encoding layer and the learned sentence encoder into the attention mechanism , which is responsible for linking and fusing information from the premise and the hypothesis words .",We first obtain a similarity matrix A ? R nm between the premise and hypothesis by,method
sentiment_analysis,42,"Different from TDLSTM + ATT , the proposed memory network approach removes the recurrent calculator over word sequence and directly apply attention mechanism on context word representations .",method,Comparison to Other Methods,0,177,22,22,0,method : Comparison to Other Methods,0.7023809523809523,0.8461538461538461,0.8461538461538461,Different from TDLSTM ATT the proposed memory network approach removes the recurrent calculator over word sequence and directly apply attention mechanism on context word representations ,26,"Therefore , the model of TDLSTM + ATT actually selects such mixed semantics of word sequence , which is weird and not an intuitive way to selectively focus on parts of contexts .","We can also find that the performance of Contex - tAVG is very poor , which means that assigning the same weight / importance to all the context words is not an effective way .",method
machine-translation,7,"The mask itself is a function of G ? ( x ) and specifies which experts are assigned to each input example : M batchwise ( X , m ) j , i = 1 if X j, i is in the top m values for to expert i 0 otherwise",APPENDICES A LOAD-BALANCING LOSS,STRICTLY BALANCED GATING,0,359,137,8,0,APPENDICES A LOAD-BALANCING LOSS : STRICTLY BALANCED GATING,0.9624664879356568,0.9072847682119204,0.5714285714285714,The mask itself is a function of G x and specifies which experts are assigned to each input example M batchwise X m j i 1 if X j i is in the top m values for to expert i 0 otherwise,42,"To obtain a sparse gating vector , we multiply G ? ( x ) component - wise with a sparse mask M ( G ? ( x ) ) and normalize the output .","As our experiments suggest and also observed in , using a batchwise function during training ( such as M batchwise ) requires modifications to the inference when we may not have a large batch of examples .",others
natural_language_inference,11,"Where the system was confused by the answers ( i.e. , when the abstracts switched the production from correct to incorrect ) , no obvious information was present in 8 of the 11 cases , suggesting that the model had difficulty coping with unrelated background information .",analysis,Qualitative Analysis,0,178,17,17,0,analysis : Qualitative Analysis,0.644927536231884,0.8095238095238095,0.8095238095238095,Where the system was confused by the answers i e when the abstracts switched the production from correct to incorrect no obvious information was present in 8 of the 11 cases suggesting that the model had difficulty coping with unrelated background information ,43,"In 11 of the 14 corrections , obvious information is present in the Wikipedia abstracts that reinforced the correct answer .","In 3 of the 11 , plausibly relevant information was present in the abstract of the correct answer , yet the model still made the incorrect answer change .",result
sentiment_analysis,50,One possible explanation is that the label distribution is extremely unbalanced on these two datasets .,ablation,Ablation Tests,0,118,5,5,0,ablation : Ablation Tests,0.7283950617283951,0.625,0.625,One possible explanation is that the label distribution is extremely unbalanced on these two datasets ,16,3 ) Transfer of the embedding layer is more helpful on D3 and D4 .,Sentiment information is not adequately captured by Glo Ve word embeddings .,result
topic_models,0,There exist similar approximation techniques based on Quasi Monte Carlo sampling .,model,model,0,226,36,36,0,model : model,0.5485436893203883,0.972972972972973,0.972972972972973,There exist similar approximation techniques based on Quasi Monte Carlo sampling ,12,"In our proposed Bayesian SMM , we also encountered the same problem , and we approximated F using the re-parametrization trick ( Section III ) .","Unlike in LDA or CTM , Bayesian SMM does not require to make mean - field approximation , because the topic - word mixture is not Discrete thus eliminating the need for discrete latent variable z .",method
text_summarization,5,We utilize a widely - used Information Retrieval ( IR ) platform to find out candidate soft templates from the training corpus .,introduction,introduction,1,32,23,23,0,introduction : introduction,0.12698412698412698,0.6388888888888888,0.6388888888888888,We utilize a widely used Information Retrieval IR platform to find out candidate soft templates from the training corpus ,20,"We call our summarization system Re 3 Sum , which consists of three modules : Retrieve , Rerank and Rewrite .","Then , we extend the seq2seq model to jointly learn template saliency measurement ( Rerank ) and final summary generation ( Rewrite ) .",introduction
text_summarization,10,"Coverage loss is L cov ( ? ) = ti min (? t , i ,? t , i ) . Finally , the total loss is a weighted combination of cross - entropy loss and coverage loss :",model,Baseline Pointer+Coverage Model,0,67,22,18,0,model : Baseline Pointer+Coverage Model,0.25475285171102663,0.23655913978494625,0.9,Coverage loss is L cov ti min t i t i Finally the total loss is a weighted combination of cross entropy loss and coverage loss ,27,"We maintain a coverage vector ct = t?1 t=0 ? t that sums over all of the previous time steps attention distributions ? t , and this is added as input to the attention mechanism .",where ? is a tunable hyperparameter .,method
text_summarization,2,"In this work we use a two - layer stacked bi-directional Long Short - Term Memory networks as the encoder , where the input to the second layer is the concatenation of hidden states from the forward and backward passes of the first layer .",approach,The Basic Framework,0,62,11,4,0,approach : The Basic Framework,0.22302158273381287,0.08396946564885496,0.14285714285714285,In this work we use a two layer stacked bi directional Long Short Term Memory networks as the encoder where the input to the second layer is the concatenation of hidden states from the forward and backward passes of the first layer ,43,"An encoder condenses the entire source text to a continuous vector ; it also learns a vector representation for each unit of the source text ( e.g. , words as units ) .",We obtain the hidden states of the second layer ; they are denoted by he i .,method
relation_extraction,7,"To evaluate the effect of entity - wise attention combined with word - level attention , we utilize BGRU in three settings on our tree parsed data and original data .",experiment,Effect of the STP,0,210,21,9,0,experiment : Effect of the STP,0.8108108108108109,0.5121951219512195,0.5294117647058824,To evaluate the effect of entity wise attention combined with word level attention we utilize BGRU in three settings on our tree parsed data and original data ,28,2 ) The SDP method is not appropriate to handle low - quality sentences where key relation words are not in the SDP .,One setting is to use WLA mechanism only ( BGRU ) .,experiment
sentiment_analysis,35,"Empirically , extensive experiments demonstrate that the proposed MGAN model can achieve superior performances on two AT - level datasets from SemEval ' 14 ABSA challenge and an ungrammatical AT - level twitter dataset .",introduction,introduction,0,48,34,34,0,introduction : introduction,0.1935483870967742,0.9714285714285714,0.9714285714285714,Empirically extensive experiments demonstrate that the proposed MGAN model can achieve superior performances on two AT level datasets from SemEval 14 ABSA challenge and an ungrammatical AT level twitter dataset ,31,"Moreover , we build a large - scale multidomain dataset named YelpAspect with 100K samples for each domain to serve as highly beneficial source domains .","Our contributions of this paper are four - fold : ( 1 ) to the best of our knowledge , a novel transfer setting cross both domain and granularity is first proposed for aspect - level sentiment analysis ; ( 2 ) a new large - scale , multi-domain AC - level dataset is constructed ; ( 3 ) the novel Coarse 2 Fine attention is proposed to effectively reduce the aspect granularity gap between tasks ; ( 4 ) empirical studies verify the effectiveness of the proposed model on three AT - level benchmarks .",introduction
sentiment_analysis,28,"Moreover , essentially every training algorithm of RNN is the truncated BPTT , which affects the model 's ability to capture dependencies over longer time scales .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.12222222222222222,0.4137931034482759,0.4137931034482759,Moreover essentially every training algorithm of RNN is the truncated BPTT which affects the model s ability to capture dependencies over longer time scales ,25,"RNNs , such as LSTM , are very expressive , but they are hard to parallelize and backpropagation through time ( BPTT ) requires large amounts of memory and computation .","Although LSTM can alleviate the vanishing gradient problem to a certain extent and thus maintain long distance information , this usually requires a large amount of training data .",introduction
natural_language_inference,75,The motivation for this is that new evidence can be combined into the query to focus on and retrieve more pertinent information in subsequent accesses .,model,Model Description,0,70,21,21,0,model : Model Description,0.3448275862068966,0.13636363636363635,0.7241379310344828,The motivation for this is that new evidence can be combined into the query to focus on and retrieve more pertinent information in subsequent accesses ,26,The key addressing equation is transformed accordingly to use the updated query :,"Finally , after a fixed number H hops , the resulting state of the controller is used to compute a final prediction over the possible outputs :",method
natural_language_inference,80,Accuracies of the models on the training set and test set of SNLI .,result,result,0,176,7,7,0,result : result,0.6068965517241379,0.35,0.35,Accuracies of the models on the training set and test set of SNLI ,14,Note that the difference between DR - BiLSTM and Chen et al. :,"DR - BiLSTM ( Ensemble ) achieves the accuracy of 89.3 % , the best result observed on SNLI , while DR - BiLSTM ( Single ) obtains the accuracy of 88.5 % , which considerably outperforms the previous non-ensemble models .",result
text_summarization,5,"As can be seen , with different templates given , our model is likely to generate dissimilar summaries .",evaluation,Effect of Templates,1,222,61,20,0,evaluation : Effect of Templates,0.8809523809523809,0.9682539682539684,0.9090909090909092,As can be seen with different templates given our model is likely to generate dissimilar summaries ,17,"For the sake of comparison , we also present the 2 - best results of Open NMT with beam search .","In contrast , the 2 - best results of Open NMT is almost the same , and often a shorter summary is only apiece of the other one .",result
sentiment_analysis,36,"SVM : It is a traditional support vector machine based model with extensive feature engineering ; AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ; AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ; IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ; CNN - ASP :",experiment,Experimental Setup,1,153,12,11,0,experiment : Experimental Setup,0.612,0.3428571428571429,0.3235294117647059,SVM It is a traditional support vector machine based model with extensive feature engineering AdaRNN It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree AE LSTM and ATAE LSTM AE LSTM is a simple LSTM model incorporating the target embedding as input while ATAE LSTM extends AE LSTM with attention IAN IAN employs two LSTMs to learn the representations of the context and the target phrase interactively CNN ASP ,77,"SVM : It is a traditional support vector machine based model with extensive feature engineering ; AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ; AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ; IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ; CNN - ASP :","SVM : It is a traditional support vector machine based model with extensive feature engineering ; AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ; AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ; IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ; CNN - ASP :",experiment
natural_language_inference,88,"It is also possible to have a more structured relational reasoning module by stacking multiple memory and hidden layers in an alternating fashion , resembling a stacked LSTM or a multi-hop memory network .",system description,Long Short-Term Memory-Network,0,111,39,29,0,system description : Long Short-Term Memory-Network,0.4493927125506073,0.5909090909090909,0.90625,It is also possible to have a more structured relational reasoning module by stacking multiple memory and hidden layers in an alternating fashion resembling a stacked LSTM or a multi hop memory network ,34,"Although it is appealing to provide direct supervision for the attention layer , e.g. , with evidence collected from a dependency treebank , we treat it as a submodule being optimized within the larger network in a downstream task .",This can be achieved by feeding the output h kt of the lower layer k as input to the upper layer ( k + 1 ) .,method
text_generation,5,Comparing the NLL results of Comparison with related methods :,result,Semi-supervised VAE results,0,221,37,12,0,result : Semi-supervised VAE results,0.7491525423728813,0.4065934065934066,0.4444444444444444,Comparing the NLL results of Comparison with related methods ,10,"This classification accuracy and NLL trade off once again verifies our conjecture : with small contextual window size , the decoder is forced to use the encoder information , hence the latent representation is better , they denotes the LSTM is initialized with a sequence autoencoder and a language model .","We compare Semisupervised VAE with the methods from , which represent the previous state - of - the - art for semisupervised sequence learning .",result
natural_language_inference,27,"In addition , we can combine this method with other data augmentation methods , such as , the type swap method , to acquire more diversity in paraphrases .",model,English to French NMT,0,175,131,34,0,model : English to French NMT,0.5177514792899408,0.9849624060150376,0.9444444444444444,In addition we can combine this method with other data augmentation methods such as the type swap method to acquire more diversity in paraphrases ,25,The diversity can be improved by both sampling during the beam search decoding and paraphrasing questions and answers in the dataset as well .,"In our experiments , we observe that the proposed data augmentation can bring non-trivial improvement in terms of accuracy .",method
natural_language_inference,74,where is elementwise product .,model,model,0,61,11,11,0,model : model,0.2723214285714285,0.9166666666666666,0.9166666666666666,where is elementwise product ,5,"To predict the discource marker between S 1 and S 2 , we combine the representations of them with some linear operation :",Finally we project r to a vector of label size ( the total number of discourse markers in the dataset ) and use softmax function to normalize the probability distribution .,method
natural_language_inference,38,propose self - aware representation and multi-hop query - sensitive pointer to predict the answer span .,system description,Attention Based Models for Machine Reading,0,180,7,7,0,system description : Attention Based Models for Machine Reading,0.9523809523809524,0.7,0.7,propose self aware representation and multi hop query sensitive pointer to predict the answer span ,16,propose a bi-directional attention flow to achieve a query - aware context representation .,propose iterarively inferring the answer with a dynamic number of steps trained with reinforcement learning .,method
natural_language_inference,81,"Honey gets its sweetness from the monosaccharides fructose and glucose , and has about the same relative sweetness as granulated sugar .",APPENDIX,Answer town,0,359,151,71,0,APPENDIX : Answer town,0.8527315914489311,0.7089201877934272,0.5338345864661654,Honey gets its sweetness from the monosaccharides fructose and glucose and has about the same relative sweetness as granulated sugar ,21,"The variety of honey produced by honey bees ( the genus "" Apis "" ) is the most well - known , due to its worldwide commercial production and human consumption .",It has attractive chemical properties for baking and a distinctive flavor that leads some people to prefer it to sugar and other sweeteners .,others
question_answering,4,"Overall , we propose DECAPROP ( Densely Connected Attention Propagation ) , a novel architecture for reading comprehension .",introduction,introduction,1,41,30,30,0,introduction : introduction,0.15953307392996108,0.9375,0.9375,Overall we propose DECAPROP Densely Connected Attention Propagation a novel architecture for reading comprehension ,15,"Therefore , this enables multiple bidirectional attention calls to be executed without much concern , allowing us to efficiently connect multiple layers together .","DECAPROP achieves a significant gain of 2.6 % ? 14.2 % absolute improvement in F1 score over the existing state - of - the - art on four challenging RC datasets , namely News QA , Quasar - T , Search QA and Narrative QA .",introduction
semantic_role_labeling,4,Another finding is that ELMo improves the model performance for span boundary identification .,introduction,introduction,0,31,23,23,0,introduction : introduction,0.10333333333333332,0.8518518518518519,0.8518518518518519,Another finding is that ELMo improves the model performance for span boundary identification ,14,Empirical analysis on these results shows that the label prediction ability of our span - based model is better than that of the CRF - based model .,"In summary , our main contributions include :",introduction
sentiment_analysis,46,"In this work , we propose a Multi- sentimentresource Enhanced Attention Network ( MEAN ) for sentence - level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi -path attention mechanism .",introduction,introduction,1,15,8,8,0,introduction : introduction,0.12295081967213115,0.5,0.5,In this work we propose a Multi sentimentresource Enhanced Attention Network MEAN for sentence level sentiment classification to integrate many kinds of sentiment linguistic knowledge into deep neural networks via multi path attention mechanism ,35,"Despite its usefulness , to date , the sentiment linguistic knowledge has been underutilized in most recent deep neural network models ( e.g. , CNNs and LSTMs ) .","Specifically , we first design a coupled word embedding module to model the word representation from character - level and word - level semantics .",introduction
natural_language_inference,17,"Nevertheless , we observe that DCRL constantly makes more accurate prediction on answer spans , especially when SCST already points a rough boundary .",analysis,analysis,0,234,5,5,0,analysis : analysis,0.9,0.21739130434782608,0.21739130434782608,Nevertheless we observe that DCRL constantly makes more accurate prediction on answer spans especially when SCST already points a rough boundary ,22,"For example , the first example shows that both four and two are retrieved when the questions asks for how many .","In the second example , SCST takes the whole phrase after Dyrrachium as its location .",result
paraphrase_generation,1,"The parameters of the decoder distribution ? are defined by the outputs of another feedforward neural networks , akin to the VAE encoder model .",methodology,Variational Autoencoder (VAE),0,57,16,11,0,methodology : Variational Autoencoder (VAE),0.2579185520361991,0.5925925925925926,0.5,The parameters of the decoder distribution are defined by the outputs of another feedforward neural networks akin to the VAE encoder model ,23,"The VAE also consists of a decoder model , which is another distribution p ? ( x |z ) that takes as input a random latent code z and produces an observation x .","The parameters of the decoder distribution ? are defined by the outputs of another feedforward neural networks , akin to the VAE encoder model .",method
natural_language_inference,24,"In summary , we think it is useful to perform some approximate hyper - parameter tuning for the number of steps , but it is not necessary to find the exact optimal value .",result,SAN:,0,172,40,30,0,result : SAN:,0.7350427350427351,0.5,0.42857142857142855,In summary we think it is useful to perform some approximate hyper parameter tuning for the number of steps but it is not necessary to find the exact optimal value ,31,"In fact , the EM / F1 scores drop slightly , but considering that the random initialization results in show a standard deviation of 0.142 and a spread of 0.426 ( for EM ) , we believe that the T = 10 result does not statistically differ from the T = 5 result .","Finally , we test SAN on two Adversarial SQuAD datasets , AddSent and Add OneSent , where the passages contain auto - generated adversarial distracting sentences to fool computer systems thatare developed to answer questions about the passages .",result
natural_language_inference,5,"The vocabulary size in the WiKiQA , TREC - QA and QNLI dataset are 30,104 , 56,908 and 154,442 , respectively .",implementation,implementation,1,119,9,9,0,implementation : implementation,0.8095238095238095,0.6,0.6,The vocabulary size in the WiKiQA TREC QA and QNLI dataset are 30 104 56 908 and 154 442 respectively ,21,"In both datasets , we apply 8 latent clusters .","When applying the TL , the vocabulary size is set to 154,442 , and the dimension of the context projection weight matrix is set to 300 .",experiment
text-to-speech_synthesis,2,We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.028571428571428567,0.8333333333333334,0.8333333333333334,We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance ,25,"We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively - trained speaker encoder to the multispeaker TTS task , and is able to synthesize natural speech from speakers unseen during training .","Finally , we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training , indicating that the model has learned a high quality speaker representation .",abstract
machine-translation,1,The first variant replaces the convolutional decoder with a recurrent one that is similarly stacked and dynamically unfolded .,model,model,0,119,8,8,0,model : model,0.5920398009950248,0.2424242424242425,0.2424242424242425,The first variant replaces the convolutional decoder with a recurrent one that is similarly stacked and dynamically unfolded ,19,We may consider two variants of the ByteNet that use recurrent networks for one or both of the networks ( see ) .,"The second variant also replaces the convolutional encoder with a recurrent encoder , e.g. a bidirectional RNN .",method
natural_language_inference,21,Sentence 1 is Families have some dogs in front of a carousel and sentence 2 is volleyball match is in progress between ladies .,model,Sentence Classifications,0,263,37,10,0,model : Sentence Classifications,0.906896551724138,0.6379310344827587,0.9090909090909092,Sentence 1 is Families have some dogs in front of a carousel and sentence 2 is volleyball match is in progress between ladies ,24,We select two sentences from SNLI test set as examples for this case study .,"shows that1 ) semantically important words such as nouns and verbs usually get large attention , but stop words ( am , is , are , etc. ) do not ; 2 ) globally important words , e.g. , volleyball , match , ladies in sentence 1 and dog , front , carousel in sentence 2 , get large attention from all other words ; 3 ) if a word is important to only some of the other words ( e.g. to constitute a phrase or sense - group ) , it gets large attention only from these words , e.g. , attention between progress , between in sentence1 , and attention between families , have in sentence 2 .",method
sentiment_analysis,27,where Wac ? R 2 d hid 2 d hid is the attention weight matrix .,methodology,Context to aspect attention,0,128,51,13,0,methodology : Context to aspect attention,0.463768115942029,0.6,0.8666666666666667,where Wac R 2 d hid 2 d hid is the attention weight matrix ,15,The process can be formulated as follows :,where Wac ? R 2 d hid 2 d hid is the attention weight matrix .,method
natural_language_inference,71,The activations of the update gates for GORU and GRU on the parenthesis task .,experiment,Parenthesis Task,0,149,50,14,0,experiment : Parenthesis Task,0.6930232558139535,0.8333333333333334,0.875,The activations of the update gates for GORU and GRU on the parenthesis task ,15,"According to the histogram of activations shown in , both models behave very similarly , and when the model receives noise as input , the activations of its :","In those bar-charts we visualize the number of activations thatare greater than 0.7 normalized by the total number of units in the gate , As can be seen in the top two plots , the activations of the update gate peaks and becomes almost one when the input is noise when the magnitude of the noise input in the bottom plot is above the red bar .",experiment
natural_language_inference,61,"Besides LSTM , a ention mechanisms could also be used to boost thee ectiveness of sentence encoding .",introduction,introduction,0,34,27,27,0,introduction : introduction,0.21794871794871795,0.5294117647058824,0.5294117647058824,Besides LSTM a ention mechanisms could also be used to boost thee ectiveness of sentence encoding ,17,"is model applied parameters of one Bi - LSTM to initialize the next Bi - LSTM to convey information , which shown beer results than the model with a single Bi - LSTM .","model developed by Ghaeini et al . added selfa ention to LSTM model , and achieved beer performance .",introduction
natural_language_inference,78,"In the second setting , cross sentence is allowed .",experiment,Experimental Setup,0,177,9,9,0,experiment : Experimental Setup,0.6413043478260869,0.4090909090909091,0.8181818181818182,In the second setting cross sentence is allowed ,9,The first setting dis allows cross sentence at - tention .,The last ( third ) setting is a comparison between model ensembles while the first two settings only comprise single models .,experiment
machine-translation,3,Data Voc BLEU RNNsearch 4.5M 50K 16.5 RNNsearch-LV 4.5M 500K 16.9 SMT 4.5 M Full 20.7 Deep - Att ( Ours ) 4.5M 160K 20.6 : English - to - German task : BLEU scores of single neural models .,method,Methods,0,253,2,2,0,method : Methods,0.8083067092651757,0.1,0.6666666666666666,Data Voc BLEU RNNsearch 4 5M 50K 16 5 RNNsearch LV 4 5M 500K 16 9 SMT 4 5 M Full 20 7 Deep Att Ours 4 5M 160K 20 6 English to German task BLEU scores of single neural models ,42, ,We also list the conventional SMT system for comparison .,method
relation-classification,4,"To study the contribution of each component in the C - GCN model , we ran an ablation study on the TACRED dev set ) .",ablation,Ablation Study,1,179,2,2,0,ablation : Ablation Study,0.683206106870229,0.1,0.3333333333333333,To study the contribution of each component in the C GCN model we ran an ablation study on the TACRED dev set ,23, ,We find that : The entity representations and feedforward layers contribute 1.0 F 1 .,result
sentence_classification,2,We experiment on using only one additional context ( N = 1 ) and using all ten languages at once ( N = 10 ) .,experiment,Experimental Setting,1,151,13,12,0,experiment : Experimental Setting,0.5992063492063492,0.4642857142857143,0.4444444444444444,We experiment on using only one additional context N 1 and using all ten languages at once N 10 ,20,Tokenization is done using the polyglot library 7 .,"For N = 1 , we only show the accuracy of the best classifier for conciseness .",experiment
text_summarization,12,We use Stochastic Gradient Descent ( SGD ) with minibatch to learn the model parameter ?.,model,Objective Function,0,128,49,6,0,model : Objective Function,0.5614035087719298,1.0,1.0,We use Stochastic Gradient Descent SGD with minibatch to learn the model parameter ,14,where D denotes a set of parallel sentencesummary pairs and ? is the model parameter ., ,method
sentiment_analysis,11,UAR is a popular metric that is used when dealing with imbalanced classes .,baseline,bc-LSTM:,0,280,26,21,0,baseline : bc-LSTM:,0.813953488372093,0.8125,0.7777777777777778,UAR is a popular metric that is used when dealing with imbalanced classes ,14,We report scores using weighted accuracy ( WAA ) and unweighted recall ( UAR ) .,Results are an average of 10 runs with varied weight initializations .,result
sentence_compression,1,"labels : 1 , if a word is to be retained in the compression , 0 if a word is to be deleted , or EOS , which is the output label used for the "" GO "" input and the end - of - sentence final period .",baseline,Baseline,0,95,58,58,0,baseline : Baseline,0.4545454545454545,0.6744186046511628,0.6744186046511628,labels 1 if a word is to be retained in the compression 0 if a word is to be deleted or EOS which is the output label used for the GO input and the end of sentence final period ,40,"Note that this basic structure is then unrolled 120 times , with the standard dependences from LSTM networks .","In the simplest implementation , that we call LSTM , the input layer has 259 dimensions .",result
sentence_compression,2,"These and other recent applications of bi - LSTMs were constructed for solving a single task in isolation , however .",system description,Words FIRST PASS REGRESSIONS,0,55,36,11,0,system description : Words FIRST PASS REGRESSIONS,0.5445544554455446,0.7346938775510204,0.4583333333333333,These and other recent applications of bi LSTMs were constructed for solving a single task in isolation however ,19,"Bi - LSTMs have already been used for finegrained sentiment analysis , syntactic chunking , and semantic role labeling .","We instead train deep bi - LSTMs to solve additional tasks to sentence compression , namely CCG - tagging and gaze prediction , using the additional tasks to regularize our sentence compression model .",method
relation_extraction,8,"where the index j ranges from 1 to s + w ? 1 . Outof - range input values q i , where i < 1 or i > s , are taken to be zero .",methodology,Convolution,0,133,49,14,0,methodology : Convolution,0.4944237918215613,0.4757281553398058,0.7777777777777778,where the index j ranges from 1 to s w 1 Outof range input values q i where i 1 or i s are taken to be zero ,29,The convolution operation involves taking the dot product of w with each w- gram in the sequence q to obtain another sequence c ? R s+w?1 :,"where the index j ranges from 1 to s + w ? 1 . Outof - range input values q i , where i < 1 or i > s , are taken to be zero .",method
natural_language_inference,55,"Instead , we consider the following general approach : given that we are predicting a path , we can predict its elements in turn using a beam search , and hence avoid scoring all candidates .",training,Inference,0,121,16,13,0,training : Inference,0.8231292517006803,0.64,0.5909090909090909,Instead we consider the following general approach given that we are predicting a path we can predict its elements in turn using a beam search and hence avoid scoring all candidates ,32,We do not add all such quadruplets since this would lead to very large candidate sets .,"Specifically , our model first ranks relation types using Eq. ( 1 ) , i.e. selects which relation types are the most likely to be expressed in q .",experiment
sentiment_analysis,40,"For each method , the maximum number of training iterations is 100 , and the model with the minimum training error is utilized for testing .",method,Compared Methods,0,162,16,16,0,method : Compared Methods,0.726457399103139,0.9411764705882352,0.9411764705882352,For each method the maximum number of training iterations is 100 and the model with the minimum training error is utilized for testing ,24,We produce its results on all four datasets with the code released by the authors .,We will discuss different settings of RAM later .,method
natural_language_inference,62,"The reasons for these achievements , we believe , are as follows :",analysis,Analysis,0,213,3,3,0,analysis : Analysis,0.9508928571428572,0.21428571428571427,0.21428571428571427,The reasons for these achievements we believe are as follows ,11,"According to the experimental results , KAR is not only comparable in performance with the state - of the - art MRC models , but also superior to them in terms of both the hunger for data and the robust - ness to noise .",KAR is designed to utilize the pre-extracted inter-word semantic connections from the data enrichment method .,result
natural_language_inference,73,"The parameters in ReSAN can be divided into two parts , ? r for the RSS modules and ? s for the rest parts which includes word embeddings , soft self - attention module , and classification / regression layers .",training,Model Training,0,144,2,2,0,training : Model Training,0.5496183206106869,0.05263157894736842,0.05263157894736842,The parameters in ReSAN can be divided into two parts r for the RSS modules and s for the rest parts which includes word embeddings soft self attention module and classification regression layers ,34, ,"The parameters in ReSAN can be divided into two parts , ? r for the RSS modules and ? s for the rest parts which includes word embeddings , soft self - attention module , and classification / regression layers .",experiment
natural_language_inference,80,"Next , the model employs a soft attention mechanism to extract relevant information from these encodings .",introduction,introduction,1,32,23,23,0,introduction : introduction,0.1103448275862069,0.7931034482758621,0.7931034482758621,Next the model employs a soft attention mechanism to extract relevant information from these encodings ,16,"Given a premise u and a hypothesis v , our model first encodes them considering dependency on each other .","The augmented sentence representations are then passed to the inference stage , which uses a similar dependent reading strategy in both directions , i.e. u ? v and v ? u .",introduction
natural_language_inference,9,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .",model,MODEL DETAILS,1,212,32,32,0,model : MODEL DETAILS,0.6424242424242425,0.4050632911392405,0.9411764705882352,The loss is minimized by stochastic gradient descent for maximally 500 epochs but training is early stopped if the loss on the development data does not decrease for 50 epochs ,31,The loss function is the cross entropy between v and the one - hot vector of the true answer .,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,method
named-entity-recognition,0,It takes ON 4 for one iteration as shown in .,system description,Classifiers,0,43,21,21,0,system description : Classifiers,0.15867158671586715,0.11797752808988765,0.5384615384615384,It takes ON 4 for one iteration as shown in ,11,"Unfortunately , in terms of computational costs , the solver is highly complex .",This is infeasible for the case of large N ( e.g. it takes 2000 + hours for a case of N = 13000 ) .,method
text_summarization,1,We set t- th focus guide m guide t to 1 if t-th source token x t is focused in target sequence y and 0 otherwise .,training,Training,0,122,4,4,0,training : Training,0.5083333333333333,0.19047619047619047,0.19047619047619047,We set t th focus guide m guide t to 1 if t th source token x t is focused in target sequence y and 0 otherwise ,28,We instead create focus guide and use it to independently and directly train the SELECTOR and the generator . ) is a simple proxy of whether a source token is focused during generation .,"During training , m guide acts as a target for SELECTOR and is a given input for generator ( teacher forcing ) .",experiment
text_generation,2,"Automatically generating coherent and semantically meaningful text has many applications in machine translation , dialogue systems , image captioning , etc .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.011428571428571429,0.2,0.2,Automatically generating coherent and semantically meaningful text has many applications in machine translation dialogue systems image captioning etc ,19, ,"Recently , by combining with policy gradient , Generative Adversarial Nets ( GAN ) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation .",abstract
paraphrase_generation,0,We also validated our method by evaluating the obtained embeddings for a sentiment analysis task .,abstract,abstract,0,12,10,10,0,abstract : abstract,0.05063291139240506,0.8333333333333334,0.8333333333333334,We also validated our method by evaluating the obtained embeddings for a sentiment analysis task ,16,This loss is used in combination with a sequential encoder - decoder network .,The proposed method results in semantic embeddings and outperforms the state - of - the - art on the paraphrase generation and sentiment analysis task on standard datasets .,abstract
natural_language_inference,55,"Freebase who is born in the location brighouse ? edward barber generated questions ( brighouse , location.location . people born here , edward barber ) and associated triples who is the producer of the recording rhapsody in b minor , op. 79 , no. as rephrasings of each other : harvested a set of 2 M distinct questions from WikiAnswers , which were grouped into 350 k paraphrase clusters .",system description,WebQuestions,0,63,38,30,0,system description : WebQuestions,0.42857142857142855,0.5757575757575758,1.0,Freebase who is born in the location brighouse edward barber generated questions brighouse location location people born here edward barber and associated triples who is the producer of the recording rhapsody in b minor op 79 no as rephrasings of each other harvested a set of 2 M distinct questions from WikiAnswers which were grouped into 350 k paraphrase clusters ,61,"influence node.influenced , helmut newton )", ,method
named-entity-recognition,7,Then our transition - based system learns to construct this forest through a sequence of shift - reduce actions .,introduction,introduction,1,19,9,9,0,introduction : introduction,0.12025316455696206,0.5,0.5,Then our transition based system learns to construct this forest through a sequence of shift reduce actions ,18,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .",shows an example of such a forest .,introduction
natural_language_inference,94,We embed each word from the context and question with a learned embedding space of dimension d .,method,method,0,83,15,15,0,method : method,0.216710182767624,0.11811023622047245,0.11811023622047245,We embed each word from the context and question with a learned embedding space of dimension d ,18,"The over all model is illustrated in , and the layers are described in further detail below .",We also obtain contextaware embeddings for each word via the pretrained embedding from language models ( ELMo ) ( 1024 dimensions ) .,method
sentiment_analysis,25,The sentence in shows the reviewer 's different attitude towards two aspects : food and delivery .,dataset,Datasets and Experiment Preparation,0,130,6,6,0,dataset : Datasets and Experiment Preparation,0.5855855855855856,0.21428571428571427,0.21428571428571427,The sentence in shows the reviewer s different attitude towards two aspects food and delivery ,16,The sentences which have different sentiment labels for different aspects or targets in the sentence are more common in review data than in standard sentiment classification benchmark .,"Therefore , to access how the models perform on review sentences more accurately , we create small but difficult datasets , which are made up of the sentences having opposite or different sentiments on different aspects / targets .",experiment
natural_language_inference,100,"Then from hidden layer 2 to output layer , we sum over all outputs of nodes in hidden layer 2 weighted by the outputs of softmax gate functions , which also form the question attention network .",training,Forward Propagation Prediction,0,219,34,9,0,training : Forward Propagation Prediction,0.5983606557377049,0.6415094339622641,1.0,Then from hidden layer 2 to output layer we sum over all outputs of nodes in hidden layer 2 weighted by the outputs of softmax gate functions which also form the question attention network ,35,"is the number of nodes in hidden layer 1 . rt is the model parameter from hidden layer 1 to hidden layer 2 , where we feed the linear combination of outputs of nodes in hidden layer 1 to an extra activation function comparing with Equation 2 .", ,experiment
text-classification,1,"Without the input and output gates , the LSTM formulation can be simplified to :",system description,More simplifications,0,114,68,24,0,system description : More simplifications,0.4453125,0.34,0.7272727272727273,Without the input and output gates the LSTM formulation can be simplified to ,14,"It is intuitive , in particular , that pooling can make the output gate unnecessary ; the role of the output gate is to prevent undesirable information from entering the output ht , and such irrelevant information can be filtered out by max - pooling .",This is equivalent to fixing it and o t to all ones .,method
natural_language_inference,18,Cnt means that the result is obtained from a combination of a lexical overlap feature and the output from the distributional model .,experiment,Dataset & Setup for Answer Sentence Selection,0,163,37,9,0,experiment : Dataset & Setup for Answer Sentence Selection,0.5970695970695971,0.4625,0.3103448275862069,Cnt means that the result is obtained from a combination of a lexical overlap feature and the output from the distributional model ,23,LCLR is the SVM - based classifier trained using a set of features .,In order to investigate the effectiveness of our NASM model we also implemented two strong baseline modelsa vanilla LSTM model ( LSTM ) and an LSTM model with a deterministic attention mechanism ( LSTM + Att ) .,experiment
text_generation,1,Note that the oracle model we used is a random initialized LSTM which is publicly available 2 .,experiment,Simulation on synthetic data,0,173,9,6,0,experiment : Simulation on synthetic data,0.6313868613138686,0.08737864077669902,0.17142857142857146,Note that the oracle model we used is a random initialized LSTM which is publicly available 2 ,18,"In the simulation , we firstly collect 10 , 000 sequential data generated by the oracle model ( or true model ) as the training set .","During learning , we randomly select one training sentence and one generated sentence from Rank GAN to form the input set C ? .",experiment
natural_language_inference,72,"Compared to automated dataset construction , crowdsourcing is more likely to provide highquality queries and answers .",dataset,Related datasets,0,74,18,18,0,dataset : Related datasets,0.2371794871794872,0.9,0.9,Compared to automated dataset construction crowdsourcing is more likely to provide highquality queries and answers ,16,"These datasets exist primarily for the general domain ; for specialized domains where background knowledge is crucial , crowdsourcing is intuitively less suitable , although some positive precedent exists for example in crowdsourcing annotations of radiology reports .","On the other hand , human question generation may also lead to less varied datasets as questions would tend to be of wh -type ; for cloze datasets , the questions maybe more varied and might require readers to possess a different set of skills .",experiment
natural_language_inference,88,The regularization constant was 1E - 4 and the mini-batch size was 5 .,model,Models,1,197,12,12,0,model : Models,0.7975708502024291,0.22641509433962265,0.5454545454545454,The regularization constant was 1E 4 and the mini batch size was 5 ,14,The initial learning rate was set to 2E - 3 .,dropout rate of 0.5 was applied to the neural network classifier .,method
natural_language_inference,56,Unless otherwise specified we use 96 hidden units for all hidden layers and all MLPs are 3 ReLU layers followed by a linear layer .,experiment,Sudoku experimental details,0,279,2,2,0,experiment : Sudoku experimental details,0.8303571428571429,0.03389830508474576,0.15384615384615385,Unless otherwise specified we use 96 hidden units for all hidden layers and all MLPs are 3 ReLU layers followed by a linear layer ,25, ,"Denote the digit for cell j d j ( 0 - 9 , 0 if not given ) , and the row and column position row j ( 1 - 9 ) and column j ( 1 - 9 ) respectively ..",experiment
text-classification,7,"With feature property as part of the information extracted by capsules , we may generalize the model better to multi-label text classification without an over extensive amount of labeled data .",ablation,Single-Label to Multi-Label Text Classification,0,164,14,9,0,ablation : Single-Label to Multi-Label Text Classification,0.6748971193415638,0.25,0.375,With feature property as part of the information extracted by capsules we may generalize the model better to multi label text classification without an over extensive amount of labeled data ,31,"In this section , we investigate the capability of capsule network on multi-label text classification by using only the single - label samples as training data .",The evaluation is carried on the Reuters - 21578 dataset .,result
relation-classification,9,"ii ) We perform extensive experimentation to investigate the performance of finetuning versus task - specific architectures atop frozen embeddings , and the effect of having an in - domain vocabulary .",introduction,introduction,0,21,12,12,0,introduction : introduction,0.14285714285714285,0.9230769230769232,0.9230769230769232,ii We perform extensive experimentation to investigate the performance of finetuning versus task specific architectures atop frozen embeddings and the effect of having an in domain vocabulary ,28,SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text .,"iii ) We evaluate SCIBERT on a suite of tasks in the scientific domain , and achieve new state - of the - art ( SOTA ) results on many of these tasks .",introduction
natural_language_inference,1,"Memory Networks are learning systems centered around a memory component that can be read and written to , with a particular focus on cases where the relationship between the input and response languages ( here natural language ) and the storage language ( here , the facts from KBs ) is performed by embedding all of them in the same vector space .",introduction,introduction,1,25,18,18,0,introduction : introduction,0.09157509157509157,0.72,0.72,Memory Networks are learning systems centered around a memory component that can be read and written to with a particular focus on cases where the relationship between the input and response languages here natural language and the storage language here the facts from KBs is performed by embedding all of them in the same vector space ,57,"Second , in sections 3 and 4 , we present an embedding - based QA system developed under the framework of Memory Networks ( Mem NNs ) .",The setting of the simple QA corresponds to the elementary operation of performing a single lookup in the memory .,introduction
sentiment_analysis,44,In this approach relation identification and nuclearity assignment is done simultaneously .,introduction,introduction,0,23,12,12,0,introduction : introduction,0.10454545454545454,0.3870967741935484,0.3870967741935484,In this approach relation identification and nuclearity assignment is done simultaneously ,12,3 ) Nucleus - Nucleus : Both Discourse Units are Nuclei .,Tree of a sample sentence .,introduction
text-classification,4,"The adaptive context - sensitive filter generation mechanism proposed here bears close resemblance to attention mechanism ) widely adopted in the NLP community , in the sense that both methods intend to incorporate rich contextual information into text representations .",model,Connections to attention mechanism,0,134,87,2,0,model : Connections to attention mechanism,0.6063348416289592,0.9666666666666668,0.4,The adaptive context sensitive filter generation mechanism proposed here bears close resemblance to attention mechanism widely adopted in the NLP community in the sense that both methods intend to incorporate rich contextual information into text representations ,37, ,"However , attention is typically operated on top of the hidden units preprocessed by CNN or LSTM layers , and assigns different weights to each unit according to a context vector .",method
natural_language_inference,9,"This is in contrast with most RNN - based models that can not be parallelized , where computing the candidate hidden state at time t explicitly requires the previous hidden state .",model,PARALLELIZATION,0,126,86,3,0,model : PARALLELIZATION,0.3818181818181817,0.8958333333333334,0.2307692307692308,This is in contrast with most RNN based models that can not be parallelized where computing the candidate hidden state at time t explicitly requires the previous hidden state ,30,An important advantage of QRN is that the recurrent updates in Equation 3 and 5 can be computed in parallel across time .,"In QRN , the final reduced queries ( h t ) can be decomposed into computing over candidate reduced queries ( h t ) , without looking at the previous reduced query .",method
natural_language_inference,60,"Hence , we take the LEAR embeddings by , which do very well on the HyperLex lexical entailment evaluation dataset .",model,Specialization,0,169,12,5,0,model : Specialization,0.8756476683937824,0.3333333333333333,0.4166666666666667,Hence we take the LEAR embeddings by which do very well on the HyperLex lexical entailment evaluation dataset ,19,"After all , one of the main motivations behind work on lexical entailment is that it allows for better downstream textual entailment .","We compare their best - performing embeddings against the original embeddings that were used for specialization , derived from the BOW2 embeddings of .",method
text-to-speech_synthesis,1,"shows that the inference latency barely increases with the length of the predicted mel-spectrogram for FastSpeech , while increases largely in Transformer TTS .",result,Results,0,153,14,14,0,result : Results,0.6986301369863014,0.3333333333333333,0.6363636363636364,shows that the inference latency barely increases with the length of the predicted mel spectrogram for FastSpeech while increases largely in Transformer TTS ,24,We also visualize the relationship between the inference latency and the length of the predicted melspectrogram sequence in the test set .,This indicates that the inference speed of our method is not sensitive to the length of generated audio due to parallel generation .,result
sentiment_analysis,3,Emotion Detection in Text :,system description,Knowledge Base in Conversations:,0,56,6,6,0,system description : Knowledge Base in Conversations:,0.1917808219178082,0.4,0.42857142857142855,Emotion Detection in Text ,5,"By contrast , our graph attention mechanism is dynamic and selects context - aware knowledge entities that balances between relatedness and affectiveness .",There is a trend moving from traditional machine learning methods to deep learning methods for emotion detection in text .,method
machine-translation,0,The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder - Decoder improves the translation performance .,introduction,introduction,0,23,15,15,0,introduction : introduction,0.1050228310502283,0.5555555555555556,0.5555555555555556,The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder Decoder improves the translation performance ,21,The model is then used as apart of a standard phrase - based SMT system by scoring each phrase pair in the phrase table .,We qualitatively analyze the trained RNN Encoder - Decoder by comparing its phrase scores with those given by the existing translation model .,introduction
question_answering,5,Our method proceeds in two phases .,method,METHOD,0,54,3,3,0,method : METHOD,0.1956521739130435,0.03260869565217391,0.1875,Our method proceeds in two phases ,7,"Given a question q , we are trying to find the correct answer a g to q using information retrieved from the web .","First , we run an IR model ( with the help of a search engine such as google or bing ) to find the top - N web passages p 1 , p 2 , . . . , p N most related to the question .",method
natural_language_inference,97,The sequential sliding window is related to the original MCTest baseline by .,model,Sequential Sliding Window,0,137,55,2,0,model : Sequential Sliding Window,0.4691780821917808,0.4910714285714286,0.18181818181818185,The sequential sliding window is related to the original MCTest baseline by ,13, ,"Our sliding window decays from its focus word according to a Gaussian distribution , which we extend by assigning a trainable weight to each location in the window .",method
named-entity-recognition,0,"Such case is worth improvement , as there may exist outlier elements in real data .",system description,Classifiers,0,46,24,24,0,system description : Classifiers,0.16974169741697415,0.1348314606741573,0.6153846153846154,Such case is worth improvement as there may exist outlier elements in real data ,15,"Moreover , the representation loss is only robust against outlier samples .","In this paper , we propose an accelerated robust subset selection method to highly raise the speed on the one hand , and to boost the robustness on the other .",method
sentiment_analysis,18,Multi- task learning can help to reduce the amount of data required for learning and to improve the model generalization ability .,model,Target Representation,0,109,43,23,0,model : Target Representation,0.4560669456066946,0.581081081081081,1.0,Multi task learning can help to reduce the amount of data required for learning and to improve the model generalization ability ,22,The learning process can also be viewed as multi-task learning where the unsupervised objective shown as Eq. 9 is an auxiliary task ., ,method
question_answering,1,The model has about 2.6 million parameters .,experiment,QUESTION ANSWERING EXPERIMENTS,1,177,14,14,0,experiment : QUESTION ANSWERING EXPERIMENTS,0.5583596214511041,0.15555555555555556,0.21212121212121213,The model has about 2 6 million parameters ,9,The hidden state size ( d ) of the model is 100 .,"We use the AdaDelta ( Zeiler , 2012 ) optimizer , with a minibatch size of 60 and an initial learning rate of 0.5 , for 12 epochs .",experiment
sentiment_analysis,13,Experimental results demonstrate that the proposed posttraining is highly effective 1 .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.039568345323741004,1.0,1.0,Experimental results demonstrate that the proposed posttraining is highly effective 1 ,12,"To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis .", ,abstract
text-to-speech_synthesis,1,"As demonstrated by the samples , FastSpeech can adjust the voice speed from 0.5x to 1.5 x smoothly , with stable and almost unchanged pitch .",result,Method Repeats Skips Error Sentences Error Rate,1,169,30,8,0,result : Method Repeats Skips Error Sentences Error Rate,0.771689497716895,0.7142857142857143,0.4,As demonstrated by the samples FastSpeech can adjust the voice speed from 0 5x to 1 5 x smoothly with stable and almost unchanged pitch ,26,We also attach several audio samples in the supplementary material for reference .,a ) 1.5 x Voice Speed ( b ) 1.0 x Voice Speed ( c ) 0.5 x,result
sentiment_analysis,13,Training loss is the cross entropy on the polarities .,system description,Aspect Sentiment Classification,0,132,56,13,0,system description : Aspect Sentiment Classification,0.4748201438848921,0.56,0.8125,Training loss is the cross entropy on the polarities ,10,"Softmax is applied along the dimension of labels on [ CLS ] : l 4 ? [ 0 , 1 ] 3 .","As a summary of these tasks , insufficient supervised training data significantly limits the performance gain across these 3 review - based tasks .",method
natural_language_inference,95,We argue that this content model is necessary for our answer verification process .,ablation,Necessity of the Content Model,0,202,25,3,0,ablation : Necessity of the Content Model,0.8632478632478633,0.78125,0.3,We argue that this content model is necessary for our answer verification process ,14,"In our model , we compute the answer representation based on the content probabilities predicted by a separate content model instead of directly using the boundary probabilities .",plots the predicted content probabilities as well as the boundary probabilities start probability end probability content probability :,result
text_summarization,4,"Overall , we argue that when the input text is short ( e.g. a sentence ) , both encoders perform comparably , otherwise when the input text is long ( e.g. a document ) , the CNN - based encoder performs better .",model,Entity encoding submodule,0,117,47,25,0,model : Entity encoding submodule,0.4534883720930232,0.5108695652173914,0.7352941176470589,Overall we argue that when the input text is short e g a sentence both encoders perform comparably otherwise when the input text is long e g a document the CNN based encoder performs better ,36,"The CNN - based encoder is good as it minimizes the noise by totally ignoring far entities when dis ambiguating , however determining the appropriate filter sizes h needs engineering .",It is obvious that not all entities need to be dis ambiguated .,method
natural_language_inference,19,"The two datasets have the same format , but sentences in the MultiNLI dataset are much longer than those in SNLI dataset .",dataset,Dataset,0,161,4,4,0,dataset : Dataset,0.6313725490196078,0.6666666666666666,0.6666666666666666,The two datasets have the same format but sentences in the MultiNLI dataset are much longer than those in SNLI dataset ,22,"The SNLI dataset consists of 549,367 / 9,842 / 9,824 ( train / valid / test ) premise and hypothesis pairs ; and the MultiNLI dataset , 392,702 / 9,815 / 9,832 / 9,796 / 9,847 ( train / valid matched / valid mismatched / test matched / test mismatched ) sentence pairs .","In addition , MultiNLI dataset consists of various genre information .",experiment
natural_language_inference,37,"Second , if the model only sees paragraphs that contain answers , it might become too confident in heuristics or patterns thatare only effective when it is known a priori that an answer exists .",method,Confidence Method,0,95,12,12,0,method : Confidence Method,0.3696498054474709,0.2222222222222222,0.7058823529411765,Second if the model only sees paragraphs that contain answers it might become too confident in heuristics or patterns thatare only effective when it is known a priori that an answer exists ,33,"As a result , nothing prevents models from producing scores thatare arbitrarily all larger or all smaller for one paragraph than another .","For example , in we observe that the model will assign high confidence values to spans that strongly match the category of the answer , even if the question words do not match the context .",method
sentiment_analysis,50,The training objective is also cross entropy minimization as shown in equation and the parameter set is,approach,Transfer Approaches,0,62,6,6,0,approach : Transfer Approaches,0.382716049382716,0.3157894736842105,0.3157894736842105,The training objective is also cross entropy minimization as shown in equation and the parameter set is,17,This network is the same as the LSTM + ATT apart from the lack of the attention layer .,"In this setting , we first train on document - level examples .",method
sentiment_analysis,4,"After the read operation at each hop , memories are updated for the next hop .",methodology,Multi-hop Memory,0,195,90,22,0,methodology : Multi-hop Memory,0.598159509202454,0.9183673469387756,0.7333333333333333,After the read operation at each hop memories are updated for the next hop ,15,"Finally , the representation of the test utterance is updated by consolidating itself with the weighted memory m as :","For this purpose , a GRU network , GRU m , takes the r th memory cells M ( r ) as input and reprocesses this sequence to generate memories M ( r+ 1 ) , i.e. , M ( r+1 ) = GRU m ( M ( r ) ) .",method
text-classification,7,"For testing , Reuters - Multi - label only uses the multi-label documents in testing dataset , while Reuters - Full includes all documents in test set .",ablation,Single-Label to Multi-Label Text Classification,0,169,19,14,0,ablation : Single-Label to Multi-Label Text Classification,0.6954732510288066,0.3392857142857143,0.5833333333333334,For testing Reuters Multi label only uses the multi label documents in testing dataset while Reuters Full includes all documents in test set ,24,"For dev and training , we only use the single - label documents in the Reuters dev and training sets .",The characteristics of these two datasets are described in .,result
relation_extraction,12,Results on Cross - Sentence n- ary Relation Extraction,experiment,Results on Cross-Sentence n-ary Relation Extraction,0,181,27,1,0,experiment : Results on Cross-Sentence n-ary Relation Extraction,0.5501519756838906,0.8181818181818182,0.14285714285714285,Results on Cross Sentence n ary Relation Extraction,8, , ,experiment
natural_language_inference,41,"Qualitatively , sample quality is high ( see 6 ) .",experiment,ConvAI2,0,181,29,9,0,experiment : ConvAI2,0.7042801556420234,0.90625,0.75,Qualitatively sample quality is high see 6 ,8,"BART outperforms the best previous work , which leverages BERT , by roughly 6.0 points on all ROUGE metrics - representing a significant advance in performance on this problem .","We evaluate dialogue response generation on CONVAI2 , in which agents must generate responses conditioned on both the previous context and a textually - specified persona .",experiment
sentiment_analysis,22,"In addition , most existing methods ignore the position information of the aspect when encoding the sentence .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03056768558951965,0.625,0.625,In addition most existing methods ignore the position information of the aspect when encoding the sentence ,17,"On this basis , we also propose a succinct hierarchical attention based mechanism to fuse the information of targets and the contextual words .","In this paper , we argue that the position - aware representations are beneficial to this task .",abstract
sentiment_analysis,26,This is because our memory module has been designed to leverage informative prior information and to reweight its signals based on this assumption .,analysis,Results and Analysis,0,188,42,42,0,analysis : Results and Analysis,0.7673469387755102,0.5384615384615384,0.6461538461538462,This is because our memory module has been designed to leverage informative prior information and to reweight its signals based on this assumption ,24,"When we feed random sentiment embeddings into them , not unexpectedly , in many cases the results do not improve much .","Hence , it is important to feed genuine sentiment cues into the memory module .",result
natural_language_inference,20,Our proposed architecture follows a sentence embedding - based approach for NLI introduced by .,model,Model Architecture,0,37,2,2,0,model : Model Architecture,0.1522633744855967,0.05882352941176471,0.2857142857142857,Our proposed architecture follows a sentence embedding based approach for NLI introduced by ,14, ,"The model illustrated in contains sentence embeddings for the two input sentences , where the output of the sentence embeddings are combined using a heuristic introduced by , putting together the concatenation ( u , v ) , absolute element - wise difference |u ? v| , and element - wise product u * v.",method
natural_language_inference,39,"Our network architecture is composed of spatial max pooling layers , spatial convolutional layers ( Conv ) with a small filter size of 3 3 plus stride 1 and padding 1 .",system description,Similarity Classification with Deep,0,121,58,9,0,system description : Similarity Classification with Deep,0.587378640776699,0.8656716417910447,0.5,Our network architecture is composed of spatial max pooling layers spatial convolutional layers Conv with a small filter size of 3 3 plus stride 1 and padding 1 ,29,We therefore use a deep homogeneous architecture which has repetitive convolution and pooling layers .,"We adopt this filter size because it is the smallest one to capture the space of left / right , up / down , and center ; the padding and stride is used to preserve the spatial input resolution .",method
semantic_role_labeling,4,This point distinguishes between their model and ours .,model,model,0,280,9,9,0,model : model,0.9333333333333332,0.5625,0.5625,This point distinguishes between their model and ours ,9,"In other words , while their model seeks to select an appropriate label for each span ( label selection ) , our model seeks to select appropriate spans for each label ( span selection ) .","Frame Net span - based model For FrameNetstyle SRL , used a segmental RNN , combining bidirectional RNNs with semi-Markov CRFs .",method
natural_language_inference,78,An ensemble of CAFE models achieve competitive re-sult on the MultiNLI dataset .,experiment,Experimental Results,1,230,28,28,0,experiment : Experimental Results,0.8333333333333334,0.7179487179487181,0.7179487179487181,An ensemble of CAFE models achieve competitive re sult on the MultiNLI dataset ,14,We also outperform the ESIM + Read model .,"On SciTail , our proposed CAFE model achieves state - of - the - art performance .",experiment
sentiment_analysis,27,"Moreover , there are no explicit edges in our task .",system description,Graph convolutional network,0,76,21,5,0,system description : Graph convolutional network,0.2753623188405797,0.9545454545454546,0.8333333333333334,Moreover there are no explicit edges in our task ,10,The final output of each GCN node is designed to be the classifier of the corresponding aspect in our task .,"Thus , we need to define the edges from scratch .",method
natural_language_inference,10,We designed a novel sentence encoder architecture that learns to compose task - specific trees from plain text data .,introduction,introduction,0,32,23,23,0,introduction : introduction,0.1350210970464135,0.7931034482758621,0.7931034482758621,We designed a novel sentence encoder architecture that learns to compose task specific trees from plain text data ,19,The contributions of our work are as follows :,We showed from experiments that the proposed architecture outperforms or is competitive to state - of - the - art models .,introduction
sentiment_analysis,23,c- TBCNN is slightly worse .,training,Training Details,0,202,16,16,0,training : Training Details,0.6917808219178082,0.9411764705882352,0.9411764705882352,c TBCNN is slightly worse ,6,"51.4 % accuracy , outperforming the previous state - of - the - art result , achieved by the RNN based on long - short term memory .","It achieves 50.4 % accuracy , ranking third in the state - of - the - art list ( including our d- TBCNN model ) .",experiment
natural_language_inference,69,We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.017391304347826087,0.4,0.4,We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods ,26,"Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods , but currently no resources exist to train and test this capability .","In our task , a model learns to seek and combine evidence - effectively performing multihop , alias multi-step , inference .",abstract
sentiment_analysis,13,We detail ReviewRC in Sec.,introduction,introduction,0,41,30,30,0,introduction : introduction,0.1474820143884892,0.6818181818181818,0.6818181818181818,We detail ReviewRC in Sec ,6,"This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) in the domains of laptop and restaurant .",". Given the wide spectrum of domains ( types of products or services ) in online businesses and the prohibitive cost of annotation , ReviewRC can only be considered to have a limited number of annotated examples for supervised training , which still leaves the domain challenges partially unresolved .",introduction
sentiment_analysis,19,The results shown above clearly show the efficacy of using SuBiLSTMs in existing models geared towards four different sentence modeling tasks .,model,Comparison of SuBiLSTM and SuBiLSTM-Tied,0,124,4,2,0,model : Comparison of SuBiLSTM and SuBiLSTM-Tied,0.8104575163398693,0.5,0.3333333333333333,The results shown above clearly show the efficacy of using SuBiLSTMs in existing models geared towards four different sentence modeling tasks ,22, ,"The relative performance of SuBiL - STM and SuBiLSTM - Tied are fairly close to each other , as shown by the relative gains in .",method
text-classification,5,One of the main benefits of transfer learning is being able to train a model for,analysis,Analysis,0,194,7,7,0,analysis : Analysis,0.7698412698412699,1.0,1.0,One of the main benefits of transfer learning is being able to train a model for,16,We fine - tune the classifier for 50 epochs and train all methods but ULMFiT with early stopping ., ,result
natural_language_inference,0,"Finally , removing the character embeddings , which were only used for WDW and CBT , leads to a reduction of about 1 % in the performance .",model,Model Accuracy,1,183,8,6,0,model : Model Accuracy,0.9289340101522844,0.5714285714285714,1.0,Finally removing the character embeddings which were only used for WDW and CBT leads to a reduction of about 1 in the performance ,24,"Next , we observe a substantial drop when removing tokenspecific attentions over the query in the GA module , which allow gating individual tokens in the document only by parts of the query relevant to that token rather than the over all query representation .", ,method
natural_language_inference,35,"These capabilities complement each other , but previous studies can not use and control different styles within a model .",introduction,introduction,0,23,11,11,0,introduction : introduction,0.08745247148288972,0.5238095238095238,0.5238095238095238,These capabilities complement each other but previous studies can not use and control different styles within a model ,19,"Moreover , to satisfy various information needs , intelligent agents should be capable of answering one question in multiple styles , such as wellformed sentences , which make sense even without the context of the question and passages , and concise phrases .","In this study , we propose Masque , a generative model for multi-passage RC .",introduction
natural_language_inference,8,"Notice that here we did not add more n-gram features , since we would like to see how much the distributional models contributed in this task , rather than n-gram overlapping .",experiment,experiment,0,158,10,10,0,experiment : experiment,0.7488151658767772,0.4761904761904762,0.4761904761904762,Notice that here we did not add more n gram features since we would like to see how much the distributional models contributed in this task rather than n gram overlapping ,32,"We integrate this feature with our distributional models by training a logistic regression classifier with three features : word co-occurrence count , word co-occurrence count weighted by IDF value and QA matching probability as provided by the distributional model .","- BFGS was used to train the logistic regression classifier , with L2 regulariser of 0.01 .",experiment
natural_language_inference,49,"Though our experiments show ResNet works well in the architecture , we choose Dense Net because it is effective in saving parameters .",model,DENSELY INTERACTIVE INFERENCE NETWORK,0,104,58,34,0,model : DENSELY INTERACTIVE INFERENCE NETWORK,0.4094488188976378,0.8529411764705882,0.7727272727272727,Though our experiments show ResNet works well in the architecture we choose Dense Net because it is effective in saving parameters ,22,Layer : We adopt DenseNet as convolutional feature extractor in DIIN .,"One interesting observation with ResNet is that if we remove the skip connection in residual structure , the model does not converge at all .",method
natural_language_inference,72,"When categorizing the skills necessary to find the right answer , we observe that a large number of comprehension skills get activated and that prior knowledge in the form of the ability to perform lexico - grammatical inferences matters the most .",introduction,introduction,0,47,39,39,0,introduction : introduction,0.15064102564102566,0.8125,0.8125,When categorizing the skills necessary to find the right answer we observe that a large number of comprehension skills get activated and that prior knowledge in the form of the ability to perform lexico grammatical inferences matters the most ,40,"Second , we look at how well humans perform on this task , by asking both a medical expert and a novice to answer a portion of the validation set .","This suggests that for our dataset and possibly for domain - specific datasets more generally , more background knowledge should be incorporated in machine comprehension models .",introduction
sentiment_analysis,8,"RMSE is calculated frame by frame and we take both , the average and standard deviation as features .",methodology,Machine Learning Models:,0,85,39,5,0,methodology : Machine Learning Models:,0.3632478632478632,0.4382022471910113,1.0,RMSE is calculated frame by frame and we take both the average and standard deviation as features ,18,We use standard Root Mean Square Energy ( RMSE ) to represent speech energy using the equation :,"We use this feature to represent the "" silent "" portion in the audio signal .",method
natural_language_inference,29,"have n't started it yet , but it 's very easy to clean "" .",ablation,ablation,0,336,19,19,0,ablation : ablation,0.9205479452054794,0.7037037037037037,0.7037037037037037,have n t started it yet but it s very easy to clean ,14,"It looks good , comfortable and cheap . "" and the right is "" The color looks good and the texture is great .","In this figure , we can see that there is a very strong interaction between question word ?? ( cleaning ) and phrase in review ?????? ( very easy to clean ) .",result
question_answering,2,Memex QA provides 4 answer choices and only one correct answer for each question .,architecture,Cross Sequence Interaction,1,183,104,34,0,architecture : Cross Sequence Interaction,0.6606498194945848,0.9541284403669724,0.8717948717948718,Memex QA provides 4 answer choices and only one correct answer for each question ,15,"The metadata is incomplete and GPS , the photo title , the album title and description may not present in every photo .",The dataset also provides more than one ground truth grounding images for each question .,method
natural_language_inference,62,This layer maps the lexicon embeddings to the context embeddings .,architecture,Overall Architecture,0,106,11,11,0,architecture : Overall Architecture,0.4732142857142857,0.14285714285714285,0.28205128205128205,This layer maps the lexicon embeddings to the context embeddings ,11,Context Embedding Layer .,"For both the passage and the question , we process the lexicon embeddings ( i.e. LP for the passage and L Q for the question ) with a shared bidirectional LSTM ( BiLSTM ) , whose hidden state dimensionality is 1 2 d .",method
sentiment_analysis,9,"The pre-trained BERT model is designed to improve performance for most NLP tasks , and The LCF - ATEPC model deploys two independent BERT - Shared layers thatare aimed to extract local and global context features .",model,BERT-Shared Layer,0,84,14,2,0,model : BERT-Shared Layer,0.30324909747292417,0.175,0.4,The pre trained BERT model is designed to improve performance for most NLP tasks and The LCF ATEPC model deploys two independent BERT Shared layers thatare aimed to extract local and global context features ,35, ,"For pre-trained BERT , the fine - tuning learning process is indispensable .",method
relation_extraction,12,We also evaluate our model on the SemEval dataset under the same settings as .,model,Model F1,1,230,43,11,0,model : Model F1,0.6990881458966566,0.9347826086956522,0.7857142857142857,We also evaluate our model on the SemEval dataset under the same settings as ,15,The performance gap between GCNs with pruned trees and AGGCNs with full trees empirically show that the AGGCN model is better at distinguishing relevant from irrelevant information for learning a better graph representation .,Results are shown in .,method
relation_extraction,9,We observe that our novel attentionbased architecture achieves new state - of - the - art results on this relation classification dataset .,experiment,Experimental Setup,1,153,16,15,0,experiment : Experimental Setup,0.7320574162679426,0.5925925925925926,0.6,We observe that our novel attentionbased architecture achieves new state of the art results on this relation classification dataset ,20,The choices generated by this process are given in . approaches .,"Att - Input - CNN relies only on the primal attention at the input level , performing standard max - pooling after the convolution layer to generate the network output w O , in which the new objective function is utilized .",experiment
natural_language_inference,50,Pairwise Hinge Loss .,approach,Optimization and Learning,0,150,58,4,0,approach : Optimization and Learning,0.473186119873817,0.7733333333333333,0.19047619047619047,Pairwise Hinge Loss ,4,"Our model learns via a pairwise ranking loss , which is well suited for metric - based learning algorithms .",Our network minimizes the pairwise hinge loss which is defined as follows :,method
semantic_parsing,2,"We wish to estimate p ( y|x ) , the conditional probability of meaning representation y given input x .",system description,Problem Formulation,0,62,7,7,0,system description : Problem Formulation,0.21305841924398625,0.12962962962962962,0.4117647058823529,We wish to estimate p y x the conditional probability of meaning representation y given input x ,18,"Let x = x 1 x | x | denote a natural language expression , and y = y 1 y |y | it s meaning representation .",We decompose p ( y|x ) into a twostage generation process :,method
natural_language_inference,31,These design choices slowdown the model by a large amount and can be replaced by much more lightweight and equally effective ones .,introduction,introduction,0,23,16,16,0,introduction : introduction,0.08333333333333333,0.5517241379310345,0.5517241379310345,These design choices slowdown the model by a large amount and can be replaced by much more lightweight and equally effective ones ,23,"We question the necessity of many slow components in text matching approaches presented in previous literature , including complicated multi-way alignment mechanisms , heavy distillations of alignment results , external syntactic features , or dense connections to connect stacked blocks when the model is going deep .","Meanwhile , we highlight three key components for an efficient text matching model .",introduction
natural_language_inference,19,The dataset used in the experiments are SNLI and MultiNLI datasets .,dataset,Dataset,0,159,2,2,0,dataset : Dataset,0.6235294117647059,0.3333333333333333,0.3333333333333333,The dataset used in the experiments are SNLI and MultiNLI datasets ,12, ,"The SNLI dataset consists of 549,367 / 9,842 / 9,824 ( train / valid / test ) premise and hypothesis pairs ; and the MultiNLI dataset , 392,702 / 9,815 / 9,832 / 9,796 / 9,847 ( train / valid matched / valid mismatched / test matched / test mismatched ) sentence pairs .",experiment
sentence_compression,1,"Boolean features indicating syntactic relations between selected tokens ( i.e. , siblings , parents , leaves , etc. ) .",baseline,Baseline,0,63,26,26,0,baseline : Baseline,0.3014354066985646,0.3023255813953488,0.3023255813953488,Boolean features indicating syntactic relations between selected tokens i e siblings parents leaves etc ,15,Dependency labels of taken and dropped tokens and their parent dependencies .,Dependency label of the least common ancestor in the dependency tree between a batch of dropped tokens .,result
natural_language_inference,87,We adopt the Whole Word Masking BERT as the baseline 6 .,implementation,Implementation,1,129,5,5,0,implementation : Implementation,0.7329545454545454,0.4166666666666667,0.4166666666666667,We adopt the Whole Word Masking BERT as the baseline 6 ,12,"For MRC model implementation ,","The initial learning rate is set in { 8e -6 , 1 e - 5 , 2 e - 5 , 3 e - 5 } with warm - up rate of 0.1 and L2 weight decay of 0.01 .",experiment
sentiment_analysis,35,"In summary , the source network acts as a "" teacher "" , which consists of three - level attention hops ( C2A + C2F + PaS ) for the AC task , while the target network is like a "" student "" that only uses two basic attention hops ( C2A + PaS ) for the AT task .",system description,An Overview of the MGAN model,0,79,20,10,0,system description : An Overview of the MGAN model,0.3185483870967742,0.20408163265306128,0.9090909090909092,In summary the source network acts as a teacher which consists of three level attention hops C2A C2F PaS for the AC task while the target network is like a student that only uses two basic attention hops C2A PaS for the AT task ,45,"After obtaining aspect - specific representations , the knowledge transfer between the two tasks is via the contrastive feature alignment .","In the following sections , we introduce each component of MGAN in details .",method
natural_language_inference,7,"Given the above three components , the complete question - focused passage word embedding for pi is their concatenation :",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,101,50,22,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5674157303370787,0.9433962264150944,1.0,Given the above three components the complete question focused passage word embedding for pi is their concatenation ,18,This representation is a bidirectional generalization of the question representation recently proposed by for a different question - answering task ., ,method
topic_models,0,"Perplexity is an intrinsic measure for topic models , .",result,result,0,319,10,10,0,result : result,0.7742718446601942,0.2083333333333333,0.2083333333333333,Perplexity is an intrinsic measure for topic models ,9,"In all the further experiments , we initialized 7 both the prior and variational distributions to N ( 0 , diag ( 0.1 ) ) .",It is computed as an average of every test document according to :,result
natural_language_inference,4,MIXED OBJECTIVE USING SELF - CRITICAL POLICY LEARNING,system description,MIXED OBJECTIVE USING SELF-CRITICAL POLICY LEARNING,0,74,49,16,0,system description : MIXED OBJECTIVE USING SELF-CRITICAL POLICY LEARNING,0.37373737373737376,0.5051546391752577,0.25,MIXED OBJECTIVE USING SELF CRITICAL POLICY LEARNING,7,"This is akin to transformer networks , which achieved stateof - the - art results on machine translation using deep self - attention layers to help model long - range dependencies , and residual networks , which achieved state - of - the - art results in image classification through the addition of skip layer connections to facilitate signal propagation and alleviate gradient degradation .",The DCN produces a distribution over the start position of the answer and a distribution over the end position of the answer .,method
sentiment_analysis,4,"Self - dependencies , also known as emotional inertia , deal with the aspect of emotional influence that speakers have on themselves during conversations .",introduction,introduction,0,23,14,14,0,introduction : introduction,0.0705521472392638,0.3181818181818182,0.3181818181818182,Self dependencies also known as emotional inertia deal with the aspect of emotional influence that speakers have on themselves during conversations ,22,Emotional dynamics in conversations consist of two important properties : self and inter-personal dependencies ( Morris and .,"On the other hand , inter-personal dependencies relate to the emotional influences that the counterparts induce into a speaker .",introduction
text-classification,5,"In our experiments , we use the state - of - theart language model AWD - LSTM , a regular LSTM ( with no attention , short - cut connections , or other sophisticated additions ) with various tuned dropout hyperparameters .",system description,Universal Language Model Fine-tuning,0,72,21,21,0,system description : Universal Language Model Fine-tuning,0.2857142857142857,0.22105263157894736,0.875,In our experiments we use the state of theart language model AWD LSTM a regular LSTM with no attention short cut connections or other sophisticated additions with various tuned dropout hyperparameters ,32,") It works across tasks varying in document size , number , and label type ; 2 ) it uses a single architecture and training process ; 3 ) it requires no custom feature engineering or preprocessing ; and 4 ) it does not require additional in - domain documents or labels .","Analogous to CV , we expect that downstream performance can be improved by using higherperformance language models in the future .",method
sentiment_analysis,27,Bidirectional Long Short - Term Memory ( Bi - LSTM ),methodology,Bidirectional Long Short-Term Memory (Bi-LSTM),0,97,20,1,0,methodology : Bidirectional Long Short-Term Memory (Bi-LSTM),0.35144927536231885,0.2352941176470588,0.125,Bidirectional Long Short Term Memory Bi LSTM ,8, , ,method
natural_language_inference,83,danced terribly and broke a friend 's toe .,result,Correct Ending Weights,0,223,8,5,0,result : Correct Ending Weights,0.7287581699346405,0.1509433962264151,0.13157894736842105,danced terribly and broke a friend s toe ,9,finally decided to tag along last Saturday .,"The next weekend , I was asked to please stay home .",result
natural_language_inference,49,Different components of encoder can be combined to obtain a better sentence matrix representation .,model,INTERACTIVE INFERENCE NETWORK,0,59,13,12,0,model : INTERACTIVE INFERENCE NETWORK,0.23228346456692914,0.19117647058823528,0.5217391304347826,Different components of encoder can be combined to obtain a better sentence matrix representation ,15,"For instance , a model can adopt bidirectional recurrent neural network to model the temporal interaction on both direction , recursive neural network ( also known as TreeRNN ) to model the compositionality and the recursive structure of language , or self - attention to model the long - term dependency on sentence .",. Interaction Layer creates an word - by - word interaction tensor by both premise and hypothesis representation matrix .,method
semantic_role_labeling,4,One popular approach to it is based on BIO tagging schemes .,introduction,introduction,0,14,6,6,0,introduction : introduction,0.04666666666666667,0.2222222222222222,0.2222222222222222,One popular approach to it is based on BIO tagging schemes ,12,key to the argument span prediction is how to represent and model spans .,State - of - the - art neural SRL models adopt this approach .,introduction
natural_language_inference,81,"New Zealand is situated some east of Australia across the Tasman Sea and roughly south of the Pacific island are as of New Caledonia , Fiji , and Tonga .",APPENDIX,Answer town,0,398,190,110,0,APPENDIX : Answer town,0.9453681710213776,0.892018779342723,0.8270676691729323,New Zealand is situated some east of Australia across the Tasman Sea and roughly south of the Pacific island are as of New Caledonia Fiji and Tonga ,28,"The country geographically comprises two main landmassesthat of the North Island , or Te Ika - a - Mui , and the South Island , or Te Waipounamuand numerous smaller islands .","Because of its remoteness , it was one of the last lands to be settled by humans .",others
natural_language_inference,23,commonly used function is multiplicative attention ) :,architecture,FULLY-AWARE FUSION NETWORK,0,129,57,5,0,architecture : FULLY-AWARE FUSION NETWORK,0.25146198830409355,0.4014084507042254,0.25,commonly used function is multiplicative attention ,7,"To fully utilize history - of - word in attention , we need a suitable attention scoring function S (x , y ) .","where U , V ? R kd h , and k is the attention hidden size .",method
text_generation,3,"For example , the auto - encoders can be replaced by variational auto-encoders to ensure that the distribution of utterance representations is normal , which has a better generalization ability .",analysis,Error Analysis,0,137,7,7,0,analysis : Error Analysis,0.9716312056737588,1.0,1.0,For example the auto encoders can be replaced by variational auto encoders to ensure that the distribution of utterance representations is normal which has a better generalization ability ,29,"Therefore , we would like to explore more approaches to address this problem in the future work .", ,result
machine-translation,3,"Unlike conventional statistical machine translation ( SMT ) systems which consist of multiple separately tuned components , NMT models encode the source sequence into continuous representation space and generate the target sequence in an end - to - end fashon .",introduction,introduction,0,15,3,3,0,introduction : introduction,0.04792332268370607,0.1,0.1,Unlike conventional statistical machine translation SMT systems which consist of multiple separately tuned components NMT models encode the source sequence into continuous representation space and generate the target sequence in an end to end fashon ,36,Neural machine translation ( NMT ) has attracted a lot of interest in solving the machine translation ( MT ) problem in recent years .,"Moreover , NMT models can also be easily adapted to other tasks such as dialog systems , question answering systems and image caption generation .",introduction
relation_extraction,10,The algorithmic span pruning process limits our model spans which are entirely within a single sentence and have a max length of L = 10 tokens .,model,Model Complexity,0,162,18,3,0,model : Model Complexity,0.84375,0.4186046511627907,0.3333333333333333,The algorithmic span pruning process limits our model spans which are entirely within a single sentence and have a max length of L 10 tokens ,26,Section 3.1 describes our span generation process and Section 4 describes our algorithmic span pruning process .,"While our model creates representations for spans ( instead of just tokens ) , it achieves the dual goals of being memory efficient and capturing most ( more than 99.95 % ) entities and relations in the space of the spans considered .",method
natural_language_inference,80,"Such models only consider encoding a hypothesis depending on the premise , disre - arXiv : 1802.05577v2 [ cs.CL ] 11 Apr 2018 garding the dependency aspects in the opposite direction .",introduction,introduction,0,29,20,20,0,introduction : introduction,0.1,0.6896551724137931,0.6896551724137931,Such models only consider encoding a hypothesis depending on the premise disre arXiv 1802 05577v2 cs CL 11 Apr 2018 garding the dependency aspects in the opposite direction ,29,"So far , they have only explored dependency aspects during the encoding stage , while ignoring its benefit during inference .",We propose a dependent reading bidirectional LSTM ( DR - BiLSTM ) model to address these limitations .,introduction
text_summarization,2,"by the parser may affect the "" Struct + 2 Way + Relation "" results .",result,Results,0,243,31,31,0,result : Results,0.8741007194244604,0.9393939393939394,0.9393939393939394,by the parser may affect the Struct 2 Way Relation results ,12,DRGD describes a deep recurrent generative decoder learning latent structure of summary sequences via variational inference .,"However , because the Gigaword dataset does not provide gold - standard annotations for parse trees , we could not easily verify this and will leave it for future work .",result
natural_language_inference,79,% Weighted Bag - of - bigrams 5.67 % Paragraph Vector 3.82 %,model,Model,0,188,3,3,0,model : Model,0.7014925373134329,0.047619047619047616,1.0, Weighted Bag of bigrams 5 67 Paragraph Vector 3 82 ,12,Error rate Vector Averaging 10.25 % Bag - of - words 8.10 % Bag - of - bigrams, ,method
relation_extraction,11,All the non-neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous .,performance,Performance Comparison,1,220,6,6,0,performance : Performance Comparison,0.8870967741935484,0.6,0.6,All the non neural baselines could not perform well as the features used by them are mostly derived from NLP tools which can be erroneous ,26,"Overall , we find that RESIDE achieves higher precision over the entire recall range on both the datasets .",RESIDE outperforms PCNN + ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model .,result
natural_language_inference,40,"On the bAbi QA tasks , our model solves 15 out of the 20 tasks with only 1000 training examples per task .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.032490974729241874,0.875,0.875,On the bAbi QA tasks our model solves 15 out of the 20 tasks with only 1000 training examples per task ,22,"We apply our model to several text comprehension tasks and achieve new state - of - the - art results on all considered benchmarks , including CNN , bAbi , and LAMBADA .",Analysis of the learned representations further demonstrates the ability of our model to encode fine - grained entity information across a document .,abstract
natural_language_inference,39,"For the SICK and MSRVID experiments , we used 300 - dimension Glo Ve word embeddings .",experiment,Experimental Setup,1,155,25,25,0,experiment : Experimental Setup,0.7524271844660194,0.625,0.625,For the SICK and MSRVID experiments we used 300 dimension Glo Ve word embeddings ,15,"In all cases , we performed optimization using RMSProp with backpropagation , with a learning rate fixed to 10 ? 4 .","For the STS2014 , WikiQA , and TrecQA experiments , we used 300 dimension PARAGRAM - SL999 embeddings from and the PARAGRAM - PHRASE embeddings from , trained on word pairs from the Paraphrase Database ( PPDB ) .",experiment
named-entity-recognition,6,"To overcome those limitations , we propose an alternative approach where we embed annotations mined from Wikipedia into a vector space from which we compute a feature vector that represent words .",system description,Motivation,0,49,18,18,0,system description : Motivation,0.2311320754716981,0.9,0.9,To overcome those limitations we propose an alternative approach where we embed annotations mined from Wikipedia into a vector space from which we compute a feature vector that represent words ,31,"For instance , words such as "" eat "" , "" directed "" or "" born "" are words that typically appear after a mention of type PER .",This vector compactly and efficiently encodes both gazetteer and lexical information .,method
sentiment_analysis,2,"We regard the cross-entropy as the loss function , and the formula is as follows :",training,Model training,0,112,4,4,0,training : Model training,0.4933920704845815,0.5,0.5,We regard the cross entropy as the loss function and the formula is as follows ,16,"In our work , let y i be the correct sentiment polarity , which is represented by one - hot vector , and y i denotes the predicted sentiment polarity for the given sentence .",where ? is the regularization factor and ? contains all the parameters .,experiment
sentiment_analysis,43,"In this section , we conduct experiments to evaluate our two hypotheses : ( 1 ) whether the wordlevel interaction between aspect and context can help relieve the information loss and improve the performance .",experiment,Experiments,0,163,2,2,0,experiment : Experiments,0.6791666666666667,0.13333333333333333,0.6666666666666666,In this section we conduct experiments to evaluate our two hypotheses 1 whether the wordlevel interaction between aspect and context can help relieve the information loss and improve the performance ,31, ,"2 ) whether the relationship among the aspects , which have the same context and different sentiment polarities , can bring extra useful information .",experiment
named-entity-recognition,8,"At least partly due to this advantage , OpenAI achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark .",system description,Unsupervised Fine-tuning Approaches,0,57,18,5,0,system description : Unsupervised Fine-tuning Approaches,0.14728682170542634,0.4615384615384616,0.35714285714285715,At least partly due to this advantage OpenAI achieved previously state of the art results on many sentencelevel tasks from the GLUE benchmark ,24,The advantage of these approaches is that few parameters need to be learned from scratch .,Left - to - right language model - BERT BERT E E 1 E ...,method
natural_language_inference,88,"For a fair comparison against previous work , we report results with different hidden / memory dimensions ( i.e. , 100 , 300 , and 450 ) .",model,Natural Language Inference,0,226,41,19,0,model : Natural Language Inference,0.9149797570850202,0.7735849056603774,0.6129032258064516,For a fair comparison against previous work we report results with different hidden memory dimensions i e 100 300 and 450 ,22,The mini- batch size was set to 16 or 32 .,We compared variants of our model against different types of LSTMs ( see the second block in ) .,method
relation-classification,2,introduce a quadratic scoring layer to model the two tasks simultaneously .,introduction,introduction,0,26,16,16,0,introduction : introduction,0.08813559322033898,0.5,0.5,introduce a quadratic scoring layer to model the two tasks simultaneously ,12,"propose a neural joint model based on LSTMs where they model the whole sentence at once , but still they do not have a principled way to deal with multiple relations .","The limitation of this approach is that only a single relation can be assigned to a token , while the time complexity for the entity recognition task is increased compared to the standard approaches with linear complexity .",introduction
text_summarization,5,"It is interesting to see that , while the ROUGE - 2 score of Random templates is zero , our model can still generate acceptable summaries with Random templates .",evaluation,Effect of Templates,0,207,46,5,0,evaluation : Effect of Templates,0.8214285714285714,0.7301587301587301,0.2272727272727273,It is interesting to see that while the ROUGE 2 score of Random templates is zero our model can still generate acceptable summaries with Random templates ,27,"As illustrated in , the more high - quality templates are provided , the higher ROUGE scores are achieved .",It seems that Re 3 Sum can automatically judge whether the soft templates are trustworthy and ignore the seriously irrelevant ones .,result
machine-translation,8,"This is done by visualizing the annotation weights ? ij from Eq. , as in .",analysis,ALIGNMENT,0,149,4,3,0,analysis : ALIGNMENT,0.4501510574018127,0.125,0.25,This is done by visualizing the annotation weights ij from Eq as in ,14,The proposed approach provides an intuitive way to inspect the ( soft - ) alignment between the words in a generated translation and those in a source sentence .,"This is done by visualizing the annotation weights ? ij from Eq. , as in .",result
sentiment_analysis,48,denotes that the results are extracted from the original paper .,system description,Model Configuration & Classifiers,0,156,8,8,0,system description : Model Configuration & Classifiers,0.6610169491525424,0.25,0.25,denotes that the results are extracted from the original paper ,11,Better results are in bold .,"In this work , the KL weight is set to be 1e - 4 .",method
text_summarization,6,Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states .,abstract,abstract,0,8,5,5,0,abstract : abstract,0.03053435114503817,0.8333333333333334,0.8333333333333334,Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states ,17,Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables .,Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state - of the - art methods .,abstract
text-to-speech_synthesis,1,"We find that removing sequence - level knowledge distillation results in - 0.325 CMOS , which demonstrates the effectiveness of sequence - level knowledge distillation .",system,1D Convolution in FFT Block,1,196,12,8,0,system : 1D Convolution in FFT Block,0.8949771689497716,1.0,1.0,We find that removing sequence level knowledge distillation results in 0 325 CMOS which demonstrates the effectiveness of sequence level knowledge distillation ,23,"We conduct CMOS evaluation to compare the performance of FastSpeech with and without sequence - level knowledge distillation , as shown in .", ,method
natural_language_inference,37,Our paragraph selection method chooses the paragraph that has the smallest TF - IDF cosine distance with the question .,method,Paragraph Selection,0,44,6,2,0,method : Paragraph Selection,0.17120622568093385,0.2,0.2857142857142857,Our paragraph selection method chooses the paragraph that has the smallest TF IDF cosine distance with the question ,19, ,"Document frequencies are computed using just the paragraphs within the relevant documents , not the entire corpus .",method
natural_language_inference,42,These questions are more challenging because they might have multiple correct answers and require higher levels of abstraction .,evaluation,FABIR and BiDAF Statistics,0,268,43,15,0,evaluation : FABIR and BiDAF Statistics,0.9273356401384084,0.8113207547169812,0.6,These questions are more challenging because they might have multiple correct answers and require higher levels of abstraction ,19,"Other "" questions include alternatives , such as "" Name a type of ... "" or "" Does it ... "" or even questions with typos , such as "" Hoe was ... "" , which should be "" How was ... "" .","For instance , to respond to a question such as "" Name an ingredient ... "" , the model would need a deep understanding of the semantics of the word "" ingredients "" to identify "" tomatoes "" or "" cheese "" as possible answers .",result
natural_language_inference,47,"Our ability to evaluate standard classifier - base NLI models , however , was limited to those which were designed to scale to SNLI 's size without modification , so a more complete comparison of approaches will have to wait for future work .",system description,Our data as a platform for evaluation,0,113,78,4,0,system description : Our data as a platform for evaluation,0.5255813953488372,0.9873417721518988,0.8,Our ability to evaluate standard classifier base NLI models however was limited to those which were designed to scale to SNLI s size without modification so a more complete comparison of approaches will have to wait for future work ,40,"In par - ticular , since it is dramatically larger than any existing corpus of comparable quality , we expect it to be suitable for training parameter - rich models like neural networks , which have not previously been competitive at this task .","In this section , we explore the performance of three classes of models which could scale readily : ( i ) models from a well - known NLI system , the Excitement Open Platform ; ( ii ) variants of a strong but simple feature - based classifier model , which makes use of both unlexicalized and lexicalized features , and ( iii ) distributed representation models , including a baseline model and neural network sequence models .",method
paraphrase_generation,1,VAE Input ( Encoder ),model,Model Architecture,0,79,11,11,0,model : Model Architecture,0.3574660633484163,0.4230769230769231,0.4230769230769231,VAE Input Encoder ,4,"In particular , our model consists of three LSTM encoders and one LSTM decoder ( thus a total of four LSTMs ) , which are employed by our VAE based architecture as follows :","As shown in , two of the LSTM encoders are used on the VAE 's input side .",method
natural_language_inference,73,All weight matrices are initialized by and the biases are initialized as zeros .,training,Model Training,0,179,37,37,0,training : Model Training,0.683206106870229,0.9736842105263158,0.9736842105263158,All weight matrices are initialized by and the biases are initialized as zeros ,14,"We use Adadelta as optimizer , which performs more stable than Adam on ReSAN .",We use 300D Glo Ve 6B pre-trained vectors,experiment
relation-classification,7,"BioBERT outperformed the state - of - the - art models on six out of nine datasets , and BioBERT v 1.1 ( PubMed ) outperformed the state - of - the - art models by 0.62 in terms of micro averaged F1 score .",experiment,Experimental results,1,139,22,5,0,experiment : Experimental results,0.6984924623115578,0.6875,0.3333333333333333,BioBERT outperformed the state of the art models on six out of nine datasets and BioBERT v 1 1 PubMed outperformed the state of the art models by 0 62 in terms of micro averaged F1 score ,38,"On the other hand , BioBERT achieves higher scores than BERT on all the datasets .","The relatively low scores on the LINNAEUS dataset can be attributed to the following : ( i ) the lack of a silver - standard dataset for training previous state - of - the - art models and ( ii ) different training / test set splits used in previous work , which were unavailable .",experiment
natural_language_inference,95,"Based on this representation , we employ a Pointer Network to predict the start and end position of the answer in the module of answer boundary prediction ( Section 2.2 ) .",introduction,introduction,0,63,55,55,0,introduction : introduction,0.2692307692307692,0.9482758620689656,0.9482758620689656,Based on this representation we employ a Pointer Network to predict the start and end position of the answer in the module of answer boundary prediction Section 2 2 ,30,"Following , we compute the question - aware representation for each passage ( Section 2.1 ) .","At the same time , with the answer content model ( Section 2.3 ) , we estimate whether each word should be included in the answer and thus obtain the answer representations .",introduction
natural_language_inference,41,"For example , it improves performance by 6 ROUGE over previous work on XSum .",introduction,introduction,0,26,15,15,0,introduction : introduction,0.10116731517509728,0.6521739130434783,0.6521739130434783,For example it improves performance by 6 ROUGE over previous work on XSum ,14,"It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD , and achieves new state - of - the - art results on a range of abstractive dialogue , question answering , and summarization tasks .",BART also opens up new ways of thinking about fine tuning .,introduction
sentiment_analysis,1,"The symmetric adjacency matrix design is mostly motivated to reduce the number of model parameters and prevent overfitting , especially in subject - dependent classifications where lesser training data is available .",ablation,Ablation Study,0,351,6,6,0,ablation : Ablation Study,0.8863636363636364,0.42857142857142855,0.42857142857142855,The symmetric adjacency matrix design is mostly motivated to reduce the number of model parameters and prevent overfitting especially in subject dependent classifications where lesser training data is available ,30,"The global connection models the asymmetric difference between neuronal activities in the left and right hemispheres and have been shown to reveal certain emotions , , .","Our NodeDAT regularizer has a noticeable positive impact on the performance of our model , which demonstrates that domain adaptation is significantly helpful in crosssubject classification .",result
sentiment_analysis,5,"Subsequently , another RNN summarizes all the asymmetric discrepancy information and produces a global deep representation in each directional stream ; ( 3 ) Finally , we integrate the global features from horizon and vertical streams with learnable linear transformation matrices and use a classifier to map this representation into the label space .",introduction,introduction,0,54,45,45,0,introduction : introduction,0.2037735849056604,0.8653846153846154,0.8653846153846154,Subsequently another RNN summarizes all the asymmetric discrepancy information and produces a global deep representation in each directional stream 3 Finally we integrate the global features from horizon and vertical streams with learnable linear transformation matrices and use a classifier to map this representation into the label space ,49,These operations will model the discrepancy information from different aspects .,"Considering the tremendous data distribution shift of EEG emotional signal , especially in the case of subject - independent task where the source ( training ) and target ( testing ) data come from different subjects , we leverage a domain discriminator that works cooperatively with the classifier to encourage the emotion - related but domain - invariant data representation appeared .",introduction
text-to-speech_synthesis,2,"This suggests that additional care must betaken to disentangle speaker identity from prosody within the synthesis network , perhaps by integrating a prosody encoder as in , or by training on randomly paired reference and target utterances from the same speaker .",experiment,experiment,0,130,39,39,0,experiment : experiment,0.5306122448979592,0.375,0.375,This suggests that additional care must betaken to disentangle speaker identity from prosody within the synthesis network perhaps by integrating a prosody encoder as in or by training on randomly paired reference and target utterances from the same speaker ,40,"This effect is larger on LibriSpeech , which contains more varied prosody .","To evaluate how well the synthesized speech matches that from the target speaker , we paired each synthesized utterance with a randomly selected ground truth utterance from the same speaker .",experiment
natural_language_inference,49,Intra-sentence attention + ( 11 ) 86.8 13 . BiMPM 86.9 14 . NTI - SLSTM-LSTM 87.3 15 . re-read LSTM 87.5 16 . ESIM 88.0 17 . ESIM ensemble with syntactic tree - LSTM 88.6 18 . BiMPM ( ensemble ) 88,experiment,EXPERIMENT ON SNLI,0,178,64,12,0,experiment : EXPERIMENT ON SNLI,0.7007874015748031,0.9142857142857144,1.0,Intra sentence attention 11 86 8 13 BiMPM 86 9 14 NTI SLSTM LSTM 87 3 15 re read LSTM 87 5 16 ESIM 88 0 17 ESIM ensemble with syntactic tree LSTM 88 6 18 BiMPM ensemble 88,39,decomposable attention model 86.3 12 ., ,experiment
sentiment_analysis,22,"IAN interactively learns attentions in the contexts and targets , and generates the representations for targets and contexts separately .",method,Model,1,145,11,2,0,method : Model,0.6331877729257642,0.5789473684210527,1.0,IAN interactively learns attentions in the contexts and targets and generates the representations for targets and contexts separately ,19, , ,method
part-of-speech_tagging,7,"In this paper , we a ) evaluate the effectiveness of different representations in bi - LSTMs , b ) compare these models across a large set of languages and under varying conditions ( data size , label noise ) and c) propose a novel bi - LSTM model with auxiliary loss ( LOGFREQ ) .",introduction,introduction,0,28,19,19,0,introduction : introduction,0.22399999999999998,1.0,1.0,In this paper we a evaluate the effectiveness of different representations in bi LSTMs b compare these models across a large set of languages and under varying conditions data size label noise and c propose a novel bi LSTM model with auxiliary loss LOGFREQ ,45,These performance gains transfer into general improvements for morphologically rich languages ., ,introduction
sentence_compression,2,"Indeed , in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets .",introduction,introduction,1,15,9,9,0,introduction : introduction,0.1485148514851485,0.6923076923076923,0.6923076923076923,Indeed in this work we use gaze data from readers of the Dundee Corpus to improve sentence compression results on several datasets ,23,Our proposed model does not require that the gaze data and the compression data come from the same source .,"While not explored here , an intriguing potential of this work is in deriving sentence simplification models that are personalized for individual users , based on their reading behavior .",introduction
relation_extraction,12,-- Bidir DAG LSTM 75.6 77.3 76.9 76.4 51.7 50.7 GS GLSTM,model,B T B Single Cross Single Cross Cross Cross,0,193,6,3,0,model : B T B Single Cross Single Cross Cross Cross,0.5866261398176292,0.13043478260869565,0.1153846153846154, Bidir DAG LSTM 75 6 77 3 76 9 76 4 51 7 50 7 GS GLSTM,18,Feature - Based 74.7 77.7 73.9 75.2 -- SPTree -- 75.9 75.9 -- Graph LSTM-EMBED 80.6 74.3 76.5 -- Graph LSTM-FULL 77.9 80.7 75.6 76.7 --00000000000000000 + multi-task - 82.0 - 78.5,80 : Average test accuracies in five - fold validation for binary - class n-ary relation extraction and multi-class n-ary relation extraction .,method
natural_language_inference,47,"Our annotators generally followed suit , writing sentences that , while structurally diverse , share topic / focus ( theme / rheme ) structure with their premises .",analysis,Analysis and discussion,0,189,21,21,0,analysis : Analysis and discussion,0.8790697674418605,0.525,0.8076923076923077,Our annotators generally followed suit writing sentences that while structurally diverse share topic focus theme rheme structure with their premises ,21,"For the most part , the original image prompts contained a focal element that the caption writer identified with a syntactic subject , following information structuring conventions associating subjects and topics in English .","This promotes a coherent , situation - specific construal of each sentence pair .",result
natural_language_inference,54,stituency tree and dependency tree into consideration .,introduction,introduction,0,12,5,5,0,introduction : introduction,0.052401746724890834,0.2272727272727273,0.2272727272727273,stituency tree and dependency tree into consideration ,8,"Although deep learning based methods demonstrated great potentials for question answering , none them take syntactic information of the sentences such as con - * Authors contributed equally to this work .",Such techniques have been proven to be useful in many natural language understanding tasks in the past and illustrated noticeable improvements such as the work by .,introduction
sentiment_analysis,9,"The polarity of each aspect on the Laptops , Restaurants and datasets maybe positive , neutral , and negative , and the conflicting labels of polarity are not considered .",dataset,Datasets and Hyperparameters Setting,0,178,6,6,0,dataset : Datasets and Hyperparameters Setting,0.6425992779783394,0.375,0.375,The polarity of each aspect on the Laptops Restaurants and datasets maybe positive neutral and negative and the conflicting labels of polarity are not considered ,26,"We reformatted the origin dataset and annotated each sample with the IOB labels for ATE task and polarity labels for APC tasks , respectively .","The reviews in the four Chinese datasets have been purged , with each aspect maybe positive or negative binary polarity .",experiment
natural_language_inference,92,Varying the size of solution set at training .,analysis,Analysis,0,187,8,8,0,analysis : Analysis,0.6493055555555556,0.2285714285714285,0.2285714285714285,Varying the size of solution set at training ,9,"The gap between MML and our method is marginal when | Z | = 0 or 1 , and gradually increases as | Z | grows .","To see how our learning method works with respect to the size of a solution set ( | Z | ) of the training data , particularly with large | Z| , we take 5 subsets of the training set on WIKISQL with | Z | = 3 , 10 , 30 , 100 , 300 .",result
natural_language_inference,2,"This is because for longer answers , the model is notable to decide the exact boundaries ( low EM score ) but manages to predict some correct words which partially overlap with the reference answer ( relatively higher F1 score ) .",analysis,Quantitative Error Analysis,0,241,6,6,0,analysis : Quantitative Error Analysis,0.8992537313432836,0.4,1.0,This is because for longer answers the model is notable to decide the exact boundaries low EM score but manages to predict some correct words which partially overlap with the reference answer relatively higher F1 score ,37,The gap between F1 and EM also increases for longer answers ., ,result
text_summarization,9,amples highlight typical mistakes of the models .,result,Results,0,145,40,40,0,result : Results,0.9477124183006536,0.9302325581395348,0.9302325581395348,amples highlight typical mistakes of the models ,8,"is the input , G is the true headline , A is ABS + , and R is RAS - ELMAN .","In Sentence 3 both models take literally the figurative use of the idiom "" a silence that spoke volumes , "" and produce fluent but nonsensical summaries .",result
natural_language_inference,63,"where is element - wise multiplication and J p ,q is p by q matrix of ones .",model,Operation Name Vector,0,111,71,16,0,model : Operation Name Vector,0.6166666666666667,0.9342105263157896,1.0,where is element wise multiplication and J p q is p by q matrix of ones ,17,"After finding the location to write with this weight , write operation on the memory is performed as follows :", ,method
sentiment_analysis,15,Recursive Neural Tensor Networks take as input phrases of any length .,introduction,introduction,1,28,16,16,0,introduction : introduction,0.1037037037037037,0.7272727272727273,0.7272727272727273,Recursive Neural Tensor Networks take as input phrases of any length ,12,"In order to capture the compositional effects with higher accuracy , we propose a new model called the Recursive Neural Tensor Network ( RNTN ) .",They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor - based composition function .,introduction
natural_language_inference,15,"Furthermore , this component increased the test performance by working as a regularizer in our experiments .",method,Bottleneck component,0,98,40,6,0,method : Bottleneck component,0.4336283185840708,0.8,1.0,Furthermore this component increased the test performance by working as a regularizer in our experiments ,16,"Autoencoder is a compression technique that reduces the number of features while retaining the original information , which can be used as a distilled semantic knowledge in our model .", ,method
natural_language_inference,59,"Our basic decomposable attention model DECATT word without pre-trained embeddings is better than most of the models , all of which used GloVe embeddings .",result,Results,1,116,4,4,0,result : Results,0.8592592592592593,0.25,0.25,Our basic decomposable attention model DECATT word without pre trained embeddings is better than most of the models all of which used GloVe embeddings ,25,"We observe that the simple FFNN baselines work better than more complex Siamese and Multi - Perspective CNN or LSTM models , more so if character n-gram based embeddings are used .",An interesting observation is that DECATT char model without any pretrained embeddings outperforms DE - CATT glove that uses task - agnostic GloVe embeddings .,result
natural_language_inference,50,The negative sampling rate is tuned from 2 to 8 .,evaluation,Hyperparameters.,1,216,21,8,0,evaluation : Hyperparameters.,0.6813880126182965,0.75,0.5333333333333333,The negative sampling rate is tuned from 2 to 8 ,11,"L2 regularization is tuned amongst { 0.001 , 0.0001 , 0.00001 }.","Finally , the margin ? is tuned amongst { 1 , 2 , 5 , 10 , 20 }.",result
sentence_classification,0,"Making use of a method , tool , approach or dataset",method,Method,0,101,2,2,0,method : Method,0.3782771535580524,0.125,0.2,Making use of a method tool approach or dataset,9, ,Fold differences were calculated by a mathematical model described in .,method
topic_models,0,"We do this from the following viewpoints : ( a ) Graphical models illustrating the dependency of random and observed variables , ( b ) assumptions of distributions over random variables and their limitations , and ( c ) approximations made during the inference and their consequences .",introduction,introduction,0,47,25,25,0,introduction : introduction,0.11407766990291265,0.7575757575757576,0.7575757575757576,We do this from the following viewpoints a Graphical models illustrating the dependency of random and observed variables b assumptions of distributions over random variables and their limitations and c approximations made during the inference and their consequences ,39,"Given the significant prior research in PTMs and related algorithms for learning representations , it is important to draw precise relations between the presented model and former works .","The contributions of this paper are as follows : ( a ) we present Bayesian subspace multinomial model and analyse its relation to popular models such as latent Dirichlet allocation ( LDA ) , correlated topic model ( CTM ) , paragraph vector ( PV - DBOW ) and neural variational document model ( NVDM ) , ( b ) we adapt tricks from for faster and efficient variational inference of the proposed model , ( c ) we combine optimization techniques from , and use them to train the proposed model , ( d ) we propose a generative Gaussian classifier that exploits uncertainty in the posterior distribution of document embeddings , ( e ) we provide experimental results on both text and speech data showing that the proposed document representations achieve state - of - theart perplexity scores , and ( f ) with our proposed classification systems , we illustrate robustness of the model to over-fitting and at the same time obtain superior classification results when compared systems based on state - of - the - art unsupervised models .",introduction
negation_scope_resolution,0,proposed a simple regular expression algorithm ( NegEx ) to detect negation cues .,system description,Rule-Based Approaches,0,52,6,6,0,system description : Rule-Based Approaches,0.2260869565217392,0.1935483870967742,0.1935483870967742,proposed a simple regular expression algorithm NegEx to detect negation cues ,12,They showed that it was possible to apply computational methods to detect negation cues in a sentence .,"They posited that medical language is lexically less ambiguous and hence a rule - based system can be applied , and that a simpler system than the one proposed by also performed well .",method
natural_language_inference,34,"The fusion process is iteratively performed at each hop through the document tokens and entities , and the final resulting answer is then obtained from document tokens .",system description,Original Entity Graph,1,58,34,31,0,system description : Original Entity Graph,0.19661016949152546,0.7906976744186046,0.775,The fusion process is iteratively performed at each hop through the document tokens and entities and the final resulting answer is then obtained from document tokens ,27,"We not only aggregate information from documents to the entity graph ( doc2 graph ) , but also propagate the information of the entity graph back to document representations ( graph2doc ) .","The fusion process of doc2 graph and graph2doc along with the dynamic entity graph jointly improve the interaction between the information of documents and the entity graph , leading to a less noisy entity graph and thus more accurate answers .",method
named-entity-recognition,8,"It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in .",model,model,0,259,10,10,0,model : model,0.6692506459948321,0.35714285714285715,0.35714285714285715,It has long been known that increasing the model size will lead to continual improvements on large scale tasks such as machine translation and language modeling which is demonstrated by the LM perplexity of held out training data shown in ,41,"By contrast , BERT BASE contains 110M parameters and BERT LARGE contains 340M parameters .","However , we believe that this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks , provided that the model has been sufficiently pre-trained .",method
machine-translation,2,summarizes our results and compares our translation quality and training costs to other model architectures from the literature .,result,Machine Translation,0,192,14,13,0,result : Machine Translation,0.8571428571428571,0.9333333333333332,0.9285714285714286,summarizes our results and compares our translation quality and training costs to other model architectures from the literature ,19,"We set the maximum output length during inference to input length + 50 , but terminate early when possible .","We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single - precision floating - point capacity of each GPU 5 .",result
natural_language_inference,11,"Compared to previously published state of the art systems , our models acquit themselves quite well on the MultiNLI benchmark , and competitively on the SNLI benchmark .",model,Model,1,150,15,15,0,model : Model,0.5434782608695652,0.5769230769230769,0.5769230769230769,Compared to previously published state of the art systems our models acquit themselves quite well on the MultiNLI benchmark and competitively on the SNLI benchmark ,26,"When providing additional background knowledge from ConceptNet , our BiLSTM based models improve substantially , while the ESIM - based models improve only on the more difficult MultiNLI dataset .","In parallel to this work , Gong et al. ( 2017 ) developed a novel task - specific architecture for RTE that achieves slightly better performance on MultiNLI than our ESIM + p + q + A based models .",method
natural_language_inference,33,Ablation specifics are presented in section 9 of the Appendix .,evaluation,EVALUATION STRATEGY,0,134,10,10,0,evaluation : EVALUATION STRATEGY,0.5134099616858238,0.072992700729927,1.0,Ablation specifics are presented in section 9 of the Appendix ,11,We present performance ablations when adding more tasks and increasing the number of hidden units in our GRU ( + L ) ., ,result
text_summarization,5,"Since OpenNMT prefers selecting source words around the predicate to form the summary , it fails on this sentence .",evaluation,Effect of Templates,0,214,53,12,0,evaluation : Effect of Templates,0.8492063492063492,0.8412698412698413,0.5454545454545454,Since OpenNMT prefers selecting source words around the predicate to form the summary it fails on this sentence ,19,"In Example 1 , there is no predicate in the source sentence .","By contract , Re 3 Sum rewrites the template and produces an informative summary .",result
sentiment_analysis,25,Attention layer involves exponential operation and normalization of all alignment scores of all the words in the sentence .,introduction,introduction,0,30,18,18,0,introduction : introduction,0.13513513513513514,0.6,0.6,Attention layer involves exponential operation and normalization of all alignment scores of all the words in the sentence ,19,LSTM processes one token in a step .,"Moreover , some models needs the positional information between words and targets to produce weighted LSTM , which can be unreliable in noisy review text .",introduction
text_generation,1,"We compare our approach with the state - of - the - art methods including maximum likelihood estimation ( MLE ) , policy gradient with BLEU ( PG - BLEU ) , and SeqGAN .",experiment,Simulation on synthetic data,0,183,19,16,0,experiment : Simulation on synthetic data,0.6678832116788321,0.18446601941747573,0.4571428571428571,We compare our approach with the state of the art methods including maximum likelihood estimation MLE policy gradient with BLEU PG BLEU and SeqGAN ,25,"The lower the NLL score is , the higher probability the generated sentence will pass the Turing test .","The PG - BLEU computes the BLEU score to measure the similarity between the generated sentence and the human - written sentences , then takes the BLEU score as the reward to update the generator with policy gradient .",experiment
text_summarization,4,The scores are then passed to a softmax and are used to pool the encoder states using weighted sum .,model,Base model,0,90,20,12,0,model : Base model,0.3488372093023256,0.21739130434782608,0.8571428571428571,The scores are then passed to a softmax and are used to pool the encoder states using weighted sum ,20,"The context vector ct is computed using the additive attention mechanism , which matches the current decoder state st and each encoder state hi to get an importance score .","The final pooled vector is the context vector , as shown in the equations below .",method
sentiment_analysis,35,"Motivated by autoencoders ) , we introduce an auxiliary pseudo - label prediction task for the source task .",system description,Context2Aspect (C2A) Attention,0,114,55,22,0,system description : Context2Aspect (C2A) Attention,0.4596774193548387,0.5612244897959183,0.3384615384615385,Motivated by autoencoders we introduce an auxiliary pseudo label prediction task for the source task ,16,"Based on this observation , we can capture more specific semantics of the source aspect and its position information conditioned on its context .","In this task , a source aspect a sis not only regarded as a sequence of aspect words , but also as a pseudo - label ( category of the aspect ) y c , where c ? C and C is a set of aspect categories .",method
semantic_role_labeling,4,People start their own business ...,performance,performance,0,214,21,21,0,performance : performance,0.7133333333333334,0.42857142857142855,0.9545454545454546,People start their own business ,6,Consider the following two sentences :, ,result
natural_language_inference,1,"The dataset is available from http://fb.ai/babi. which would result in trivial uninformative questions , such as , Name a person who is an actor ?.",dataset,The SimpleQuestions dataset,0,70,13,13,0,dataset : The SimpleQuestions dataset,0.2564102564102564,0.6190476190476191,0.6190476190476191,The dataset is available from http fb ai babi which would result in trivial uninformative questions such as Name a person who is an actor ,26,This filtering step is crucial to remove facts,The threshold was set to 10 .,experiment
text_summarization,4,"Second , the linked entities may also be too common to be considered an entity .",system description,Possible issues,0,64,25,9,0,system description : Possible issues,0.248062015503876,0.8064516129032258,0.6,Second the linked entities may also be too common to be considered an entity ,15,"We discover that 71.0 % of the top 100 entities and 53.6 % of the entities picked at random have dis ambiguation pages , which shows that most entities are prone to ambiguity problems .",This may introduce errors and irrelevance to the summary .,method
semantic_role_labeling,2,"While the CoNLL 2005 shared task assumes gold predicates as input , this information is not available in many downstream applications .",model,Predicate Detection,0,76,40,2,0,model : Predicate Detection,0.3392857142857143,0.9302325581395348,0.4,While the CoNLL 2005 shared task assumes gold predicates as input this information is not available in many downstream applications ,21, ,"We propose a simple model for end - to - end SRL , where the system first predicts a set of predicate words v from the input sentence w .",method
natural_language_inference,6,"We use the first two million sentences in Arabic , Russian and Chinese .",training,Training data,0,194,6,6,0,training : Training data,0.782258064516129,0.1,0.1875,We use the first two million sentences in Arabic Russian and Chinese ,13,The size varies from 400 k to 2M sentences depending on the language pair .,OpenSubtitles2018 : A parallel corpus of movie subtitles in 57 languages .,experiment
question_generation,1,"Further , we observe that the proposed approach substantially improves over state - of - the - art benchmarks on the quantitative metrics ( BLEU , METEOR , ROUGE , and CIDEr ) .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.02295918367346939,1.0,1.0,Further we observe that the proposed approach substantially improves over state of the art benchmarks on the quantitative metrics BLEU METEOR ROUGE and CIDEr ,25,The generated questions show a remarkable similarity to the natural questions as validated by a human study ., ,abstract
sentiment_analysis,11,The attention - based filtering in each hop provides a refined context representation of the histories .,result,Results,0,299,13,13,0,result : Results,0.8691860465116279,0.2452830188679245,0.7647058823529411,The attention based filtering in each hop provides a refined context representation of the histories ,16,The second graph shows that multiple hops on the histories indeed lead to an improvement in performance .,Models with hops in the range of 3 ? 10 outperform the single layer variant .,result
natural_language_inference,82,Factorization ( 2 ) relies on the hypothesis that there exists a fixed vector for each candidate answer representing its meaning .,model,Model,0,34,8,8,0,model : Model,0.2982456140350877,0.7272727272727273,0.7272727272727273,Factorization 2 relies on the hypothesis that there exists a fixed vector for each candidate answer representing its meaning ,20,"in which a vector u ( D , q ) is learned to represent the status of a reader after reading a document and a query , and this vector is used to retrieve an answer by coupling with the answer vector v ( a ) .","However , as we argued in Section 1 , an entity surface does not possess meaning ; rather , it serves as an anchor to link pieces of information about it .",method
natural_language_inference,56,BaBi ablation results .,ablation,bAbI ablation experiments,0,245,11,11,0,ablation : bAbI ablation experiments,0.7291666666666666,0.3235294117647059,1.0,BaBi ablation results ,4,We also do pseudo-ablation experiments with fewer steps by measuring at each step of the RRN ., ,result
sentiment_analysis,15,The parent vectors are again given as features to a classifier .,model,Recursive Neural Models,0,96,7,7,0,model : Recursive Neural Models,0.3555555555555556,0.06542056074766353,0.3181818181818182,The parent vectors are again given as features to a classifier ,12,Recursive neural models will then compute parent vectors in a bottom up fashion using different types of compositionality functions g.,"For ease of exposition , we will use the tri-gram in this figure to explain all models .",method
sentiment_analysis,9,"Additionally , we apply a tanh activation function for the MHSA learning process , which significantly enhanced featurecapture capability .",model,Multi-Head Self-Attention,0,103,33,16,0,model : Multi-Head Self-Attention,0.37184115523465705,0.4125,1.0,Additionally we apply a tanh activation function for the MHSA learning process which significantly enhanced featurecapture capability ,18,"The "" ; "" means feature concatenation of each head . ? ? ? ? is the parameter matrices for projection .", ,method
semantic_parsing,2,Notice that we use different parameters for the scoring networks ?( ) .,training,SELECT Clause,0,226,117,31,0,training : SELECT Clause,0.7766323024054983,0.9915254237288136,0.96875,Notice that we use different parameters for the scoring networks ,11,"where l L yt / r R yt represents the first / last copying index of cond yt is l/r , the probabilities are normalized to 1 , and ?( ) is the scoring network defined in Equation .","The copied span is represented by the concatenated vector [? l ,? r ] , which is fed into a one - layer neural network and then used as the input to the next LSTM unit in the decoder .",experiment
question_answering,0,We adopt GGNNs for semantic parsing to learn a vector representation of a semantic graph .,system description,Gated Graph Neural Networks,0,109,60,3,0,system description : Gated Graph Neural Networks,0.3694915254237288,0.6666666666666666,0.21428571428571427,We adopt GGNNs for semantic parsing to learn a vector representation of a semantic graph ,16,Gate Graph Neural Networks ( GGNN ) process graphs by iteratively updating representations of graph nodes based on the neighboring nodes and relations .,give a formulation of GGNNs for graphs with labeled nodes and typed directed edges .,method
text_generation,1,"In the experiment , we evaluate the performance of the language generator by averaging BLEU scores to measure the similarity between the generated sentences and the human - written sentences in the validation set .",experiment,Results on COCO image captions,0,247,83,8,0,experiment : Results on COCO image captions,0.9014598540145984,0.8058252427184466,0.4,In the experiment we evaluate the performance of the language generator by averaging BLEU scores to measure the similarity between the generated sentences and the human written sentences in the validation set ,33,"Since the proposed RankGAN focuses on unconditional GANs that do not consider any prior knowledge as input , we train our model on the captions of the training set without conditioning on specific images .",shows the performance comparison of different methods .,experiment
text_generation,3,"In the training stage , besides the auto - encoder loss and the mapping loss , we also use an end - toend loss J 4 ( ? , ? , ? ) :",training,Training and Testing,0,74,7,7,0,training : Training and Testing,0.524822695035461,0.5384615384615384,0.5384615384615384,In the training stage besides the auto encoder loss and the mapping loss we also use an end toend loss J 4 ,23,"Finally , t is sent to the target - decoder for response generation .","where x is the source input , y is the target response , and T is the length of response sequence .",experiment
sarcasm_detection,0,"Before constructing this subcorpus we first remove from consideration all comments thatare not complete sentences and not between 2 and 50 tokens long , allowing for cleaner comments in the evaluation .",evaluation,Evaluation Task,0,126,5,5,0,evaluation : Evaluation Task,0.7159090909090909,0.2631578947368421,0.625,Before constructing this subcorpus we first remove from consideration all comments thatare not complete sentences and not between 2 and 50 tokens long allowing for cleaner comments in the evaluation ,31,"Performance on this task is measured by average precision , recall and F 1 scores .","Although the responses are still largely non-sarcastic , the proportion of sarcastic comments is much greater here as each datapoint must correspond to a thread where at least one sarcastic annotation occurred .",result
natural_language_inference,42,"It defines three different matrices U , K and V thatare associated with queries , keys , and values , respectively .",system description,Google's Transformer,0,60,20,3,0,system description : Google's Transformer,0.2076124567474049,0.11428571428571427,0.17647058823529413,It defines three different matrices U K and V thatare associated with queries keys and values respectively ,18,The Transformer is a machine translation model introduced in [ 2 ] that achieved state - of - the - art results by combining feedforward neural networks with a multiplicative attention mechanism applied over position - encoded embedding vectors .,Every attention operation in the Transformer is performed by multiplying these matrices as shown in .,method
natural_language_inference,81,"The superfamily Apoidea is a major group within the Hymenoptera , which includes two traditionally recognized lineages , the "" sphecoid "" wasps , and the bees .",APPENDIX,Answer town,0,354,146,66,0,APPENDIX : Answer town,0.8408551068883611,0.6854460093896714,0.4962406015037594,The superfamily Apoidea is a major group within the Hymenoptera which includes two traditionally recognized lineages the sphecoid wasps and the bees ,23,The study of bees including honey bees is known as melittology .,"Molecular phylogeny demonstrates that the bees arose from within the Crabronidae , so that grouping is paraphyletic .",others
sentiment_analysis,23,"This subsection describes training details for d - TBCNN , where hyperparameters are chosen by validation .",training,Training Details,0,188,2,2,0,training : Training Details,0.6438356164383562,0.1176470588235294,0.1176470588235294,This subsection describes training details for d TBCNN where hyperparameters are chosen by validation ,15, ,"c- TBCNN is mostly tuned synchronously ( e.g. , optimization algorithm , activation function ) with some changes in hyperparameters .",experiment
machine-translation,4,"For the encoder and decoder , we follow the newly emerged Transformer .",model,Model Architecture,0,61,4,4,0,model : Model Architecture,0.2552301255230125,0.1,0.2857142857142857,For the encoder and decoder we follow the newly emerged Transformer ,12,"It consists of seven sub networks : including two encoders Enc sand Enc t , two decoders Dec sand Dec t , the local discriminator D l , and the global discriminators D g 1 and D g 2 .","Specifically , the encoder is composed of a stack of four identical layers",method
sarcasm_detection,1,This CNN is capable of modeling only the content of a comment .,model,Baseline Models,0,262,7,7,0,model : Baseline Models,0.7844311377245509,0.2692307692307692,0.2692307692307692,This CNN is capable of modeling only the content of a comment ,13,CNN : We compare our model with this individual CNN version .,The architecture is similar to the CNN used in CASCADE ( see Section 3.2 ) .,method
natural_language_inference,36,"ESIM is simple with satisfactory performance , and thus is widely chosen as the baseline model .",system description,Text Comprehension,0,70,34,34,0,system description : Text Comprehension,0.3333333333333333,0.53125,0.9444444444444444,ESIM is simple with satisfactory performance and thus is widely chosen as the baseline model ,16,"Notably , proposed an enhanced sequential inference model ( ESIM ) , which employed recursive architectures in both local inference modeling and inference composition , as well as syntactic parsing information , for a sequential inference model .",proposed to transfer the LSTM encoder from the neural machine translation ( NMT ) to the NLI task to contextualize word vectors .,method
natural_language_inference,75,We discuss choices of feature map in Sec. 3.2 .,model,Model Description,0,64,15,15,0,model : Model Description,0.31527093596059114,0.0974025974025974,0.5172413793103449,We discuss choices of feature map in Sec 3 2 ,11,"Addressing : during addressing , each candidate memory is assigned a relevance probability by comparing the question to each key :","Value Reading : in the final reading step , the values of the memories are read by taking their weighted sum using the addressing probabilities , and the vector o is returned :",method
natural_language_inference,0,Let D ( X ( K?1 ) ) be the full output of final layer document Bi - GRU .,model,Answer Prediction,0,86,25,2,0,model : Answer Prediction,0.4365482233502538,0.5208333333333334,0.2,Let D X K 1 be the full output of final layer document Bi GRU ,16, ,"To obtain the probability that a particular token in the document answers the query , we take an inner-product between these two , and pass through a softmax layer :",method
text_generation,5,"First , we propose the use of a dilated CNN as a new decoder for VAE .",introduction,introduction,0,40,29,29,0,introduction : introduction,0.13559322033898305,0.8787878787878788,0.8787878787878788,First we propose the use of a dilated CNN as a new decoder for VAE ,16,Our contributions are as follows :,"We then empirically evaluate several dilation architectures with different capacities , finding that reduced contextual capacity leads to stronger reliance on latent representations .",introduction
sentiment_analysis,44,"Next , for each node of the discourse tree , a sentiment label was extracted from the corresponding labeled syntactic tree by finding a subtree that exactly ( or almost exactly 3 ) matches the text span represented by the node in the discourse tree .",system description,Corpora,0,105,15,15,0,system description : Corpora,0.4772727272727273,1.0,1.0,Next for each node of the discourse tree a sentiment label was extracted from the corresponding labeled syntactic tree by finding a subtree that exactly or almost exactly 3 matches the text span represented by the node in the discourse tree ,42,"For each sentence in the dataset , a Discourse Tree was created using .", ,method
natural_language_inference,83,The final prediction corresponds to the option with greater score .,baseline,Baselines,0,188,18,18,0,baseline : Baselines,0.6143790849673203,0.7826086956521741,0.7826086956521741,The final prediction corresponds to the option with greater score ,11,Here P k represents the probability obtained from the k th logistic regression .,Aspect - aware Ensemble :,result
natural_language_inference,77,The binary word in question feature is computed on lemmas provided by spacy and restricted to alphanumeric words that are not stopwords .,model,model,1,145,5,5,0,model : model,0.5311355311355311,0.09615384615384616,0.15151515151515152,The binary word in question feature is computed on lemmas provided by spacy and restricted to alphanumeric words that are not stopwords ,23,As pre-processing steps we lowercase all inputs and tokenize it using spacy 4 .,"Throughout all experiments we use a hidden dimensionality of n = 150 , dropout at the input embeddings with the same mask for all words and a rate of 0.2 and 300 - dimensional fixed word - embeddings from Glove .",method
sentiment_analysis,41,"In our experiments , we use AdaGrad as our optimization method , which has improved the robustness of SGD on large scale learning task remarkably in a distributed environment .",training,Model Training,0,143,28,28,0,training : Model Training,0.6412556053811659,0.9655172413793104,0.9655172413793104,In our experiments we use AdaGrad as our optimization method which has improved the robustness of SGD on large scale learning task remarkably in a distributed environment ,28,"The percentage of outof - vocabulary words is about 5 % , and they are randomly initialized from U (?? , ? ) , where ? = 0.01 .","AdaGrad adapts the learning rate to the parameters , performing larger updates for infrequent parameters and smaller updates for frequent parameters .",experiment
text-to-speech_synthesis,0,where the likelihood P ( y | x ; ? ) can be factored by the chainrule and formulated as the cross - entropy between the one - hot label and per-token probability :,system description,Token-Level Knowledge Distillation,0,65,30,5,0,system description : Token-Level Knowledge Distillation,0.4012345679012346,0.5263157894736842,0.5,where the likelihood P y x can be factored by the chainrule and formulated as the cross entropy between the one hot label and per token probability ,28,G2P model based on sequence to sequence learning aims to minimize the negative log - likelihood loss on corpus D:,"Ty is the length of the target sequence , | V | is the vocabulary size of the phonemes , yt is the t- th target token in the phoneme sequence , and 1 { } is the indicator function indicating the id of the phoneme in vocabulary .",method
sentiment_analysis,19,The performance of our models is still someway off from MultiTask ; but they use a training dataset which is two orders of magnitude larger with a complex set of learning objectives .,system description,General Sentence Representation,0,85,33,11,0,system description : General Sentence Representation,0.5555555555555556,0.5409836065573771,0.7857142857142857,The performance of our models is still someway off from MultiTask but they use a training dataset which is two orders of magnitude larger with a complex set of learning objectives ,32,"It is interesting to note that both models perform comparably on an average , although SuBiLSTM has twice as many param - eters as SuBiLSTM - Tied .",Thoughts ( Logeswaran and Lee 2018 ) also uses a much larger unsupervised dataset .,method
natural_language_inference,39,"where ? is the logistic sigmoid activation , W * , U * and b * are learned weight matrices and biases .",model,Context Modeling,0,56,9,9,0,model : Context Modeling,0.27184466019417475,0.5625,0.5625,where is the logistic sigmoid activation W U and b are learned weight matrices and biases ,17,"At time step t , given an input x t , previous output h t? 1 , input gate it , output gate o t and forget gate ft , LST M ( x t , h t?1 ) outputs the hidden state ht based on the equations below :","where ? is the logistic sigmoid activation , W * , U * and b * are learned weight matrices and biases .",method
question_generation,1,"To understand the progress towards multimedia vision and language understanding , a visual Turing test was proposed by that was aimed at visual question answering .",introduction,introduction,0,11,2,2,0,introduction : introduction,0.02806122448979592,0.06666666666666668,0.06666666666666668,To understand the progress towards multimedia vision and language understanding a visual Turing test was proposed by that was aimed at visual question answering ,25, ,Visual ) is a natural extension for VQA .,introduction
semantic_role_labeling,2,All 8 layer ablations suffer a loss of more than 1.7 in absolute F 1 compared to the full model .,result,Results,1,110,10,10,0,result : Results,0.4910714285714286,0.5882352941176471,1.0,All 8 layer ablations suffer a loss of more than 1 7 in absolute F 1 compared to the full model ,22,"Orthonormal parameter initialization is surprisingly important - without this , the model achieves only 65 F1 within the first 50 epochs .", ,result
question_answering,3,The results reported are the best performing models on the heldout set .,method,Our Methods,0,209,34,14,0,method : Our Methods,0.7464285714285714,1.0,1.0,The results reported are the best performing models on the heldout set ,13,We tune the dimensionality of the DCU cell within a range of 100 ? 300 in denominations of 50 ., ,method
sentiment_analysis,32,This demonstrates the effectiveness of separately modeling the targets via LSTM networks and interactive attention .,analysis,Analysis of IAN Model,0,188,28,28,0,analysis : Analysis of IAN Model,0.8173913043478261,0.5957446808510638,1.0,This demonstrates the effectiveness of separately modeling the targets via LSTM networks and interactive attention ,16,"Conversely , average / max pooling , used in other methods , usu - ally lose more information in modeling long targets compared with shorter target .", ,result
relation_extraction,6,We use the complete English Wikipedia corpus to generate training and evaluation data .,system description,Data generation with Wikidata,0,49,4,4,0,system description : Data generation with Wikidata,0.3769230769230769,0.1176470588235294,0.2222222222222222,We use the complete English Wikipedia corpus to generate training and evaluation data ,14,. It contains more than 28 million entities and 160 million re - .,Wikipedia and Wikidata are tightly integrated which enables us to employ manual wiki annotations to extract high quality data .,method
part-of-speech_tagging,5,Impact of the Sentence - based Character Model,ablation,Impact of the Sentence-based Character Model,0,172,14,1,0,ablation : Impact of the Sentence-based Character Model,0.8514851485148515,0.3783783783783784,0.04166666666666666,Impact of the Sentence based Character Model,7, , ,result
relation_extraction,2,The pre-trained BERT model is a multi - layer bidirectional Transformer encoder .,methodology,Pre-trained Model BERT,0,49,3,2,0,methodology : Pre-trained Model BERT,0.362962962962963,0.25,0.18181818181818185,The pre trained BERT model is a multi layer bidirectional Transformer encoder ,13, ,The design of input representation of BERT is to be able to represent both a single text sentence and a pair of text sentences in one token sequence .,method
sentiment_analysis,2,"In PBAN , the position information is regarded as the inputs of the Bi - GRU , so it can help calculate the weights of different words in aspect term and improve the final representation of the sentence .",dataset,Datasets,0,158,27,27,0,dataset : Datasets,0.6960352422907489,0.9310344827586208,0.9310344827586208,In PBAN the position information is regarded as the inputs of the Bi GRU so it can help calculate the weights of different words in aspect term and improve the final representation of the sentence ,36,"As we have mentioned in previous sections , an aspect term contains several words and different words in aspect term should have different contributions to the final representation of sentence .","Moreover , when different aspect terms contain the same word , our proposed position information can effectively identify the current aspect term without confusion while MemNet can not .",experiment
question_answering,1,"Both of these attentions , which will be discussed below , are derived from a shared similarity matrix , S ? R T J , between the contextual embeddings of the context ( H ) and the query ( U ) , where S tj indicates the similarity between t- th context word and j- th query word .",model,Attention Flow,0,51,19,10,0,model : Attention Flow,0.1608832807570978,0.18811881188118806,0.14492753623188406,Both of these attentions which will be discussed below are derived from a shared similarity matrix S R T J between the contextual embeddings of the context H and the query U where S tj indicates the similarity between t th context word and j th query word ,49,"In this layer , we compute attentions in two directions : from context to query as well as from query to context .",The similarity matrix is computed by,method
named-entity-recognition,2,"We believe the decrease in the Bi - LSTM - CRF model occurs because of the nature of the OntoNotes dataset compared to contains a particularly high proportion of ambiguous entities , 7 perhaps leading to more benefit from document context that helps with dis ambiguation .",model,Model,0,208,12,12,0,model : Model,0.976525821596244,0.9230769230769232,0.9230769230769232,We believe the decrease in the Bi LSTM CRF model occurs because of the nature of the OntoNotes dataset compared to contains a particularly high proportion of ambiguous entities 7 perhaps leading to more benefit from document context that helps with dis ambiguation ,44,"For the first time , we see the score decrease when more context is added to the Bi - LSTM - CRF model , though the ID - CNN , whose sentence model a lower score than that of the Bi - LSTM - CRF , sees an increase .","In this scenario , adding the wider context may just add noise to the high - scoring Bi - LSTM - CRF model , whereas the less accurate dilated model can still benefit from the refined predictions of the iterated dilated convolutions .",method
natural_language_inference,44,The decoder module obtains the scores for start and end position of the answer span by calculating bilinear similarities between document encodings and question encodings as follows .,model,model,0,213,9,9,0,model : model,0.7447552447552448,0.8181818181818182,0.8181818181818182,The decoder module obtains the scores for start and end position of the answer span by calculating bilinear similarities between document encodings and question encodings as follows ,28,"It first takes the document and the question as inputs , obtains document embeddings D ? R L d h d , question embeddings Q ? R Lqh d and question - aware document embeddings D q ? R L d h d , where D q is defined as Equation 1 , and finally obtains document encodings D enc and question encodings Q enc as Equation 3 .","The over all architecture is similar to Document Reader in DrQA ( Chen et al. , 2017 ) , except they are different in obtaining embeddings and use different hyperparameters .",method
natural_language_inference,40,"Suppose we have S sequences X 1 , . . . , XS .",method,Multiple Sequences,0,137,60,7,0,method : Multiple Sequences,0.4945848375451263,0.9230769230769232,0.5833333333333334,Suppose we have S sequences X 1 XS ,9,"Also , we would like to preserve the order of the original sequences in the decomposed DAGs .","One way to do this is as follows : for each permutation of the collection of sequences ( X k1 , X k2 , . . . , X k S ) , treat it as a single long sequence and decompose into forward and backward subgraphs as described in Section 3.1 .",method
text_summarization,14,"Hence , it can not guarantee the semantic correctness of the summary as a whole .",introduction,introduction,0,16,5,5,0,introduction : introduction,0.07017543859649122,0.2083333333333333,0.2083333333333333,Hence it can not guarantee the semantic correctness of the summary as a whole ,15,"Despite substantial improvements on this task , most of the existing researches typically aim to improve word overlap between the generated summary and the references , which is measured by n-gram matching metrics ( e.g. , ROUGE ) .","Therefore , in some cases , the summary giving high matching scores may contain critical error messages , which makes the summary fail to capture the correct information with respect to the source sentence .",introduction
natural_language_inference,25,LM - augmented token re-embedding ( TR + LM ),system description,LM-augmented token re-embedding (TR+LM),0,35,17,1,0,system description : LM-augmented token re-embedding (TR+LM),0.3365384615384616,0.6296296296296297,0.09090909090909093,LM augmented token re embedding TR LM ,8, , ,method
question_similarity,0,"We now discuss our model structure , which is shown in .",model,Model Structure,0,50,2,2,0,model : Model Structure,0.3623188405797101,0.0625,0.5,We now discuss our model structure which is shown in ,11, ,"As the figure shows , the model structure can be divided into the following components / layers : input layer , sequence representation extraction layer , merging layer and decision layer .",method
text_summarization,6,Gigawords is an English sentence summarization dataset prepared based on Annotated Gigawords 1 by extracting the first sentence from articles with the headline to form a sourcesummary pair .,experiment,Datesets,0,169,4,3,0,experiment : Datesets,0.6450381679389313,0.2222222222222222,0.17647058823529413,Gigawords is an English sentence summarization dataset prepared based on Annotated Gigawords 1 by extracting the first sentence from articles with the headline to form a sourcesummary pair ,29,We train and evaluate our framework on three popular datasets .,We directly download the prepared dataset used in .,experiment
sentiment_analysis,2,IAN : IAN considers the separate modeling of aspect terms and sentences respectively .,dataset,Datasets,1,140,9,9,0,dataset : Datasets,0.6167400881057269,0.3103448275862069,0.3103448275862069,IAN IAN considers the separate modeling of aspect terms and sentences respectively ,13,The other design of ATAE - LSTM is the same as AE - LSTM .,"IAN is able to interactively learn attentions in the contexts and aspect terms , and generates the representations for aspect terms and contexts separately .",experiment
sentiment_analysis,49,A gated multi-modal embedded LSTM with temporal attention ( GME - LSTM ( A ) ) for word - level fusion of multi-modality inputs .,analysis,Comparative Analysis,0,201,36,3,0,analysis : Comparative Analysis,0.7944664031620553,0.5142857142857142,0.17647058823529413,A gated multi modal embedded LSTM with temporal attention GME LSTM A for word level fusion of multi modality inputs ,21,For MOSI datasets we compare the performance of our proposed approach with the the following state - of - the - art systems : i ) . - LSTM - based sequence model to capture the contextual information of the utterances ; ii ) . - Tensor level fusion technique for combining all the three modalities ; iii ) .,and iv ) . - Multiple attention blocks for capturing the information across the three modalities .,result
sentiment_analysis,40,"For example , in "" I bought a mobile phone , it s camera is wonderful but the battery life is short "" , there are three opinion targets , "" camera "" , "" battery life "" , and "" mobile phone "" .",introduction,introduction,0,12,3,3,0,introduction : introduction,0.05381165919282512,0.08823529411764706,0.08823529411764706,For example in I bought a mobile phone it s camera is wonderful but the battery life is short there are three opinion targets camera battery life and mobile phone ,31,"The goal of aspect sentiment analysis is to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .","The reviewer has a positive sentiment on the "" camera "" , a negative sentiment on the * Corresponding author .",introduction
relation-classification,1,"Then , based on our tagging scheme , we study different end - toend models to extract entities and their relations directly , without identifying entities and relations separately .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.024390243902439025,0.6666666666666666,0.6666666666666666,Then based on our tagging scheme we study different end toend models to extract entities and their relations directly without identifying entities and relations separately ,26,"To tackle this problem , we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem .",We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods .,abstract
natural_language_inference,19,"However , it is a very deep structured LSTM model with 140.2 m parameters .",result,MultiNLI Results,0,192,20,8,0,result : MultiNLI Results,0.7529411764705882,0.24096385542168675,0.7272727272727273,However it is a very deep structured LSTM model with 140 2 m parameters ,15,showed the best performance with 74.5 % accuracy in Matched Test .,"In our model , the inference layer is simply composed of 1 layer of 300D in order to focus on the training of sentence encoder .",result
natural_language_inference,30,"On WikiAnswers , users can tag pairs of questions as rephrasing of each other .",training,Questions Generation,0,110,36,25,0,training : Questions Generation,0.4263565891472868,0.8,0.8064516129032258,On WikiAnswers users can tag pairs of questions as rephrasing of each other ,14,"To overcome this issue , we again follow and supplement our training data with an indirect supervision signal made of pairs of question paraphrases collected from the WikiAnswers website .","harvested a set of 18 M of these question - paraphrase pairs , with 2.4 M distinct questions in the corpus .",experiment
natural_language_inference,40,"Sequential data appears in many real world applications involving natural language , videos , speech and financial markets .",introduction,introduction,0,12,2,2,0,introduction : introduction,0.04332129963898917,0.05128205128205128,0.05128205128205128,Sequential data appears in many real world applications involving natural language videos speech and financial markets ,17, ,Predictions involving such data require accurate modeling of dependencies between elements of the sequence which maybe arbitrarily far apart .,introduction
text-classification,7,"Formally , each corresponding vote can be computed by :",model,Child-Parent Relationships,0,80,42,9,0,model : Child-Parent Relationships,0.3292181069958848,0.44680851063829785,0.75,Formally each corresponding vote can be computed by ,9,"The first one shares weights W t 1 ? RN dd across child capsules in the layer below , where N is the number of parent capsules in the layer above .",where u i is a child - capsule in the layer below and b j|i is the capsule bias term .,method
sentiment_analysis,48,This implementation guarantees that the improvement does not come from learning to generate .,ablation,Effect of Sharing Embeddings,0,214,12,6,0,ablation : Effect of Sharing Embeddings,0.9067796610169492,0.48,0.6666666666666666,This implementation guarantees that the improvement does not come from learning to generate ,14,"In the aforementioned experiments , the embedding layer is not shared between the classifier and autoencoder .","To verify if sharing embedding will benefit , we also conducted experiments with sharing embedding , as illustrated in 4 .",result
machine-translation,1,"Similarly , the Path T column corresponds to the length of the shortest path between an input target token and any output target token .",model,model,0,133,22,22,0,model : model,0.6616915422885572,0.6666666666666666,0.6666666666666666,Similarly the Path T column corresponds to the length of the shortest path between an input target token and any output target token ,24,The Path S column corresponds to the length in layer steps of the shortest path between a source token and any output target token .,Shorter paths lead to better forward and backward signal propagation .,method
sentiment_analysis,6,Characteristic of the Datasets,dataset,Characteristic of the Datasets,0,225,1,1,0,dataset : Characteristic of the Datasets,0.7785467128027682,0.1111111111111111,0.1111111111111111,Characteristic of the Datasets,4, , ,experiment
natural_language_inference,91,": The thin - stack algorithm operating on the input sequence x = ( Spot , sat , down ) and the transition sequence shown in the rightmost column .",implementation,Implementation issues,0,117,23,23,0,implementation : Implementation issues,0.5021459227467812,0.4107142857142857,0.6216216216216216, The thin stack algorithm operating on the input sequence x Spot sat down and the transition sequence shown in the rightmost column ,24,down 1 2 3 reduce 4 ( sat down ) 1 4 reduce 5 ( Spot ( sat down ) ),[t ] shows the top of the stack at each step t.,experiment
machine-translation,9,"Let ? ? R |V | H be the original embedding matrix , where each embedding vector has H dimensions .",system description,CODE LEARNING WITH GUMBEL-SOFTMAX,0,125,29,2,0,system description : CODE LEARNING WITH GUMBEL-SOFTMAX,0.4355400696864111,0.4833333333333333,0.060606060606060615,Let R V H be the original embedding matrix where each embedding vector has H dimensions ,17, ,"Let ? ? R |V | H be the original embedding matrix , where each embedding vector has H dimensions .",method
relation-classification,2,"where the superscript ( r ) is used for the notation of the relation task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) , is the hidden size of the LSTM , b is the size of the label embeddings and l the layer width .",model,Relation extraction as multi-head selection,0,149,44,13,0,model : Relation extraction as multi-head selection,0.5050847457627119,0.676923076923077,0.5416666666666666,where the superscript r is used for the notation of the relation task f is an elementwise activation function i e relu tanh is the hidden size of the LSTM b is the size of the label embeddings and l the layer width ,44,We calculate the score between tokens w i and w j given a label r k as follows :,"where the superscript ( r ) is used for the notation of the relation task , f ( ) is an elementwise activation function ( i.e. , relu , tanh ) , is the hidden size of the LSTM , b is the size of the label embeddings and l the layer width .",method
natural_language_inference,95,"We adopt the official evaluation metrics , including ROUGE - L and .",implementation,Implementation Details,0,157,14,14,0,implementation : Implementation Details,0.6709401709401709,0.8235294117647058,0.875,We adopt the official evaluation metrics including ROUGE L and ,11,shows the results of our system and other state - of - the - art models on the MS - MARCO test set .,"As we can see , for both metrics , our single model outperforms all the other competing models with an evident margin , which is extremely hard considering the near - human per - formance .",experiment
natural_language_inference,83,Maria smelled the fresh Autumn air and decided to celebrate .,result,Pam,0,259,44,3,0,result : Pam,0.8464052287581699,0.8301886792452831,0.25,Maria smelled the fresh Autumn air and decided to celebrate ,11,Pam was upset at herself .,She wanted to make candy apples .,result
machine-translation,3,"Because of the large model size , we use strong L 2 regularization to constrain the parameter matrix v in the following way :",model,Optimization,0,217,17,6,0,model : Optimization,0.6932907348242812,0.53125,0.2857142857142857,Because of the large model size we use strong L 2 regularization to constrain the parameter matrix v in the following way ,23,We refer all the parameters not used for recurrent computation as non-recurrent part of the model .,"Here r is the regularization strength , l is the corresponding learning rate , g stands for the gradients of v.",method
natural_language_inference,46,Natural language understanding seeks to create models that read and comprehend text .,introduction,introduction,0,12,2,2,0,introduction : introduction,0.04040404040404042,0.3333333333333333,0.3333333333333333,Natural language understanding seeks to create models that read and comprehend text ,13, ,"common strategy for assessing the language understanding capabilities of comprehension models is to demonstrate that they can answer questions about documents they read , akin to how reading comprehension is tested in children when they are learning to read .",introduction
natural_language_inference,30,Any advance on this difficult topic would bring a huge leap forward in building new ways of accessing knowledge .,introduction,introduction,0,13,3,3,0,introduction : introduction,0.050387596899224806,0.0967741935483871,0.0967741935483871,Any advance on this difficult topic would bring a huge leap forward in building new ways of accessing knowledge ,20,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .","An important development in this are a has been the creation of large - scale Knowledge Bases ( KBs ) , such as Freebase and DBpedia which store huge amounts of general - purpose information .",introduction
text_summarization,13,"By keeping an encoded representation of each part of the input , we "" attend "" to the relevant part each time we produce an output from the decoder .",introduction,introduction,0,11,4,4,0,introduction : introduction,0.041198501872659166,0.2352941176470588,0.2352941176470588,By keeping an encoded representation of each part of the input we attend to the relevant part each time we produce an output from the decoder ,27,popular variant of sequence - to - sequence models are attention models .,"In practice , this means computing attention weights for all encoder hidden states , then taking the weighted average as our new context vector .",introduction
semantic_role_labeling,4,"main difference is that while they model P ( r|i , j ) , we model P (i , j|r ) .",model,model,0,278,7,7,0,model : model,0.9266666666666666,0.4375,0.4375,main difference is that while they model P r i j we model P i j r ,18,They also used BiLSTMs to induce span representations in an end - to - end fashion .,"In other words , while their model seeks to select an appropriate label for each span ( label selection ) , our model seeks to select appropriate spans for each label ( span selection ) .",method
relation_extraction,8,"In relation extraction , an input sentence can be divided into three segments based on the two selected entities .",methodology,Piecewise Max Pooling,0,148,64,11,0,methodology : Piecewise Max Pooling,0.5501858736059481,0.6213592233009708,0.55,In relation extraction an input sentence can be divided into three segments based on the two selected entities ,19,"In addition , single max pooling is not sufficient to capture the structural information between two entities .","Therefore , we propose a piecewise max pooling procedure that returns the maximum value in each segment instead of a single maximum value .",method
relation-classification,4,"As shown in , the performance of all three models peaks when K = 1 , outperforming their respective dependency path - based counterpart ( K = 0 ) .",experiment,Effect of Path-centric Pruning,1,171,36,4,0,experiment : Effect of Path-centric Pruning,0.6526717557251909,0.8571428571428571,0.4444444444444444,As shown in the performance of all three models peaks when K 1 outperforming their respective dependency path based counterpart K 0 ,23,"We experimented with K ? { 0 , 1 , 2 , ?} on the TACRED dev set , and also include results when the full tree is used .",This confirms our hypothesis in Section 3 that incorporating off - path information is crucial to relation extraction .,experiment
named-entity-recognition,0,"For the same task of N = 13000 , it costs our method 1.8 minutes , achieving a 68429 times acceleration compared with the speed of Nie 's method .",system description,Accelerated Robust Subset Selection (ARSS),0,104,82,6,0,system description : Accelerated Robust Subset Selection (ARSS),0.3837638376383764,0.4606741573033708,0.05882352941176471,For the same task of N 13000 it costs our method 1 8 minutes achieving a 68429 times acceleration compared with the speed of Nie s method ,28,"Although the resulted objective is challenging to solve , a speedup algorithm is proposed to dramatically save the computational costs .","To boost the robustness against outliers in both samples and features , we formulate the discrepancy between X and XA via the p ( 0 < p < 1 ) - norm .",method
sentiment_analysis,16,Coupled Interaction ( CI ) :,approach,approach,0,167,45,45,0,approach : approach,0.5251572327044025,0.5487804878048781,0.5487804878048781,Coupled Interaction CI ,4,We thus will not repeat similar discussions .,This proposed technique associates the TCS interaction with an additional set of context representation .,method
natural_language_inference,68,"To verify this hypothesis , we analysed the performance in terms of both exact match and F 1 score with respect to the answer length on the development set .",result,FURTHER ANALYSES,0,206,21,4,0,result : FURTHER ANALYSES,0.8273092369477911,0.525,0.17391304347826084,To verify this hypothesis we analysed the performance in terms of both exact match and F 1 score with respect to the answer length on the development set ,29,"First , we suspect that longer answers are harder to predict .","For example , for questions whose answers contain more than 9 tokens , the F 1 score of the boundary model drops to around 55 % and the exact match score drops to only around 30 % , compared to the F 1 score and exact match score of close to 72 % and 67 % , respectively , for questions with single - token answers .",result
natural_language_inference,80,"RNNs are the natural solution for variable length sequence modeling , consequently , we utilize a bidirectional LSTM ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ) for encoding the given sentences .",model,model,0,63,7,7,1,model : model,0.2172413793103448,0.109375,0.109375,RNNs are the natural solution for variable length sequence modeling consequently we utilize a bidirectional LSTM BiLSTM Hochreiter and Schmidhuber 1997 for encoding the given sentences ,27,The task is to predict a label y that indicates the logical relationship between premise u and hypothesis v.,"For ease of presentation , we only describe how we encode u depending on v. The same procedure is utilized for the reverse direction ( v|u ) .",method
machine-translation,4,This indicates that it deserves more efforts to investigate the temporal order information in self - attention mechanism .,ablation,Ablation study,0,223,9,9,0,ablation : Ablation study,0.9330543933054394,0.75,0.75,This indicates that it deserves more efforts to investigate the temporal order information in self attention mechanism ,18,"When we remove the directional self - attention , we getup to - 0.3 BLEU points decline .",The GANs also significantly improve the translation performance of our system .,result
natural_language_inference,18,"According to , we can see that the questions starting with ' how ' have more white are as , which indicates higher variances or more uncertainties are in these dimensions .",experiment,Q1 how old was sue lyon when she made lolita,0,201,75,10,0,experiment : Q1 how old was sue lyon when she made lolita,0.7362637362637363,0.9375,0.6666666666666666,According to we can see that the questions starting with how have more white are as which indicates higher variances or more uncertainties are in these dimensions ,28,All the log standard deviations are initialised as zero before training .,"By contrast , the questions starting with ' what ' have black squares in almost every dimension .",experiment
natural_language_inference,56,Some of the tasks we evaluate on can be efficiently and perfectly solved by hand - crafted algorithms that operate on the symbolic level .,introduction,introduction,0,33,20,20,0,introduction : introduction,0.09821428571428573,0.7142857142857143,0.7142857142857143,Some of the tasks we evaluate on can be efficiently and perfectly solved by hand crafted algorithms that operate on the symbolic level ,24,This paper chiefly develops the relational reasoning side of that interface .,"For example , 9 - by - 9 Sudokus can be solved in a fraction of a second with constraint propagation and search or with dancing links .",introduction
question-answering,4,"The word - by - word perspective focuses on similarity matches between individual words from hypothesis and text , at various scales .",introduction,introduction,1,30,19,19,0,introduction : introduction,0.10309278350515463,0.5135135135135135,0.5135135135135135,The word by word perspective focuses on similarity matches between individual words from hypothesis and text at various scales ,20,"The semantic perspective compares the hypothesis to sentences in the text viewed as single , self - contained thoughts ; these are represented using a sum and transformation of word embedding vectors , similarly to in .","As in the semantic perspective , we consider matches over complete sentences .",introduction
sentiment_analysis,6,Contextual Unimodal and Multimodal Classification,method,Contextual Unimodal and Multimodal Classification,0,66,8,1,0,method : Contextual Unimodal and Multimodal Classification,0.2283737024221453,0.12121212121212124,0.3333333333333333,Contextual Unimodal and Multimodal Classification,5, , ,method
text-to-speech_synthesis,1,"In order to train the duration predictor , we extract the ground - truth phoneme duration from an autoregressive teacher TTS model , as shown in .",system description,Duration Predictor,0,96,35,7,0,system description : Duration Predictor,0.4383561643835616,0.7777777777777778,0.4117647058823529,In order to train the duration predictor we extract the ground truth phoneme duration from an autoregressive teacher TTS model as shown in ,24,"Note that the trained duration predictor is only used in the TTS inference phase , because we can directly use the phoneme duration extracted from an autoregressive teacher model in training ( see following discussions ) .",We describe the detailed steps as follows :,method
text_summarization,10,the hoops have written the sfa for an ' understanding ' of the decision .: john hartson was once on the end of a major hampden injustice while playing for celtic .,abstract,abstract,0,243,24,24,0,abstract : abstract,0.9239543726235742,0.5853658536585366,0.5853658536585366,the hoops have written the sfa for an understanding of the decision john hartson was once on the end of a major hampden injustice while playing for celtic ,29,"however , no action was taken against offender josh meekings .",but he can not see any point in his old club writing to the scottish football association over the latest controversy at the national stadium .,abstract
sentiment_analysis,8,"Humans are able to resolve ambiguity in most cases because we can efficiently comprehend information from multiple domains ( henceforth , referred to as modalities ) , namely , speech , text and visual .",introduction,introduction,0,15,4,4,0,introduction : introduction,0.0641025641025641,0.26666666666666666,0.26666666666666666,Humans are able to resolve ambiguity in most cases because we can efficiently comprehend information from multiple domains henceforth referred to as modalities namely speech text and visual ,29,"For instance , the phrase "" This is awesome "" could be said under either happy or sad settings .","With the rise of deep learning algorithms , there have been multiple attempts to tackle the task of Speech Emotion Recognition ( SER ) as in [ 2 ] and .",introduction
natural_language_inference,23,The results of various attention functions on SQuAD development set are shown in .,result,Multiplicative attention:,0,251,23,4,0,result : Multiplicative attention:,0.4892787524366472,0.36507936507936506,0.3076923076923077,The results of various attention functions on SQuAD development set are shown in ,14,"We consider the activation function f ( x ) to be max ( 0 , x ) .",It is clear that the symmetric form consistently outperforms all alternatives .,result
sentiment_analysis,23,few too long or too short sentences are grouped together for smoothing ; the numbers of sentences in each group vary from 126 :,model,The Effect of Sentence Lengths,0,253,17,4,0,model : The Effect of Sentence Lengths,0.8664383561643836,0.34,0.25,few too long or too short sentences are grouped together for smoothing the numbers of sentences in each group vary from 126 ,23,"Sentences are split into 7 groups by length , with granularity 5 .",Visualizing how features ( after convolution ) are related to the sentiment of a sentence .,method
natural_language_inference,30,"with f ( ) a function mapping words from questions into Contrary to previous work modeling KBs with embeddings ( e.g. ) , in our model , an entity does not have the same embedding when appearing in the lefthand or in the right - hand side of a triple .",architecture,architecture,0,125,6,6,0,architecture : architecture,0.4844961240310077,0.0967741935483871,0.17647058823529413,with f a function mapping words from questions into Contrary to previous work modeling KBs with embeddings e g in our model an entity does not have the same embedding when appearing in the lefthand or in the right hand side of a triple ,45,The scoring function is then :,"Since , g ( ) sums embeddings of all constituents of a triple , we need to use 2 embeddings per entity to encode for the fact that relationships in the KB are not symmetric and so that appearing as a left - hand or right - hand entity is different .",method
sentiment_analysis,39,"In this example , more than one restaurant are discussed and restaurants for which opinions are expressed , are explicitly mentioned .",introduction,introduction,0,47,33,33,0,introduction : introduction,0.19183673469387755,0.8048780487804879,0.8048780487804879,In this example more than one restaurant are discussed and restaurants for which opinions are expressed are explicitly mentioned ,20,"The design of the space is good in Boqueria but the service is horrid , on the other hand , the staff in Gremio are very friendly and the food is always delicious . """,We call these target entities .,introduction
sentiment_analysis,10,"Now , when a party speaks , we update his / her state q i with context ct which contains relevant information on all the preceding utterances , rendering explicit listener state update unnecessary .",model,Our Model,0,107,47,47,0,model : Our Model,0.4163424124513619,0.7966101694915254,0.8392857142857143,Now when a party speaks we update his her state q i with context ct which contains relevant information on all the preceding utterances rendering explicit listener state update unnecessary ,31,"In other words , a silent party has no influence in a conversation .",This is shown in .,method
natural_language_inference,83,"Event - sequence , Sentimenttrajectory , and Topical Consistency .",system description,Measuring Consistency,0,64,9,3,0,system description : Measuring Consistency,0.20915032679738568,0.16363636363636366,0.13636363636363635,Event sequence Sentimenttrajectory and Topical Consistency ,7,Our approach analyzes the following aspects of story understanding :,"For a story , or any piece of text , to be coherent , it needs to describe a meaningful or ' mutually entailing ' sequence of events .",method
text-classification,3,The embedding layer was initialized using 300 - dimensional CBOW Word2vec embeddings trained on the 3B - word UMBC WebBase corpus with standard hyperparameters,experiment,Experimental setup,1,68,7,7,0,experiment : Experimental setup,0.5528455284552846,0.13725490196078433,0.5384615384615384,The embedding layer was initialized using 300 dimensional CBOW Word2vec embeddings trained on the 3B word UMBC WebBase corpus with standard hyperparameters,22,"These models were used for both topic categorization and polarity detection tasks , with slight hyperparameter variations given their different natures ( mainly in their text size ) which were fixed across all datasets .","For the topic categorization task we used the BBC news dataset 5 , 20 News , Reuters 6 and The code for this CNN implementation is the same as in , which is available at https://github.com/pilehvar/sensecnn",experiment
sentiment_analysis,10,"We tried two listener state update mechanisms : Simply keep the state of the listener unchanged , that is",model,Our Model,0,100,40,40,0,model : Our Model,0.3891050583657588,0.6779661016949152,0.7142857142857143,We tried two listener state update mechanisms Simply keep the state of the listener unchanged that is,17,Listener Update : Listener state models the listeners ' change of state due to the speaker 's utterance .,"Employ another GRU cell GRU L to update the listener state based on listener visual cues ( facial expression ) v i , t and it s context ct , as",method
relation_extraction,3,"On this single relation task , the out - of - box BERT achieves a reasonable result after finetuning .",method,Method,0,128,5,5,0,method : Method,0.9343065693430656,0.5555555555555556,0.5555555555555556,On this single relation task the out of box BERT achieves a reasonable result after finetuning ,17,"From the results in , our proposed techniques also outperforms the state - of - the - art on this single - relation benchmark .","Adding the entity - aware attention gives about 8 % improvement , due to the availability of the entity information during encoding .",method
natural_language_inference,1,This section provides an extensive evaluation of our MemNNs implementation against state - of the - art QA methods as well as an empirical study of the impact of using multiple training sources on the prediction performance .,experiment,Experiments,0,216,2,2,0,experiment : Experiments,0.7912087912087912,0.6666666666666666,0.6666666666666666,This section provides an extensive evaluation of our MemNNs implementation against state of the art QA methods as well as an empirical study of the impact of using multiple training sources on the prediction performance ,36, ,"details the dimensions of the test sets of WebQuestions , SimpleQuestions and Reverb which we used for evaluation .",experiment
natural_language_inference,27,"For the self - attention - layer , we adopt the multi-head attention mechanism defined in which , for each position in the input , called the query , computes a weighted sum of all positions , or keys , in the input based on the similarity between the query and key as measured by the dot product .",model,MODEL OVERVIEW,0,88,44,36,0,model : MODEL OVERVIEW,0.2603550295857988,0.3308270676691729,0.631578947368421,For the self attention layer we adopt the multi head attention mechanism defined in which for each position in the input called the query computes a weighted sum of all positions or keys in the input based on the similarity between the query and key as measured by the dot product ,52,"The kernel size is 7 , the number of filters is d = 128 and the number of conv layers within a block is 4 .",The number of heads is 8 throughout all the layers .,method
natural_language_inference,68,Here we adopt Ptr- Net in order to construct answers using tokens from the input text .,method,MATCH-LSTM,0,70,20,16,0,method : MATCH-LSTM,0.2811244979919679,0.17543859649122806,1.0,Here we adopt Ptr Net in order to construct answers using tokens from the input text ,17,The pointer mechanism has inspired some recent work on language processing ., ,method
natural_language_inference,51,"Following practice , we also remove temporal linkage for faster training .",Details on Few-shot Learning Task,Details on bAbI Task,0,260,6,4,0,Details on Few-shot Learning Task : Details on bAbI Task,0.981132075471698,0.5454545454545454,0.4444444444444444,Following practice we also remove temporal linkage for faster training ,11,The batch size is 32 and we adopt layer normalization ( Lei to DNC 's layers .,The details of hyper - parameters are listed in .,others
temporal_information_extraction,1,"As discussed in existing work , the structure of a temporal graph is constrained by some rather simple rules :",introduction,introduction,0,26,17,17,0,introduction : introduction,0.10116731517509728,0.3863636363636364,0.3863636363636364,As discussed in existing work the structure of a temporal graph is constrained by some rather simple rules ,19,"No one was hurt , but firefighters ordered the evacuation of nearby homes and said they 'll monitor the shifting ground .. . .",must be after A.,introduction
sentiment_analysis,4,The input to this network consists of pre-trained word embeddings extracted from the 300 - dimensional FastText embeddings .,methodology,Textual Features,0,121,16,5,0,methodology : Textual Features,0.3711656441717792,0.16326530612244894,0.5555555555555556,The input to this network consists of pre trained word embeddings extracted from the 300 dimensional FastText embeddings ,19,"For our purpose , we utilize a simple CNN with a single convolutional layer followed by max - pooling .","The convolution layer consists of three filters with sizes f 1 t , f 2 t , f 3 t with f out feature maps each .",method
relation_extraction,8,Effect of piecewise max pooling and multi-instance learning .,evaluation,Manual Evaluation,0,260,24,24,0,evaluation : Manual Evaluation,0.966542750929368,0.8571428571428571,0.8571428571428571,Effect of piecewise max pooling and multi instance learning ,10,These results demonstrate that the proposed piecewise max pooling technique is beneficial and :,can effectively capture structural information for relation extraction .,result
sentiment_analysis,39,"In the following , we argue that this task is both very relevant in practice , and raises interesting modelling questions .",introduction,introduction,0,52,38,38,0,introduction : introduction,0.21224489795918366,0.9268292682926828,0.9268292682926828,In the following we argue that this task is both very relevant in practice and raises interesting modelling questions ,20,Targeted aspect - based sentiment analysis handles extracting the target entities as well as different aspects and their relevant sentiments .,To facilitate research on this task we introduce the SentiHood dataset .,introduction
sentiment_analysis,1,"The majority of existing classifiers are topology - invariant classifiers such as SVM , k - Nearest Neighbors ( KNN ) , DBNs and RNNs , which do not take the topological structure of EEG features into account when learning the EEG representations .",system description,EEG-Based Emotion Recognition,0,80,8,8,0,system description : EEG-Based Emotion Recognition,0.202020202020202,0.03864734299516908,0.5333333333333333,The majority of existing classifiers are topology invariant classifiers such as SVM k Nearest Neighbors KNN DBNs and RNNs which do not take the topological structure of EEG features into account when learning the EEG representations ,37,EEG classifiers can be broadly divided into topologyinvariant classifiers and topology - aware ones .,"In contrast , topology - aware classifiers such as CNNs , , , and GNNs consider the inter-channel topological relations and learn EEG representations for each channel by aggregating features from nearby channels using convolutional operations either in the Euclidean space or in the non-Euclidean space .",method
natural_language_inference,37,shows some qualitative examples of this phenomenon .,method,Confidence Method,0,91,8,8,0,method : Confidence Method,0.3540856031128405,0.14814814814814814,0.4705882352941176,shows some qualitative examples of this phenomenon ,8,Our experiments in Section 5 show that in practice these models can be very poor at providing good confidence scores .,We hypothesize that there are two key reasons a model 's confidence scores might not be well calibrated .,method
paraphrase_generation,1,"In this section , we first provide a brief overview of VAE , and then describe our framework .",methodology,Methodology,0,46,5,5,0,methodology : Methodology,0.2081447963800905,0.1851851851851852,1.0,In this section we first provide a brief overview of VAE and then describe our framework ,17,This enables us to generate paraphrase ( s ) specific to an input sentence at test time ., ,method
sentiment_analysis,29,"Following the linear layer , we use a softmax layer to compute the probability of the sentence s with sentiment polarity c ? C towards an aspect a as :",method,Final Classification,0,106,45,5,0,method : Final Classification,0.6091954022988506,0.8490566037735849,0.38461538461538464,Following the linear layer we use a softmax layer to compute the probability of the sentence s with sentiment polarity c C towards an aspect a as ,28,where W land bl are the weight matrix and bias respectively .,"Following the linear layer , we use a softmax layer to compute the probability of the sentence s with sentiment polarity c ? C towards an aspect a as :",method
question_answering,4,We initialize the word embeddings with 300D Glo Ve embeddings and are fixed during training .,experiment,Experimental Setup,1,186,10,10,0,experiment : Experimental Setup,0.7237354085603113,0.7142857142857143,0.7142857142857143,We initialize the word embeddings with 300D Glo Ve embeddings and are fixed during training ,16,We apply variational dropout in - between RNN layers .,The size of the character embeddings is set to 8 and the character RNN is set to the same as the word - level RNN encoders .,experiment
sarcasm_detection,1,"We further compare our proposed user -profiling method with that of CUE - CNN , with absolute differences shown in the bottom row of .",model,Baseline Models,0,278,23,23,0,model : Baseline Models,0.8323353293413174,0.8846153846153846,0.8846153846153846,We further compare our proposed user profiling method with that of CUE CNN with absolute differences shown in the bottom row of ,23,Its improved performance on the Main imbalanced dataset also reflects its robustness towards class imbalance and establishes it as a real - world deployable network .,"Since CUE - CNN generates its user embeddings using a method similar to the ParagraphVector , we test the importance of personality features being included in our user profiling .",method
natural_language_inference,100,Most existing text matching models do not explicitly model question focus .,introduction,introduction,0,45,35,35,0,introduction : introduction,0.12295081967213115,0.8333333333333334,0.8333333333333334,Most existing text matching models do not explicitly model question focus ,12,"For example , given a question like "" Where was the first burger king restaurant opened ? "" , it is critical for the answer to talk about "" burger "" , "" king "" , "" open "" , etc .","For example , models based on CNNs treat all the question terms as equally important when matching to answer terms .",introduction
natural_language_inference,1,"To this end , we introduce a new dataset of 100 k questions that we use in conjunction with existing benchmarks .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.02197802197802198,0.8,0.8,To this end we introduce a new dataset of 100 k questions that we use in conjunction with existing benchmarks ,21,"This paper studies the impact of multitask and transfer learning for simple question answering ; a setting for which the reasoning required to answer is quite easy , as long as one can retrieve the correct evidence given a question , which can be difficult in large - scale conditions .","We conduct our study within the framework of Memory Networks ( Weston et al. , 2015 ) because this perspective allows us to eventually scale up to more complex reasoning , and show that Memory Networks can be successfully trained to achieve excellent performance .",abstract
part-of-speech_tagging,7,"In order to evaluate the effect of modeling sub - token information , we examine accuracy rates at different frequency rates .",result,Results,0,91,21,21,0,result : Results,0.728,0.525,0.8076923076923077,In order to evaluate the effect of modeling sub token information we examine accuracy rates at different frequency rates ,20,"Finally , the bi - LSTM tagger is competitive on WSJ , cf .","shows absolute improvements in accuracy of bi - LSTM w + cover mean log frequency , for different language families .",result
natural_language_inference,0,"N-gram statistics , for instance , computed over the entire corpus are no longer useful in such an anonymized corpus .",dataset,Datasets,0,116,6,6,0,dataset : Datasets,0.5888324873096447,0.5454545454545454,0.5454545454545454,N gram statistics for instance computed over the entire corpus are no longer useful in such an anonymized corpus ,20,"Further , entities within each article were anonymized to make the task purely a comprehension one .",The next two datasets are formed from two different subsets of the Children 's Book Test ( CBT ),experiment
machine-translation,7,"To reduce computation , we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,324,102,5,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8686327077747991,0.6754966887417219,0.15625,To reduce computation we decrease the number of LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively ,25,Our model is a modified version of the GNMT model described in .,We insert MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,others
sentiment_analysis,26,"Given rows representing discrete words , we rely on weight matrices W ? R hd with region size h.",approach,Dual-Module Memory based CNNs,0,69,48,10,0,approach : Dual-Module Memory based CNNs,0.2816326530612245,0.5714285714285714,0.21739130434782608,Given rows representing discrete words we rely on weight matrices W R hd with region size h ,18,We perform convolutional operations on these matrices via linear filters .,"Given rows representing discrete words , we rely on weight matrices W ? R hd with region size h.",method
text_summarization,12,"where W a , U a , W r , Ur , V rand W o are weight matrices .",model,Summary Decoder,0,121,42,10,0,model : Summary Decoder,0.5307017543859649,0.8571428571428571,0.9090909090909092,where W a U a W r Ur V rand W o are weight matrices ,16,The readout state is then passed through a maxout hidden layer to predict the next word with a softmax layer over the decoder vocabulary .,"Readout state rt is a 2 d - dimensional vector , and the maxout layer ( Equation 16 ) picks the max value for every two numbers in rt and produces a d-dimensional vector mt .",method
sentence_compression,2,We use the Dundee Corpus as our eye -tracking corpus with tokenization and measures similar to the Dundee Treebank .,experiment,Gaze data,0,71,3,2,0,experiment : Gaze data,0.7029702970297029,0.2,0.25,We use the Dundee Corpus as our eye tracking corpus with tokenization and measures similar to the Dundee Treebank ,20, ,The corpus contains eye - tracking recordings often native English - speaking subjects reading 20 newspaper articles from The Independent .,experiment
natural_language_inference,62,"That is to say , when only a subset of the training examples are available , KAR outperforms the state - of the - art MRC models by a large margin , and is still reasonably robust to noise .",experiment,Model Comparison in the Hunger for Data,0,210,38,8,0,experiment : Model Comparison in the Hunger for Data,0.9375,1.0,1.0,That is to say when only a subset of the training examples are available KAR outperforms the state of the art MRC models by a large margin and is still reasonably robust to noise ,35,"As shown in and , with the above KAR , SAN , and QANet trained on the same training subsets , we also evaluate their performance on the adversarial sets , and still find that KAR performs much better than SAN and QANet .", ,experiment
natural_language_inference,83,Example from the story - cloze task : predict the correct ending to a given short story out of provided options .,introduction,introduction,0,24,14,14,0,introduction : introduction,0.0784313725490196,0.3111111111111111,0.3111111111111111,Example from the story cloze task predict the correct ending to a given short story out of provided options ,20,Correct Ending : He was scolded . :,"ing such as identifying character personas , interpersonal relationships , plotpatterns , narrative structures ) .",introduction
part-of-speech_tagging,3,"For sequence labeling ( or general structured prediction ) tasks , it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence .",architecture,CRF,0,61,30,2,0,architecture : CRF,0.30049261083743845,0.6122448979591837,0.14285714285714285,For sequence labeling or general structured prediction tasks it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence ,33, ,"For example , in POS tagging an adjective is more likely to be followed by a noun than a verb , and in NER with standard BIO2 annotation ( Tjong Kim Sang and Veenstra , 1999 ) I - ORG can not follow I - PER .",method
text-classification,5,We are interested in a model that performs robustly across a diverse set of tasks .,hyperparameters,Hyperparameters,0,160,2,2,0,hyperparameters : Hyperparameters,0.6349206349206349,0.2222222222222222,0.2222222222222222,We are interested in a model that performs robustly across a diverse set of tasks ,16, ,"To this end , if not mentioned otherwise , we use the same set of hyperparameters across tasks , which we tune on the IMDb validation set .",experiment
natural_language_inference,54,"The syntactic collection C ( p ) for dependency tree is defined as p 's children , each represented by its word embedding concatenated with a vec-tor that uniquely identifies the dependency label .",system description,Structural Embedding of Dependency Trees (SEDT),0,89,34,6,0,system description : Structural Embedding of Dependency Trees (SEDT),0.3886462882096071,0.5964912280701754,0.5454545454545454,The syntactic collection C p for dependency tree is defined as p s children each represented by its word embedding concatenated with a vec tor that uniquely identifies the dependency label ,32,"For example , in the dependency tree plotted in , the link from "" unit "" to "" Conference "" indicates that the target node is a nominal subject ( i.e. NSUBJ ) of the source node .",The processing order A ( p ) for dependency tree is then defined to be the dependent 's original order in the sentence .,method
part-of-speech_tagging,2,"For Spanish and Dutch , we use the 64 - dimensional Polyglot embeddings .",performance,COMPARISON WITH STATE-OF-THE-ART RESULTS,1,162,31,6,0,performance : COMPARISON WITH STATE-OF-THE-ART RESULTS,0.9101123595505618,0.7560975609756098,0.375,For Spanish and Dutch we use the 64 dimensional Polyglot embeddings ,12,"On the English datasets , following previous works that are based on neural networks , we experiment with both the 50 - dimensional SENNA embeddings and the 100 - dimensional GloVe embeddings and use the development set to choose the embeddings for different tasks and settings .",We set the hidden state dimensions to be 300 for the word - level GRU .,result
relation_extraction,8,"The lower sentence , however , does not express this relation but is still selected as a training instance .",introduction,introduction,0,26,13,13,0,introduction : introduction,0.0966542750929368,0.2888888888888889,0.2888888888888889,The lower sentence however does not express this relation but is still selected as a training instance ,18,"For instance , the upper sentence indeed expresses the "" company / founders "" relation in .",This will hinder the performance of a model trained on such noisy data .,introduction
natural_language_inference,58,We choose GRU model as the internal state controller .,training,CNN and Daily Mail Datasets,0,161,30,26,0,training : CNN and Daily Mail Datasets,0.4805970149253732,0.38461538461538464,0.4642857142857143,We choose GRU model as the internal state controller ,10,"W doc 1 , W doc 2 ) ; Internal State Controller :",The number of hidden units in the GRU state controller is 256 for CNN and 384 for Daily Mail .,experiment
natural_language_inference,66,"Im- portant matching results will be "" remembered "" by the LSTM while non-essential ones will be "" forgotten . """,model,Our Model,0,117,25,25,0,model : Our Model,0.41935483870967744,0.8620689655172413,0.8620689655172413,Im portant matching results will be remembered by the LSTM while non essential ones will be forgotten ,18,This LSTM models the matching between the premise and the hypothesis .,"We use the concatenation of a k , which is the attention - weighted version of the premise for the k th word in the hypothesis , and ht k , the hidden state for the k th word itself , as input to the mLSTM .",method
sentence_classification,1,The RNN encoder in the sentence encoding layer is set as LSTM for the PubMed datasets and gated recurrent unit ( GRU ) for the NICTA - PIBOSO dataset .,training,Training Settings,0,122,15,15,0,training : Training Settings,0.7052023121387283,0.9375,0.9375,The RNN encoder in the sentence encoding layer is set as LSTM for the PubMed datasets and gated recurrent unit GRU for the NICTA PIBOSO dataset ,27,"The window sizes of the CNN encoder in the sentence encoding layer are 2 , 3 , 4 and 5 .",Code for this work is available online 9 .,experiment
part-of-speech_tagging,2,Code is available at https://github.com/kimiyoung/transfer 1 ar Xiv:1703.06345v1 [ cs.CL ],abstract,abstract,1,11,9,9,0,abstract : abstract,0.06179775280898876,1.0,1.0,Code is available at https github com kimiyoung transfer 1 ar Xiv 1703 06345v1 cs CL ,17,These improvements lead to improvements over the current state - of the - art on several well - studied tasks ., ,abstract
natural_language_inference,67,"During test , we test on the full length of passage , so that we do n't prune out the potential candidates .",implementation,implementation,0,146,12,12,0,implementation : implementation,0.7156862745098039,0.5454545454545454,0.5454545454545454,During test we test on the full length of passage so that we do n t prune out the potential candidates ,22,This step reduced the training set size by about 1.6 % .,"We trained the model for at most 30 epochs , and in case the accuracy did not improve for 10 epochs , we stopped training .",experiment
natural_language_inference,1,Memory Network consists of a memory ( an indexed array of objects ) and a neural network that is trained to query it given some inputs ( usually questions ) .,system description,Memory Networks for Simple QA,0,80,2,2,0,system description : Memory Networks for Simple QA,0.29304029304029305,0.025,0.4,Memory Network consists of a memory an indexed array of objects and a neural network that is trained to query it given some inputs usually questions ,27, ,"It has four components : Input map ( I ) , Generalization ( G ) , Output map ( O ) and Response ( R ) which we detail below .",method
sentiment_analysis,44,"In contrast to traditional syntactic and semantic parsing , Discourse Parsing can generate structures that cover not only a single sentence but also multi-sentential text .",introduction,introduction,0,30,19,19,0,introduction : introduction,0.13636363636363635,0.6129032258064516,0.6129032258064516,In contrast to traditional syntactic and semantic parsing Discourse Parsing can generate structures that cover not only a single sentence but also multi sentential text ,26,"The Sentiment annotation ( over Discourse Tree structure ) of a sentence from Sentiment Treebank dataset contained in the resulting Discourse Tree can benefit many other NLP tasks including but not restricted to automatic summarization ( e.g. , , , ) , machine translation ( e.g. , , ) and question answering ( e.g. , ) .","However , the focus of this paper is on sentence level Discourse Parsing , leaving the study of extensions to multi-sentential text as future work .",introduction
sentiment_analysis,50,"Following previous work , we remove samples with conflicting polarities in all datasets",dataset,Datasets and Experimental Settings,0,79,3,3,0,dataset : Datasets and Experimental Settings,0.4876543209876543,0.13636363636363635,0.13636363636363635,Following previous work we remove samples with conflicting polarities in all datasets,12,"We run experiments on four benchmark aspectlevel datasets , taken from SemEval 2014 , SemEval 2015 , and SemEval 2016 .",Statistics of the resulting datasets are presented in .,experiment
text_generation,0,"For simplicity , we use the standard LSTM as the generator , while it is worth noticing that most of the RNN variants , such as the gated recurrent unit ( GRU ) ) and soft attention mechanism , can be used as a generator in SeqGAN .",implementation,Model Implementations,0,281,12,12,0,implementation : Model Implementations,0.8672839506172839,0.24489795918367346,0.32432432432432434,For simplicity we use the standard LSTM as the generator while it is worth noticing that most of the RNN variants such as the gated recurrent unit GRU and soft attention mechanism can be used as a generator in SeqGAN ,41,"where [ h , x ] is the vector concatenation and is the elementwise product .","The standard way of training an RNN G ? is the maximum likelihood estimation ( MLE ) , which involves minimizing the negative log - likelihood ? T t=1 log G ? ( y t = x t | {x 1 , . . . , x t?1 }) for a generated sequence ( y 1 , . . . , y T ) given input ( x 1 , . . . , x T ) .",experiment
natural_language_inference,46,"We additionally collected a second reference answer for each question by asking annotators to judge whether a question is answerable , given the summary , and provide an answer if it was .",method,method,0,123,23,23,0,method : method,0.4141414141414141,0.46,0.46,We additionally collected a second reference answer for each question by asking annotators to judge whether a question is answerable given the summary and provide an answer if it was ,31,Answer selection and multiple - choice question answering are frequently used .,All but 2.3 % of the questions were judged as answerable .,method
text_summarization,6,We only reserve those pairs with scores no less than 3 .,experiment,Datesets,0,181,16,15,0,experiment : Datesets,0.6908396946564885,0.8888888888888888,0.8823529411764706,We only reserve those pairs with scores no less than 3 ,12,There is a score in range 1 ? 5 labeled by human to indicate how relevant an article and its summary is .,"The size of the three sets are 2.4 M , 8.7 k , and 725 respectively .",experiment
natural_language_inference,88,"Following Section 3.2 , C and H denote the target memory tape and hidden tape , which store representations of the target symbols that have been processed so far .",system description,Deep Attention Fusion Deep fusion combines,0,128,56,5,0,system description : Deep Attention Fusion Deep fusion combines,0.5182186234817814,0.8484848484848485,0.3333333333333333,Following Section 3 2 C and H denote the target memory tape and hidden tape which store representations of the target symbols that have been processed so far ,29,We use different notation to represent the two sets of attention .,The computation of intra-attention follows Equations ( 4 ) - ( 9 ) .,method
natural_language_inference,27,"Despite their success , these models are often slow for both training and inference due to the sequential nature of RNNs .",abstract,abstract,0,6,3,3,0,abstract : abstract,0.017751479289940832,0.3333333333333333,0.3333333333333333,Despite their success these models are often slow for both training and inference due to the sequential nature of RNNs ,21,Current end - to - end machine reading and question answering ( Q&A ) models are primarily based on recurrent neural networks ( RNNs ) with attention .,"We propose a new Q&A architecture called QANet , which does not require recurrent networks :",abstract
text-classification,7,Bd be the filter shared in different sliding windows .,model,Primary Capsule Layer,0,63,25,6,0,model : Primary Capsule Layer,0.25925925925925924,0.26595744680851063,0.75,Bd be the filter shared in different sliding windows ,10,Let W b ? R,"For each matrix multiplication , we have a window sliding over each Ngram vector denoted as M i ? R B , then the corresponding N - gram phrases in the form of capsule are produced with",method
natural_language_inference,5,This task has been extensively investigated by researchers because it is a fundamental task that can be applied to other QA - related tasks .,introduction,introduction,0,16,8,8,0,introduction : introduction,0.10884353741496597,0.3478260869565217,0.3478260869565217,This task has been extensively investigated by researchers because it is a fundamental task that can be applied to other QA related tasks ,24,"In particular , our model is designed to undertake an answer-selection task that chooses the sentence that is most relevant to the question from a list of answer candidates .","However , most previous answer-selection studies have employed small datasets compared with the large datasets employed for other natural language processing ( NLP ) tasks .",introduction
sentiment_analysis,8,We see from the confusion matrix ( ) that our text - based models are able to distinguish the six emotions fairly well along with the end - to - end trained TRE .,result,RESULTS,0,210,14,14,0,result : RESULTS,0.8974358974358975,0.4666666666666667,0.6086956521739131,We see from the confusion matrix that our text based models are able to distinguish the six emotions fairly well along with the end to end trained TRE ,29,This could be attributed to the richness of TFIDF vectors known to capture word - sentence correlation .,"We observe that "" sad "" is the toughest for textual features to identify very clearly .",result
question_answering,4,The DECAENC accepts the inputs P and Q from the input encoder .,system description,Densely Connected Attention Encoder (DECAENC),0,98,55,2,0,system description : Densely Connected Attention Encoder (DECAENC),0.3813229571984436,0.5288461538461539,0.10526315789473684,The DECAENC accepts the inputs P and Q from the input encoder ,13, ,DECAENC is a multi - layered encoder with k layers .,method
sentiment_analysis,31,The key step of PG - CNN is described in equation .,system description,Parameterized Gates,0,90,46,6,0,system description : Parameterized Gates,0.5660377358490566,0.8214285714285714,0.5454545454545454,The key step of PG CNN is described in equation ,11,"Similar with PF - CNN , PG - CNN also utilizes a convolutional neural network to extract feature ? t from aspect terms , which instead is used as agate in the CNN applied on the sentence .","Instead of using a non-linear function fin equation ( 1 ) , we use agate ?(? t v i:i+h?1 + b) to control how much information passing to the next layer , where ?( ) is sigmoid function .",method
sentiment_analysis,34,We decided to keep only the tweets with confidence level over 50 % and ignore the rest .,experiment,Experimental Setup,0,106,10,10,0,experiment : Experimental Setup,0.7162162162162162,0.8333333333333334,0.8333333333333334,We decided to keep only the tweets with confidence level over 50 and ignore the rest ,17,"In addition , Ar - SAS has a confidence value for each label .","After this step , we end up with 17,784 tweets in the ArSAS dataset labelled with three sentiment labels .",experiment
relation-classification,2,This means that relations of other pairs of entities in the same sentence - which could be helpful in deciding on the relation type for a particular pair - are not taken into account .,introduction,introduction,0,24,14,14,0,introduction : introduction,0.08135593220338982,0.4375,0.4375,This means that relations of other pairs of entities in the same sentence which could be helpful in deciding on the relation type for a particular pair are not taken into account ,33,"Note that the aforementioned works examine pairs of entities for relation extraction , rather than modeling the whole sentence directly .","propose a neural joint model based on LSTMs where they model the whole sentence at once , but still they do not have a principled way to deal with multiple relations .",introduction
natural_language_inference,95,shows an example from MS - MARCO .,introduction,introduction,0,19,11,11,0,introduction : introduction,0.0811965811965812,0.1896551724137931,0.1896551724137931,shows an example from MS MARCO ,7,"One of the intrinsic challenges for such multipassage MRC is that since all the passages are question - related but usually independently written , it 's probable that multiple confusing answer candidates ( correct or incorrect ) exist .",We can see that all the answer candidates have semantic matching with the question while they are literally different and some of them are even incorrect .,introduction
natural_language_inference,73,"After going through the entire sequence , it receives a loss value from the classification problem , which can be regarded as the negative delay reward to train the agent .",training,Model Training,0,157,15,15,0,training : Model Training,0.5992366412213741,0.3947368421052632,0.3947368421052632,After going through the entire sequence it receives a loss value from the classification problem which can be regarded as the negative delay reward to train the agent ,29,"In particular , RSS plays as an agent and takes action of whether to select a token or not .","Since the over all goal of RSS is to select a small subset of tokens for better efficiency and meanwhile retain useful information , a penalty limiting the number of selected tokens is included in the reward R , i.e. , where ? is the penalty weight and is fine - tuned with values from { 0.005 , 0.01 , 0.02 } in all experiments .",experiment
semantic_role_labeling,3,They used simplified input and output layers compared with .,model,Labeling Confusion,0,244,51,9,0,model : Labeling Confusion,0.9242424242424242,0.7611940298507462,0.36,They used simplified input and output layers compared with ,10,improved further with highway LSTMs and constrained decoding .,also proposed a bidirectional LSTM based model .,method
text-to-speech_synthesis,2,"Results are shown in , comparing the proposed model to baseline multispeaker models that utilize a lookup table of speaker embeddings similar to , but otherwise have identical architectures to the proposed synthesizer network .",experiment,experiment,0,122,31,31,0,experiment : experiment,0.4979591836734694,0.2980769230769231,0.2980769230769231,Results are shown in comparing the proposed model to baseline multispeaker models that utilize a lookup table of speaker embeddings similar to but otherwise have identical architectures to the proposed synthesizer network ,33,"Each sample was rated by a single rater , and each evaluation was conducted independently : the outputs of different models were not compared directly .","The proposed model achieved about 4.0 MOS in all datasets , with the VCTK model obtaining a MOS about 0.2 points higher than the LibriSpeech model when evaluated on seen speakers .",experiment
sentiment_analysis,42,"On both datasets , the proposed approach could obtain comparable accuracy compared to the state - of - art feature - based SVM system .",method,Comparison to Other Methods,1,181,26,26,0,method : Comparison to Other Methods,0.7182539682539683,1.0,1.0,On both datasets the proposed approach could obtain comparable accuracy compared to the state of art feature based SVM system ,21,"The best performances are achieved when the model contains seven and nine hops , respectively .", ,method
machine-translation,6,"In deep learning approaches for NLP tasks , word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters .",introduction,introduction,0,13,4,4,0,introduction : introduction,0.044673539518900345,0.11428571428571427,0.11428571428571427,In deep learning approaches for NLP tasks word embeddings act as the inputs of the neural network and are usually trained together with neural network parameters ,27,"Different from classic one - hot representation , the learned word embeddings contain semantic information which can measure the semantic similarity between words , and can also be transferred into other learning tasks .","As the inputs of the neural network , word embeddings carryall the information of words that will be further processed by the network , and the quality of embeddings is critical and highly impacts the final performance of the learning task .",introduction
text_summarization,0,"After each decoding step , a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect .",introduction,introduction,1,55,40,40,0,introduction : introduction,0.1827242524916944,0.7843137254901961,0.7843137254901961,After each decoding step a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect ,23,"Then , we treat the decoder attention weights as the focused aspect of the generated summary , a.k.a. , "" decoder focused aspect "" .","Given this distance , a goal tracker provides the goal to the decoder to induce it to reduce this distance .",introduction
natural_language_inference,21,We initialize the word embedding in x by 300D Glo Ve 6B pre-trained vectors .,experiment,Experiments,1,185,11,11,0,experiment : Experiments,0.6379310344827587,0.215686274509804,0.55,We initialize the word embedding in x by 300D Glo Ve 6B pre trained vectors ,16,"All weight matrices are initialized by Glorot Initialization , and the biases are initialized with 0 .","The Out - of - Vocabulary words in training set are randomly initialized by uniform distribution between ( ? 0.05 , 0.05 ) .",experiment
natural_language_inference,96,We ensemble the models by concatenating their final representations and passing it through an MLP .,system description,Stylistic models for adversarial filtering,0,152,94,12,0,system description : Stylistic models for adversarial filtering,0.38974358974358975,0.7833333333333333,0.4615384615384616,We ensemble the models by concatenating their final representations and passing it through an MLP ,16,. A bidirectional LSTM over the 100 most common words in the second sentence ; uncommon words are replaced by their POS tags .,"On every adversarial iteration , the ensemble is trained jointly to minimize cross -entropy .",method
sentiment_analysis,24,is the cross - entropy loss applied to each token .,method,Learning,0,134,79,6,0,method : Learning,0.4308681672025724,0.5895522388059702,0.09836065573770493,is the cross entropy loss applied to each token ,10,"where T denotes the maximum number of iterations in the message passing mechanism , Na denotes the total number of aspect - level training instances , n i denotes the number of tokens contained in the ith training instance , and y ae i , j ( y as i , j ) denotes the one - hot encoding of the gold label for AE ( AS ) .","In aspect - level datasets , only aspect terms have sentiment annotations .",method
named-entity-recognition,8,"However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .",ablation,ablation,0,386,25,25,0,ablation : ablation,0.9974160206718348,0.9615384615384616,0.9615384615384616,However as expected using only the MASK strategy was problematic when applying the featurebased approach to NER ,18,From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies .,"Interestingly , using only the RND strategy performs much worse than our strategy as well .",result
text-to-speech_synthesis,2,"We trained a new eval - only speaker encoder with the same network topology as Section 2.1 , but using a different training set of 28 M utterances from 113K speakers .",experiment,experiment,0,154,63,63,0,experiment : experiment,0.6285714285714286,0.6057692307692307,0.6057692307692307,We trained a new eval only speaker encoder with the same network topology as Section 2 1 but using a different training set of 28 M utterances from 113K speakers ,31,"As an objective metric of the degree of speaker similarity between synthesized and ground truth audio for unseen speakers , we evaluated the ability of a limited speaker verification system to distinguish synthetic from real speech .",Using a different model for evaluation ensured that metrics were not only valid on a specific speaker embedding space .,experiment
sentiment_analysis,27,"In our model , we employ two Bi - LSTM separately to get the sentence contextual hidden output",methodology,Bidirectional Long Short-Term Memory (Bi-LSTM),0,103,26,7,0,methodology : Bidirectional Long Short-Term Memory (Bi-LSTM),0.37318840579710144,0.3058823529411765,0.875,In our model we employ two Bi LSTM separately to get the sentence contextual hidden output,16,We concatenate both the forward and the backward hidden state to form the final representation :,"Note that , the Bi - LSTM for each different aspect shares the parameters .",method
sentiment_analysis,42,This phenomenon might be caused by the neglect of location information .,model,Visualize Attention Models,0,215,9,9,0,model : Visualize Attention Models,0.8531746031746031,0.6923076923076923,0.6923076923076923,This phenomenon might be caused by the neglect of location information ,12,"As a result , the model incorrectly predicts the polarity towards "" food "" as negative .","From ( b ) , we can find that the weight of "" great "" is increased when the location of context word is considered .",method
part-of-speech_tagging,7,We want to compare POS taggers under varying conditions .,system description,Tagging with bi-LSTMs,0,60,32,32,0,system description : Tagging with bi-LSTMs,0.48,0.9142857142857144,0.9142857142857144,We want to compare POS taggers under varying conditions ,10,The code is released at : https : //github.com/bplank/bilstm-aux,We hence use three different types of taggers : our implementation of a bi -LSTM ; TNT - a second order HMM with suffix trie handling for OOVs .,method
part-of-speech_tagging,3,"The embedding we used is Stanford 's Glo Ve with dimension 100 , the same as Section 4.2 . illustrates the performance of our model on different subsets of words , together with the baseline LSTM - CNN model for comparison .",analysis,analysis,0,175,8,8,0,analysis : analysis,0.8620689655172413,0.8,0.8,The embedding we used is Stanford s Glo Ve with dimension 100 the same as Section 4 2 illustrates the performance of our model on different subsets of words together with the baseline LSTM CNN model for comparison ,39,informs the statistics of the partition on each corpus .,The largest improvements appear on the OOBV subsets of both the two corpora .,result
natural_language_inference,27,The speedup gain makes our model the most promising candidate for scaling up to larger datasets .,introduction,introduction,0,42,30,30,0,introduction : introduction,0.1242603550295858,0.9375,0.9375,The speedup gain makes our model the most promising candidate for scaling up to larger datasets ,17,"This combination maintains good accuracy , while achieving up to 13x speedup in training and 9x per training iteration , compared to the RNN counterparts .","To improve our result on SQuAD , we propose a novel data augmentation technique to enrich the training data by paraphrasing .",introduction
sentiment_analysis,47,TD - LSTM obtains an improvement of 1 - 2 % over LSTM when target signals are taken into consideration .,performance,System Performance Comparison,1,162,10,10,0,performance : System Performance Comparison,0.7534883720930232,0.1724137931034483,0.5882352941176471,TD LSTM obtains an improvement of 1 2 over LSTM when target signals are taken into consideration ,18,"Among LSTM based neural networks described in this paper , the basic LSTM approach performs the worst .","Because of the introduction of attention mechanism , AE - LSTM and ATAE - LSTM achieve better results than TD - LSTM , and ATAE - LSTM is slightly better among the two .",result
natural_language_inference,36,"In the last decade , the MRC tasks have evolved from the early cloze - style test to spanbased answer extraction from passage .",system description,Text Comprehension,0,42,6,6,0,system description : Text Comprehension,0.2,0.09375,0.16666666666666666,In the last decade the MRC tasks have evolved from the early cloze style test to spanbased answer extraction from passage ,22,"Textual entailment aims for a deep understanding of text and reasoning , which shares the similar genre of machine reading comprehension , though the task formations are slightly different .",The former has restrictions that each answer should be a single word in the document and the original sentence without the answer part is taken as the query .,method
text_summarization,11,"Moreover , the analysis shows : Structure of our proposed Convolutional Gated Unit .",introduction,introduction,0,27,20,20,0,introduction : introduction,0.1875,0.9090909090909092,0.9090909090909092,Moreover the analysis shows Structure of our proposed Convolutional Gated Unit ,12,"We conduct experiments on LCSTS and Gigaword , two benchmark datasets for sentence summarization , which shows that our model outperforms the state - of - theart methods with ROUGE - 2 F1 score 26.8 and 17.8 respectively .","We implement 1 - dimensional convolution with a structure similar to the Inception over the outputs of the RNN encoder , where k refers to the kernel size .",introduction
natural_language_inference,30,Using the scoring function S ( ) allows to directly query the KB without needing to define an intermediate structured logical representation for questions as in semantic parsing systems .,system description,Task Definition,0,72,12,9,0,system description : Task Definition,0.2790697674418605,0.8571428571428571,0.8181818181818182,Using the scoring function S allows to directly query the KB without needing to define an intermediate structured logical representation for questions as in semantic parsing systems ,28,"To handle multiple answer , we instead present the results as a ranked list , rather than taking the top prediction , and evaluate that instead .","We aim at learning S ( ) , with no human - labeled supervised data in the form ( question , answer ) pairs , but only by indirect supervision , generated either automatically or collaboratively .",method
sentence_classification,1,"However , even today , a significant portion of scientific abstracts is still unstructured , which causes great difficulty in information retrieval .",introduction,introduction,0,14,7,7,0,introduction : introduction,0.08092485549132948,0.25,0.25,However even today a significant portion of scientific abstracts is still unstructured which causes great difficulty in information retrieval ,20,"This process can be expedited if the abstracts are structured ; that is , if the rhetorical structural elements of scientific abstracts such as purpose , methods , results , and conclusions ) are explicitly stated .","In this paper , we develop a machine - learning based approach to automatically categorize sentences in scientific abstracts into rhetorical sections so that the desired information can be efficiently retrieved .",introduction
question_similarity,0,"Then , it applies sequence weighted attention 6 proposed by on the outputs of the second ON - LSTM layer to get the final question representation .",model,Sequence Representation Extractor,0,61,13,3,0,model : Sequence Representation Extractor,0.4420289855072464,0.40625,0.2,Then it applies sequence weighted attention 6 proposed by on the outputs of the second ON LSTM layer to get the final question representation ,25,"This component takes the ELMo embeddings related to each word in the question as an input and feeds them into two a special kind of bidirectional LSTM layers called Ordered Neurons LSTM ( ON - LSTM ) 5 introduced in with 256 hidden units , 20 % dropout rate , and 8 as the chunk size for each of them .",This component uses the same weights to compute representations for each question in the pair .,method
topic_models,0,Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the data better as compared to the state - of - the - art neural variational document model on ( Fisher ) speech and ( 20 Newsgroups ) text corpora .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.021844660194174758,0.35,0.35,Our intrinsic evaluation using perplexity measure shows that the proposed Bayesian SMM fits the data better as compared to the state of the art neural variational document model on Fisher speech and 20 Newsgroups text corpora ,37,We also present a generative Gaussian linear classifier for topic identification that exploits the uncertainty in document embeddings .,Our topic identification experiments show that the proposed systems are robust to over-fitting on unseen test data .,abstract
sentiment_analysis,31,"With h s k such filters and bias terms , we can get a feature matrix ? t ? R hsk , where h sis the filter width applied on sentences .",system description,Parameterized Filters,0,78,34,9,0,system description : Parameterized Filters,0.490566037735849,0.6071428571428571,0.6,With h s k such filters and bias terms we can get a feature matrix t R hsk where h sis the filter width applied on sentences ,28,"where wt ? R htk , b tare the convolutional filter , bias term for CNN t . ht is the width of filters applied on aspect targets .","With h s k such filters and bias terms , we can get a feature matrix ? t ? R hsk , where h sis the filter width applied on sentences .",method
natural_language_inference,39,"First , instead of using sentence modeling , we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences .",introduction,introduction,1,18,11,11,0,introduction : introduction,0.08737864077669902,0.6470588235294118,0.6470588235294118,First instead of using sentence modeling we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences ,21,Our contribution is twofold :,"This is inspired by our own intuitions of how people recognize textual similarity : given two sentences sent 1 and sent 2 , a careful reader might look for corresponding semantic units , which we operationalize in our pairwise word interaction modeling technique ( Sec. 5 ) .",introduction
sentiment_analysis,39,This representation can be a BoW or a distributional representation .,baseline,Baseline,0,159,7,7,0,baseline : Baseline,0.6489795918367347,0.2058823529411765,0.875,This representation can be a BoW or a distributional representation ,11,l is a representation of location l .,Each method that we propose here define their own specific representation fore l .,result
part-of-speech_tagging,5,"As mentioned in the previous section , the character and word - based encoding models have their own tagging loss functions , which are trained independently and joined via the meta-BiLSTM .",training,Training Schema,0,94,2,2,0,training : Training Schema,0.4653465346534654,0.10526315789473684,0.10526315789473684,As mentioned in the previous section the character and word based encoding models have their own tagging loss functions which are trained independently and joined via the meta BiLSTM ,30, ,", the loss of each model is minimized independently by separate optimizers with their own hyperparameters .",experiment
natural_language_inference,27,"When paraphrasing , we keep the question q unchanged ( to avoid accidentally changing its meaning ) and generate new triples of ( d , q , a ) such that the new document d has the new answer a in it .",model,English to French NMT,0,160,116,19,0,model : English to French NMT,0.4733727810650887,0.8721804511278195,0.5277777777777778,When paraphrasing we keep the question q unchanged to avoid accidentally changing its meaning and generate new triples of d q a such that the new document d has the new answer a in it ,36,"Remember that , each training example of SQuAD is a triple of ( d , q , a ) in which document dis a multi-sentence paragraph that has the answer a .",The procedure happens in two steps : ( i ) document paraphrasing - paraphrase d into d and ( b ) answer extraction - extract a from d that closely matches a .,method
natural_language_inference,79,"Each of these paragraphs is also known as a "" snippet "" which summarizes the content of a web page and how a web page matches the query .",model,Information Retrieval with Paragraph Vectors,0,209,24,4,0,model : Information Retrieval with Paragraph Vectors,0.7798507462686567,0.3809523809523809,0.14285714285714285,Each of these paragraphs is also known as a snippet which summarizes the content of a web page and how a web page matches the query ,27,"Here , we have a dataset of paragraphs in the first 10 results returned by a search engine given each of 1,000,000 most popular queries .","From such collection , we derive a new dataset to test vector representations of paragraphs .",method
semantic_role_labeling,0,"In this more realistic setting , where the predicate must be predicted , our model achieves state - of - the - art performance on PropBank .",introduction,introduction,0,21,13,13,0,introduction : introduction,0.2,0.9285714285714286,0.9285714285714286,In this more realistic setting where the predicate must be predicted our model achieves state of the art performance on PropBank ,22,"To the best of our knowledge , this is the first span - based SRL model that does not assume that predicates are given .","It also reinforces the strong performance of similar span embedding methods for coreference , suggesting that this style of models could be used for other span - span relation tasks , such as syntactic parsing , relation extraction , and QA - SRL .",introduction
sentiment_analysis,12,"Adopting our method , attention weights of "" cute "" in training instances with neural or negative polarity are significantly decreased .",model,model,0,198,14,14,0,model : model,0.8839285714285714,0.875,0.875,Adopting our method attention weights of cute in training instances with neural or negative polarity are significantly decreased ,19,"For the second test sentence , since the context word "" cute "" occurs in training instances mostly with positive polarity , TNet - ATT directly focuses on this word and then incorrectly predicts the sentence sentiment as positive .","Specifically , in these instances , the average weight of "" cute "" is reduced to 0.07 times of the original .",method
sentence_classification,0,"Conversely , the results on the FUTUREWORK category are the lowest .",result,Results,1,199,21,21,0,result : Results,0.7453183520599251,0.9545454545454546,0.9545454545454546,Conversely the results on the FUTUREWORK category are the lowest ,11,"For example on ACL - ARC , the results on the BACKGROUND category are the highest as this category is the most common .",This category has the fewest data points ( see distribution of the categories in ) and thus it is harder for the model to learn the optimal parameters for correct classification in this category .,result
natural_language_inference,17,"The question attention for the j - th context word is then : softmax ( E :j ) , which is used to compute an attended question vector ? j = V softmax ( E :j ) .",architecture,Alignment Architecture for MRC,0,155,112,112,0,architecture : Alignment Architecture for MRC,0.5961538461538461,0.8175182481751825,0.8615384615384616,The question attention for the j th context word is then softmax E j which is used to compute an attended question vector j V softmax E j ,29,"First , the similarity matrix E ? R nm is computed using Eq. 1 , where the multiplicative product with nonlinearity is applied as attention function :","To efficiently fuse the attentive information into the context , an heuristic fusion function , denoted as o = fusion ( x , y ) , is proposed as",method
natural_language_inference,53,We augment with our model trained on both SNLI and MultiNLI ( All NLI ) .,model,Task transfer,0,198,57,37,0,model : Task transfer,0.9519230769230768,0.9344262295081968,0.902439024390244,We augment with our model trained on both SNLI and MultiNLI All NLI ,14,"With 433K sentence pairs , MultiNLI improves upon SNLI in its coverage : it contains ten distinct genres of written and spoken English , covering most of the complexity of the language .",We observe a significant boost in performance over all compared to the model trained only on SLNI .,method
natural_language_inference,67,The result shows that ( 1 ) our chunk representations are powerful enough to differentiate even a huge amount of chunks when no constraints are applied ; and ( 2 ) the proposed POS - trie reduces the search space at the cost of a small drop in performance .,experiment,Experiments,0,163,7,7,0,experiment : Experiments,0.7990196078431373,1.0,1.0,The result shows that 1 our chunk representations are powerful enough to differentiate even a huge amount of chunks when no constraints are applied and 2 the proposed POS trie reduces the search space at the cost of a small drop in performance ,44,"Finally , combining the DCR model with the proposed POS - trie constraints yields a score similar to the one obtained using the DCR model with all possible n-gram chunks .", ,experiment
sentiment_analysis,41,"For instance , "" The appetizers are ok , but the service is slow . "" , for aspect taste , the polarity is positive while for service , the polarity is negative .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03139013452914798,0.5555555555555556,0.5555555555555556,For instance The appetizers are ok but the service is slow for aspect taste the polarity is positive while for service the polarity is negative ,26,"In this paper , we reveal that the sentiment polarity of a sentence is not only determined by the content but is also highly related to the concerned aspect .","Therefore , it is worthwhile to explore the connection between an aspect and the content of a sentence .",abstract
natural_language_inference,19,"Both in and models , the inference layer was set very complex in order to improve the MultiNLI accuracy .",result,MultiNLI Results,0,194,22,10,0,result : MultiNLI Results,0.7607843137254902,0.26506024096385544,0.9090909090909092,Both in and models the inference layer was set very complex in order to improve the MultiNLI accuracy ,19,"In our model , the inference layer is simply composed of 1 layer of 300D in order to focus on the training of sentence encoder .","Taking this into consideration , it can be seen that our Distance - based Self - Attention Network performs competitively given its simpler structure .",result
question-answering,7,"We set the batch size to 32 , the initial learning rate to 3e - 4 and l 2 regularizer strength to 1 e - 5 , and trained each model for 50 epochs .",analysis,Document Sentiment Analysis,1,209,10,10,0,analysis : Document Sentiment Analysis,0.76,0.15625,0.4761904761904762,We set the batch size to 32 the initial learning rate to 3e 4 and l 2 regularizer strength to 1 e 5 and trained each model for 50 epochs ,31,We used one - layer LSTM with 100 hidden units for the read / write modules and the pre-trained 100 - D Glove 6B vectors for this task .,The write / read neural nets and the document - level NSE / LSTM were regularized by 15 % dropouts and the softmax layer by 20 % dropouts .,result
sentiment_analysis,12,Input : D : the initial training corpus ; ? init : the initial model parameters ; ? : the entropy threshold of attention weight distribution ; K : the maximum number of training iterations ; 1 :,approach,Details of Our Approach,0,89,24,10,0,approach : Details of Our Approach,0.39732142857142855,0.3333333333333333,0.2127659574468085,Input D the initial training corpus init the initial model parameters the entropy threshold of attention weight distribution K the maximum number of training iterations 1 ,27,: Neural ASC Model Training with Automatically Mined Attention Supervision Information .,Input : D : the initial training corpus ; ? init : the initial model parameters ; ? : the entropy threshold of attention weight distribution ; K : the maximum number of training iterations ; 1 :,method
natural_language_inference,29,"In total , our dataset contains cover 469,953 products and 38 product categories .",system description,EXPERIMENTAL SETUP 5.1 Research questions,0,252,157,18,0,system description : EXPERIMENTAL SETUP 5.1 Research questions,0.6904109589041096,0.98125,0.8571428571428571,In total our dataset contains cover 469 953 products and 38 product categories ,14,We remove all QA pairs without any relevant review and split the whole dataset into training and testing set .,The average length of question is 9.03 words and ground truth answer is 10.3 words .,method
text_summarization,10,"We take our baseline and 3 way multi-task models , plus the pointer - coverage model from .",evaluation,Generalizability Results (DUC-2002),0,171,9,3,0,evaluation : Generalizability Results (DUC-2002),0.6501901140684411,0.6923076923076923,0.42857142857142855,We take our baseline and 3 way multi task models plus the pointer coverage model from ,17,"Next , we also tested our model 's generalizability / transfer skills , where we take the models trained on CNN / DailyMail and directly test them on DUC - 2002 .",We only retune the beam- size for each of these three models separately ( based on DUC - 2003 as the validation set ) .,result
natural_language_inference,62,"However , we believe that this is both unnecessary and distracting , since each passage word has nothing to do with many of the other passage words .",architecture,Knowledge Aided Self Attention,0,161,66,4,0,architecture : Knowledge Aided Self Attention,0.71875,0.8571428571428571,0.26666666666666666,However we believe that this is both unnecessary and distracting since each passage word has nothing to do with many of the other passage words ,26,"If we simply follow the self attentions of other works , then for each passage word pi , we should fuse it s coarse memory g pi ( i.e. the i - th column in G ) with the coarse memories of all the other passage words .",Thus we use the preextracted general knowledge to guarantee that the fusion of coarse memories for each passage word will only involve a precise subset of the other passage words .,method
natural_language_inference,94,"In , and 10 ( see next page ) , we demonstrate extracted commonsense examples for questions that require commonsense to reach an answer .",training,training,0,272,7,7,0,training : training,0.7101827676240209,0.059322033898305086,0.059322033898305086,In and 10 see next page we demonstrate extracted commonsense examples for questions that require commonsense to reach an answer ,21,"For both Narrative QA and WikiHop , we reached these parameters via tuning on the full , official validation set .",We bold words in the question and in the extracted commonsense in cases where the commonsense knowledge explicitly bridges gaps between implicitly connected words in the context or question .,experiment
natural_language_inference,86,"In following parts , we start with a brief definition of the MC task ( Section 2 ) , followed by the details of our MPCM model ( Section 3 ) .",introduction,introduction,0,33,20,20,0,introduction : introduction,0.19298245614035087,0.9523809523809524,0.9523809523809524,In following parts we start with a brief definition of the MC task Section 2 followed by the details of our MPCM model Section 3 ,26,Experimental result on the test set of SQuAD shows that our model achieves a competitive result on the leaderboard .,Then we evaluate our model on the SQuAD dataset ( Section 4 ) .,introduction
semantic_role_labeling,4,The pioneering neural SRL model was proposed by .,system description,system description,0,262,19,19,0,system description : system description,0.8733333333333333,0.6785714285714286,0.6785714285714286,The pioneering neural SRL model was proposed by ,9,Neural models State - of - the - art SRL models use neural networks based on the BIO tagging approach .,They use convolutional neural networks ( CNNs ) and CRFs .,method
sentiment_analysis,0,These types of data are the same as those used in the ARE and TRE cases .,model,Multimodal Dual Recurrent Encoder (MDRE),0,81,40,4,0,model : Multimodal Dual Recurrent Encoder (MDRE),0.4550561797752809,0.6557377049180327,0.3076923076923077,These types of data are the same as those used in the ARE and TRE cases ,17,"In this study , we consider multiple modalities , such as MFCC features , prosodic features and transcripts , which contain sequential audio information , statistical audio information and textual information , respectively .",The MDRE model employs two RNNs to encode data from the audio signal and textual inputs independently .,method
sentiment_analysis,40,TD - LSTM - A : We developed TD - LSTM to make it have one attention on the outputs of 3 https://github.com/svn2github/word2vec,method,Compared Methods,1,158,12,12,0,method : Compared Methods,0.7085201793721974,0.7058823529411765,0.7058823529411765,TD LSTM A We developed TD LSTM to make it have one attention on the outputs of 3 https github com svn2github word2vec,23,"We reproduce its results on the tweet dataset with our embeddings , and also run it for the other three datasets .","forward and backward LSTMs , respectively .",method
sentiment_analysis,35,"Thus , for fair comparison , we use the source code of DAN and MMD , and extend them to M - DAN and M - MMD that utilize target supervised information and have higher performances , respectively .",baseline,baseline,0,212,17,17,0,baseline : baseline,0.8548387096774194,1.0,1.0,Thus for fair comparison we use the source code of DAN and MMD and extend them to M DAN and M MMD that utilize target supervised information and have higher performances respectively ,33,The original DAN and MMD are unsupervised domain adaptation methods ., ,result
sentiment_analysis,23,"We regard sub-sentences as individual samples during training , like and .",dataset,The Task and Dataset,0,183,10,10,0,dataset : The Task and Dataset,0.6267123287671232,0.7692307692307693,0.7692307692307693,We regard sub sentences as individual samples during training like and ,12,RNNs deal with them naturally during the recursive process .,"The training set therefore has more than 150,000 entries in total .",experiment
text-to-speech_synthesis,2,The better generalization of the LibriSpeech model suggests that training the synthesizer on only 100 speakers is insufficient to enable high quality speaker transfer .,experiment,experiment,0,151,60,60,0,experiment : experiment,0.6163265306122448,0.5769230769230769,0.5769230769230769,The better generalization of the LibriSpeech model suggests that training the synthesizer on only 100 speakers is insufficient to enable high quality speaker transfer ,25,"However , the LibriSpeech model synthesized VCTK speakers with significantly higher speaker similarity than the VCTK model is able to synthesize LibriSpeech speakers .","As an objective metric of the degree of speaker similarity between synthesized and ground truth audio for unseen speakers , we evaluated the ability of a limited speaker verification system to distinguish synthetic from real speech .",experiment
relation_extraction,12,"After applying the AGGCN model over the dependency tree , we obtain hidden representations of all tokens .",system description,AGGCNs for Relation Extraction,0,140,85,2,0,system description : AGGCNs for Relation Extraction,0.425531914893617,0.8585858585858586,0.125,After applying the AGGCN model over the dependency tree we obtain hidden representations of all tokens ,17, ,"Given these representations , the goal of relation extraction is to predict a relation among entities .",method
text_summarization,2,Struct + Input concatenates structural embeddings of position i ( flattened into one vector s e i ) with the source word embedding xi and uses them as a new form of input to the encoder : .,approach,Structural info,0,100,49,8,0,approach : Structural info,0.3597122302158273,0.3740458015267176,0.5,Struct Input concatenates structural embeddings of position i flattened into one vector s e i with the source word embedding xi and uses them as a new form of input to the encoder ,34,"In this work , we compare two settings :",Structural embeddings are important complements to existing neural architectures .,method
natural_language_inference,8,"surveys published results on this task , and places our best models in the context of the current state - of - the - art results .",experiment,experiment,0,165,17,17,0,experiment : experiment,0.7819905213270142,0.8095238095238095,0.8095238095238095,surveys published results on this task and places our best models in the context of the current state of the art results ,23,"This does not apply to our models , where performance of the models increases .",The table also includes three baseline models provided in .,experiment
natural_language_inference,52,"To help with observed overfitting , we tried removing the dense layers and received a small boost to 32. 52 % P@1 ( line 4 ) .",performance,QA Performance,0,190,20,20,0,performance : QA Performance,0.7279693486590039,0.4081632653061225,0.7692307692307693,To help with observed overfitting we tried removing the dense layers and received a small boost to 32 52 P 1 line 4 ,24,"Using this implementation , the performance on the test set was 31.50 % P@1 .","The lower performance of their model , which relies exclusively on latent representations of the data , underscores the benefit of including explicit features alongside latent features in a deep - learning approach for this domain 11 .",result
natural_language_inference,96,"The generality of adversarial filtering allows it to be applied to build future datasets , ensuring that they serve as reliable benchmarks .",introduction,introduction,0,44,34,34,0,introduction : introduction,0.11282051282051282,0.9714285714285714,0.9714285714285714,The generality of adversarial filtering allows it to be applied to build future datasets ensuring that they serve as reliable benchmarks ,22,"Second , our proposed adversarial filtering methodology allows for cost-effective construction of a large - scale dataset while substantially reducing known annotation artifacts .", ,introduction
relation-classification,1,"Hence , the extracted result from this sentence is { United States e 1 , Country - President r , Trump e 2 } , which called triplet here .",introduction,introduction,0,26,18,18,0,introduction : introduction,0.10569105691056913,0.4390243902439024,0.4390243902439024,Hence the extracted result from this sentence is United States e 1 Country President r Trump e 2 which called triplet here ,23,"Entity "" Apple Inc "" has no obvious relationship with the other entities in this sentence .","In this paper , we focus on the extraction of triplets thatare composed of two entities and one relation between these two entities .",introduction
natural_language_inference,41,"In contrast , XSum is highly abstractive , and extractive models perform poorly .",experiment,ConvAI2,0,179,27,7,0,experiment : ConvAI2,0.6964980544747081,0.84375,0.5833333333333334,In contrast XSum is highly abstractive and extractive models perform poorly ,12,"Nevertheless , BART outperforms all existing work .","BART outperforms the best previous work , which leverages BERT , by roughly 6.0 points on all ROUGE metrics - representing a significant advance in performance on this problem .",experiment
relation_extraction,9,"By minimizing this pairwise loss function iteratively ( see Section 3.5 ) , ? ? ( S , y ) are encouraged to decrease , while ? ? ( S , ? ? ) increase .",model,Classification Objective,0,76,29,17,0,model : Classification Objective,0.3636363636363637,0.3295454545454545,0.8947368421052632,By minimizing this pairwise loss function iteratively see Section 3 5 S y are encouraged to decrease while S increase ,21,"Based on a distance function motived byword analogies , we minimize the gap between predicted outputs and ground - truth labels , while maximizing the distance with the selected incorrect class .","By minimizing this pairwise loss function iteratively ( see Section 3.5 ) , ? ? ( S , y ) are encouraged to decrease , while ? ? ( S , ? ? ) increase .",method
relation-classification,1,"For example , if the relation type "" Country - President "" in is "" Company - Founder "" , then there will be four entities in the given sentence with the same relation type .",method,From Tag Sequence To Extracted Results,0,94,29,8,0,method : From Tag Sequence To Extracted Results,0.3821138211382114,0.4142857142857143,0.8,For example if the relation type Country President in is Company Founder then there will be four entities in the given sentence with the same relation type ,28,"Besides , if a sentence contains two or more triplets with the same relation type , we combine every two entities into a triplet based on the nearest principle .","United States "" is closest to entity "" Trump "" and the "" Apple Inc "" is closest to "" Jobs "" , so the results will be { United States , Company - Founder , Trump } and { Apple Inc , Company - Founder , Steven Paul Jobs } .",method
sentiment_analysis,2,"We firstly get the hidden contextual representation of the aspect term by the left Bi - GRU , and get the hidden contextual representation of inputs ( i.e. , the concatenation of word embedding and position embedding ) by the right Bi - GRU structure .",model,Word Representation,0,87,38,22,0,model : Word Representation,0.3832599118942731,0.6440677966101694,0.5116279069767442,We firstly get the hidden contextual representation of the aspect term by the left Bi GRU and get the hidden contextual representation of inputs i e the concatenation of word embedding and position embedding by the right Bi GRU structure ,41,"sentence should be represented differently based on different words in aspect term , because different words may have different effects on the final representation of the sentence .","Here , we regard the position embedding as the part of the inputs , because it intuitively represents the relative distance of words in a sentence to the current aspect term as mentioned in section 2.1 .",method
natural_language_inference,31,"Since paraphrase identification is asymmetric task where two input sequences can be swapped with no effect to the label of the text pair , in hyperparameter tuning we validate between two symmet -",result,Results on Paraphrase Identification,0,158,3,3,0,result : Results on Paraphrase Identification,0.572463768115942,0.10714285714285714,1.0,Since paraphrase identification is asymmetric task where two input sequences can be swapped with no effect to the label of the text pair in hyperparameter tuning we validate between two symmet ,32,Results on Quora dataset are listed in ., ,result
sentiment_analysis,6,This level consists of a contextual LSTM network similar to Level - 1 but independent in training and computation .,architecture,Hierarchical Framework,0,173,29,7,0,architecture : Hierarchical Framework,0.5986159169550173,0.5087719298245614,0.6363636363636364,This level consists of a contextual LSTM network similar to Level 1 but independent in training and computation ,19,Individual LSTM networks are used for each modality .,"Output from each LSTM network in Level - 1 are concatenated and fed into this LSTM network , thus providing an inherent fusion scheme ( see ) .",method
text-to-speech_synthesis,1,"We can also control the break between words by adjusting the duration of the space characters in the sentence , so as to adjust part of prosody of the synthesized speech .",system description,Length Regulator,0,89,28,13,0,system description : Length Regulator,0.4063926940639269,0.6222222222222222,1.0,We can also control the break between words by adjusting the duration of the space characters in the sentence so as to adjust part of prosody of the synthesized speech ,31,"where ? is a hyperparameter to determine the length of the expanded sequence H mel , thereby controlling the voice speed .", ,method
natural_language_inference,41,Deletion appears to outperform masking on generation tasks .,result,Results,0,137,10,10,0,result : Results,0.5330739299610895,0.4,1.0,Deletion appears to outperform masking on generation tasks ,9,"The successful methods either use token deletion or masking , or self - attention masks .", ,result
semantic_parsing,2,"The SQL query over 48.3 59.4 SQLNET 61 COUNT ( President ) WHERE ( College = A ) "" , but the query over table "" College Number of Presidents "" would be "" SELECT Number of Presidents WHERE ( College = A ) "" .",analysis,Results and Analysis,0,274,23,23,0,analysis : Results and Analysis,0.9415807560137456,0.6764705882352942,0.6764705882352942,The SQL query over 48 3 59 4 SQLNET 61 COUNT President WHERE College A but the query over table College Number of Presidents would be SELECT Number of Presidents WHERE College A ,34,"Consider the question "" how many presidents are graduated from A "" .",We also examine the predicted sketches themselves in .,result
machine-translation,1,"Second , the size of the source representation should be linear in the length of the source string , i.e. it should be resolution preserving , and not have constant size .",model,Desiderata,0,67,17,6,0,model : Desiderata,0.3333333333333333,0.2786885245901639,0.12,Second the size of the source representation should be linear in the length of the source string i e it should be resolution preserving and not have constant size ,30,The use of operations that run in parallel along the sequence length can also be beneficial for reducing computation time .,This is to avoid burdening the model with an additional memorization step before translation .,method
sentence_classification,1,This error analysis suggests that one of the biggest model error sources could be from the debatable gold standard labels of the dataset .,result,Results and Discussion,0,144,21,21,0,result : Results and Discussion,0.8323699421965318,0.5,0.5121951219512195,This error analysis suggests that one of the biggest model error sources could be from the debatable gold standard labels of the dataset ,24,presents a few examples of prediction errors that are produced by our HSLN - RNN model trained on the PubMed 20 k dataset .,"For example , the sentence "" Depressive disorders are one of the leading components of the global burden of disease with a prevalence of up to 14 % in the general population . "" is indeed introducing the background of the problem ( depressive disorders ) on which this article is going to focus ; however , the gold label classifies it into the Objective category .",result
natural_language_inference,11,shows the results of our RTE experiments .,model,Model,1,147,12,12,0,model : Model,0.532608695652174,0.4615384615384616,0.4615384615384616,shows the results of our RTE experiments ,8,"We found that the second layer did not improve performance , suggesting that pooling over word / lemma occurrences in a given context between layers , is a powerful , yet simple technique .","In general , the introduction of our refinement strategy almost always helps , both with and without external knowledge .",method
sentiment_analysis,26,This risk seems more pronounced for larger - dimensional sentiment embeddings .,analysis,Results and Analysis,0,182,36,36,0,analysis : Results and Analysis,0.7428571428571429,0.4615384615384616,0.5538461538461539,This risk seems more pronounced for larger dimensional sentiment embeddings ,11,This suggests that a simple concatenation may harm the model 's ability to harness the semantic information carried by regular word vectors .,"In contrast , with our DM - MCNNs approach , the sentiment information is provided to the model in a separate memory module that makes multiple passes over this data before combining it with the regular CNN module 's signals .",result
named-entity-recognition,0,"Thus , the results depend tightly on the initialization , and they are highly unstable for large K ( i.e. the number of centers or selected samples ) .",system description,Classifiers,0,35,13,13,0,system description : Classifiers,0.12915129151291513,0.07303370786516854,0.3333333333333333,Thus the results depend tightly on the initialization and they are highly unstable for large K i e the number of centers or selected samples ,26,Both methods employ an EM - like algorithm .,"Recently , there are a few methods that assume exemplars are the samples that can best represent the whole dataset .",method
machine-translation,2,One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .,model,Why Self-Attention,0,135,94,9,0,model : Why Self-Attention,0.6026785714285714,0.8623853211009175,0.375,One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network ,27,Learning long - range dependencies is a key challenge in many sequence transduction tasks .,"The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long - range dependencies .",method
sarcasm_detection,1,Its improved performance on the Main imbalanced dataset also reflects its robustness towards class imbalance and establishes it as a real - world deployable network .,model,Baseline Models,1,277,22,22,1,model : Baseline Models,0.8293413173652695,0.8461538461538461,0.8461538461538461,Its improved performance on the Main imbalanced dataset also reflects its robustness towards class imbalance and establishes it as a real world deployable network ,25,CASCADE comfortably beats the state - of - the - art neural models CNN - SVM and CUE - CNN .,"We further compare our proposed user -profiling method with that of CUE - CNN , with absolute differences shown in the bottom row of .",method
sentence_compression,1,"In particular , we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models ( LSTMs ) to output surprisingly readable and informative compressions .",introduction,introduction,1,19,12,12,0,introduction : introduction,0.09090909090909093,0.7058823529411765,0.7058823529411765,In particular we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models LSTMs to output surprisingly readable and informative compressions ,36,"In this paper we research the following question : can a robust compression model be built which only uses tokens and has no access to syntactic or other linguistic information ? While phenomena like long - distance relations may seem to make generation of grammatically correct compressions impossible , we are going to present an evidence to the contrary .","Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings , in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges .",introduction
natural_language_inference,19,"But in their study , not considered at all was the distance between words , an important feature when learning the local dependency to help understand the context of input text .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.031372549019607836,0.4615384615384616,0.4615384615384616,But in their study not considered at all was the distance between words an important feature when learning the local dependency to help understand the context of input text ,30,It showed good performance with various data by using forward and backward directional information in a sentence .,"We propose Distance - based Self - Attention Network , which considers the word distance by using a simple distance mask in order to model the local dependency without losing the ability of modeling global dependency which attention has inherent .",abstract
question_answering,5,"Specifically , we first use softmax to re-normalize the top - 5 answer scores provided by the two strength - based rankers and the one coverage - based reranker ; we then weighted sum up the scores for the same answer and select the answer with the largest score as the final prediction .",method,EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0,143,92,63,0,method : EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0.5181159420289855,1.0,1.0,Specifically we first use softmax to re normalize the top 5 answer scores provided by the two strength based rankers and the one coverage based reranker we then weighted sum up the scores for the same answer and select the answer with the largest score as the final prediction ,50,The full re-ranker is a weighted combination of the outputs of the above different re-rankers without further training .,"We conduct experiments on three publicly available open - domain QA datasets , namely , Quasar - T , Search QA and Trivia QA .",method
natural_language_inference,56,"where pi ? [ 0 , 1 ] 2 is the position , N n ? { 0 , ... , n ? 1 } , c i ? N 8 is the color , mi ? N 8 is the marker , s ? N 16 is the marker or color of the start object , and n ? N 8 is the number of jumps .",ablation,Pretty-CLEVR experimental details,0,249,15,4,0,ablation : Pretty-CLEVR experimental details,0.7410714285714286,0.4411764705882353,0.17391304347826084,where pi 0 1 2 is the position N n 0 n 1 c i N 8 is the color mi N 8 is the marker s N 16 is the marker or color of the start object and n N 8 is the number of jumps ,48,We compute each node feature vector x i as,"where pi ? [ 0 , 1 ] 2 is the position , N n ? { 0 , ... , n ? 1 } , c i ? N 8 is the color , mi ? N 8 is the marker , s ? N 16 is the marker or color of the start object , and n ? N 8 is the number of jumps .",result
natural_language_inference,76,"The idea is to use the same model ( i.e. same structure and weights ) to attend over the premise conditioned on the hypothesis , as well as to attend over the hypothesis conditioned on the premise , by simply swapping the two sequences .",method,WORD-BY-WORD ATTENTION,0,84,56,12,0,method : WORD-BY-WORD ATTENTION,0.5753424657534246,0.9824561403508772,0.9230769230769232,The idea is to use the same model i e same structure and weights to attend over the premise conditioned on the hypothesis as well as to attend over the hypothesis conditioned on the premise by simply swapping the two sequences ,42,"Inspired by bidirectional LSTMs that read a sequence and its reverse for improved encoding , we introduce two - way attention for RTE .",This produces two sentence - pair representations that we concatenate for classification .,method
natural_language_inference,70,The primary system achieves the highest F 1 and accuracy on both tuning and test stages .,result,Subtask C Model:,1,157,35,14,0,result : Subtask C Model:,0.8971428571428571,0.660377358490566,0.4375,The primary system achieves the highest F 1 and accuracy on both tuning and test stages ,17,"Differently from what observed in the tuning stage , on the official test set the contrastive systems achieve a higher MAP and would have ranked second .","Considering these two metrics , our primary submission is over all the best model .",result
natural_language_inference,97,"In experiments , our Parallel - Hierarchical model achieves state - of - the - art accuracy on MCTest , outperforming these existing methods .",introduction,introduction,0,47,36,36,0,introduction : introduction,0.16095890410958905,0.972972972972973,0.972972972972973,In experiments our Parallel Hierarchical model achieves state of the art accuracy on MCTest outperforming these existing methods ,19,"Models designed specifically for MCTest include those of , and more recently , , and .","Below we describe related work , the mathematical details of our model , and our experiments , then analyze our results .",introduction
natural_language_inference,75,The remaining performance drop seems to be split roughly equally between conjunctions ( 74 % ) and coreference ( 76 % ) .,model,WikiMovies,0,184,135,24,0,model : WikiMovies,0.9064039408866996,0.8766233766233766,0.9230769230769232,The remaining performance drop seems to be split roughly equally between conjunctions 74 and coreference 76 ,17,Moving to a larger number of templates does not deteriorate performance much ( 80 % ) .,The hardest synthetic dataset combines these ( All Templates + Conj. + Coref. ) and is actually harder than using the real Wikipedia documents ( 72.5 % vs. 76.2 % ) .,method
semantic_role_labeling,2,"Words outside argument spans have the tag O , and words at the beginning and inside of argument spans with role r have the tags Br and Ir respectively .",model,Model,0,29,6,6,0,model : Model,0.12946428571428573,0.4615384615384616,0.5,Words outside argument spans have the tag O and words at the beginning and inside of argument spans with role r have the tags Br and Ir respectively ,29,Each y i ? y belongs to a discrete set of BIO tags T .,Let n = | w| = | y | be the length of the sequence .,method
natural_language_inference,11,SQuAD T - Wiki T - Web Net ( A ) .,model,Model,0,137,2,2,0,model : Model,0.4963768115942029,0.07692307692307693,0.07692307692307693,SQuAD T Wiki T Web Net A ,8, ,"Wikipedia ( W ) yields further , significant improvements on TriviaQA , slightly outperforming the current state of the art model .",method
sentiment_analysis,32,Aspect - level sentiment classification is a fundamental task in natural language processing and catches many researchers ' attention .,introduction,introduction,0,14,4,4,0,introduction : introduction,0.060869565217391314,0.1111111111111111,0.1111111111111111,Aspect level sentiment classification is a fundamental task in natural language processing and catches many researchers attention ,18,"For example , Given the mentioned targets : staff , pizza and beef cubes , and their context sentence "" a group of friendly staff , the pizza is not bad , but the beef cubes are not worth the money ! "" , the sentiment polarity for the three targets , staff , pizza and beef cubes , are positive , neutral and negative respectively .","Traditional approaches mainly focus on designing a set of features such as bag - of - words , sentiment lexicon to train a classifier ( e.g. , SVM ) for aspect - level sentiment classification .",introduction
semantic_role_labeling,3,"In this section , we will describe DEEPATT in detail .",system description,Deep Attentional Neural Network for SRL,0,53,15,2,0,system description : Deep Attentional Neural Network for SRL,0.20075757575757566,0.13761467889908258,0.4,In this section we will describe DEEPATT in detail ,10, ,The main component of our deep network consists of N identical layers .,method
natural_language_inference,18,"To validate this , we compute the stratified MAP scores based on different question type .",experiment,Q1 how old was sue lyon when she made lolita,0,204,78,13,0,experiment : Q1 how old was sue lyon when she made lolita,0.7472527472527473,0.975,0.8666666666666667,To validate this we compute the stratified MAP scores based on different question type ,15,"Intuitively , it is more difficult to understand and answer the questions starting with ' how ' than the others , while the ' what ' questions commonly have explicit words indicating the possible answers .",The MAP of ' how ' questions is 0.524 which is the lowest among the five groups .,experiment
prosody_prediction,0,We use a batch size of 64 and train the model for 5 epochs .,experiment,Experimental Setup,0,96,16,14,0,experiment : Experimental Setup,0.5,0.6666666666666666,0.6363636363636364,We use a batch size of 64 and train the model for 5 epochs ,15,We use a dropout of 0.2 between the layers of the BiLSTM .,"For the SVM we use Minitagger 4 implementation by using each dimension of the pre-trained 300D Glo Ve 840B word embeddings as features , with context - size 1 , i.e. including the previous and the next word in the context .",experiment
natural_language_inference,54,"Although the methods proposed in the paper are demonstrated using syntactic trees , we note that similar approaches can be used to encode other types of tree structured information such as knowledge graphs and ontology relations .",introduction,introduction,0,29,22,22,0,introduction : introduction,0.12663755458515286,1.0,1.0,Although the methods proposed in the paper are demonstrated using syntactic trees we note that similar approaches can be used to encode other types of tree structured information such as knowledge graphs and ontology relations ,36,"Our focus is to show adding structural embedding can improve baseline models , rather than directly compare to published SQuAD results .", ,introduction
natural_language_inference,81,"arXiv preprint ar Xiv : 1808.09920 , 2018 .",abstract,abstract,0,30,28,28,0,abstract : abstract,0.07125890736342043,0.7368421052631579,0.7368421052631579,arXiv preprint ar Xiv 1808 09920 2018 ,8,Question answering by reasoning across documents with graph convolutional networks .,"Sang Keun Lee Deunsol Yoon , Dongbok Lee. Dynamic self - attention :",abstract
text-classification,1,"However , this shortcoming of CNN can be alleviated by having multiple convolution layers with distinct region sizes .",system description,Experiments (supervised),0,153,107,30,0,system description : Experiments (supervised),0.59765625,0.535,0.5,However this shortcoming of CNN can be alleviated by having multiple convolution layers with distinct region sizes ,18,We attribute to this fact the small improvements of oh - 2 LSTMp over oh - CNN in .,We show in the table above that one - hot CNNs with two layers ( of 1000 feature maps each ) with two different region sizes 4 rival oh - 2 LST Mp .,method
relation_extraction,5,"We further show that this difference is due to each model 's competitive advantage : dependency - based models are better at handling sentences with entities farther apart , while sequence models can better leverage local word patterns regardless of parsing quality ( see also .",ablation,Complementary Strengths of GCNs and PA-LSTMs,0,197,19,13,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7490494296577946,0.95,0.9285714285714286,We further show that this difference is due to each model s competitive advantage dependency based models are better at handling sentences with entities farther apart while sequence models can better leverage local word patterns regardless of parsing quality see also ,42,This complementary performance explains the gain we see in when the two models are combined .,We include further analysis in the supplementary material .,result
question_answering,5,The goal of the re-ranker is to rank this list of candidates so that the top - ranked candidates are more likely to be the correct answer a g .,method,METHOD,0,64,13,13,0,method : METHOD,0.2318840579710145,0.14130434782608695,0.8125,The goal of the re ranker is to rank this list of candidates so that the top ranked candidates are more likely to be the correct answer a g ,30,"Given a question q , suppose we have a baseline open - domain QA system that can generate the top - K answer candidates a 1 , . . . , a K , each being a text span in some passage pi .","With access to these additional features , the re-ranking step has the potential to prioritize answers not easily discoverable by the base system alone .",method
named-entity-recognition,1,"Thus , the output buffer contains a single vector representation for each labeled chunk that is generated , regardless of its length .",training,Representing Labeled Chunks,0,118,50,5,0,training : Representing Labeled Chunks,0.5700483091787439,0.5882352941176471,1.0,Thus the output buffer contains a single vector representation for each labeled chunk that is generated regardless of its length ,21,"This function is given as g ( u , . . . , v , r y ) , where r y is a learned embedding of a label type .", ,experiment
relation-classification,2,"solve the entity classification task ( which is different from NER since in entity classification the boundaries of the entities are known and only the type of the entity should be predicted ) and relation extraction problems using an approximation of a global normalization objective ( i.e. , CRF ) : they replicate the context of the sentence ( left and right part of the entities ) to feed one entity pair at a time to a CNN for relation extraction .",system description,Joint entity and relation extraction,0,98,51,10,0,system description : Joint entity and relation extraction,0.3322033898305085,0.8793103448275862,0.5882352941176471,solve the entity classification task which is different from NER since in entity classification the boundaries of the entities are known and only the type of the entity should be predicted and relation extraction problems using an approximation of a global normalization objective i e CRF they replicate the context of the sentence left and right part of the entities to feed one entity pair at a time to a CNN for relation extraction ,75,propose the use of a lot of hand - crafted features along with RNNs .,"Thus , they do not simultaneously infer other potential entities and relations within the same sentence .",method
natural_language_inference,17,"To enhance the ability of capturing complex interactions among inputs , we stack two more aligning blocks with the reattention mechanism as follows",architecture,Alignment Architecture for MRC,0,169,126,126,0,architecture : Alignment Architecture for MRC,0.65,0.9197080291970804,0.9692307692307692,To enhance the ability of capturing complex interactions among inputs we stack two more aligning blocks with the reattention mechanism as follows,22,Multi-round Alignments with Reattention .,where align t denote the t - th block .,method
machine-translation,3,All the other layers have the same r = 2 .,model,Optimization,0,220,20,9,0,model : Optimization,0.7028753993610224,0.625,0.42857142857142855,All the other layers have the same r 2 ,10,The two embedding layers are not regularized .,The parameters of the recurrent computation part are initialized to zero .,method
natural_language_inference,92,"The learning challenge is then to determine which solution in the set is the correct one , while estimating a complete QA model .",introduction,introduction,1,26,16,16,0,introduction : introduction,0.09027777777777778,0.6666666666666666,0.6666666666666666,The learning challenge is then to determine which solution in the set is the correct one while estimating a complete QA model ,23,"We demonstrate that for many recently introduced tasks , which we group into three categories as given in , it is relatively easy to precompute a discrete , task - specific set of possible solutions that contains the correct solution along with a modest number of spurious options .","We model the set of possible solutions as a discrete latent variable , and develop a learning strategy that uses hard - EM - style parameter updates .",introduction
named-entity-recognition,8,"The General Language Understanding Evaluation ( GLUE ) benchmark ( Wang et al. , 2018 a ) is a collection of diverse natural language understanding tasks .",experiment,GLUE,1,156,4,2,1,experiment : GLUE,0.4031007751937984,0.056338028169014086,0.028985507246376805,The General Language Understanding Evaluation GLUE benchmark Wang et al 2018 a is a collection of diverse natural language understanding tasks ,22, ,Detailed descriptions of GLUE datasets are included in Appendix B.1 .,experiment
natural_language_inference,29,"At the beginning , for a product , we assume there is a question",system description,PROBLEM FORMULATION,0,98,3,3,0,system description : PROBLEM FORMULATION,0.2684931506849315,0.01875,0.3,At the beginning for a product we assume there is a question,12,"Before introducing our answer generation task for product - aware question , we introduce our notation and key concepts .",where a k i is the name of i - th attribute and av i is the attribute content .,method
text-to-speech_synthesis,1,"Specifically , we extract attention alignments from an encoder - decoder based teacher model for phoneme duration prediction , which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.0365296803652968,0.5454545454545454,0.5454545454545454,Specifically we extract attention alignments from an encoder decoder based teacher model for phoneme duration prediction which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel spectrogram sequence for parallel mel spectrogram generation ,45,"In this work , we propose a novel feed - forward network based on Transformer to generate mel-spectrogram in parallel for TTS .","Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality , nearly eliminates the problem of word skipping and repeating in particularly hard cases , and can adjust voice speed smoothly .",abstract
named-entity-recognition,4,"This supplement contains details of the model architectures , training routines and hyper - parameter choices for the state - of - the - art models in Section 4 .",system description,system description,0,190,2,2,0,system description : system description,0.6985294117647058,0.060606060606060615,0.060606060606060615,This supplement contains details of the model architectures training routines and hyper parameter choices for the state of the art models in Section 4 ,25, ,All of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked RNNs - LSTMs in every case except the SQuAD model that uses GRUs .,method
natural_language_inference,76,"Word - by - word attention seems to also work well when words in the premise and hypothesis are connected via deeper semantics or common - sense knowledge ( "" snow "" can be found "" outside "" and a "" mother "" is an "" adult "" , 3e and 3 g ) .",analysis,QUALITATIVE ANALYSIS,0,135,15,15,0,analysis : QUALITATIVE ANALYSIS,0.9246575342465754,0.8333333333333334,0.8333333333333334,Word by word attention seems to also work well when words in the premise and hypothesis are connected via deeper semantics or common sense knowledge snow can be found outside and a mother is an adult 3e and 3 g ,41,"It is also noteworthy that irrelevant parts of the premise , such as words capturing little meaning or whole uninformative relative clauses , are correctly neglected for determining entailment ( "" which also has a rope leading out of it "" , 3 b ) .","Furthermore , the model is able to resolve one - to - many relationships ( "" kids "" to "" boy "" and "" girl "" , 3 d )",result
sentiment_analysis,25,CNN is widely used on text classification task .,method,Compared Methods,1,158,6,6,0,method : Compared Methods,0.7117117117117117,0.42857142857142855,0.42857142857142855,CNN is widely used on text classification task ,9,"The sentiment lexicons improve the performance significantly , but it requires large scale labeled data : 183 thousand Yelp reviews , 124 thousand Amazon laptop reviews , 56 million tweets , and 3 sentiment lexicons labeled manually .","It can not directly capture aspectspecific sentiment information on ACSA task , but it provides a very strong baseline for sentiment classification .",method
sentence_compression,1,Obama do n't know nuttin ' about it .,experiment,Experiments,0,170,40,40,0,experiment : Experiments,0.8133971291866029,0.5555555555555556,0.5555555555555556,Obama do n t know nuttin about it ,9,"Whatever the crisis or embarrassment to his administration , Pres .",Obama do n't know nuttin .,experiment
part-of-speech_tagging,4,"It gives highly competitive results compared with topperformance systems on WSJ , OntoNotes 5.0 and CCGBank without external training .",introduction,introduction,0,40,32,32,0,introduction : introduction,0.17167381974248927,0.9411764705882352,0.9411764705882352,It gives highly competitive results compared with topperformance systems on WSJ OntoNotes 5 0 and CCGBank without external training ,20,"On standard benchmarks for POS tagging , NER and CCG supertagging , our model achieves significantly better accuracies and higher efficiencies than BiLSTM - CRF and BiLSTM - softmax with similar number of parameters .","In addition to accuracy and efficiency , BiLSTM - LAN is also more interpretable than BiLSTM - CRF thanks to visualizable label embeddings and label distributions .",introduction
natural_language_inference,57,"To learn f c and f i , we use the approach of except , since we are embedding into RN + , we constrain the embedding vectors to have nonnegative entries by taking their absolute value .",result,IMAGE AND CAPTION EMBEDDINGS,0,127,25,2,0,result : IMAGE AND CAPTION EMBEDDINGS,0.7383720930232558,0.5555555555555556,0.3333333333333333,To learn f c and f i we use the approach of except since we are embedding into RN we constrain the embedding vectors to have nonnegative entries by taking their absolute value ,34, ,"Thus , to embed images , we use f i ( i ) = | W i CNN ( i ) | ( 6 ) where W i is a learned N 4096 matrix , N being the dimensionality of the embedding space .",result
natural_language_inference,76,"Furthermore , let e L ? R L be a vector of 1s and h N be the last output vector after the premise and hypothesis were processed by the two LSTMs respectively .",method,ATTENTION,0,61,33,8,0,method : ATTENTION,0.4178082191780822,0.5789473684210527,0.42105263157894735,Furthermore let e L R L be a vector of 1s and h N be the last output vector after the premise and hypothesis were processed by the two LSTMs respectively ,32,"Let Y ? R kL be a matrix consisting of output vectors [ h 1 h L ] that the first LSTM produced when reading the L words of the premise , where k is a hyperparameter denoting the size of embeddings and hidden layers .","Furthermore , let e L ? R L be a vector of 1s and h N be the last output vector after the premise and hypothesis were processed by the two LSTMs respectively .",method
question_similarity,0,This idea can also be applied to search engines in order to find documents relevant to a query .,introduction,introduction,0,13,5,5,0,introduction : introduction,0.09420289855072464,0.3125,0.3125,This idea can also be applied to search engines in order to find documents relevant to a query ,19,"Also , in answer sentence selection task , it is utilized to determine the relevance between question - answer pairs and rank the answers sentences from the most relevant to the least .",new task has been proposed by Mawdoo3 1 company with a new dataset provided by their data annotation team for Semantic Question Similarity ( SQS ) for the Arabic language .,introduction
temporal_information_extraction,1,"Among all the ten possible pairs of nodes , only three TLINKs were annotated .",introduction,introduction,0,35,26,26,0,introduction : introduction,0.13618677042801555,0.5909090909090909,0.5909090909090909,Among all the ten possible pairs of nodes only three TLINKs were annotated ,14,shows the actual human annotations provided by TE3 .,"Even if we only look at main events in consecutive sentences and at events in the same sentence , there are still quite a few missing TLINKs , e.g. , the one between hurt and cascaded and the one between monitor and ordered .",introduction
natural_language_inference,42,"In this section , we present FABIR 's architecture and the main design decisions we have made to develop a lighter and faster question - answering model .",system description,III. FABIR,0,82,42,3,0,system description : III. FABIR,0.2837370242214533,0.24,0.6,In this section we present FABIR s architecture and the main design decisions we have made to develop a lighter and faster question answering model ,26,"Also , their model probably has a higher number of learned parameters due to the increased number of layers .","In particular , we introduce the convolutional attention , the column - wise cross-attention , and the reduction layer , which build on the Transformer model to enable its application to question - answering .",method
natural_language_inference,53,"We also consider a model BiGRU - last that concatenates the last hidden state of a forward GRU , and the last hidden state of a backward GRU to have the same architecture as for SkipThought vectors .",architecture,LSTM and GRU,0,65,7,4,0,architecture : LSTM and GRU,0.3125,0.25,1.0,We also consider a model BiGRU last that concatenates the last hidden state of a forward GRU and the last hidden state of a backward GRU to have the same architecture as for SkipThought vectors ,36,"For a sequence of T words ( w 1 , . . . , w T ) , the network computes a set of T hidden representations", ,method
sentiment_analysis,8,TFIDF is a numerical statistic that shows the correlation between a word and a document in a collection or corpus .,methodology,Implementation Details,0,98,52,3,0,methodology : Implementation Details,0.4188034188034188,0.5842696629213483,0.12,TFIDF is a numerical statistic that shows the correlation between a word and a document in a collection or corpus ,21,Term Frequency - Inverse Document Frequency ( TFIDF ) :,It consists of two parts :,method
sentiment_analysis,5,"In this section , we investigate the performance of using different pairwise operations in BiHDM , as shown in Eq. .",experiment,Different pairwise operations,0,212,77,2,0,experiment : Different pairwise operations,0.8,0.8850574712643678,0.18181818181818185,In this section we investigate the performance of using different pairwise operations in BiHDM as shown in Eq ,19, ,"Here , we denote the subtraction , division and inner product variants as BiHDM - S , BiHDM - D , and BiHDM - I , respectively .",experiment
sentiment_analysis,45,"In ABSA , the target - aspect pairs {t , a} become only aspects a .",methodology,Task description,0,42,8,6,0,methodology : Task description,0.2916666666666667,0.14814814814814814,0.8571428571428571,In ABSA the target aspect pairs t a become only aspects a ,13,"In TABSA , a sentence s usually consists of a series of words : {w 1 , , w m } , and some of the words {w i 1 , , w i k } are pre-identified targets {t 1 , , t k } , following , we set the task as a 3 class classification problem : given the sentence s , a set of target entities T and a fixed aspect set A = { general , price , transitlocation , saf ety } , predict the sentiment polarity y ? { positive , negative , none } over the full set of the target - aspect pairs { ( t , a ) : t ? T , a ? A }. As we can see in , the gold standard polarity of ( LOCATION2 , price ) is negative , while the polarity of ( LOCATION1 , price ) is none .",This setting is equivalent to learning subtasks 3 ( Aspect Category Detection ) and subtask 4 ( Aspect Category Polarity ) of SemEval - 2014 Task 4 2 at the same time .,method
passage_re-ranking,1,The official pre-trained BERT 21.8 19.8 - Conv-KNRM,training,training,0,60,4,4,0,training : training,0.821917808219178,0.3636363636363637,0.3636363636363637,The official pre trained BERT 21 8 19 8 Conv KNRM,11,"However , there is an important difference .","29 . models 2 were pre-trained on the full Wikipedia , and therefore they have seen , although in an unsupervised way , Wikipedia documents thatare used in the test set of TREC - CAR .",experiment
sentiment_analysis,25,"After removing duplicates , the statistics are show in .",dataset,Datasets and Experiment Preparation,0,142,18,18,0,dataset : Datasets and Experiment Preparation,0.6396396396396397,0.6428571428571429,0.6428571428571429,After removing duplicates the statistics are show in ,9,"We assign a sentence a positive label if p > 0 , a negative label if p < 0 , or a neutral label if p = 0 .","The resulting dataset has 8 aspects : restaurant , food , drinks , ambience , service , price , misc and location .",experiment
natural_language_inference,90,"Our approach uses attention to decompose the problem into subproblems that can be solved separately , thus making it trivially parallelizable .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.03333333333333333,0.6,0.6,Our approach uses attention to decompose the problem into subproblems that can be solved separately thus making it trivially parallelizable ,21,We propose a simple neural architecture for natural language inference .,"On the Stanford Natural Language Inference ( SNLI ) dataset , we obtain state - of - the - art results with almost an order of magnitude fewer parameters than previous work and without relying on any word - order information .",abstract
sentiment_analysis,8,"Since we are using feature vectors as input , we do not need another decoder network to transform it back from hidden to output space thereby reducing network size .",model,model,0,155,20,20,0,model : model,0.6623931623931624,1.0,1.0,Since we are using feature vectors as input we do not need another decoder network to transform it back from hidden to output space thereby reducing network size ,29,We feed the feature vectors as input to the network and finally pass the output of the LSTM network through a softmax layer to get probability scores for each of the six emotion classes ., ,method
natural_language_inference,32,"We concatenate the output from the the previous IF layer with the attention vector , and feed it to the last BiLSTM layer .",architecture,architecture,0,128,31,31,0,architecture : architecture,0.2929061784897025,0.8157894736842105,0.8157894736842105,We concatenate the output from the the previous IF layer with the attention vector and feed it to the last BiLSTM layer ,23,We apply fully - aware attention on the context itself ( self - attention ) .,"We use the same answer span selection method to estimate the start and end probabilities PS i , j , PE i , j of the j - th context token for the i - th question .",method
sentiment_analysis,11,Our goal is to infer the emotion of utterances present in a dyadic conversation .,system description,Task Definition,0,78,2,2,0,system description : Task Definition,0.2267441860465116,0.10526315789473684,0.10526315789473684,Our goal is to infer the emotion of utterances present in a dyadic conversation ,15, ,Let us define a dyadic conversation to bean asynchronous exchange of utterances between two persons Pa and Pb .,method
machine-translation,8,The hidden state s i of the decoder given the annotations from the encoder is computed by,model,DECODER,0,264,51,2,0,model : DECODER,0.7975830815709971,0.4322033898305085,0.07692307692307693,The hidden state s i of the decoder given the annotations from the encoder is computed by,17, ,is the word embedding matrix for the target language .,method
natural_language_inference,70,This paper describes the KeLP system participating in the SemEval - 2016 cQA challenge .,introduction,introduction,0,13,2,2,0,introduction : introduction,0.07428571428571429,0.1111111111111111,0.1111111111111111,This paper describes the KeLP system participating in the SemEval 2016 cQA challenge ,14, ,"In this task , participants are asked to automatically provide good answers in a c QA setting .",introduction
sentiment_analysis,9,"Therefore , when conducting transfer learning on aspect - based sentiment analysis , those proposed models often fall into the dilemma of lacking aspect extraction method on targeted tasks because there is not enough research support .",introduction,introduction,0,22,8,8,0,introduction : introduction,0.07942238267148015,0.21052631578947367,0.21052631578947367,Therefore when conducting transfer learning on aspect based sentiment analysis those proposed models often fall into the dilemma of lacking aspect extraction method on targeted tasks because there is not enough research support ,34,"However , most of the proposed models for aspect - based sentiment analysis tasks only focus on improving the classification accuracy of aspect polarity and ignore the research of aspect term extraction .",The APC task is a kind of classification problem .,introduction
sentiment_analysis,18,But intuitively not all words are equally important for determining the polarity of a target .,model,Syntax-based Attention Mechanism,0,112,46,3,0,model : Syntax-based Attention Mechanism,0.4686192468619247,0.6216216216216216,0.17647058823529413,But intuitively not all words are equally important for determining the polarity of a target ,16,"The attention mechanism used in previous works gives equal importance to all context words , where the attention weight is merely a measure of semantic association between the target and the context word .","Words that appear near the target or have a modifier relation to the target , for example , are more important and should receive higher weight .",method
part-of-speech_tagging,0,Accuracy 97.27 97.28 97.29 97.50 97.78 97.55 97.55 97.55 Ours - Baseline 97.54 Ours - Adversarial 97.58 .,model,Model,0,129,2,2,0,model : Model,0.5243902439024389,0.2,0.2,Accuracy 97 27 97 28 97 29 97 50 97 78 97 55 97 55 97 55 Ours Baseline 97 54 Ours Adversarial 97 58 ,26, ,"We apply dropout to input embeddings and BiLSTM outputs for both baseline and adversarial training , with dropout rate 0.5 .",method
text-to-speech_synthesis,1,We describe the detailed steps as follows :,system description,Duration Predictor,0,97,36,8,0,system description : Duration Predictor,0.4429223744292237,0.8,0.4705882352941176,We describe the detailed steps as follows ,8,"In order to train the duration predictor , we extract the ground - truth phoneme duration from an autoregressive teacher TTS model , as shown in .",We first train an autoregressive encoder - attention - decoder based Transformer TTS model following .,method
natural_language_inference,74,"We can intuitively speculate that "" but "" and "" although "" have direct connections with the contradiction label ( which drops most significantly ) while "" because "" has some links with the entailment label .",analysis,Semantic Analysis,0,190,5,5,0,analysis : Semantic Analysis,0.8482142857142857,0.4166666666666667,0.7142857142857143,We can intuitively speculate that but and although have direct connections with the contradiction label which drops most significantly while because has some links with the entailment label ,29,"As we can see , there is a sharp decline of accuracy when removing "" but "" , "" because "" and "" although "" .","We observe that some discourse markers such as "" if "" or "" before "" contribute much less than other words which have strong logical hints , although they actually improve the performance of the model .",result
semantic_role_labeling,4,Recent span - based model,model,model,0,275,4,4,0,model : model,0.9166666666666666,0.25,0.25,Recent span based model,4,"For inference , several effective methods have been proposed , such as structural constraint inference by using integer linear programming or dynamic programming .","Avery recent work , , proposed a span - based SRL model similar to our model .",method
sentiment_analysis,50,"Specifically , we explore two transfer methods to incorporate this sort of knowledge - pretraining and multi-task learning .",introduction,introduction,1,21,14,14,0,introduction : introduction,0.12962962962962962,0.8235294117647058,0.8235294117647058,Specifically we explore two transfer methods to incorporate this sort of knowledge pretraining and multi task learning ,18,"In this paper , we hypothesize that aspect - level sentiment classification can be improved by employing knowledge gained from document - level sentiment classification .","In our experiments , we find that both methods are helpful and combining them achieves significant improvements over attentionbased LSTM models trained only on aspect - level data .",introduction
sentiment_analysis,41,"Vector v a i ? R dais represented for the embedding of aspect i , where d a is the dimension of aspect embedding .",system description,LSTM with Aspect Embedding (AE-LSTM),0,83,45,5,0,system description : LSTM with Aspect Embedding (AE-LSTM),0.3721973094170404,0.5844155844155844,0.625,Vector v a i R dais represented for the embedding of aspect i where d a is the dimension of aspect embedding ,23,"To make the best use of aspect information , we propose to learn an embedding vector for each aspect .","Vector v a i ? R dais represented for the embedding of aspect i , where d a is the dimension of aspect embedding .",method
natural_language_inference,48,The result of this alternating search is fed back into the iterative inference process to seed the next search step .,introduction,introduction,1,23,17,17,0,introduction : introduction,0.10407239819004524,0.7391304347826086,0.7391304347826086,The result of this alternating search is fed back into the iterative inference process to seed the next search step ,21,"This phase involves a novel alternating attention mechanism ; it first attends to some parts of the query , then finds their corresponding matches by attending to the document .","This permits our model to reason about different parts of the query in a sequential way , based on the information that has been gathered previously from the document .",introduction
sentiment_analysis,23,Such results show that structures are important when modeling sentences ; tree - based convolution can capture these structural information more effectively than RNNs .,performance,Performance,0,211,8,8,0,performance : Performance,0.7226027397260274,0.2962962962962963,0.8,Such results show that structures are important when modeling sentences tree based convolution can capture these structural information more effectively than RNNs ,23,"In a more controlled comparison - with shallow architectures and the basic interaction ( linearly transformed and non-linearly squashed ) - TBCNNs , of both variants , consistently outperform RNNs to a large extent ( 50.4 - 51.4 % versus 43.2 % ) ; they also consistently outperform "" flat "" CNNs by more than 10 % .",We also observe d- TBCNN achieves higher performance than c - TBCNN .,result
sentiment_analysis,45,1 ) detect the mention of an aspect a for the target t; ( 2 ) determine the positive or negative sentiment polarity y for detected target - aspect pairs .,dataset,Datasets,0,94,5,5,0,dataset : Datasets,0.6527777777777778,0.5555555555555556,0.5555555555555556,1 detect the mention of an aspect a for the target t 2 determine the positive or negative sentiment polarity y for detected target aspect pairs ,27,"Ultimately , given a sentence sand the target tin the sentence , we need to :",1 ) detect the mention of an aspect a for the target t; ( 2 ) determine the positive or negative sentiment polarity y for detected target - aspect pairs .,experiment
natural_language_inference,13,"Given a passage , there are several questions with four options each .",dataset,Datasets,0,148,4,4,0,dataset : Datasets,0.6462882096069869,0.6666666666666666,0.6666666666666666,Given a passage there are several questions with four options each ,12,RACE ( Reading Comprehension from Examinations ) is a recently proposed dataset that is constructed from real world examinations .,"The authors argue that RACE is more challenging compared to popular benchmarks ( e.g. , SQuAD ) as more multi-sentence and compositional reasoning is required .",experiment
natural_language_inference,9,system is evaluated on the accuracy of getting the correct answers to the questions .,experiment,EXPERIMENTS,0,166,5,5,0,experiment : EXPERIMENTS,0.5030303030303029,0.2631578947368421,0.2631578947368421,system is evaluated on the accuracy of getting the correct answers to the questions ,15,story can be as short as two sentences and as long as 200 + sentences .,"The answers are single words or lists ( e.g. "" football , apple "" ) .",experiment
sentence_compression,1,"For training , we unfold the network 120 times and make sure that none of our training instances is longer than that .",baseline,Baseline,0,119,82,82,0,baseline : Baseline,0.5693779904306221,0.9534883720930232,0.9534883720930232,For training we unfold the network 120 times and make sure that none of our training instances is longer than that ,22,"Therefore , the search space at decoding time is exponential on the length of the input , and we have used a beam - search procedure as described in .","The learning rate is initialized at 2 , with a decay factor of 0.96 every 300,000 traning steps .",result
natural_language_inference,30,"Otherwise , the pattern is chosen randomly .",training,Questions Generation,0,93,19,8,0,training : Questions Generation,0.36046511627906974,0.4222222222222222,0.25806451612903225,Otherwise the pattern is chosen randomly ,7,"Note only triples with a *-in.r relation ( denoted r- in in ) can generate from the pattern where did er ? , for example , and similar for other constraints .","Except for these exceptions , we used all 16 seed questions for all triples hence generating approximately 16 14M questions stored in a training set we denote D.",experiment
natural_language_inference,46,"on the performance of span retrieval models , including the neural models discussed below .",baseline,baseline,0,159,5,5,0,baseline : baseline,0.5353535353535354,0.15151515151515152,0.15151515151515152,on the performance of span retrieval models including the neural models discussed below ,14,The answer oracle provides an upper bound We lowercase both the candidates and the references and remove the end of sentence marker and the final full stop .,"When using the question as the query , we obtain generalization results of IR methods .",result
natural_language_inference,83,"For instance , our language model of over all narrative sentiments indicates that while happy stories mostly have happy endings ( with a conditional probability of 74 % ) , the reverse is not true .",result,Correct Ending Weights,0,234,19,16,0,result : Correct Ending Weights,0.7647058823529411,0.3584905660377358,0.42105263157894735,For instance our language model of over all narrative sentiments indicates that while happy stories mostly have happy endings with a conditional probability of 74 the reverse is not true ,31,than making hard assumptions .,"In particular , sad stories ( with over all negative sentiments ) end with a negative sentiment in only 52 % of the cases .",result
text_generation,0,"It is worth noticing that most of the RNN variants , such as the gated recurrent unit ( GRU ) ) and soft attention mechanism , can be used as a generator in SeqGAN .",system description,The Generative Model for Sequences,0,148,86,7,0,system description : The Generative Model for Sequences,0.4567901234567901,0.8037383177570093,1.0,It is worth noticing that most of the RNN variants such as the gated recurrent unit GRU and soft attention mechanism can be used as a generator in SeqGAN ,30,"To deal with the common vanishing and exploding gradient problem of the backpropagation through time , we leverage the Long Short - Term Memory ( LSTM ) cells to implement the update function gin Eq. ( 9 ) .", ,method
sentiment_analysis,32,"Next , we use the LSTM networks to learn the hidden word semantics , since words in a sentence have strong dependence on each other , and LSTM is good at learning long - term dependencies and can avoid gradient vanishing and expansion problems .",system description,Interactive Attention Networks,0,65,15,15,0,system description : Interactive Attention Networks,0.2826086956521739,0.32608695652173914,0.32608695652173914,Next we use the LSTM networks to learn the hidden word semantics since words in a sentence have strong dependence on each other and LSTM is good at learning long term dependencies and can avoid gradient vanishing and expansion problems ,41,"In our model , we choose the latter strategy .","Formally , given the input word embedding wk , previous cell state c k?1 and previous hidden state h k?1 , the current cell state ck and current hidden state h kin the LSTM networks are updated as :",method
temporal_information_extraction,1,"This distinction emphasizes that vague is a consequence of lack of background / contextual information , rather than a concrete relation type to be trained on .",training,Missing Annotations,0,166,91,15,0,training : Missing Annotations,0.6459143968871596,0.8584905660377359,0.5,This distinction emphasizes that vague is a consequence of lack of background contextual information rather than a concrete relation type to be trained on ,25,"For example , if a before TLINK can be established given a sentence , then it always holds as before regardless of other events around it , but if a TLINK is vague given a sentence , it may still change to other types afterwards if a connection can later be established through other nodes from the context .","Fourth , without the vague classifier , the predicted temporal graph tends to become more densely connected , thus the global transitivity constraints can be more effective in correcting local mistakes .",experiment
natural_language_inference,41,"We present BART , a denoising autoencoder for pretraining sequence - to - sequence models .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.01556420233463035,0.2222222222222222,0.2222222222222222,We present BART a denoising autoencoder for pretraining sequence to sequence models ,13, ,"BART is trained by ( 1 ) corrupting text with an arbitrary noising function , and ( 2 ) learning a model to reconstruct the original text .",abstract
sentiment_analysis,22,Then the weighted representations are fed into the Bi - GRU .,performance,performance,0,166,13,13,0,performance : performance,0.7248908296943232,0.2452830188679245,0.2452830188679245,Then the weighted representations are fed into the Bi GRU ,11,"Bi - GRU - PW first weights the word embeddings of each word in the sentence based on the distance from the target , as did in ) .",Bi - GRU - PE concatenates the word embeddings and the position embeddings of each word as inputs to the Bi - GRU when modelling the sentence .,result
natural_language_inference,41,"BART also provides a 1.1 BLEU increase over a back - translation system for machine translation , with only target language pretraining .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.03891050583657588,0.8888888888888888,0.8888888888888888,BART also provides a 1 1 BLEU increase over a back translation system for machine translation with only target language pretraining ,22,"It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD , achieves new stateof - the - art results on a range of abstractive dialogue , question answering , and summarization tasks , with gains of up to 6 ROUGE .","We also report ablation experiments that replicate other pretraining schemes within the BART framework , to better measure which factors most influence end - task performance .",abstract
natural_language_inference,53,Binary and multi-class classification,evaluation,Evaluation of sentence representations,0,107,15,15,0,evaluation : Evaluation of sentence representations,0.5144230769230769,0.32608695652173914,0.625,Binary and multi class classification,5,"The tool uses Adam ( Kingma and to fit a logistic regression classifier , with batch size 64 .","We use a set of binary classification tasks ( see ) that covers various types of sentence classification , including sentiment analysis ( MR , SST ) , question - type ( TREC ) , product reviews ( CR ) , subjectivity / objectivity ( SUBJ ) and opinion polarity ( MPQA ) .",result
semantic_role_labeling,4,"Specifically , for each predicted span in the development set , we collect 10 nearest neighbor spans with their gold labels from the training set .",performance,Function F ?,0,232,39,17,0,performance : Function F ?,0.7733333333333333,0.7959183673469388,0.6296296296296297,Specifically for each predicted span in the development set we collect 10 nearest neighbor spans with their gold labels from the training set ,24,"To investigate a relation between the span representations and predicted labels , we qualitatively analyze nearest neighbors of each span representation with its predicted label .","shows 10 nearest neighbors of a span "" across the border "" for the predicate "" move "" .",result
machine-translation,3,The group size is the same as the length of the input sequence .,system description,Network,0,144,102,65,0,system description : Network,0.4600638977635783,0.85,0.7831325301204819,The group size is the same as the length of the input sequence ,14,"At the last LSTM layers , there are two groups of vectors representing the source sequence .",Interface : Prior encoder - decoder models and attention models are different in their method of extracting the representations of the source sequences .,method
question_generation,1,) controls the separation margin between these and is obtained through validation data .,method,Mixture Module,0,133,53,16,0,method : Mixture Module,0.3392857142857143,0.5824175824175825,1.0, controls the separation margin between these and is obtained through validation data ,14,"Here D + , D ? represent the euclidean distance between the target and supporting sample , and target and opposing sample respectively .", ,method
sentiment_analysis,8,"Communication is the key to human existence and more often than not , we have to deal with ambiguous situations .",introduction,introduction,0,13,2,2,0,introduction : introduction,0.05555555555555555,0.13333333333333333,0.13333333333333333,Communication is the key to human existence and more often than not we have to deal with ambiguous situations ,20, ,"For instance , the phrase "" This is awesome "" could be said under either happy or sad settings .",introduction
sentiment_analysis,27,"However , these attention works model each aspect separately in one sentence , which may loss some sentiment dependency information on multiple aspects case .",system description,Aspect-level sentiment classification,0,71,16,16,0,system description : Aspect-level sentiment classification,0.2572463768115942,0.7272727272727273,1.0,However these attention works model each aspect separately in one sentence which may loss some sentiment dependency information on multiple aspects case ,23,"Song et al. propose an attentional encoder network , which eschews recurrence and apply multi-head attention for the modeling between context and aspect .", ,method
text-classification,7,"Capsule networks achieve competitive results over the compared baseline methods on 4 out of 6 datasets , which shows the effectiveness of capsule networks for text classification .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.02880658436213992,0.625,0.625,Capsule networks achieve competitive results over the compared baseline methods on 4 out of 6 datasets which shows the effectiveness of capsule networks for text classification ,27,series of experiments are conducted with capsule networks on six text classification benchmarks .,We additionally show that capsule networks exhibit significant improvement when transfer single - label to multi-label text classification over the competitors .,abstract
text_summarization,11,The global encoding allows the encoder output at each time step to become new representation vector with further connection to the global source side information .,system description,Convolutional Gated Unit,0,55,26,17,0,system description : Convolutional Gated Unit,0.3819444444444444,0.6341463414634146,0.53125,The global encoding allows the encoder output at each time step to become new representation vector with further connection to the global source side information ,26,"Based on the convolution and self - attention , the gated unit sets agate to filter the source annotations from the RNN encoder , in order to select information relevant to the global semantic meaning .","For convolution , we implement a structure similar to inception .",method
relation_extraction,0,We compute the output layer representations as :,model,Entity detection,0,89,35,15,0,model : Entity detection,0.34765625,0.4929577464788733,0.9375,We compute the output layer representations as ,8,with a dense representation bk .,We decode the output sequence from left to right in a greedy manner .,method
natural_language_inference,46,"In particular , the task often requires referring to larger parts of the story , in addition to knowing at least some background about entities .",analysis,FRANK (to Dana),0,246,16,5,0,analysis : FRANK (to Dana),0.8282828282828283,0.6153846153846154,0.3333333333333333,In particular the task often requires referring to larger parts of the story in addition to knowing at least some background about entities ,24,We found that the retrieval is challenging even for humans not familiar with the presented narrative .,"This makes the search procedure , based on only a short question , a challenging and interesting task in itself .",result
named-entity-recognition,2,The gap between our greedy model and those using Viterbi decoding is wider than on CoNLL .,model,Model,0,201,5,5,0,model : Model,0.943661971830986,0.38461538461538464,0.38461538461538464,The gap between our greedy model and those using Viterbi decoding is wider than on CoNLL ,17,"Our greedy model is out - performed by the Bi - LSTM - CRF reported in Chiu and Nichols ( 2016 ) as well as our own re-implementation , which appears to be the new state - of - the - art on this dataset .","We believe this is due to the more diverse set of entities in OntoNotes , which also tend to be much longer - the average length of a multi-token named entity segment in CoNLL is about one token shorter than in OntoNotes .",method
natural_language_inference,23,This component fuses all higher - level information in the question Q to the context C through fully - aware attention on history - of - word .,architecture,Low-level fusion:?,0,172,100,7,0,architecture : Low-level fusion:?,0.33528265107212474,0.7042253521126759,0.2258064516129032,This component fuses all higher level information in the question Q to the context C through fully aware attention on history of word ,24,are the understanding vectors for Q. Fully - Aware Multi - level Fusion : Higher - level .,"Since the proposed attention scoring function for fully - aware attention is constrained to be symmetric , we need to identify the common history - of - word for both C , Q .",method
natural_language_inference,22,Context : The Broncos took an early lead in Super Bowl 50 and never trailed .,model,Summarization Layer,0,102,57,17,0,model : Summarization Layer,0.4636363636363636,0.8636363636363636,0.7727272727272727,Context The Broncos took an early lead in Super Bowl 50 and never trailed ,15,"Hence we obtain S C ? R 2 d C , which is fused with context encoding C via a gate :","Newton was limited by Denver 's defense , which sacked him seven times and forced him into three turnovers , including a fumble which they recovered for a touchdown .",method
phrase_grounding,0,"Furthermore , methods based on bounding boxes often extract features separately for each bounding box , inducing a high computational cost .",system description,Grounding natural language in images,0,40,7,7,0,system description : Grounding natural language in images,0.17777777777777778,0.175,0.4666666666666667,Furthermore methods based on bounding boxes often extract features separately for each bounding box inducing a high computational cost ,20,"These works often operate in a fully supervised setting , where the mapping between sentences and bounding boxes has to be provided at training time which is not always available and is costly to gather .","Therefore , some works choose not to rely on bounding boxes and propose to formalize the localization problem as finding a spatial heatmap for the referring expression .",method
relation_extraction,2,"For the parameters of the pre-trained BERT model , please refer to for details .",system description,Parameter Settings,0,97,5,5,0,system description : Parameter Settings,0.7185185185185186,0.8333333333333334,1.0,For the parameters of the pre trained BERT model please refer to for details ,15,"For the pre-trained BERT model , we use the uncased basic model .", ,method
relation_extraction,5,"3 ) F 1 drops by 10.3 when we remove the feedforward layers , the LSTM component and the dependency structure altogether .",ablation,Ablation Study,1,183,5,5,0,ablation : Ablation Study,0.6958174904942965,0.25,0.8333333333333334,3 F 1 drops by 10 3 when we remove the feedforward layers the LSTM component and the dependency structure altogether ,22,"2 ) When we remove the dependency structure ( i.e. , setting to I ) , the score drops by 3.2 F 1 .","4 ) Removing the pruning ( i.e. , using full trees as input ) further hurts the result by another 9.7 F 1 .",result
text-classification,6,"As shown Eq. 1 , we first compute the cosine similarity of the two sentence embeddings and then use arccos to convert the cosine similarity into an angular distance .",training,Transfer Tasks,0,79,19,14,0,training : Transfer Tasks,0.5337837837837838,0.9047619047619048,0.875,As shown Eq 1 we first compute the cosine similarity of the two sentence embeddings and then use arccos to convert the cosine similarity into an angular distance ,29,"For the pairwise semantic similarity task , we directly assess the similarity of the sentence embeddings produced by our two encoders .","sim ( u , v ) = 1 ? arccos u v | | u || | | v|| /? ( 1 )",experiment
text_generation,5,We find directly optimizing U ( x ) does not perform well for unsupervised clustering and we need to initialize the encoder with LSTM language model .,result,Unsupervised clustering results,0,243,59,7,0,result : Unsupervised clustering results,0.823728813559322,0.6483516483516484,0.1794871794871795,We find directly optimizing U x does not perform well for unsupervised clustering and we need to initialize the encoder with LSTM language model ,25,"Since GMM can easily get stuck in poor local optimum , we run each model ten times and report the best result .",The model only works well for Yahoo data set .,result
natural_language_inference,52,"Finally , we also use a set of four IR - based features which are assigned at the level of the answer candidate ( i.e. , these features are identical for each of the candidate justifications for that answer choice ) .",model,Semi-Lexicalized Discourse features (lexDisc):,0,119,37,12,0,model : Semi-Lexicalized Discourse features (lexDisc):,0.4559386973180077,0.6607142857142857,0.5714285714285714,Finally we also use a set of four IR based features which are assigned at the level of the answer candidate i e these features are identical for each of the candidate justifications for that answer choice ,38,IR - based features ( IR ++ ):,"Using the same query method as described in Section 4.1 , for each question and answer candidate we retrieve a set of indexed documents .",method
sentence_classification,0,The hyperparameters ? i are tuned for best performance on the validation set of the respective datasets using a 0.0 to 0.3 grid search .,implementation,Implementation,0,160,7,7,0,implementation : Implementation,0.599250936329588,0.4117647058823529,0.4117647058823529,The hyperparameters i are tuned for best performance on the validation set of the respective datasets using a 0 0 to 0 3 grid search ,26,"For each of scaffold tasks , we use a single - layer MLP with 20 hidden nodes , ReLU activation and a Dropout rate of 0.2 between the hidden and input layers .",The hyperparameters ? i are tuned for best performance on the validation set of the respective datasets using a 0.0 to 0.3 grid search .,experiment
natural_language_inference,46,"We found that this approach lead to the best performance of the subsequently applied model on the validation set , irrespective of the number of chunks .",experiment,experiment,0,221,34,34,0,experiment : experiment,0.7441077441077442,0.7906976744186046,0.7906976744186046,We found that this approach lead to the best performance of the subsequently applied model on the validation set irrespective of the number of chunks ,26,"Finally , we also consider the cosine similarity of TF - IDF representations .","Note that we used the answer as the query on the training , and the question for validation and test .",experiment
sentence_classification,0,"On the smaller dataset , our best model takes approximately 30 minutes per epoch to train ( training time without ELMo is significantly faster ) .",implementation,Implementation,0,166,13,13,0,implementation : Implementation,0.6217228464419475,0.7647058823529411,0.7647058823529411,On the smaller dataset our best model takes approximately 30 minutes per epoch to train training time without ELMo is significantly faster ,23,We use Beaker 12 for running the experiments .,It is known that multiple runs of probabilistic deep learning models can have variance in over all scores .,experiment
natural_language_inference,95,The results of our model and several baseline systems on the test set of DuReader are shown in .,result,Results on DuReader,0,162,2,2,0,result : Results on DuReader,0.6923076923076923,0.125,0.125,The results of our model and several baseline systems on the test set of DuReader are shown in ,19, ,The BiDAF and Match - LSTM models are provided as two baseline systems .,result
natural_language_inference,3,"This is informative pattern for the relation prediction of these two sentences , whose ground truth is contradiction .",result,Understanding Behaviors of Neurons in C-LSTMs,0,186,19,12,0,result : Understanding Behaviors of Neurons in C-LSTMs,0.8942307692307693,0.76,0.6666666666666666,This is informative pattern for the relation prediction of these two sentences whose ground truth is contradiction ,18,"The activation in the patch , including the word pair "" ( red , green ) "" , is much higher than others .","An interesting thing is there are two words describing color in the sentence "" A person in a red shirt and black pants hunched over . "" .",result
sentiment_analysis,51,) Recursive networks :,model,model,1,129,4,4,0,model : model,0.86,0.3636363636363637,0.3636363636363637, Recursive networks ,4,"In this method , the word vectors pretrained on large text corpus such as Wikipedia dump are averaged to get the document vector , which is then fed to the sentiment classifier to compute the sentiment score .",Various types of recursive neural networks ( RNN ) have been applied on SST .,method
natural_language_inference,24,", to map both the passage and question lexical encodings into the same number of dimensions .",system description,Proposed model: SAN,0,58,31,31,0,system description : Proposed model: SAN,0.2478632478632479,0.3406593406593407,0.492063492063492, to map both the passage and question lexical encodings into the same number of dimensions ,17,"To prevent this , we employ an idea inspired by : use two separate two - layer positionwise Feed- Forward Networks ( FFN ) , F F N ( x ) = W 2 ReLU ( W 1 x + b 1 ) + b",Note that this FFN has fewer :,method
semantic_parsing,2,We compare our model ( COARSE2FINE ) against several previously published systems as well as various baselines .,analysis,Results and Analysis,0,253,2,2,0,analysis : Results and Analysis,0.8694158075601375,0.05882352941176471,0.05882352941176471,We compare our model COARSE2FINE against several previously published systems as well as various baselines ,16, ,"Specifically , we report results with a model which decodes meaning representations in one stage ( ONESTAGE ) without leveraging sketches .",result
text-classification,0,"To date , almost all techniques of text classification are based on words , in which simple statistics of some ordered word combinations ( such as n-grams ) usually perform the best .",introduction,introduction,0,15,4,4,0,introduction : introduction,0.06607929515418502,0.25,0.25,To date almost all techniques of text classification are based on words in which simple statistics of some ordered word combinations such as n grams usually perform the best ,30,The range of text classification research goes from designing the best features to choosing the best possible machine learning classifiers .,"On the other hand , many researchers have found convolutional networks ( ConvNets ) are useful in extracting information from raw signals , ranging from computer vision applications to speech recognition and others .",introduction
named-entity-recognition,4,Variational dropout is added to the input of both biLSTM layers .,model,model,0,260,39,39,0,model : model,0.9558823529411764,0.7647058823529411,0.7647058823529411,Variational dropout is added to the input of both biLSTM layers ,12,"During training , we use a CRF loss and at test time perform decoding using the Viterbi algorithm while ensuring that the output tag sequence is valid .",During training the gradients are rescaled if their 2 norm exceeds 5.0 and parameters updated using Adam with constant learning rate of 0.001 .,method
sentiment_analysis,16,"Looking at sentences ( 3 ) and ( 4 ) , we further see the importance of this problem and also why relying on attention mechanism alone is insufficient .",introduction,introduction,0,45,33,33,0,introduction : introduction,0.14150943396226415,0.825,0.825,Looking at sentences 3 and 4 we further see the importance of this problem and also why relying on attention mechanism alone is insufficient ,25,"Instead , its sentiment polarity is dependent on the given target .","In these two sentences , sentiment contexts are both "" high "" ( i.e. , same attention ) , but sentence ( 3 ) is negative and sentence ( 4 ) is positive simply because their target aspects are different .",introduction
text_generation,5,We use residual connection to speedup convergence and enable training of deeper models .,system description,Residual connection:,0,106,38,2,0,system description : Residual connection:,0.3593220338983051,0.5,0.2222222222222222,We use residual connection to speedup convergence and enable training of deeper models ,14, ,We use a residual block ( shown to the right ) similar to that of .,method
sentiment_analysis,17,The first two metrics are measures of correlation against human evaluations of semantic relatedness .,experiment,Semantic Relatedness,0,165,18,5,0,experiment : Semantic Relatedness,0.7333333333333333,0.6666666666666666,0.35714285714285715,The first two metrics are measures of correlation against human evaluations of semantic relatedness ,15,"Following , we use Pearson 's r , Spearman 's ? and mean squared error ( MSE ) as evalua - tion metrics .",We compare our models against a number of non -LSTM baselines .,experiment
relation-classification,0,https://dumps.wikimedia.org/enwiki/ 20150901/,experiment,End-to-end Relation Extraction Results,0,172,20,9,0,experiment : End-to-end Relation Extraction Results,0.7610619469026548,0.4651162790697674,0.28125,https dumps wikimedia org enwiki 20150901 ,7,stanford-corenlp-full-2015-04-20.zip,"We did not tune the precision - recall trade - offs , but doing so can specifically improve precision further .",experiment
natural_language_inference,29,"To evaluate the performance of our dataset and the proposed framework , we compare our model with the following baselines :",evaluation,evaluation,0,268,13,13,0,evaluation : evaluation,0.7342465753424657,0.4814814814814815,0.4814814814814815,To evaluate the performance of our dataset and the proposed framework we compare our model with the following baselines ,20,"In order to prove the effectiveness of each module in PAAG , we conduct some ablation models shown in .",1 ) S2SA : Sequence - to - sequence framework has been proposed for language generation task .,result
natural_language_inference,90,"Data preprocessing : Following Bowman et al. ( 2015 ) , we remove examples labeled "" - "" ( no gold label ) from the dataset , which leaves 549,367 pairs for training , 9,842 for development , and 9,824 for testing .",implementation,Implementation Details,0,113,3,3,1,implementation : Implementation Details,0.7533333333333333,0.17647058823529413,0.17647058823529413,Data preprocessing Following Bowman et al 2015 we remove examples labeled no gold label from the dataset which leaves 549 367 pairs for training 9 842 for development and 9 824 for testing ,34,The method was implemented in TensorFlow .,"We use the tokenized sentences from the non-binary parse provided in the dataset and prepend each sentence with a "" NULL "" token .",experiment
text_generation,2,"'m not going to be very proud of the other countries , "" he said .",SeqGAN,SeqGAN,0,331,13,13,0,SeqGAN : SeqGAN,0.9457142857142856,0.40625,0.40625, m not going to be very proud of the other countries he said ,15,"It is hard to buy on the Olympics , but we probably do n't see a lot of it .","He said the U. N. intelligence industry will not comment on the ground , which would be sensitive to the European Union .",others
natural_language_inference,94,The t th reasoning cell 's inputs are the previous step 's output ( {c t? 1 i } n i=1 ) and the embedded question ( {e Q i } m i =1 ) .,method,method,0,89,21,21,0,method : method,0.2323759791122716,0.16535433070866146,0.16535433070866146,The t th reasoning cell s inputs are the previous step s output c t 1 i n i 1 and the embedded question e Q i m i 1 ,31,"Our reasoning layer is composed of k reasoning cells ( see ) , where each incrementally updates the context representation .",It first creates step- specific context and query encodings via cell - specific bidirectional LSTMs :,method
natural_language_inference,96,"The bad ending is semantically or grammatically malformed , e.g. ' the man is getting out of the horse .",analysis,18.1%,0,278,19,3,0,analysis : 18.1%,0.7128205128205128,0.4871794871794872,1.0,The bad ending is semantically or grammatically malformed e g the man is getting out of the horse ,19,The bad ending seems redundant ; it is entailed by the context .,Ambiguous Both endings seem equally likely .,result
text_summarization,9,"is the input , G is the true headline , A is ABS + , and R is RAS - ELMAN .",result,Results,0,144,39,39,0,result : Results,0.9411764705882352,0.9069767441860463,0.9069767441860463,is the input G is the true headline A is ABS and R is RAS ELMAN ,17,Example sentence summaries produced on Gigaword .,amples highlight typical mistakes of the models .,result
text_generation,0,Compute Q ( a = yt ; s = Y1:t?1 ) by Eq. :,system description,SeqGAN via Policy Gradient,0,126,64,49,0,system description : SeqGAN via Policy Gradient,0.3888888888888889,0.5981308411214953,0.765625,Compute Q a yt s Y1 t 1 by Eq ,11,fort in 1 : T do 10 :,end for 12:,method
natural_language_inference,67,"First , replacing the word - by - word attention with Attentive Reader style attention decreases the EM score by about 4.5 % , showing the strength of our proposed attention mechanism .",experiment,Experiments,1,159,3,3,0,experiment : Experiments,0.7794117647058824,0.42857142857142855,0.42857142857142855,First replacing the word by word attention with Attentive Reader style attention decreases the EM score by about 4 5 showing the strength of our proposed attention mechanism ,29,We also did ablation tests on our DCR model .,"Second , we remove the features in input to see the contribution of each feature .",experiment
natural_language_inference,94,We also conducted experiments testing the effectiveness of our commonsense selection and incorporation techniques .,ablation,ablation,0,227,2,2,0,ablation : ablation,0.5926892950391645,0.25,0.25,We also conducted experiments testing the effectiveness of our commonsense selection and incorporation techniques ,15, ,"We first tried to naively add ConceptNet information by initializing the word embeddings with the ConceptNet - trained embeddings , NumberBatch ( Speer and Havasi , 2012 ) ( we also change embedding size from 256 to 300 ) .",result
sentiment_analysis,48,Effect of Labeled Data,ablation,Effect of Labeled Data,0,204,2,1,0,ablation : Effect of Labeled Data,0.8644067796610171,0.08,0.2,Effect of Labeled Data,4, , ,result
part-of-speech_tagging,4,This makes our task essentially to represent a full - exponential search space without making Markov assumptions .,introduction,introduction,1,22,14,14,0,introduction : introduction,0.0944206008583691,0.4117647058823529,0.4117647058823529,This makes our task essentially to represent a full exponential search space without making Markov assumptions ,17,"One main challenge , however , is that the number of possible label sequences is exponential to the size of input .",We tackle this challenge using a hierarchicallyrefined representation of marginal label distributions .,introduction
natural_language_inference,45,"Intuitively speaking , when the gate g has high values , more information flows from the character - level representation to the final representation ; when the gate g has low values , the final representation is dominated by the word - level representation .",system description,WORD-CHARACTER FINE-GRAINED GATING,0,107,39,17,0,system description : WORD-CHARACTER FINE-GRAINED GATING,0.5376884422110553,0.609375,0.7391304347826086,Intuitively speaking when the gate g has high values more information flows from the character level representation to the final representation when the gate g has low values the final representation is dominated by the word level representation ,39,An illustration of our fine - grained gating mechanism is shown in .,"Though also use agate to choose between word - level and character - level representations , our method is different in two ways .",method
machine-translation,7,"The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600,000 images .",abstract,abstract,0,40,38,38,0,abstract : abstract,0.10723860589812333,0.9268292682926828,0.9268292682926828,The existing literature on conditional computation deals with relatively small image recognition data sets consisting of up to 600 000 images ,22,Model capacity is most critical for very large data sets .,"It is hard to imagine that the labels of these images provide a sufficient signal to adequately train a model with millions , let alone billions of parameters .",abstract
natural_language_inference,30,This is cheap but also causes the data to be noisy .,training,Questions Generation,0,113,39,28,0,training : Questions Generation,0.437984496124031,0.8666666666666667,0.9032258064516128,This is cheap but also causes the data to be noisy ,12,These pairs have been labeled collaboratively .,"Hence , estimated that only 55 % of the pairs were actual paraphrases .",experiment
natural_language_inference,75,This can come at the cost of lower recall due to malformed or completely missing triplets .,model,Doc,0,144,95,15,0,model : Doc,0.7093596059113301,0.6168831168831169,0.75,This can come at the cost of lower recall due to malformed or completely missing triplets ,17,An IE - KB representation has attractive properties such as more precise and compact expressions of facts and logical key - value pairings based on subjectverb - object groupings .,For IE we use standard open - source software followed by some task - specific engineering to improve the results .,method
sentiment_analysis,3,Our KET variants KET SingleSelfAttn and KET StdAttn perform comparably with the best baselines on all datasets except IEMOCAP .,baseline,Comparison with Baselines,1,246,14,14,0,baseline : Comparison with Baselines,0.8424657534246576,0.8235294117647058,0.8235294117647058,Our KET variants KET SingleSelfAttn and KET StdAttn perform comparably with the best baselines on all datasets except IEMOCAP ,20,"This finding indicates that our model is robust across datasets with varying training sizes , context lengths and domains .","However , both variants perform noticeably worse than KET on all datasets except EC , validating the importance of our proposed hierarchical self - attention and dynamic context - aware affective graph attention mechanism .",result
natural_language_inference,2,"To implicitly infer the answer type , we also propose a max-attentional question aggregation mechanism to encode a question vector based on the important words in a question .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.033582089552238806,0.7777777777777778,0.7777777777777778,To implicitly infer the answer type we also propose a max attentional question aggregation mechanism to encode a question vector based on the important words in a question ,29,Multi-factor attentive encoding using tensor - based transformation aggregates meaningful facts even when they are located in multiple sentences .,"During prediction , we incorporate sequence - level encoding of the first wh-word and its immediately following word as an additional source of question type information .",abstract
text_summarization,8,Bottom - Up Attention 0.5 53.3 24.8 6.5 : % Novel shows the percentage of words in a summary that are not in the source document .,analysis,Analysis and Discussion,0,240,13,13,0,analysis : Analysis and Discussion,0.8391608391608392,0.38235294117647056,0.38235294117647056,Bottom Up Attention 0 5 53 3 24 8 6 5 Novel shows the percentage of words in a summary that are not in the source document ,28,"Similar to , we find that the decrease in ROUGE - 2 indicates alack of fluency and grammaticality of the generated summaries .",The last three columns show the part - of - speech tag distribution of the novel words in generated summaries .,result
sentiment_analysis,15,"When an n-gram is given to the compositional models , it is parsed into a binary tree and each leaf node , corresponding to a word , is represented as a vector .",model,Recursive Neural Models,0,94,5,5,0,model : Recursive Neural Models,0.3481481481481481,0.04672897196261682,0.2272727272727273,When an n gram is given to the compositional models it is parsed into a binary tree and each leaf node corresponding to a word is represented as a vector ,31,displays this approach .,Recursive neural models will then compute parent vectors in a bottom up fashion using different types of compositionality functions g.,method
natural_language_inference,27,"Specifically , we utilize the publicly available codebase 3 provided by , which replicates the Google 's NMT ( GNMT ) systems .",model,DATA AUGMENTATION BY BACKTRANSLATION,0,136,92,9,0,model : DATA AUGMENTATION BY BACKTRANSLATION,0.40236686390532544,0.6917293233082706,0.6428571428571429,Specifically we utilize the publicly available codebase 3 provided by which replicates the Google s NMT GNMT systems ,19,"In this work , we consider attention - based neural machine translation ( NMT ) models ; , which have demonstrated excellent translation quality , as the core models of our data augmentation pipeline .",We train 4 - layer GNMT models on the public WMT data for both English - French 4 ( 36M sentence pairs ) and English - German 5 ( 4.5 M sentence pairs ) .,method
natural_language_inference,7,"However , RASOR assigns almost as much probability mass to it 's incorrect third prediction "" British "" as it does to the top scoring correct prediction "" Egyptian "" .",model,Learning objective EM F1,0,170,45,33,0,model : Learning objective EM F1,0.9550561797752808,0.9782608695652174,0.9705882352941176,However RASOR assigns almost as much probability mass to it s incorrect third prediction British as it does to the top scoring correct prediction Egyptian ,26,"The top predictions for both examples are all valid syntactic constituents , and they all have the correct semantic category .","This showcases a common failure case for RASOR , where it can find an answer of the correct type close to a phrase that overlaps with the question - but it can not accurately represent the semantic dependency on that phrase .",method
text-to-speech_synthesis,2,"In order to address safety concerns consistent with principles such as , we verify that voices generated by the proposed model can easily be distinguished from real voices .",introduction,introduction,0,15,7,7,0,introduction : introduction,0.061224489795918366,0.18421052631578946,0.18421052631578946,In order to address safety concerns consistent with principles such as we verify that voices generated by the proposed model can easily be distinguished from real voices ,28,"However , it is also important to note the potential for misuse of this technology , for example impersonating someone 's voice without their consent .","Synthesizing natural speech requires training on a large number of high quality speech - transcript pairs , and supporting many speakers usually uses tens of minutes of training data per speaker .",introduction
natural_language_inference,79,"There , we have p ( w t |w t?k , ... , w t+k ) = e yw ti e yi",system description,Learning Vector Representation of Words,0,63,17,14,0,system description : Learning Vector Representation of Words,0.23507462686567165,0.5,0.4666666666666667,There we have p w t w t k w t k e yw ti e yi,17,"The prediction task is typically done via a multiclass classifier , such as softmax .","Each of y i is un - normalized log - probability for each output word i , computed as",method
natural_language_inference,11,We denote the hidden dimensionality of ... :,system description,Refining Word Embeddings by Reading,0,59,25,14,0,system description : Refining Word Embeddings by Reading,0.213768115942029,0.44642857142857145,0.7368421052631579,We denote the hidden dimensionality of ,7,"In the following , we define this procedure formally .","Illustration of our context - dependent , refinement strategy for word representations on an example from the SNLI dataset comprising the premise ( X 1 = {p} ) , hypothesis ( X 2 = {q} ) and additional external information inform of free - text assertions from ConceptNet ( X 1 = A ) .",method
text-classification,6,The other targets efficient inference with slightly reduced accuracy .,model,Encoders,0,42,14,5,0,model : Encoders,0.28378378378378377,0.4375,1.0,The other targets efficient inference with slightly reduced accuracy ,10,One based on the transformer architecture targets high accuracy at the cost of greater model complexity and resource consumption ., ,method
text_summarization,8,man food his first hamburger wrongfully for 36 years .,analysis,Analysis and Discussion,0,243,16,16,0,analysis : Analysis and Discussion,0.8496503496503497,0.4705882352941176,0.4705882352941176,man food his first hamburger wrongfully for 36 years ,10,typical example looks like this :,"michael hanline , 69 , was convicted of murder for the shooting of truck driver jt mcgarry in 1980 on judge charges .",result
natural_language_inference,89,"As shown in , our system consists of two components : ( 1 ) a no-answer reader for extracting candidate answers and detecting unanswerable questions , and ( 2 ) an answer verifier for deciding whether or not the extracted candidate is legitimate .",introduction,introduction,1,27,18,18,0,introduction : introduction,0.10384615384615384,0.5,0.5,As shown in our system consists of two components 1 a no answer reader for extracting candidate answers and detecting unanswerable questions and 2 an answer verifier for deciding whether or not the extracted candidate is legitimate ,38,"To address the above issue , we propose a read - then - verify system that aims to be robust to unanswerable questions in this paper .",The key contributions of our work are three - fold .,introduction
sentiment_analysis,48,http://nlp.stanford.edu/data/glove.8B.300d.zip,system description,Model Configuration & Classifiers,0,167,19,19,0,system description : Model Configuration & Classifiers,0.7076271186440678,0.59375,0.59375,http nlp stanford edu data glove 8B 300d zip,9,IAN : IAN adopts two LSTMs to derive the representations of the context and the target phrase interactively and the concatenation is fed to the softmax layer .,BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and makes use of a special gate layer to combine these two representations .,method
machine-translation,7,Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively .,APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,257,35,7,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6890080428954424,0.2317880794701987,0.7,Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively ,22,Our metrics of expert utilization change to the following :,( i ) denotes the subset of X for which G primary ( x ) i >,others
sentence_classification,1,"For individual sentence encoding , we propose the use of a CNN module as an alternative to RNN for small datasets , suffering less from over-fitting as evidenced by our experiments .",introduction,introduction,0,30,23,23,0,introduction : introduction,0.17341040462427745,0.8214285714285714,0.8214285714285714,For individual sentence encoding we propose the use of a CNN module as an alternative to RNN for small datasets suffering less from over fitting as evidenced by our experiments ,31,We remove the need for a character - based word embedding component without sacrificing performance .,"Moreover , we incorporate attention - based pooling in both RNN and CNN models to further improve the performance .",introduction
question_answering,3,"Unless stated otherwise , the encoder in the pointer layer for span prediction models also uses DCU .",method,Our Methods,0,203,28,8,0,method : Our Methods,0.725,0.8235294117647058,0.5714285714285714,Unless stated otherwise the encoder in the pointer layer for span prediction models also uses DCU ,17,The encompassing framework for DCU is the Bi- Attentive models described for MCQ - based problems and span prediction problems .,"However , for the Hybrid DCU - LSTM models , answer pointer layers use BiLSTMs .",method
machine-translation,7,"We use an attention mechanism between the encoder and decoder , with the first decoder LSTM receiving output from and providing input for the attention 5 .",APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,326,104,7,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8739946380697051,0.6887417218543046,0.21875,We use an attention mechanism between the encoder and decoder with the first decoder LSTM receiving output from and providing input for the attention 5 ,26,We insert MoE layers in both the encoder ( between layers 2 and 3 ) and the decoder ( between layers 1 and 2 ) .,All of the layers in our model have input and output dimensionality of 512 .,others
question-answering,7,The SPINN - PI model is similar to NSE in spirit that it also explicitly computes word composition .,experiment,Natural Language Inference,0,153,31,19,0,experiment : Natural Language Inference,0.5563636363636364,0.4025974025974026,0.7037037037037037,The SPINN PI model is similar to NSE in spirit that it also explicitly computes word composition ,18,"While most of the sentence encoder models rely solely on word embeddings , the dependency tree CNN and the SPINN - PI models make use of sentence parser output .","However , the composition in the SPINN - PI is guided by supervisions from a dependency parser .",experiment
sentiment_analysis,23,"Compared with human engineering , neural networks serve as away of automatic feature learning .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.06506849315068493,0.375,0.375,Compared with human engineering neural networks serve as away of automatic feature learning ,14,extend such approaches to learn sentences ' and paragraphs ' representations .,Two widely used neural sentence models are convolutional neural networks ( CNNs ) and recursive neural networks ( RNNs ) .,introduction
sentiment_analysis,41,"Besides , the attention can detect multiple keywords if more than one keyword is existing .",analysis,Qualitative Analysis,0,204,13,13,0,analysis : Qualitative Analysis,0.9147982062780268,0.52,0.9285714285714286,Besides the attention can detect multiple keywords if more than one keyword is existing ,15,"In ( a ) , "" fastest delivery times "" is a multi-word phrase , but our attention - based model can detect such phrases if service can is the input aspect .","In , tastless and too sweet are both detected .",result
relation_extraction,13,"We report results on all of the tasks from Section 3.1 , using the same task - specific training methodology for both BERT EM and BERT EM + MTB .",experiment,Experimental Evaluation,0,183,5,5,0,experiment : Experimental Evaluation,0.8591549295774648,0.19230769230769232,1.0,We report results on all of the tasks from Section 3 1 using the same task specific training methodology for both BERT EM and BERT EM MTB ,28,We train the BERT EM + MTB model by initializing the Transformer weights to the weights from BERT LARGE and use the following parameters :, ,experiment
natural_language_inference,9,0.00 1.00 0.00,model,VISUALIZATIONS,0,319,32,25,0,model : VISUALIZATIONS,0.9666666666666668,0.7441860465116279,0.6944444444444444,0 00 1 00 0 00,6,Let 's do it .,Great let me do the reservation .,method
natural_language_inference,45,where q j can be viewed as agate to filter the information in pi .,system description,DOCUMENT-QUERY FINE-GRAINED GATING,0,127,59,14,0,system description : DOCUMENT-QUERY FINE-GRAINED GATING,0.6381909547738693,0.921875,0.7368421052631579,where q j can be viewed as agate to filter the information in pi ,15,"More specifically , for pi and q j , we have",We then use an attention mechanism over I ij to output hidden states hi as follows,method
natural_language_inference,76,"Despite the success of neural networks for paraphrase detection ( e.g. , end - to - end differentiable neural architectures failed to get close to acceptable performance for RTE due to the lack of large high - quality datasets .",introduction,introduction,0,19,6,6,0,introduction : introduction,0.13013698630136986,0.4,0.4,Despite the success of neural networks for paraphrase detection e g end to end differentiable neural architectures failed to get close to acceptable performance for RTE due to the lack of large high quality datasets ,36,"State - of - the - art systems for RTE so far relied heavily on engineered NLP pipelines , extensive manual creation of features , as well as various external resources and specialized subcomponents such as negation detection ( e.g. .","An end - to - end differentiable solution to RTE is desirable , since it avoids specific assumptions about the underlying language .",introduction
natural_language_inference,34,Most previous work focus on finding evidence and answers from a single paragraph .,introduction,introduction,0,15,4,4,0,introduction : introduction,0.05084745762711865,0.3076923076923077,0.3076923076923077,Most previous work focus on finding evidence and answers from a single paragraph ,14,QA provides a quantifiable way to evaluate an NLP system 's capability on language understanding and reasoning .,It rarely tests deep reasoning capabilities of the underlying model .,introduction
natural_language_inference,28,RUM learns the Copying Memory task completely and improves the state - of - the - art result in the Recall task .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.04059040590405904,0.6923076923076923,0.6923076923076923,RUM learns the Copying Memory task completely and improves the state of the art result in the Recall task ,20,"We evaluate our model on synthetic memorization , question answering and language modeling tasks .",RUM 's performance in the bAbI Question Answering task is comparable to that of models with attention mechanism .,abstract
natural_language_inference,61,"erefore , our a ESIM model was able to capture the most important word pair in each pair of sentences .",model,Models,0,150,8,8,0,model : Models,0.9615384615384616,1.0,1.0,erefore our a ESIM model was able to capture the most important word pair in each pair of sentences ,20,"We could conclude that our a ESIM model had the higher weight than ESIM model on each keyword pair , especially in.b , where the similarity of ' happy ' and ' grin ' in aESIM model is much higher than that in ESIM model .", ,method
semantic_role_labeling,4,replaced stacked BiLSTMs with selfattention architectures .,system description,system description,0,265,22,22,0,system description : system description,0.8833333333333333,0.7857142857142857,0.7857142857142857,replaced stacked BiLSTMs with selfattention architectures ,7,"Instead of CNNs , and used stacked BiLSTMs and achieved strong performance without syntactic inputs .",improved the self - attention SRL model by incorporating syntactic information .,method
sentiment_analysis,48,The models are evaluated on two benchmarks : Restaurant ( REST ) and Laptop ( LAPTOP ) datasets from the SemEval ATSA challenge .,dataset,Datasets and Preparation,0,130,2,2,0,dataset : Datasets and Preparation,0.5508474576271186,0.1,0.1,The models are evaluated on two benchmarks Restaurant REST and Laptop LAPTOP datasets from the SemEval ATSA challenge ,19, ,"The REST dataset contains the reviews in the restaurant domain , while the LAPTOP dataset contains the reviews of Laptop products .",experiment
natural_language_inference,28,Their flexibility of taking inputs of dynamic length makes RNN particularly useful for these tasks .,introduction,introduction,0,18,3,3,0,introduction : introduction,0.06642066420664207,0.125,0.125,Their flexibility of taking inputs of dynamic length makes RNN particularly useful for these tasks ,16,"Recurrent neural networks are widely used in a variety of machine learning applications such as language modeling ) , machine translation ) and speech recognition ) .","However , the traditional RNN models such as Long Short - Term Memory ( LSTM , ) and Gated Recurrent Unit ( GRU , ) exhibit some weaknesses that prevent them from achieving human level performance :",introduction
natural_language_inference,91,We evaluate SPINN on the task of natural language inference .,experiment,NLI Experiments,0,152,2,2,0,experiment : NLI Experiments,0.6523605150214592,0.06451612903225806,0.18181818181818185,We evaluate SPINN on the task of natural language inference ,11, ,"NLI is a sentence pair classification task , in which a model reads two sentences ( a premise and a hypothesis ) , and outputs a judgment of entailment , contradiction , or neutral , reflecting the relationship between the meanings of the two sentences .",experiment
text-classification,0,Each epoch takes a fixed number of random training samples uniformly sampled across classes .,system description,Key Modules,0,52,25,22,0,system description : Key Modules,0.2290748898678414,0.7352941176470589,0.9166666666666666,Each epoch takes a fixed number of random training samples uniformly sampled across classes ,15,"The algorithm used is stochastic gradient descent ( SGD ) with a minibatch of size 128 , using momentum 0.9 and initial step size 0.01 which is halved every 3 epoches for 10 times .",This number will later be detailed for each dataset sparately .,method
natural_language_inference,27,"Each of these basic operations ( conv / self - attention / ffn ) is placed inside a residual block , shown lower - right in .",model,MODEL OVERVIEW,0,90,46,38,0,model : MODEL OVERVIEW,0.26627218934911245,0.3458646616541353,0.6666666666666666,Each of these basic operations conv self attention ffn is placed inside a residual block shown lower right in ,20,The number of heads is 8 throughout all the layers .,"For an input x and a given operation f , the output is f ( layernorm ( x ) ) + x , meaning there is a full identity path from the input to output of each block , where layernorm indicates layer - normalization proposed in .",method
question_answering,1,"We also provide an in - depth ablation study of our model on the SQuAD development set , visualize the intermediate feature spaces in our model , and analyse its performance as compared to a more traditional language model for machine comprehension .",introduction,introduction,0,32,24,24,0,introduction : introduction,0.10094637223974763,1.0,1.0,We also provide an in depth ablation study of our model on the SQuAD development set visualize the intermediate feature spaces in our model and analyse its performance as compared to a more traditional language model for machine comprehension ,40,"With a modification to only the output layer , BIDAF achieves the state - of - the - art results on the CNN / DailyMail cloze test .", ,introduction
natural_language_inference,68,This vector ? ? z i is fed into a standard one - directional LSTM to form our so - called match - LSTM :,method,Answer Pointer Layer,0,119,69,17,0,method : Answer Pointer Layer,0.4779116465863454,0.6052631578947368,0.27419354838709675,This vector z i is fed into a standard one directional LSTM to form our so called match LSTM ,20,"Next , we use the attention weight vector ? ? ? i to obtain a weighted version of the question and combine it with the current token of the passage to form a vector ? ? z i :",This vector ? ? z i is fed into a standard one - directional LSTM to form our so - called match - LSTM :,method
relation_extraction,0,We leave it for future work .,analysis,Entities,0,217,9,4,0,analysis : Entities,0.84765625,0.2647058823529412,0.3076923076923077,We leave it for future work ,7,"Since we propose a jointmodel , we can not directly apply their pretraining trick on entities separately .","mentioned in their analysis of the dataset that there were many "" UNK "" tokens in the test set which were never seen during training .",result
natural_language_inference,52,", for each answer candidate ( i.e. , IR ++ 0 = 1.0 for the top - ranked candidate in the first ranking , IR ++",model,Semi-Lexicalized Discourse features (lexDisc):,0,127,45,20,0,model : Semi-Lexicalized Discourse features (lexDisc):,0.4865900383141762,0.8035714285714286,0.9523809523809524, for each answer candidate i e IR 0 1 0 for the top ranked candidate in the first ranking IR ,22,"We then use these rankings to make a set of four reciprocal rank features , IR ++ 0 , ... , IR ++","= 0.5 for the next candidate , etc . )",method
sentence_classification,1,"For regularization , dropout ( Srivastava et al. , 2014 ) is applied to each layer .",training,Training Settings,1,117,10,10,1,training : Training Settings,0.6763005780346821,0.625,0.625,For regularization dropout Srivastava et al 2014 is applied to each layer ,13,The learning rate is initially set as 0.003 and decayed by 0.9 after each epoch .,"For the version of dropout used in practice ( e.g. , the dropout function implemented in the TensorFlow and Pytorch libraries ) , the model ensemble generated by dropout in the training phase is approximated by a single model with scaled weights in the inference phase , resulting in a gap between training and inference .",experiment
natural_language_inference,72,"In our experience , the annotation of skills proved quite challenging due to certain confusables .",analysis,Analysis of comprehension skills,0,143,13,13,0,analysis : Analysis of comprehension skills,0.4583333333333333,0.8666666666666667,0.8666666666666667,In our experience the annotation of skills proved quite challenging due to certain confusables ,15,The fraction of these cases was around 16 % .,"For example , object tracking and coreference both need to maintain the link between objects ; object tracking , which includes establishing set relations and membership , maybe overlaid with the schematic clause relation skill ( subordination ) ; and bridging inference can overlap with coreference resolution .",result
natural_language_inference,49,"The sequence length is set as a hard cutoff on all experiments : 48 for MultiNLI , 32 for SNLI and 24 for Quora Question Pair Dataset .",experiment,EXPERIMENTS SETTING,1,155,41,20,0,experiment : EXPERIMENTS SETTING,0.610236220472441,0.5857142857142857,0.8695652173913043,The sequence length is set as a hard cutoff on all experiments 48 for MultiNLI 32 for SNLI and 24 for Quora Question Pair Dataset ,26,The first scale down ratio ? in feature extraction layer is set to 0.3 and transitional scale down ratio ? is set to 0.5 .,"During the experiments on MultiNLI , we use 15 % of data from SNLI as in .",experiment
sentiment_analysis,30,"We use the following hyper - parameters for weight matrices in both directions : R ? R 3003 , H , U , V , Ware all matrices of size R 300300 , v ? R 300 , and hidden size of the GRU in Equation is 300 .",experiment,Experimental Setup,1,92,13,12,0,experiment : Experimental Setup,0.71875,0.41935483870967744,0.4,We use the following hyper parameters for weight matrices in both directions R R 3003 H U V Ware all matrices of size R 300300 v R 300 and hidden size of the GRU in Equation is 300 ,39,Training is carried out over 800 epochs with the FTRL optimiser and a batch size of 128 and learning rate of 0.05 .,Dropout is applied to the output of ? in the final classifier ( Equation ) with a rate of 0.2 .,experiment
text_summarization,3,RAS - Elman ) is a convolution encoder and an Elman RNN decoder with attention .,system description,Distant Supervision (DS) for Model Adaption,1,179,124,51,0,system description : Distant Supervision (DS) for Model Adaption,0.7396694214876033,0.9465648854961832,0.8793103448275862,RAS Elman is a convolution encoder and an Elman RNN decoder with attention ,14,is a two - layer LSTM encoder - decoder .,Seq2seq + att is two - layer BiLSTM encoder and one - layer LSTM decoder equipped with attention .,method
natural_language_inference,88,"In all cases , we train LSTMN models end - to - end with task - specific supervision signals , achieving performance comparable or better to state - of - the - art models and superior to vanilla LSTMs .",introduction,introduction,0,52,33,33,0,introduction : introduction,0.21052631578947367,1.0,1.0,In all cases we train LSTMN models end to end with task specific supervision signals achieving performance comparable or better to state of the art models and superior to vanilla LSTMs ,32,"We validate the performance of the LSTMN in language modeling , sentiment analysis , and natural language inference .", ,introduction
machine-translation,5,"In this paper , we compare the MLSTMbased models with Transformer models for English - Estonian and Estonian - English and we show that the state - of - the - art of WMT 2017 is well behind the new models .",introduction,introduction,0,20,13,13,0,introduction : introduction,0.13986013986013987,0.8125,0.8125,In this paper we compare the MLSTMbased models with Transformer models for English Estonian and Estonian English and we show that the state of the art of WMT 2017 is well behind the new models ,36,"In WMT 2017 , Tilde participated with MLSTM - based NMT systems .","Therefore , for WMT 2018 , Tilde submitted NMT systems that were trained using Transformer models .",introduction
natural_language_inference,27,"All the out - of - vocabulary words are mapped to an < UNK > token , whose embedding is trainable with random initialization .",model,MODEL OVERVIEW,0,75,31,23,0,model : MODEL OVERVIEW,0.22189349112426035,0.2330827067669173,0.4035087719298245,All the out of vocabulary words are mapped to an UNK token whose embedding is trainable with random initialization ,20,"The word embedding is fixed during training and initialized from the p 1 = 300 dimensional pre-trained Glo Ve word vectors , which are fixed during training .",The character embedding is obtained as follows :,method
relation_extraction,12,where Q and K are both equal to the collective representation h ( l?1 ) at layer l ? 1 of the AG - GCN model .,system description,Attention Guided Layer,0,104,49,24,0,system description : Attention Guided Layer,0.3161094224924012,0.494949494949495,0.7272727272727273,where Q and K are both equal to the collective representation h l 1 at layer l 1 of the AG GCN model ,24,"The output is computed as a weighted sum of the values , where the weight is computed by a function of the query with the corresponding key .",where Q and K are both equal to the collective representation h ( l?1 ) at layer l ? 1 of the AG - GCN model .,method
sentiment_analysis,19,Let ? L p represent a LSTM that ar Xiv : 1805.07340v2 [ cs.LG ],abstract,abstract,0,15,13,13,0,abstract : abstract,0.09803921568627452,0.9285714285714286,0.9285714285714286,Let L p represent a LSTM that ar Xiv 1805 07340v2 cs LG ,14,"We use s[i : j ] to denote the sequence of embeddings of the tokens from s [ i ] to s [ j ] , where j maybe less than i .",Let ? L p represent a LSTM that ar Xiv : 1805.07340v2 [ cs.LG ],abstract
natural_language_inference,64,"We built a large scale dataset to enable the transfer step , exploiting the Natural Questions dataset .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.03187250996015936,0.5454545454545454,0.5454545454545454,We built a large scale dataset to enable the transfer step exploiting the Natural Questions dataset ,17,"We demonstrate the benefits of our approach for answer sentence selection , which is a well - known inference task in Question Answering .","Our approach establishes the state of the art on two well - known benchmarks , WikiQA and TREC - QA , achieving MAP scores of 92 % and 94.3 % , respectively , which largely outperform the previous highest scores of 83.4 % and 87.5 % , obtained in very recent work .",abstract
machine-translation,5,"As NMT systems are sensitive to noise in parallel data , all parallel data were filtered using the parallel data filtering methods described by .",system,Data Filtering,0,50,27,2,0,system : Data Filtering,0.34965034965034963,0.3648648648648649,0.13333333333333333,As NMT systems are sensitive to noise in parallel data all parallel data were filtered using the parallel data filtering methods described by ,24, ,"The parallel corpora filtering methods remove sentence pairs that have indications of data corruption or low parallelity ( e.g. , source - target length ratio , content overlap , digit mismatch , language adherence , etc. ) issues .",method
natural_language_inference,42,"As suggested in , we have chosen the Adam optimizer with the same hyperparameters , except for the learning rate , which was divided by two in our implementation .",experiment,experiment,0,220,5,5,0,experiment : experiment,0.7612456747404844,0.5,0.5,As suggested in we have chosen the Adam optimizer with the same hyperparameters except for the learning rate which was divided by two in our implementation ,27,We pre-processed the texts with the NLTK Tokenizer .,"For regularization , we applied residual and attention dropout of 0.9 in processing layers and of 0.8 in the reduction layer .",experiment
named-entity-recognition,8,"To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C ? R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .",experiment,GLUE,0,158,6,4,0,experiment : GLUE,0.4082687338501292,0.08450704225352113,0.05797101449275361,To fine tune on GLUE we represent the input sequence for single sentence or sentence pairs as described in Section 3 and use the final hidden vector C R H corresponding to the first input token CLS as the aggregate representation ,42,Detailed descriptions of GLUE datasets are included in Appendix B.1 .,"To fine - tune on GLUE , we represent the input sequence ( for single sentence or sentence pairs ) as described in Section 3 , and use the final hidden vector C ? R H corresponding to the first input token ( [ CLS ] ) as the aggregate representation .",experiment
relation_extraction,4,"The first slot asks about fillers of a relation with the query entity as the subject ( Mike Penner ) , and we term this a hop - 0 slot ; the second slot asks about fillers with the system 's hop - 0 output as the subject , and we term this a hop - 1 slot .",evaluation,evaluation,0,152,18,18,0,evaluation : evaluation,0.7342995169082126,0.6206896551724138,0.6206896551724138,The first slot asks about fillers of a relation with the query entity as the subject Mike Penner and we term this a hop 0 slot the second slot asks about fillers with the system s hop 0 output as the subject and we term this a hop 1 slot ,51,slot filling system is asked to answer a series of queries with two - hop slots ) :,"System predictions are then evaluated against gold annotations , and micro-averaged precision , recall and F 1 scores are calculated at the hop - 0 and hop - 1 levels .",result
part-of-speech_tagging,1,"To address those open questions in Section 1 , we conduct detailed experiments and empirical comparisons on different state - of - the - art character embedding models across different domains .",method,method,0,193,2,2,0,method : method,0.772,0.047619047619047616,0.047619047619047616,To address those open questions in Section 1 we conduct detailed experiments and empirical comparisons on different state of the art character embedding models across different domains ,28, ,"Firstly , we use LSTM - CRF with randomly initialized word embeddings as our initial baseline .",method
natural_language_inference,72,"Realizing that the presence of entities makes the task easier for the machines , Hermann et al. anonymize the entities , also with a goal of discouraging language model solutions to the queries .",system description,Task formulation,0,115,39,7,1,system description : Task formulation,0.3685897435897436,0.8863636363636364,0.5833333333333334,Realizing that the presence of entities makes the task easier for the machines Hermann et al anonymize the entities also with a goal of discouraging language model solutions to the queries ,32,"Although this simplifies the task , it also makes it less realistic as the entities may not be recognized at test time .","In our case , it is not clear how relevant the anonymization is since we deal with medical entities , which have different properties than proper name entities .",method
natural_language_inference,97,"Finally , this word - level sliding window operates on two different views of text sentences : the sequential view , where words appear in their natural order , and the dependency view , where words are reordered based on a linearization of the sentence 's dependency graph .",introduction,introduction,0,33,22,22,0,introduction : introduction,0.11301369863013698,0.5945945945945946,0.5945945945945946,Finally this word level sliding window operates on two different views of text sentences the sequential view where words appear in their natural order and the dependency view where words are reordered based on a linearization of the sentence s dependency graph ,43,"We also use a sliding window acting on a subsentential scale ( inspired by the work of ) , which implicitly considers the linear distance between matched words .",Words are represented throughout by embedding vectors .,introduction
text_generation,2,"As such , the MANAGER is trained to mimic the transition of real text samples in the feature space .",methodology,Hierarchical Structure of G,0,126,56,34,0,methodology : Hierarchical Structure of G,0.36,0.918032786885246,0.8717948717948718,As such the MANAGER is trained to mimic the transition of real text samples in the feature space ,19,"wheref t = F (? t ) , ? t and ? t+c are states of real text , and the state - action value Q F ( s t , gt ) in Eq. is set as 1 here since the data instances used in pre-training are all real sentences .",While the WORKER is trained via maximum likelihood estimation ( MLE ) .,method
sentiment_analysis,48,"This module attempts to extract the lexical feature that is independent of the label y when given data sample ( x , a ) .",method,Transformer Encoder,0,103,41,3,0,method : Transformer Encoder,0.4364406779661017,0.6307692307692307,0.21428571428571427,This module attempts to extract the lexical feature that is independent of the label y when given data sample x a ,22,"The encoder plays the role of q ? ( z |x , a , y ) .","In this way , the z and y jointly form the representative vector for the input data .",method
sentiment_analysis,36,where h ( L ),model,Convolutional Feature Extractor,0,130,83,13,0,model : Convolutional Feature Extractor,0.52,0.8829787234042553,0.5416666666666666,where h L ,4,"Then we feed the weighted h ( L ) to the convolutional layer , i.e. , the top - most layer in , to generate the feature map c ? R n?s+1 as follows :","i:i+s?1 ? R sdim h is the concatenated vector of ? i+s?1 , and sis the kernel size .",method
natural_language_inference,27,"While adding more augmented data with French as a pivot does not provide performance gain , injecting additional augmented data En - De - En of the same amount brings another 0.2 improvement in F1 , as indicated in entry "" data augmentation 3 ( 1:1:1 ) "" .",analysis,ABALATION STUDY AND ANALYSIS,0,256,13,13,0,analysis : ABALATION STUDY AND ANALYSIS,0.7573964497041421,0.35135135135135137,0.5909090909090909,While adding more augmented data with French as a pivot does not provide performance gain injecting additional augmented data En De En of the same amount brings another 0 2 improvement in F1 as indicated in entry data augmentation 3 1 1 1 ,44,"Making the training data twice as large by adding the En - Fr - En data only ( ratio 1:1 between original training data and augmented data , as indicated by row "" data augmentation 2 ( 1:1:0 ) "" ) yields an increase in the F1 by 0.5 percent .","We may attribute this gain to the diversity of the new data , which is produced by the translator of the new language .",result
sentiment_analysis,40,mayor may not be tuned in the training of our framework .,model,Input Embedding,0,75,10,6,0,model : Input Embedding,0.336322869955157,0.16129032258064516,0.75,mayor may not be tuned in the training of our framework ,12,"The input module retrieves the word vectors from L for an input sequence and gets a list of vectors {v 1 , . . . , v t , . . . , v T } where v t ? Rd .","If it is not tuned , the model can utilize the words ' similarity revealed in the original embedding space .",method
natural_language_inference,41,Token masking is crucial,result,Results,0,134,7,7,0,result : Results,0.5214007782101168,0.28,0.7,Token masking is crucial,4,"For example , a simple language model achieves the best ELI5 performance , but the worst SQUAD results .",Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation .,result
sentiment_analysis,9,"Similar to name entity recognition ( NER ) task , the ATE task is a kind of sequence labeling task , and prepare the input based on IOB labels .",system description,Aspect Term Extraction,0,57,2,2,0,system description : Aspect Term Extraction,0.20577617328519854,0.25,0.4,Similar to name entity recognition NER task the ATE task is a kind of sequence labeling task and prepare the input based on IOB labels ,26, ,"We design the IOB labels as , , , and the labels indicate the beginning , inside and outside of the aspect terms , respectively .",method
relation-classification,7,"With the progress in natural language processing ( NLP ) , extracting valuable information from biomedical literature has gained popularity among researchers , and deep learning has boosted the development of effective biomedical text mining models .",abstract,abstract,1,6,4,4,0,abstract : abstract,0.03015075376884422,0.3076923076923077,0.3076923076923077,With the progress in natural language processing NLP extracting valuable information from biomedical literature has gained popularity among researchers and deep learning has boosted the development of effective biomedical text mining models ,33,Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows .,"However , directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora .",abstract
sentence_classification,0,"The language expressed in citation sentences is likely distinctive from regular sentences in scientific writing , and such information could also be useful for better language modeling of the citation contexts .",model,Structural scaffolds,0,54,27,8,0,model : Structural scaffolds,0.20224719101123595,0.54,0.25806451612903225,The language expressed in citation sentences is likely distinctive from regular sentences in scientific writing and such information could also be useful for better language modeling of the citation contexts ,31,"The first scaffold task that we consider is "" citation worthiness "" of a sentence , indicating whether a sentence needs a citation .","To this end , using citation markers such as "" [ 12 ] "" or "" Lee et al ( 2010 ) "" , we identify sentences in a paper that include citations and the negative samples are sentences without citation markers .",method
part-of-speech_tagging,4,"In each layer , each input words is represented together with its marginal label probabilities , and a sequence neural network is employed to model unbounded dependencies .",introduction,introduction,0,25,17,17,0,introduction : introduction,0.1072961373390558,0.5,0.5,In each layer each input words is represented together with its marginal label probabilities and a sequence neural network is employed to model unbounded dependencies ,26,"As shown in , our model consists of a multi - layer neural network .","The marginal distributions space are refined hierarchically bottom - up , where a higher layer learns a more informed label sequence distribution based on information from a lower layer .",introduction
sentiment_analysis,48,"In our implementation , we use a bidirectional encoder to construct sentences embeddings .",method,Transformer Encoder,0,105,43,5,0,method : Transformer Encoder,0.4449152542372881,0.6615384615384615,0.35714285714285715,In our implementation we use a bidirectional encoder to construct sentences embeddings ,13,"In this way , the z and y jointly form the representative vector for the input data .","It is referred as the Transformer encoder that is actually a sub-graph of the Transformer architechture , the architecture is shown in the left part of the .",method
natural_language_inference,3,Stacking C - LSTMs Blocks,model,Stacking C-LSTMs Blocks,0,121,98,1,0,model : Stacking C-LSTMs Blocks,0.5817307692307693,0.8672566371681416,0.5,Stacking C LSTMs Blocks,4, , ,method
sarcasm_detection,1,"However , a sarcastic sentence can be expressed with contextual presumptions , background and commonsense knowledge .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.01497005988023952,0.42857142857142855,0.42857142857142855,However a sarcastic sentence can be expressed with contextual presumptions background and commonsense knowledge ,15,"The literature in automated sarcasm detection has mainly focused on lexical , syntactic and semantic - level analysis of text .","In this paper , we propose CASCADE ( a ContextuAl SarCasm DEtector ) that adopts a hybrid approach of both content and context - driven modeling for sarcasm detection in online social media discussions .",abstract
relation_extraction,13,"Let s 1 = ( i , j ) and s 2 = ( k , l ) be pairs of integers such that 0 < i < j ? 1 , j < k , k ? l ? 1 , and l ? n.",system description,Overview,0,35,5,5,0,system description : Overview,0.1643192488262911,0.14285714285714285,0.35714285714285715,Let s 1 i j and s 2 k l be pairs of integers such that 0 i j 1 j k k l 1 and l n ,29,"Formally , let x = [ x 0 . . . x n ] be a sequence of tokens , where x 0 = [ CLS ] and x n = [ SEP ] are special start and end markers .","relation statement is a triple r = ( x , s 1 , s 2 ) , where the indices in s 1 and s 2 delimit entity mentions in x : the sequence [ x i . . . x j?1 ] mentions an entity , and so does the sequence [ x k . . . x l?1 ] .",method
sentence_classification,0,"The best result is achieved when we also add ELMo vectors to the input representations in ' BiLSTM - Attn w / ELMo + both scaffolds ' , achieving an F1 of 67.9 , a major improvement from the previous state - of - the - art results of 54.6 ( ?= 13.3 ) .",result,Results,1,185,7,7,0,result : Results,0.6928838951310862,0.3181818181818182,0.3181818181818182,The best result is achieved when we also add ELMo vectors to the input representations in BiLSTM Attn w ELMo both scaffolds achieving an F1 of 67 9 a major improvement from the previous state of the art results of 54 6 13 3 ,45,"When both scaffolds are used simultaneously in ' BiLSTM - Attn + both scaffolds ' , the F1 score further improves to 63.1 ( ?= 11.3 ) , suggesting that the two tasks provide complementary signal that is useful for citation intent prediction .","We note that the scaffold tasks provide major contributions on top of the ELMo - enabled baseline ( ?= 13.6 ) , demonstrating the efficacy of using structural scaffolds for citation intent prediction .",result
semantic_role_labeling,1,SA and LISA with and without ELMo attain comparable scores so we report only LISA + Glo Ve .,experiment,"Parsing, POS and predicate detection",0,187,38,5,0,"experiment : Parsing, POS and predicate detection",0.8577981651376146,0.95,0.7142857142857143,SA and LISA with and without ELMo attain comparable scores so we report only LISA Glo Ve ,18,"In we present predicate detection precision , recall and F1 on the CoNLL - 2005 and 2012 test sets .","We compare to He et al. on CoNLL - 2005 , the only cited work reporting comparable predicate detection F1 .",experiment
text_summarization,6,"Some more complex latent structures in the target summaries , such as the high - level syntactic features and latent topics , can not be modeled effectively by the deterministic operations and variables .",system description,Recurrent Generative Decoder,0,84,26,18,0,system description : Recurrent Generative Decoder,0.32061068702290074,0.5098039215686274,0.4186046511627907,Some more complex latent structures in the target summaries such as the high level syntactic features and latent topics can not be modeled effectively by the deterministic operations and variables ,31,"From our investigations , we find that the representational power of such deterministic variables are limited .","Recently , a generative model called Variational Auto - Encoders ( VAEs ) shows strong capability in modeling latent random variables and improves the performance of tasks in different fields such as sentence generation and image generation .",method
natural_language_inference,46,"Our hope is that this dataset will serve not only as a challenge for the machine reading community , but as a driver for the development of a new class of neural models which will take a significant step beyond the level of complexity which existing datasets and tasks permit .",Figure 1:,Review of Reading Comprehension Datasets and Models,0,297,24,19,0,Figure 1: : Review of Reading Comprehension Datasets and Models,1.0,1.0,1.0,Our hope is that this dataset will serve not only as a challenge for the machine reading community but as a driver for the development of a new class of neural models which will take a significant step beyond the level of complexity which existing datasets and tasks permit ,50,"Having given a quantitative and qualitative analysis of the difficulty of the more complex tasks , we suggest research directions that may help bridge the gap between existing models and hu-man performance .", ,others
natural_language_inference,83,"For the hidden aspect assignment , we needed features that could analyze the two options in light of the given context , and characterize the importance of various aspects for the given instance .",model,Hidden Coherence Model,0,145,35,35,0,model : Hidden Coherence Model,0.4738562091503268,0.7777777777777778,0.7777777777777778,For the hidden aspect assignment we needed features that could analyze the two options in light of the given context and characterize the importance of various aspects for the given instance ,32,"The features extracted for each of the K = 3 aspects , f k co , were described in Sec. 2.1 .",One way to measure an aspect 's importance is by quantifying how different the two options are with respect to that aspect .,method
sarcasm_detection,1,"This vector ? c i , j captures both syntactic and semantic information useful for the task at hand .",method,Summary of the Proposed Approach,0,80,11,6,0,method : Summary of the Proposed Approach,0.2395209580838324,0.0728476821192053,0.3333333333333333,This vector c i j captures both syntactic and semantic information useful for the task at hand ,18,CNNs generate abstract representations of text by extracting location - invariant local patterns .,"This vector ? c i , j captures both syntactic and semantic information useful for the task at hand .",method
text-classification,1,The onehot CNN and its semi-supervised extension were shown to be superior to a number of previous methods .,introduction,introduction,0,20,9,9,0,introduction : introduction,0.078125,0.2571428571428571,0.2571428571428571,The onehot CNN and its semi supervised extension were shown to be superior to a number of previous methods ,20,"document is represented as a sequence of one - hot vectors ( each of which indicates a word by the position of a 1 ) ; a convolution layer converts small regions of the document ( e.g. , "" I love it "" ) to low-dimensional vectors at every location ( embedding of text regions ) ; a pooling layer aggregates the region embedding results to a document vector by taking componentwise maximum or average ; and the top layer classifies a document vector with a linear model .","In this work , we consider a more general framework ( subsuming one - hot CNN ) which jointly trains a feature generator and a linear model , where the feature generator consists of ' region embedding + pooling ' .",introduction
semantic_parsing,0,It tests the ability of a system to generalize to not only new SQL queries and data base schemas but also new domains .,dataset,Dataset Statistics and Comparison,0,159,11,11,0,dataset : Dataset Statistics and Comparison,0.5719424460431655,1.0,1.0,It tests the ability of a system to generalize to not only new SQL queries and data base schemas but also new domains ,24,"Therefore , Spider is the only one text - to - SQL dataset that contains both data bases with multiple tables in different domains and complex SQL queries", ,experiment
sentiment_analysis,2,"This is because different words in aspect term have different effect on a sentence , and we apply the bidirectional attention mechanism to choose more useful words .",system description,Case Study,0,198,16,16,0,system description : Case Study,0.8722466960352423,0.8888888888888888,0.8888888888888888,This is because different words in aspect term have different effect on a sentence and we apply the bidirectional attention mechanism to choose more useful words ,27,"From , it is worth noting that some words such as "" good "" and "" place "" get less attention even they are closer to the current aspect term than "" but "" and "" not "" .","For instance , in this case , PBAN should pay more attention on the word "" take - out "" .",method
question_answering,4,ij * ? R 3 represents the generated features for each ij combination of P/Q .,system description,Densely Connected Attention Encoder (DECAENC),0,111,68,15,0,system description : Densely Connected Attention Encoder (DECAENC),0.4319066147859921,0.6538461538461539,0.7894736842105263,ij R 3 represents the generated features for each ij combination of P Q ,15,This densely connects all representations of P and Q across multiple layers .,ij * ? R 3 represents the generated features for each ij combination of P/Q .,method
relation_extraction,8,"In this paper , we use the Skip - gram model ( word2 vec ) 5 to train the word embeddings on the NYT corpus .",experiment,Pre-trained Word Embeddings,1,202,3,2,0,experiment : Pre-trained Word Embeddings,0.7509293680297398,0.15,0.4,In this paper we use the Skip gram model word2 vec 5 to train the word embeddings on the NYT corpus ,22, ,Word2 vec first constructs a vocabulary from the training text data and then learns vector representations of the words .,experiment
natural_language_inference,40,"The output representation at time step t is given by , where ht ? Re de , and denotes concatenation .",method,MAGE-GRUs,0,117,40,16,0,method : MAGE-GRUs,0.4223826714801444,0.6153846153846154,0.5517241379310345,The output representation at time step t is given by where ht Re de and denotes concatenation ,18,"e r , z , h and U e,e r, z , h are parameter matrices of size d e din and d e d e respectively , and be r , z , h are parameter vectors of size d e .","The output representation at time step t is given by , where ht ? Re de , and denotes concatenation .",method
natural_language_inference,23,"Then we attend for the span start using the summarized question understanding vector u Q , To use the information of the span start when we attend for the span end , we combine the context understanding vector for the span start with u Q through a GRU , where u Q is taken as the memory and i PS i u Ci as the input in GRU .",architecture,APPLICATION IN MACHINE COMPREHENSION,0,203,131,7,0,architecture : APPLICATION IN MACHINE COMPREHENSION,0.3957115009746589,0.9225352112676056,0.3888888888888889,Then we attend for the span start using the summarized question understanding vector u Q To use the information of the span start when we attend for the span end we combine the context understanding vector for the span start with u Q through a GRU where u Q is taken as the memory and i PS i u Ci as the input in GRU ,66,and w is a trainable vector .,"Then we attend for the span start using the summarized question understanding vector u Q , To use the information of the span start when we attend for the span end , we combine the context understanding vector for the span start with u Q through a GRU , where u Q is taken as the memory and i PS i u Ci as the input in GRU .",method
topic_models,0,"The classifier was fine - tuned for 50 epochs with drop - out rates from : { 0.2 , . . . , 0.6 }.",baseline,baseline,0,304,21,21,0,baseline : baseline,0.7378640776699029,0.8076923076923077,0.8076923076923077,The classifier was fine tuned for 50 epochs with drop out rates from 0 2 0 6 ,18,"More specifically , the LM was fine - tuned for 15 epochs 6 , with drop - out rates from : { 0.2 , . . . , 0.6 }.","held - out development set was used to tune the hyper - parameters ( drop - out rates , and fine - tuning epochs ) .",result
natural_language_inference,49,All models except ours share one common feature that they use LSTM as a essential building block as encoder .,experiment,EXPERIMENT ON MULTINLI,0,163,49,5,0,experiment : EXPERIMENT ON MULTINLI,0.6417322834645669,0.7,0.625,All models except ours share one common feature that they use LSTM as a essential building block as encoder ,20,RepEval 2017 workshop requires all submitted model to be sentence encoding - based model therefore alignment between sentences and memory module are not eligible for competition .,"Our approach , without using any recurrent structure , achieves the new state - of - the - art performance of 80.0 % , exceeding current state - of - the - art performance by more than 5 % .",experiment
named-entity-recognition,7,In each step .,model,Shift-Reduce System,0,53,10,5,0,model : Shift-Reduce System,0.3354430379746836,0.14705882352941174,0.2,In each step ,4,"The system 's state is defined as [ S , i , A ] which denotes stack , buffer front index and action history respectively .",an action is applied to change the system 's state .,method
sentiment_analysis,10,"Now , we employ a GRU cell GRU P to update the current speaker state q s ( ut ) , t?1 to the new state q s ( ut ) , t based on incoming utterance u t and context ct using GRU cell GRU P of output size DP q s ( ut ) , t = GRU P ( q s ( ut ) , t? 1 , ( u t ? ct ) ) , This encodes the information on the current utterance along with its context from the global GRU into the speaker 's state q s ( ut ) , which helps in emotion classification down the line .",model,Our Model,0,97,37,37,0,model : Our Model,0.3774319066147861,0.6271186440677966,0.6607142857142857,Now we employ a GRU cell GRU P to update the current speaker state q s ut t 1 to the new state q s ut t based on incoming utterance u t and context ct using GRU cell GRU P of output size DP q s ut t GRU P q s ut t 1 u t ct This encodes the information on the current utterance along with its context from the global GRU into the speaker s state q s ut which helps in emotion classification down the line ,92,"Finally , in Eq. ( 4 ) the context vector ct is calculated by pooling the previous global states with ?.","Now , we employ a GRU cell GRU P to update the current speaker state q s ( ut ) , t?1 to the new state q s ( ut ) , t based on incoming utterance u t and context ct using GRU cell GRU P of output size DP q s ( ut ) , t = GRU P ( q s ( ut ) , t? 1 , ( u t ? ct ) ) , This encodes the information on the current utterance along with its context from the global GRU into the speaker 's state q s ( ut ) , which helps in emotion classification down the line .",method
question_answering,5,This observation can be seen as multiple passages collaboratively enhancing the evidence for the correct answer .,introduction,introduction,0,27,13,13,0,introduction : introduction,0.09782608695652174,0.35135135135135137,0.35135135135135137,This observation can be seen as multiple passages collaboratively enhancing the evidence for the correct answer ,17,"For example , in , the correct answer "" danny boy "" has more passages providing evidence relevant to the question compared to the incorrect one .","Second , sometimes the question covers multiple answer aspects , which spreads over multiple passages .",introduction
semantic_parsing,2,"On the contrary , parent hidden states serve as input to the softmax classifiers of both fine and coarse meaning decoders .",training,Natural Language to Logical Form,0,146,37,28,0,training : Natural Language to Logical Form,0.5017182130584192,0.3135593220338983,0.8235294117647058,On the contrary parent hidden states serve as input to the softmax classifiers of both fine and coarse meaning decoders ,21,similar idea is also explored in the tree decoders proposed in and where parent hidden states are fed to the input gate of the LSTM units .,"Taking the meaning sketch "" ( and flight@ 1 from @ 2 ) "" as an example , the parent of "" from@2 "" is "" ( and "" .",experiment
text_summarization,1,"Guided by K focus , generator conducts parallel greedy decoding .",baseline,Baselines,0,170,17,17,0,baseline : Baselines,0.7083333333333334,0.4473684210526316,1.0,Guided by K focus generator conducts parallel greedy decoding ,10,We construct a hard - MoE of K SELECTORs with uniform mixing coefficient that infers K different focus from source sequence ., ,result
relation_extraction,12,"For binary - class n-ary relation extraction , we follow to binarize multi-class labels by grouping the four relation classes as "" Yes "" and treating "" None "" as "" No "" .",experiment,Data,0,162,8,7,0,experiment : Data,0.4924012158054712,0.2424242424242425,0.4666666666666667,For binary class n ary relation extraction we follow to binarize multi class labels by grouping the four relation classes as Yes and treating None as No ,28,"We consider two specific tasks for evaluation , i , e. , binary - class n-ary relation extraction and multi-class n-ary relation extraction .","For the sentence - level relation extraction task , we follow the experimental settings in to evaluate our model on the TACRED dataset and Semeval - 10 Task 8 .",experiment
natural_language_inference,21,"In , we compare LSTM , Bi - LSTM , Tree - LSTM and DiSAN on different sentence lengths .",model,Model,0,244,18,18,0,model : Model,0.8413793103448276,0.3103448275862069,0.8571428571428571,In we compare LSTM Bi LSTM Tree LSTM and DiSAN on different sentence lengths ,15,It is also interesting to see the performance of different models on the sentences with different lengths .,"In the range of ( 5 , 12 ) , the length range for most movie review sentences , DiSAN significantly outperforms others .",method
natural_language_inference,10,"The composition query vector is initialized by sampling from Gaussian distribution N ( 0 , 0.01 2 ) .",SST,SST,0,230,2,2,0,SST : SST,0.9704641350210972,0.2222222222222222,0.2222222222222222,The composition query vector is initialized by sampling from Gaussian distribution N 0 0 01 2 ,17, ,"The last linear transformation that outputs the unnormalized log probability for each class is initialized by sampling from uniform distribution U ( ? 0.002 , 0.002 ) .",others
natural_language_inference,67,Thus the upper bound of the exact match score of our baseline system is around 66 % ( 92 % ( the answer recall ) 72 % ) .,analysis,analysis,0,170,7,7,0,analysis : analysis,0.8333333333333334,0.3888888888888889,0.3888888888888889,Thus the upper bound of the exact match score of our baseline system is around 66 92 the answer recall 72 ,22,"When we use a window size of 10 , 92 % of the time , the ground truth answer will be included in the extracted Candidate chunk set .","From the results , we see our DCR system 's exact match score is at 62 % .",result
machine-translation,6,"Results show that with FRAGE , we achieve higher performance than the baselines in all tasks .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.030927835051546393,1.0,1.0,Results show that with FRAGE we achieve higher performance than the baselines in all tasks ,16,"We conducted comprehensive studies on ten datasets across four natural language processing tasks , including word similarity , language modeling , machine translation and text classification .", ,abstract
machine-translation,2,We trained on the standard WMT 2014 English - German dataset consisting of about 4.5 million sentence pairs .,training,Training Data and Batching,0,154,4,2,0,training : Training Data and Batching,0.6875,0.14285714285714285,0.3333333333333333,We trained on the standard WMT 2014 English German dataset consisting of about 4 5 million sentence pairs ,19, ,"Sentences were encoded using byte - pair encoding , which has a shared sourcetarget vocabulary of about 37000 tokens .",experiment
natural_language_inference,97,"Accuracy scores are divided among questions whose evidence lies in a single sentence ( single ) and across multiple sentences ( multi ) , and among the two variants .",training,Training and Model Details,0,239,25,25,0,training : Training and Model Details,0.8184931506849316,0.9615384615384616,0.9615384615384616,Accuracy scores are divided among questions whose evidence lies in a single sentence single and across multiple sentences multi and among the two variants ,25,presents the performance of featureengineered and neural methods on the MCTest test set .,"Clearly , MCTest - 160 is easier .",experiment
natural_language_inference,51,"Also , to ignore catastrophic forgetting in the state network , we use Feedforward controllers in the two baselines .",result,Few-shot Learning,0,186,53,6,0,result : Few-shot Learning,0.7018867924528301,0.7571428571428571,0.2608695652173913,Also to ignore catastrophic forgetting in the state network we use Feedforward controllers in the two baselines ,18,"In our experiment , we let the models see the training data from the while freezing others , we force "" hard "" attention over the programs by replacing the softmax function in Eq. 5 with the Gumbel - softmax .","After finishing one task , we evaluate the bit accuracy ? measured by 1 ? ( bit error per sequence / total bits per sequence ) over 4 tasks .",result
named-entity-recognition,1,"Our second intuition is that names , which may individually be quite varied , appear in regular contexts in large corpora .",training,Input Word Embeddings,0,124,56,6,0,training : Input Word Embeddings,0.5990338164251208,0.6588235294117647,0.75,Our second intuition is that names which may individually be quite varied appear in regular contexts in large corpora ,20,We therefore use a model that constructs representations of words from representations of the characters they are composed of ( 4.1 ) .,Therefore we use embed - dings learned from a large corpus thatare sensitive to word order ( 4.2 ) .,experiment
sentence_classification,2,"Because of these , we can not directly use the additive attention module .",model,Vector Fixing Module,0,115,30,11,0,model : Vector Fixing Module,0.4563492063492064,0.5660377358490566,0.3235294117647059,Because of these we can not directly use the additive attention module ,13,"Moreover , one characteristic of our problem is that the sentence vectors can be both a source and a context vector ( e.g. v scan be both sand ti in Equation 1 ) .","We extend the module such that ( 1 ) each sentence vector v k has its own projection matrix X k ? R dd , and ( 2 ) each projection matrix X k can be used as projection matrix of both the source ( e.g. when sentence k is the current source ) and the context vectors .",method
sentiment_analysis,4,This example displays the role of inter-speaker influences and how ICON processes such dependencies .,ablation,Case Studies,0,318,52,15,0,ablation : Case Studies,0.9754601226993864,1.0,1.0,This example displays the role of inter speaker influences and how ICON processes such dependencies ,16,"These characteristics are captured by the attention mechanism applied on the global memories ( generated by DGIM ) , which finds contextual information from histories thatare relevant to the test utterance u 18 .", ,result
natural_language_inference,48,The related query is formed from an excerpt 's 21st sentence by replacing a single word with an anonymous placeholder token .,system description,Task Description,0,35,6,6,0,system description : Task Description,0.1583710407239819,0.08695652173913042,0.42857142857142855,The related query is formed from an excerpt s 21st sentence by replacing a single word with an anonymous placeholder token ,22,Documents consist of 20 - sentence excerpts from these books .,The dataset is divided into four subsets depending on the type of the word replaced .,method
text_summarization,10,"Despite the strengths of the strong model described above with attention , pointer , and coverage , a good summary should also contain maximal salient information and be a directed logical entailment of the source document .",model,Two Auxiliary Tasks,0,71,26,2,0,model : Two Auxiliary Tasks,0.26996197718631176,0.27956989247311825,0.6666666666666666,Despite the strengths of the strong model described above with attention pointer and coverage a good summary should also contain maximal salient information and be a directed logical entailment of the source document ,34, ,We teach these skills to the abstractive summarization model via multi-task training with two related auxiliary tasks : question generation task and entailment generation .,method
natural_language_inference,44,These experimental results and analyses show that our approach is effective in filtering adversarial sentences and preventing wrong predictions caused by adversarial sentences .,system description,SQuAD and NewsQA,0,184,91,81,0,system description : SQuAD and NewsQA,0.6433566433566433,1.0,1.0,These experimental results and analyses show that our approach is effective in filtering adversarial sentences and preventing wrong predictions caused by adversarial sentences ,24,"While FULL selects the answer from the adversarial sentence , MINIMAL first chooses the oracle sentence , and subsequently predicts the correct answer .",The model architecture of S - Reader is divided into the encoder module and the decoder module .,method
natural_language_inference,63,We concatenate both embeddings [ e w ; e c ] to represent each word embedding as e ? R l .,model,Contextual Representation,0,54,14,6,0,model : Contextual Representation,0.3,0.18421052631578946,0.2727272727272727,We concatenate both embeddings e w e c to represent each word embedding as e R l ,18,"In addition to this , we utilize character - level embedding by applying convolution filters to address out - ofvocabulary and infrequent words problem .",We concatenate both embeddings [ e w ; e c ] to represent each word embedding as e ? R l .,method
natural_language_inference,21,"In experiments 1 , we compare DiSAN with the currently popular methods on various NLP tasks , e.g. , natural language inference , sentiment analysis , sentence classification , etc .",abstract,abstract,0,41,39,39,0,abstract : abstract,0.1413793103448276,0.8863636363636364,0.8863636363636364,In experiments 1 we compare DiSAN with the currently popular methods on various NLP tasks e g natural language inference sentiment analysis sentence classification etc ,26,"The simple architecture of DiSAN leads to fewer parameters , less computation and easier parallelization .",DiSAN achieves the highest test accuracy on the Stanford Natural Language Inference ( SNLI ) dataset among sentence - encoding models and improves the currently best result by 1.02 % .,abstract
sentiment_analysis,7,"We consider only 4 of them anger , excitement ( happiness ) , neutral and sadness so as to remain consistent with the prior research .",experiment,EXPERIMENTAL SETUP,0,50,6,6,0,experiment : EXPERIMENTAL SETUP,0.4716981132075472,0.1875,0.1875,We consider only 4 of them anger excitement happiness neutral and sadness so as to remain consistent with the prior research ,22,"The evaluation form contained 10 options ( neutral , happiness , sadness , anger , surprise , fear , disgust frustration , excited , other ) .","We consider emotions where atleast 2 experts were consistent with their decision , which is more than 70 % of the dataset , consistent with prior research .",experiment
part-of-speech_tagging,3,"Bias vectors are initialized to zero , except the bias bf for the forget gate in LSTM , which is initialized to 1.0 .",training,Parameter Initialization,0,95,15,10,0,training : Parameter Initialization,0.4679802955665025,0.42857142857142855,1.0,Bias vectors are initialized to zero except the bias bf for the forget gate in LSTM which is initialized to 1 0 ,23,Character embeddings are initialized with uniform samples from ., ,experiment
sentiment_analysis,1,The matrix can be pre-computed as a prior or estimated from noisy data .,system description,Learning with Noisy Labels,0,116,44,4,0,system description : Learning with Noisy Labels,0.2929292929292929,0.21256038647342995,0.5714285714285714,The matrix can be pre computed as a prior or estimated from noisy data ,15,The noise transition matrix specifies the probabilities of transition from each ground true label to each noisy label and is often applied to modify the crossentropy loss .,"few studies tackle noisy labels by using noise - tolerant robust loss functions , such as unhinged loss and ramp loss .",method
named-entity-recognition,6,"They are widely used as a feature source in NER , and have been successfully included in feature - based models .",system description,Motivation,0,34,3,3,0,system description : Motivation,0.16037735849056606,0.15,0.15,They are widely used as a feature source in NER and have been successfully included in feature based models ,20,Gazetteers are lists of entities thatare associated with specific NE categories .,"Typically , lists of entities are compiled from structured data sources such as DBpedia or Freebase .",method
natural_language_inference,53,The {? i } represent the score of similarity between the keys and a learned context query vector u w .,architecture,Self-attentive network,0,77,19,6,0,architecture : Self-attentive network,0.3701923076923077,0.6785714285714286,0.5454545454545454,The i represent the score of similarity between the keys and a learned context query vector u w ,19,"These are fed to an affine transformation ( W , b w ) which outputs a set of keys ( h 1 , . . . ,h T ) .",The {? i } represent the score of similarity between the keys and a learned context query vector u w .,method
sentiment_analysis,43,"On the other hand , they just use the averaged aspect vector to guide the attention , which will lose some information , especially on the aspects with multiple words .",performance,Overall Performance Comparison,0,208,17,17,0,performance : Overall Performance Comparison,0.8666666666666667,0.85,0.85,On the other hand they just use the averaged aspect vector to guide the attention which will lose some information especially on the aspects with multiple words ,28,"On one hand , they only consider to learn the attention weights on context towards the aspect , and do not consider to learn the weights on aspect words towards the context .","From another perspective , our method employs the aspect alignment loss , which can bring extra useful information from the aspectlevel interactions .",result
sentiment_analysis,38,Tanh is applied to bound values between - 1 and 1 .,experiment,Experimental Setup and Results,0,89,13,13,0,experiment : Experimental Setup and Results,0.5297619047619048,0.5909090909090909,0.5909090909090909,Tanh is applied to bound values between 1 and 1 ,11,The model processes the sequence and the final cell states of the mL - STM are used as a feature representation .,"We follow the methodology established in by training a logistic regression classifier on top of our model 's representation on datasets for tasks including semantic relatedness , text classification , and paraphrase detection .",experiment
natural_language_inference,62,The contribution of this paper is two - fold .,system description,Passage Question,0,40,14,14,0,system description : Passage Question,0.17857142857142858,0.7368421052631579,0.7777777777777778,The contribution of this paper is two fold ,9,"In this paper , we discard the existing implicit way and instead explore an explicit ( i.e. understandable and controllable ) way to utilize general knowledge .","On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage - question pair .",method
sentiment_analysis,48,"In the aforementioned experiments , the embedding layer is not shared between the classifier and autoencoder .",ablation,Effect of Sharing Embeddings,0,213,11,5,0,ablation : Effect of Sharing Embeddings,0.9025423728813561,0.44,0.5555555555555556,In the aforementioned experiments the embedding layer is not shared between the classifier and autoencoder ,16,It is questionable whether the improvement is obtained by using VAE or multitask learning ( text generation and classification ) .,This implementation guarantees that the improvement does not come from learning to generate .,result
question_generation,0,We propose to adapt the recently released Stanford Question Answering Dataset ( SQuAD ) as the training and development datasets for the question generation task .,introduction,introduction,0,23,15,15,0,introduction : introduction,0.12637362637362634,0.8823529411764706,0.8823529411764706,We propose to adapt the recently released Stanford Question Answering Dataset SQuAD as the training and development datasets for the question generation task ,24,Large - scale manually annotated passage and question pairs play a crucial role in developing question generation systems .,"In SQuAD , the answers are labeled as subsequences in the given sentences by crowed sourcing , and it contains more than 100K questions which makes it feasible to train our neural network models .",introduction
phrase_grounding,0,from different levels and choosing the one that maximizes this projection .,method,Feature Level Selection,0,132,59,8,0,method : Feature Level Selection,0.5866666666666667,0.7866666666666666,0.6153846153846154,from different levels and choosing the one that maximizes this projection ,12,This is equivalent to finding the hyperplane ( spanned by each level visual feature vectors in the common space ) that best matches the textual feature .,"Intuitively , that chosen hyperplane can be a better representation for visual feature space attended byword t.",method
natural_language_inference,33,"In an attempt to understand what information is encoded in by sentence representations , we consider six different classification tasks where the objective is to predict sentence characteristics such as length , word content and word order or syntactic properties such as active / passive , tense and the top syntactic sequence ( TSS ) from the parse tree of a sentence .",evaluation,SENTENCE CHARACTERISTICS & SYNTAX,0,245,121,2,0,evaluation : SENTENCE CHARACTERISTICS & SYNTAX,0.9386973180076628,0.8832116788321168,0.1111111111111111,In an attempt to understand what information is encoded in by sentence representations we consider six different classification tasks where the objective is to predict sentence characteristics such as length word content and word order or syntactic properties such as active passive tense and the top syntactic sequence TSS from the parse tree of a sentence ,57, ,The sentence characteristic tasks are setup in the same way as described in .,result
named-entity-recognition,6,"In practice , we found that simply concatenating a sentence ( v1 ) with its annotated version ( v 2 ) , as illustrated in , offers a simple but efficient way of combining words and entity types so that embeddings can make good use of them .",method,Embedding Words and Entity Types,0,66,15,14,0,method : Embedding Words and Entity Types,0.3113207547169811,0.3,0.4666666666666667,In practice we found that simply concatenating a sentence v1 with its annotated version v 2 as illustrated in offers a simple but efficient way of combining words and entity types so that embeddings can make good use of them ,41,"For instance , the embedding for / product / software will be trained using context words that surround all entities that were ( automatically ) labelled as / product / software in Wikipedia .",We use the FastText toolkit to learn the uncased embeddings for both words and entity types .,method
natural_language_inference,27,"However , the model is agnostic to those adversarial examples during training .",analysis,ROBUSTNESS STUDY,0,269,26,4,0,analysis : ROBUSTNESS STUDY,0.7958579881656804,0.7027027027027027,0.26666666666666666,However the model is agnostic to those adversarial examples during training ,12,"In this dataset , one or more sentences are appended to the original SQuAD context of test set , to intentionally mislead the trained models to produce wrong answers .","We focus on two types of misleading sentences , namely , Add Sent and Add OneSent .",result
named-entity-recognition,8,The final model achieves 97 % - 98 % accuracy on NSP .,model,model,0,128,50,50,0,model : model,0.33074935400516803,0.6756756756756757,0.8064516129032258,The final model achieves 97 98 accuracy on NSP ,10,"Despite its simplicity , we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI .","The vector C is not a meaningful sentence representation without fine - tuning , since it was trained with NSP .",method
natural_language_inference,13,We propose an over all architecture that utilizes MRU within a bi-attentive framework for both multiple choice and span prediction MC tasks .,introduction,introduction,0,41,29,29,0,introduction : introduction,0.17903930131004367,0.8529411764705882,0.8529411764705882,We propose an over all architecture that utilizes MRU within a bi attentive framework for both multiple choice and span prediction MC tasks ,24,"We propose MRU ( Multi-range Reasoning Units ) , a new compositional encoder which construct gates from a novel contract - and - expand operation .","MRU can be used as a standalone ( without RNNs ) for fast reading and / or together with RNN models ( i.e. , MRU - LSTM ) for more expressive reading .",introduction
relation_extraction,11,"Overall , we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available .",ablation,Effect of Relation Alias Side Information,1,241,17,10,0,ablation : Effect of Relation Alias Side Information,0.9717741935483872,0.9444444444444444,0.9090909090909092,Overall we find that RESIDE gives competitive performance even when very limited amount of relation alias information is available ,20,We find that the model performs best when aliases are provided by the KB itself .,We observe that performance improves further with the availability of more alias information .,result
sentence_compression,0,"In this paper , we aim to study how syntactic information can be incorporated into neural network models for sentence compression to improve their domain adaptability .",introduction,introduction,0,26,19,19,0,introduction : introduction,0.0931899641577061,0.5277777777777778,0.5277777777777778,In this paper we aim to study how syntactic information can be incorporated into neural network models for sentence compression to improve their domain adaptability ,26,"This is not surprising because these models do not explicitly use much syntactic information , which is more general than lexical information .",We hope to train a model that performs well on both in - domain and out - ofdomain data .,introduction
negation_scope_resolution,0,"For negation cue detection , we observe a significant gap between our model , NegBERT , and the current state - of the - art systems , while we outperform the baseline systems .",result,Results,1,213,15,15,0,result : Results,0.9260869565217392,0.6818181818181818,0.6818181818181818,For negation cue detection we observe a significant gap between our model NegBERT and the current state of the art systems while we outperform the baseline systems ,28,"On the SFU Review Corpus , we outperform the best system to date by 1.02 F1 .","We believe this is so as these datasets are fairly limited in size and scope , and for a such a task , bigger models like BERT need a lot more examples to train onto master the finer points of negation detection , while this is straightforward to handle for rule - based approaches and smaller datasets .",result
natural_language_inference,48,We note that the latter comparison maybe influenced by different training and initialization strategies .,result,Main result,0,154,21,5,0,result : Main result,0.6968325791855203,0.5675675675675675,0.2380952380952381,We note that the latter comparison maybe influenced by different training and initialization strategies ,15,Our model slightly improves over this strong baseline by 0.2 percent on validation and 0.9 percent on test .,"First , Stanford AS uses Glo Ve embeddings , pre-trained from a large external corpus .",result
natural_language_inference,97,We initialize the model 's neural networks to perform specific heuristic functions that yield decent ( thought not impressive ) performance on the dataset .,introduction,introduction,0,42,31,31,0,introduction : introduction,0.14383561643835616,0.8378378378378378,0.8378378378378378,We initialize the model s neural networks to perform specific heuristic functions that yield decent thought not impressive performance on the dataset ,23,"To facilitate learning with limited data , we also develop a unique training scheme .","Thus , the training scheme gives the model a safe , reasonable baseline from which to start learning .",introduction
named-entity-recognition,9,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .",method,Fine-tuning BioBERT,0,76,30,2,0,method : Fine-tuning BioBERT,0.3819095477386935,0.6,0.09090909090909093,With minimal architectural modification BioBERT can be applied to various downstream text mining tasks ,15, ,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",method
natural_language_inference,92,shows the results on DROP num .,method,Reading Comprehension with Discrete Reasoning,0,117,64,9,0,method : Reading Comprehension with Discrete Reasoning,0.40625,0.6530612244897959,0.8181818181818182,shows the results on DROP num ,7,"Details of the model architecture are shown in Appendix A. and 10 for QANET and BERT , respectively .","Our training strategy outperforms the First - Only baseline and MML by a large margin , consistently across two base models .",method
sentence_classification,0,We observe that our scaffold - enhanced models achieve clear improvements over the state - of - the - art approach on this task .,result,Results,1,181,3,3,0,result : Results,0.6779026217228464,0.13636363636363635,0.13636363636363635,We observe that our scaffold enhanced models achieve clear improvements over the state of the art approach on this task ,21,Our main results for the ACL - ARC dataset is shown in .,"Starting with the ' BiLSTM - Attn ' baseline with a macro F1 score of 51.8 , adding the first scaffold task in ' BiLSTM - Attn + section title scaffold ' improves the F1 score to 56.9 (?= 5.1 ) .",result
natural_language_inference,65,"As in , we define the training objective as a hinge loss :",training,Scoring and Training Procedure,0,91,10,10,0,training : Scoring and Training Procedure,0.39565217391304347,0.7142857142857143,0.7142857142857143,As in we define the training objective as a hinge loss ,12,"The input in each round is two pairs ( q , a + ) and ( q , a ? ) , where a + is aground truth answer for q , and a ? is an incorrect answer .","where m is constant margin , s ? ( q , a + ) and s ? ( q , a ? ) are scores generated by the network with parameter set ?.",experiment
natural_language_inference,46,We use start and end indices of the span achieving the highest Rouge - L score with respect to the reference answers as labels on the training set .,baseline,baseline,0,176,22,22,0,baseline : baseline,0.5925925925925926,0.6666666666666666,0.6666666666666666,We use start and end indices of the span achieving the highest Rouge L score with respect to the reference answers as labels on the training set ,28,Span prediction models can be trained by obtaining supervision on the training set from the oracle IR model .,The model is then trained to predict these spans by maximizing the probability of the indices .,result
sentiment_analysis,40,Such complications usually hinder conventional approaches to aspect sentiment analysis .,introduction,introduction,0,21,12,12,0,introduction : introduction,0.09417040358744394,0.35294117647058826,0.35294117647058826,Such complications usually hinder conventional approaches to aspect sentiment analysis ,11,", "" It s camera is not wonderful enough "" might express a neutral sentiment on "" camera "" , but not negative .","To model the sentiment of the above phraselike word sequence ( i.e. "" not wonderful enough "" ) , LSTM - based methods are proposed , such as target dependent LSTM ( TD - LSTM ) .",introduction
sentiment_analysis,0,"Recently , deep learning algorithms have successfully addressed problems in various fields , such as image classification , machine translation , speech recognition , text - to - speech generation and other machine learning related are as .",introduction,introduction,0,11,2,2,0,introduction : introduction,0.06179775280898876,0.0625,0.0625,Recently deep learning algorithms have successfully addressed problems in various fields such as image classification machine translation speech recognition text to speech generation and other machine learning related are as ,31, ,"Similarly , substantial improvements in performance have been obtained when deep learning algorithms have been applied to statistical speech processing .",introduction
sentiment_analysis,36,pability in capturing the context features .,result,Main Results,0,185,9,9,0,result : Main Results,0.74,0.6923076923076923,0.6923076923076923,pability in capturing the context features ,7,"The marker refers to p-value < 0.01 when comparing with BILSTM - ATT - G , while the marker refers to p-value < 0.01 when comparing with RAM .","Another difficulty caused by the ungrammatical sentences is that the dependency parsing might be errorprone , which will affect those methods such as AdaRNN using dependency information .",result
text_generation,4,"Table 2 ( a ) contains sentences generated from STGAN using least - squares distance in which there was no mode collapse observed , while 2 ( b ) contains examples wherein it is observed .",training,Language Generation.,0,95,21,6,0,training : Language Generation.,0.8715596330275229,0.6,0.75,Table 2 a contains sentences generated from STGAN using least squares distance in which there was no mode collapse observed while 2 b contains examples wherein it is observed ,30,The samples represent how mode collapse is manifested when using least - squares distance f- measure without minibatch discrimination .,Table 2 ( c ) shows generated sentences using gradient penalty regularizer ( GAN - GP ) .,experiment
text_generation,5,The improvement is big for small models and gradually decreases as we increase the decoder model contextual capacity .,result,Language modeling results,0,192,8,8,0,result : Language modeling results,0.6508474576271186,0.08791208791208792,0.4705882352941176,The improvement is big for small models and gradually decreases as we increase the decoder model contextual capacity ,19,"For SCNN , MCNN and LCNN , the VAE results improve over LM results from 345.3 to 337.8 , 338.3 to 336.2 , and 335.4 to 333.9 respectively .","When the model is as large as VLCNN , the improvement diminishes and the VAE result is almost the same with LM result .",result
machine-translation,3,"However , the largest batch size is constrained by the GPU memory .",model,Optimization,0,228,28,17,0,model : Optimization,0.7284345047923323,0.875,0.8095238095238095,However the largest batch size is constrained by the GPU memory ,12,We also find that larger batch size results in better convergence although the improvement is not large .,We use 4 ? 8 GPU machines ( each has 4 K40 GPU cards ) running for 10 days to train the full model with parallelization at the data batch level .,method
question-answering,7,"This model ( LSTM - LSTM ) has two LSTM for the encoder / decoder and has the soft attention neural net , which attends over the source sentence and constructs a focused encoding vector for each target word .",analysis,Machine Translation,0,228,29,8,0,analysis : Machine Translation,0.8290909090909091,0.453125,0.3333333333333333,This model LSTM LSTM has two LSTM for the encoder decoder and has the soft attention neural net which attends over the source sentence and constructs a focused encoding vector for each target word ,35,The first model is a baseline model and is similar to the one proposed in ( RNNSearch ) .,The second model is an NSE - LSTM encoder - decoder which encodes the source sentence with NSE and generates the targets with the LSTM network by using the NSE output states and the attention network .,result
natural_language_inference,16,"For a natural language inference task , NLSM is utilized to judge whether a hypothesis sentence can be inferred from a premise sentence .",introduction,introduction,0,18,5,5,0,introduction : introduction,0.08450704225352113,0.15625,0.15625,For a natural language inference task NLSM is utilized to judge whether a hypothesis sentence can be inferred from a premise sentence ,23,"For example , in a paraphrase identification task , NLSM is used to determine whether two sentences are paraphrase or not .","For question answering and information retrieval tasks , NLSM is employed to assess the relevance between query - answer pairs and rank all the candidate answers .",introduction
natural_language_inference,33,"Computer vision tasks like image captioning and visual question answering typically use CNNs pretrained on ImageNet to extract representations of the image , while several natural language tasks such as reading comprehension and sequence labeling have benefited from pretrained word embeddings thatare either fine - tuned for a specific task or held fixed .",introduction,introduction,0,15,3,3,0,introduction : introduction,0.05747126436781608,0.13636363636363635,0.13636363636363635,Computer vision tasks like image captioning and visual question answering typically use CNNs pretrained on ImageNet to extract representations of the image while several natural language tasks such as reading comprehension and sequence labeling have benefited from pretrained word embeddings thatare either fine tuned for a specific task or held fixed ,52,Transfer learning has driven a number of recent successes in computer vision and NLP .,"Many neural NLP systems are initialized with pretrained word embeddings but learn their representations of words in context from scratch , in a task - specific manner from supervised learning signals .",introduction
sentiment_analysis,4,We also find that contextually conditioned features perform better than context - less features .,training,Training Details,1,228,11,11,0,training : Training Details,0.6993865030674846,0.8461538461538461,0.8461538461538461,We also find that contextually conditioned features perform better than context less features ,14,"For visual features , however , a deeper CNN provides better representations .","Thus , in our experiments , we extract video - level contextual features for utterances from each modality using the network proposed by .",experiment
natural_language_inference,57,Published as a conference paper at ICLR 2016 similar objects are mapped to points thatare nearby in the embedding space .,introduction,introduction,0,21,14,14,0,introduction : introduction,0.12209302325581395,0.5185185185185185,0.5185185185185185,Published as a conference paper at ICLR 2016 similar objects are mapped to points thatare nearby in the embedding space ,21,"One line of work , exemplified by and first applied to the caption - image relationship by , requires the mapping to be distance - preserving : semantically 1 ar Xiv : 1511.06361v6LG ] 1 Mar 2016",symmetric distance measure such as Euclidean or cosine distance is typically used .,introduction
sentence_classification,2,"To solve the problems of the baselines discussed above , we introduce an attention - based neural multiple context fixing attachment ( MCFA ) 2 , a series of modules attached to the sentence vectors V .",model,Model,0,87,2,2,0,model : Model,0.34523809523809523,0.03773584905660377,0.3333333333333333,To solve the problems of the baselines discussed above we introduce an attention based neural multiple context fixing attachment MCFA 2 a series of modules attached to the sentence vectors V ,32, ,"MCFA attachment is used to fix the sentence vectors , by slightly modifying the per-dimension values of the vector , before concatenating them into the final feature vector .",method
sarcasm_detection,1,"After the training , this CNN model is used to infer the personality traits present in each comment .",method,Personality features,0,142,73,11,0,method : Personality features,0.4251497005988024,0.4834437086092716,0.2894736842105263,After the training this CNN model is used to infer the personality traits present in each comment ,18,"Specifically , the CNN is pre-trained on a benchmark corpus developed by which contains 2 , 400 essays and is labeled with the Big - Five personality traits , i.e. , Openness , Conscientiousness , Extraversion , Agreeableness , and Neuroticism ( OCEAN ) .",This is done by extracting the activations of the CNN 's last hidden layer vector which we call as the personality vector ? p j u i .,method
sentiment_analysis,27,The dimension of each word vector is 300 for GloVe and 768 for BERT .,experiment,Data sets and experimental settings,1,185,16,15,0,experiment : Data sets and experimental settings,0.6702898550724637,0.6666666666666666,0.6521739130434783,The dimension of each word vector is 300 for GloVe and 768 for BERT ,15,"In our implementation , we respectively use the GloVe 3 word vector and the pre-trained language model word representation BERT 4 to initialize the word embeddings .","The number of LSTM hidden units is set to 300 , and the output dimension of GCN layer is set to 600 .",experiment
sentiment_analysis,25,Attention - based LSTM with Aspect Embedding uses the embedding vectors of aspect words to selectively attend the regions of the representations generated by LSTMs .,system description,Aspect based Sentiment Analysis,0,67,23,18,0,system description : Aspect based Sentiment Analysis,0.30180180180180183,0.2875,1.0,Attention based LSTM with Aspect Embedding uses the embedding vectors of aspect words to selectively attend the regions of the representations generated by LSTMs ,25,"In this category , the model is asked to predict the sentiment polarity toward a predefined aspect category .", ,method
question-answering,3,"Therefore , the lower - level granularity is an indispensable factor for a good performance .",model,Comparing with State-of-the-art Models,0,208,38,9,0,model : Comparing with State-of-the-art Models,0.8188976377952756,0.5428571428571428,0.2195121951219512,Therefore the lower level granularity is an indispensable factor for a good performance ,14,"After adding some word overlap features between the two sentences , the performance was improved significantly ( the third row of ) .","conducted word alignment for a sentence pair based on word vectors , and measured the sentence similarity based on a couple of word alignment features .",method
natural_language_inference,33,We used minibatches of 48 examples and the Adam optimizer with a learning rate of 0.002 .,evaluation,APPENDIX 7 MODEL TRAINING,0,184,60,9,0,evaluation : APPENDIX 7 MODEL TRAINING,0.7049808429118773,0.4379562043795621,0.6923076923076923,We used minibatches of 48 examples and the Adam optimizer with a learning rate of 0 002 ,18,All models use word embeddings of 512 dimensions and GRUs with either 1500 or 2048 hidden units .,Models were trained for 7 days on an Nvidia Tesla P100 - SXM2 - 16GB GPU .,result
natural_language_inference,31,Occlusion sensitivity can help to reveal how much the model depends on each part when deciding on a specific category and we can make some speculations about how the model works based on the observations .,analysis,Analysis,0,216,33,33,0,analysis : Analysis,0.7826086956521741,0.5409836065573771,0.5409836065573771,Occlusion sensitivity can help to reveal how much the model depends on each part when deciding on a specific category and we can make some speculations about how the model works based on the observations ,36,"We use a three - block RE2 model to predict on SNLI dev set , mask one feature in one block to zeros at a time and report changes in accuracy of the three categories : entailment , neutral and contradiction .",shows the result of occlusion sensitivity .,result
sentence_compression,2,"We use three different sentence compression datasets , , , and the publically available subset of .",experiment,Compression data,0,79,11,2,0,experiment : Compression data,0.7821782178217822,0.7333333333333333,0.3333333333333333,We use three different sentence compression datasets and the publically available subset of ,14, ,"The first two consist of manually compressed newswire text in English , while the third is built heuristically from pairs of headlines and first sentences from newswire , resulting in the most aggressive compressions , as exemplified in .",experiment
natural_language_inference,58,"Thus , AS Reader can be viewed as a special case of ReasoNets with T max = 1 .",training,CNN and Daily Mail Datasets,0,170,39,35,0,training : CNN and Daily Mail Datasets,0.5074626865671642,0.5,0.625,Thus AS Reader can be viewed as a special case of ReasoNets with T max 1 ,17,"Following the The CNN and Daily Mail datasets are available at https://github.com/deepmind/rcdata settings in AS Reader , we sum up scores from the same candidate and make a prediction .","The maximum reasoning step , T max is set to 5 in experiments on both CNN and Daily Mail datasets .",experiment
semantic_role_labeling,1,We calculate a locally normalized distribution over role labels for token tin frame f using the softmax function :,model,Predicting semantic roles,0,120,87,8,0,model : Predicting semantic roles,0.5504587155963303,0.9886363636363636,0.8888888888888888,We calculate a locally normalized distribution over role labels for token tin frame f using the softmax function ,19,which can be computed in parallel across all semantic frames in an entire minibatch .,"At test time , we perform constrained decoding using the Viterbi algorithm to emit valid sequences of BIO tags , using unary scores sf t and the transition probabilities given by the training data .",method
relation_extraction,11,"The syntactic graph encoding from GCN is appended to Bi - GRU output to get the final token representation , h concat i as [ h gru i ; h gcn i k+1 ].",system description,Syntactic Sentence Encoding,0,138,71,20,0,system description : Syntactic Sentence Encoding,0.5564516129032258,0.5546875,0.7692307692307693,The syntactic graph encoding from GCN is appended to Bi GRU output to get the final token representation h concat i as h gru i h gcn i k 1 ,31,"We use ReLU as activation function f , throughout our experiments .","Since , not all tokens are equally relevant for RE task , we calculate the degree of relevance of each token using attention as used in",method
sentiment_analysis,43,We employ the stochastic gradient descent ( SGD ) optimizer to compute and update the training parameters .,training,Aspect Alignment Loss,0,160,18,17,0,training : Aspect Alignment Loss,0.6666666666666666,0.9473684210526316,0.9444444444444444,We employ the stochastic gradient descent SGD optimizer to compute and update the training parameters ,16,"Where ? ? 0 and ? ? 0 controls the influence of the aspect alignment loss and the L 2 regularization item , respectively .","In addition , we utilize dropout strategy to avoid overfitting .",experiment
text-classification,6,We explore combining the sentence and word level transfer models by concatenating their representations prior to feeding the combined representation :,model,Combined Transfer Models,0,89,2,2,0,model : Combined Transfer Models,0.6013513513513513,0.16666666666666666,0.16666666666666666,We explore combining the sentence and word level transfer models by concatenating their representations prior to feeding the combined representation ,21, ,Model performance on transfer tasks .,method
sarcasm_detection,1,Visualization of the user embedding cluster ( Section 4.6 ) provides insights for this positive trend .,ablation,Ablation Study,0,291,9,9,0,ablation : Ablation Study,0.8712574850299402,0.6,0.6,Visualization of the user embedding cluster Section 4 6 provides insights for this positive trend ,16,major boost in performance is observed ( 8 ? 12 % accuracy and F1 ) when user embeddings are introduced ( row 5 ) .,"Overall , CASCADE consisting of CNN with user embeddings and contextual discourse features provide the best performance in all three datasets ( row 6 ) .",result
natural_language_inference,75,"NNs we encode the key as the entire window , and the value as only the center word , which is not possible in the MemNN architecture .",model,Key-Value Memories,0,105,56,27,0,model : Key-Value Memories,0.5172413793103449,0.3636363636363637,0.6585365853658537,NNs we encode the key as the entire window and the value as only the center word which is not possible in the MemNN architecture ,26,"However , in Key - Value Mem","This makes sense because the entire window is more likely to be pertinent as a match for the question ( as the key ) , whereas the entity at the center is more pertinent as a match for the answer ( as the value ) .",method
sentiment_analysis,9,"Besides , the existing researches do not pay attention to the research of the Chinese - oriented ABSA task .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.036101083032490974,0.6666666666666666,0.6666666666666666,Besides the existing researches do not pay attention to the research of the Chinese oriented ABSA task ,18,Most of the existing work focuses on the subtask of aspect term polarity inferring and ignores the significance of aspect term extraction .,"Based on the local context focus ( LCF ) mechanism , this paper firstly proposes a multi-task learning model for Chineseoriented aspect - based sentiment analysis , namely LCF - ATEPC .",abstract
sentiment_analysis,2,"Finally , it concatenates the aspect term representation and context representation for predicting the sentiment polarity of the aspect terms within its contexts .",dataset,Datasets,0,142,11,11,0,dataset : Datasets,0.6255506607929515,0.3793103448275862,0.3793103448275862,Finally it concatenates the aspect term representation and context representation for predicting the sentiment polarity of the aspect terms within its contexts ,23,"IAN is able to interactively learn attentions in the contexts and aspect terms , and generates the representations for aspect terms and contexts separately .","MemNet : MemNet applies attention multiple times on the word embedding , so that more abstractive evidences could be selected from the external memory .",experiment
natural_language_inference,56,"The target is a single word , in this case "" garden "" , one - hot encoded over the full bAbI vocabulary of 177 words .",experiment,Experiments,0,87,8,8,0,experiment : Experiments,0.25892857142857145,0.07476635514018691,0.8,The target is a single word in this case garden one hot encoded over the full bAbI vocabulary of 177 words ,22,"Daniel put down the milk . """,task is considered solved if a model achieves greater than 95 % accuracy .,experiment
question-answering,8,We also further extend the boundary model with a search mechanism .,introduction,introduction,1,42,32,32,0,introduction : introduction,0.1686746987951807,0.8,0.8,We also further extend the boundary model with a search mechanism ,12,We propose two ways to apply the Ptr - Net model for our task : a sequence model and a boundary model .,Experiments on the SQuAD dataset show that our two models both outperform the best performance reported by .,introduction
sentiment_analysis,19,"We demonstrate the effectiveness of SuBiLSTM on several sentence modeling tasks in NLP - general sentence representation , text classification , textual entailment and paraphrase detection .",introduction,introduction,0,41,25,25,0,introduction : introduction,0.2679738562091503,0.6944444444444444,0.6944444444444444,We demonstrate the effectiveness of SuBiLSTM on several sentence modeling tasks in NLP general sentence representation text classification textual entailment and paraphrase detection ,24,"However , the main motivation for introducing SuBiLSTMs is to apply it to problems that require whole sentence modeling e.g. text classification , where the richer contextual information can be helpful .","In each of these tasks , we show gains by simply replacing BiLSTMs in strong base models , achieving a new state - of - the - art in fine grained sentiment classification and question classification .",introduction
natural_language_inference,40,"The DAG - RNN baseline from and the shared version of MAGE ( where edge representations are tied ) also perform worse , showing that our proposed architecture is superior .",performance,Performance Comparison,1,185,16,16,0,performance : Performance Comparison,0.6678700361010831,0.5333333333333333,0.5333333333333333,The DAG RNN baseline from and the shared version of MAGE where edge representations are tied also perform worse showing that our proposed architecture is superior ,27,"Adding the same information as one - hot features fails to improve the performance , which indicates that the inductive bias we employ on MAGE is useful .",Our motivation in incorporating the DAG structure in text comprehension models is to help the reader model long term dependencies in the input sequences .,result
sentiment_analysis,23,"Let i be the position of a word in a sentence ( i = 1 , 2 , , n ) .",analysis,Question Classification,0,159,18,17,0,analysis : Question Classification,0.5445205479452054,0.8181818181818182,0.8095238095238095,Let i be the position of a word in a sentence i 1 2 n ,16,"For kslot pooling , we can adopt an "" equal allocation "" strategy , shown in .","It s extracted feature vector is pooled to the j - th slot , if",result
relation-classification,2,"Specifically , we use the boundaries and the strict settings .",result,Results,1,250,30,29,0,result : Results,0.8474576271186439,0.7692307692307693,0.7631578947368421,Specifically we use the boundaries and the strict settings ,10,"We also report results for the DREC dataset , with two different evaluation settings .","We transform the previous results from to the boundaries setting to make them comparable to our model since in their work , they report token - based F 1 score , which is not a common evaluation metric in relation extraction problems .",result
natural_language_inference,79,"While paragraph vectors are unique among paragraphs , the word vectors are shared .",introduction,introduction,1,29,17,17,0,introduction : introduction,0.1082089552238806,0.5,0.5,While paragraph vectors are unique among paragraphs the word vectors are shared ,13,Both word vectors and paragraph vectors are trained by the stochastic gradient descent and backpropagation .,"At prediction time , the paragraph vectors are inferred by fixing the word vectors and training the new paragraph vector until convergence .",introduction
part-of-speech_tagging,5,Part - of - Speech Tagging Results,system description,Part-of-Speech Tagging Results,1,135,1,1,0,system description : Part-of-Speech Tagging Results,0.6683168316831684,0.1111111111111111,0.1111111111111111,Part of Speech Tagging Results,5, , ,method
sentiment_analysis,18,"We found the learned embeddings to be semantically meaningful , i.e. , embeddings of words that are semantically related appear close to the same aspect embedding .",introduction,introduction,0,44,30,30,0,introduction : introduction,0.18410041841004185,0.7894736842105263,0.7894736842105263,We found the learned embeddings to be semantically meaningful i e embeddings of words that are semantically related appear close to the same aspect embedding ,26,The autoencoder structure is jointly trained with a neural attention - based sentiment classifier to provide a good target representation as well as a high accuracy on the predicted sentiment .,"For example , embeddings of the words service , servers , staff , and courteous appear close to the same aspect embedding , which we interpret to represent the aspect service .",introduction
machine-translation,7,The amount of noise per component is controlled by a second trainable weight matrix W noise .,system description,GATING NETWORK,0,95,52,12,0,system description : GATING NETWORK,0.2546916890080429,1.0,1.0,The amount of noise per component is controlled by a second trainable weight matrix W noise ,17,"The noise term helps with load balancing , as will be discussed in Appendix A .", ,method
natural_language_inference,54,"However , we found that for SECT model , its mean performance reached the peak while standard deviations narrowed when window size reaches 10 .",analysis,Window Size Analysis,0,170,7,7,0,analysis : Window Size Analysis,0.74235807860262,0.175,0.5,However we found that for SECT model its mean performance reached the peak while standard deviations narrowed when window size reaches 10 ,23,In general the results illustrate that performances of the models increase with the length of the window .,We also observed that larger window size does not generate predictive results that is as good as the one with window size set to 10 .,result
natural_language_inference,38,We then show it can be stacked to give multiple hops in memory .,model,Memory Network of Full-Orientation Matching,0,73,32,6,0,model : Memory Network of Full-Orientation Matching,0.3862433862433862,0.4848484848484849,0.5454545454545454,We then show it can be stacked to give multiple hops in memory ,14,"We start by describing our model in the single layer case , which implements a single memory hop operation .","The input of this step is the representations {r Pt } n t =1 , {r Q t } m t=1 and u Q .",method
natural_language_inference,2,Each row of F measures how relevant every passage word is with respect to a given questiondependent passage encoding of a word .,architecture,Multi-factor Attentive Encoding,0,104,59,19,0,architecture : Multi-factor Attentive Encoding,0.3880597014925373,0.5412844036697247,0.59375,Each row of F measures how relevant every passage word is with respect to a given questiondependent passage encoding of a word ,23,"where F i , j represents the element in the ith row and jth column of F ? R T T .","We apply a row - wise softmax function on F to normalize the attention weights , obtaining F ? R T T .",method
natural_language_inference,100,We focus on a NMM - 1 as the example due to the space limitation .,analysis,Parameter Sensitivity Analysis,0,346,3,3,0,analysis : Parameter Sensitivity Analysis,0.9453551912568308,0.2307692307692308,0.2307692307692308,We focus on a NMM 1 as the example due to the space limitation ,15,We perform parameter sensitivity analysis of our proposed model a NMM .,"For a NMM - 1 , we fix the number of bins as 600 and change the dimension of word vectors .",result
named-entity-recognition,6,"While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02358490566037736,0.3,0.3,While some features do remain in state of the art systems lexical features have been mostly discarded with the exception of gazetteers ,23,Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features .,"In this work , we show that this is unfair : lexical features are actually quite useful .",abstract
natural_language_inference,28,"LSTM is designed to solve this problem , but gradient clipping ) is still required for training .",approach,UNITARY APPROACH,0,53,13,13,0,approach : UNITARY APPROACH,0.1955719557195572,0.3611111111111111,0.5909090909090909,LSTM is designed to solve this problem but gradient clipping is still required for training ,16,"As long as the eigenvalues of D ( k ) are of order unity , then if W has eigenvalues ? i 1 , they will cause gradient explosion ? C /?h ( T ) ? ? , while if W has eigenvalues ? i 1 , they can cause gradient vanishing , ?C /?h ( T ) ? 0 . Either situation hampers the efficiency of RNN .","Recently , by restraining the hidden - to - hidden matrix to be orthogonal or unitary , many models have overcome the problem of exploding and vanishing gradients .",method
sentiment_analysis,42,"In this work , the attention layer in one layer is essentially a weighted average compositional function , which is not powerful enough to handle the sophisticated computationality like negation , intensification and contrary in language .",model,The Need for Multiple Hops,0,133,7,3,0,model : The Need for Multiple Hops,0.5277777777777778,0.7,0.5,In this work the attention layer in one layer is essentially a weighted average compositional function which is not powerful enough to handle the sophisticated computationality like negation intensification and contrary in language ,34,It is widely accepted that computational models thatare composed of multiple processing layers have the ability to learn representations of data with multiple levels of abstraction .,Multiple computational layers allow the deep memory network to learn representations of text with multiple levels of abstraction .,method
relation-classification,9,"For relations in Sci - ERC , our results are not comparable with those in because we are performing relation classification given gold entities , while they perform joint entity and relation extraction .",result,Results,0,121,17,17,0,result : Results,0.8231292517006803,0.8095238095238095,1.0,For relations in Sci ERC our results are not comparable with those in because we are performing relation classification given gold entities while they perform joint entity and relation extraction ,31,"In addition , SCIBERT achieves new SOTA results on ACL - ARC , and the NER part of SciERC .", ,result
natural_language_inference,31,Dropout with a keep probability of 0.8 is applied before every fully - connected or convolutional layer .,implementation,Implementation Details,1,134,9,9,0,implementation : Implementation Details,0.4855072463768116,0.3333333333333333,0.39130434782608703,Dropout with a keep probability of 0 8 is applied before every fully connected or convolutional layer ,18,All other parameters are initialized with He initialization and normalized by weight normalization .,The kernel size of the convolutional encoder is set to 3 .,experiment
text_summarization,1,"Given an optimal ranking method ( oracle ) , this metric measures the upper bound of Top - 1 accuracy by comparing the best hypothesis with the target .",baseline,Metrics: Accuracy and Diversity,0,179,26,9,0,baseline : Metrics: Accuracy and Diversity,0.7458333333333333,0.6842105263157895,0.42857142857142855,Given an optimal ranking method oracle this metric measures the upper bound of Top 1 accuracy by comparing the best hypothesis with the target ,25,This measures the quality of the target distribution coverage among the Top - K generated target sequences .,"Concretely , we generate hypotheses {? 1 . . .? K } from each source x and keep the hypothesis ? best that achieves the best sentence - level metric with the target y .",result
sentiment_analysis,41,"Let H ? R d N be a matrix consisting of hidden vectors [ h 1 , . . . , h N ] that the LSTM produced , where d is the size of hidden layers and N is the length of the given sentence .",system description,Attention-based LSTM (AT-LSTM),0,91,53,5,0,system description : Attention-based LSTM (AT-LSTM),0.4080717488789238,0.6883116883116883,0.21739130434782608,Let H R d N be a matrix consisting of hidden vectors h 1 h N that the LSTM produced where d is the size of hidden layers and N is the length of the given sentence ,38,represents the architecture of an Attentionbased LSTM ( AT - LSTM ) .,"Let H ? R d N be a matrix consisting of hidden vectors [ h 1 , . . . , h N ] that the LSTM produced , where d is the size of hidden layers and N is the length of the given sentence .",method
natural_language_inference,75,"Triple Knowledge base entries have a structure of triple "" subject relation object "" ( see for examples ) .",model,Key-Value Memories,0,92,43,14,0,model : Key-Value Memories,0.4532019704433498,0.2792207792207792,0.3414634146341464,Triple Knowledge base entries have a structure of triple subject relation object see for examples ,16,"We now describe several possible variants of ? K and ? V that we tried in our experiments , for simplicity we kept ? X and ? Y fixed as bag - of - words representations .","The representation we consider is simple : the key is composed of the left - hand side entity ( subject ) and the relation , and the value is the right - hand side entity ( object ) .",method
sentiment_analysis,9,The extraction of aspects based on the features of the global context .,model,Model Architecture,0,82,12,12,0,model : Model Architecture,0.29602888086642604,0.15,1.0,The extraction of aspects based on the features of the global context ,13,The feature interactive learning ( FIL ) layer combines the learning of the interaction between local context features and global context features and predicts the sentiment polarity of aspects ., ,method
natural_language_inference,63,We achieve 68.13 EM and 70.32 F1 for short documents and 63.44 and 65.19 for long documents which are the current best results .,result,QUASAR-T:,1,151,9,5,0,result : QUASAR-T:,0.8388888888888889,0.2727272727272727,0.1724137931034483,We achieve 68 13 EM and 70 32 F1 for short documents and 63 44 and 65 19 for long documents which are the current best results ,28,"As described in , the baseline ( BiDAF + DNC ) results in a reasonable gain , however , our proposed memory controller gives more performance improvement .",TriviaQA : We compare proposed model with all the previously suggested approaches as shown in .,result
sentiment_analysis,20,"Let x be the vector with the class counts and ? the smoothing factor , we obtain class weights with w i = max ( x )",model,Class Weights,0,142,66,5,0,model : Class Weights,0.7634408602150538,0.9565217391304348,0.625,Let x be the vector with the class counts and the smoothing factor we obtain class weights with w i max x ,23,"Moreover , we introduce a smoothing factor in order to smooth out the weights in cases where the imbalances are very strong , which would otherwise lead to extremely large class weights .","Let x be the vector with the class counts and ? the smoothing factor , we obtain class weights with w i = max ( x )",method
natural_language_inference,94,"We publicly release all our code , models , and data at :",introduction,introduction,0,26,4,4,0,introduction : introduction,0.06788511749347259,0.19047619047619047,0.19047619047619047,We publicly release all our code models and data at ,11,This * Equal contribution .,https://github.com/yicheng-w/CommonSenseMultiHopQA task tests a model 's natural language understanding capabilities by asking it to answer a question based on a passage of relevant content .,introduction
machine-translation,1,We evaluate the full ByteNet on the WMT English to German translation task .,model,model,0,169,25,25,0,model : model,0.8407960199004975,0.5102040816326531,0.5102040816326531,We evaluate the full ByteNet on the WMT English to German translation task ,14,Character - Level Machine Translation,We use NewsTest 2013 for validation and NewsTest 2014 and 2015 for testing .,method
text-classification,5,"Fine- tuning has been used successfully to transfer between similar tasks , e.g. in QA , for distantly supervised sentiment analysis , or MT domains but has been shown to fail between unrelated ones .",system description,Universal Language Model Fine-tuning,0,53,2,2,0,system description : Universal Language Model Fine-tuning,0.21031746031746032,0.021052631578947368,0.08333333333333333,Fine tuning has been used successfully to transfer between similar tasks e g in QA for distantly supervised sentiment analysis or MT domains but has been shown to fail between unrelated ones ,33, ,"also fine - tune a language model , but overfit with 10 k labeled examples and require millions of in - domain documents for good performance .",method
natural_language_inference,45,"We use named entity tags , part - ofspeech tags , document frequencies , and word - level representations as the features for token properties which determine the gate .",introduction,introduction,1,33,24,24,0,introduction : introduction,0.1658291457286432,0.6,0.6,We use named entity tags part ofspeech tags document frequencies and word level representations as the features for token properties which determine the gate ,25,Each dimension of the gate controls how much information is flowed from the word - level and character - level representations respectively .,"More generally , our fine - grained gating mechanism can be used to model multiple levels of structure in language , including words , characters , phrases , sentences and paragraphs .",introduction
natural_language_inference,69,"More formally , for a given document d and answer candidate c , let cooccurrence ( d , c ) denote the total count of how often d co-occurs with c in a sample where c is also the correct answer .",system description,Document-Answer Correlations,0,135,10,10,0,system description : Document-Answer Correlations,0.39130434782608703,0.3703703703703704,0.9090909090909092,More formally for a given document d and answer candidate c let cooccurrence d c denote the total count of how often d co occurs with c in a sample where c is also the correct answer ,38,The statistic counts how often a candidate c is observed as the correct answer when a certain document is present in Sq across training set samples .,"We use this statistic to filter the dataset , by discarding samples with at least one document - candidate pair ( d , c ) for which cooccurrence ( d , c ) > 20 .",method
natural_language_inference,55,We also removed all entities appearing less than 5 times and finally obtained a Freebase set containing 14M triples made of 2.2 M entities and 7 k relation types .,system description,WebQuestions,0,44,19,11,0,system description : WebQuestions,0.2993197278911565,0.2878787878787879,0.3666666666666665,We also removed all entities appearing less than 5 times and finally obtained a Freebase set containing 14M triples made of 2 2 M entities and 7 k relation types ,31,"We used a subset , created by only keeping triples where one of the entities was appearing in either the WebQuestions training / validation set or in ClueWeb extractions .","Since the format of triples does not correspond to any structure one could find in language , we decided to transform them into automatically generated questions .",method
natural_language_inference,73,"Hard attention mechanism forces a model to concentrate solely on the important elements , entirely discarding the others .",introduction,introduction,0,24,12,12,0,introduction : introduction,0.0916030534351145,0.35294117647058826,0.35294117647058826,Hard attention mechanism forces a model to concentrate solely on the important elements entirely discarding the others ,18,"Unlike the widely - studied soft attention , in hard attention , a subset of elements is selected from an input sequence .","In fact , various NLP tasks solely rely on very sparse tokens from along text input .",introduction
natural_language_inference,70,"q , c i , which is the score of the pair q , c i provided by the model trained on Subtask A ; p o , c i , which is the score of the pair o , c i provided by the model trained on Subtask A ; p o , q , which is the score of the pair o , q provided by the model trained on Subtask B.",system description,Stacking classifiers across subtasks,0,119,90,12,0,system description : Stacking classifiers across subtasks,0.68,0.967741935483871,0.8,q c i which is the score of the pair q c i provided by the model trained on Subtask A p o c i which is the score of the pair o c i provided by the model trained on Subtask A p o q which is the score of the pair o q provided by the model trained on Subtask B ,64,"We thus developed a stacking strategy for Subtask C that uses the following scores in the classification step , w.r.t. an original question o and the comment c i from the thread of q:","q , c i , which is the score of the pair q , c i provided by the model trained on Subtask A ; p o , c i , which is the score of the pair o , c i provided by the model trained on Subtask A ; p o , q , which is the score of the pair o , q provided by the model trained on Subtask B.",method
natural_language_inference,31,We also provide a simplified version of the prediction layer .,approach,Prediction Layer,0,98,62,11,0,approach : Prediction Layer,0.355072463768116,0.96875,0.8461538461538461,We also provide a simplified version of the prediction layer ,11,"In symmetric tasks like paraphrase identification , asymmetric version of the prediction layer is used for better generalization :",Which version to use is treated as a hyperparameter .,method
sentiment_analysis,47,"Specifically , each word is represented as word embedding .",model,Left-Center-Right Separated LSTMs,0,81,11,9,0,model : Left-Center-Right Separated LSTMs,0.3767441860465116,0.24444444444444444,0.75,Specifically each word is represented as word embedding ,9,"Accordingly , the LCR - Rot model is composed of three Bi - LSTMs , i.e. , left - , center - , and right - Bi - LSTM , respectively modeling left context , target phrase and right context in the sentence .","All the word vectors are stacked in a word embedding matrix L w ? R d|V | , where d is the dimension of word vector and | V | is vocabulary size .",method
natural_language_inference,89,"We evaluate our approach on the SQuAD 2.0 dataset ( Rajpurkar , Jia , and Liang 2018 ) .",dataset,Experimental Setup Dataset,0,156,2,2,1,dataset : Experimental Setup Dataset,0.6,0.4,0.4,We evaluate our approach on the SQuAD 2 0 dataset Rajpurkar Jia and Liang 2018 ,16, ,SQuAD 2.0 is a new machine reading comprehension benchmark that aims to test the models whether they have truely understood the questions by knowing what they do n't know .,experiment
relation-classification,9,"For text classification ( i.e. CLS and REL ) , we feed the final BERT vector for the [ CLS ] token into a linear classification layer .",system description,Finetuning BERT,0,83,26,3,0,system description : Finetuning BERT,0.564625850340136,0.5531914893617021,0.2727272727272727,For text classification i e CLS and REL we feed the final BERT vector for the CLS token into a linear classification layer ,24,"We mostly follow the same architecture , optimization , and hyperparameter choices used in .","For sequence labeling ( i.e. NER and PICO ) , we feed the final BERT vector for each token into a linear classification layer with softmax output .",method
text_generation,0,"Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model .",introduction,introduction,1,34,23,23,0,introduction : introduction,0.10493827160493828,0.7931034482758621,0.7931034482758621,Unlike the work in that requires a task specific sequence score such as BLEU in machine translation to give the reward we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model ,42,The generative model is treated as an agent of reinforcement learning ( RL ) ; the state is the generated tokens so far and the action is the next token to be generated .,"To solve the problem that the gradient can not pass back to the generative model when the output is discrete , we regard the generative model as a stochastic parametrized policy .",introduction
natural_language_inference,1,"Which forest is Fires Creek in ? What is an active ingredient in childrens earache relief ? tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",introduction,introduction,0,21,14,14,0,introduction : introduction,0.07692307692307693,0.56,0.56,Which forest is Fires Creek in What is an active ingredient in childrens earache relief tators and associated to Freebase facts while the largest existing benchmark WebQuestions contains less than 6 k questions created automatically using the Google suggest API ,41,"This dataset , which is presented in Section 2 , contains more than 100 k questions written by human anno-What American cartoonist is the creator of Andy Lippincott ?","Which forest is Fires Creek in ? What is an active ingredient in childrens earache relief ? tators and associated to Freebase facts , while the largest existing benchmark , WebQuestions , contains less than 6 k questions created automatically using the Google suggest API .",introduction
question-answering,1,") it shrinks the size of the representation by half , thus quickly absorbs the differences in length for sentence representation , and 2 ) it filters out undesirable composition of words ( see Section 2.1 for some analysis ) .",model,Max-Pooling,0,45,19,4,0,model : Max-Pooling,0.23316062176165805,0.5757575757575758,1.0, it shrinks the size of the representation by half thus quickly absorbs the differences in length for sentence representation and 2 it filters out undesirable composition of words see Section 2 1 for some analysis ,37,The effects of pooling are two - fold :, ,method
natural_language_inference,73,All experiments are conducted in Python with Tensorflow and run on a Nvidia GTX 1080 Ti .,training,Model Training,1,177,35,35,0,training : Model Training,0.6755725190839694,0.9210526315789472,0.9210526315789472,All experiments are conducted in Python with Tensorflow and run on a Nvidia GTX 1080 Ti ,17,rand ? s are then optimized simultaneously to pursue a better performance by selecting critical token pairs and exploring their dependencies .,"We use Adadelta as optimizer , which performs more stable than Adam on ReSAN .",experiment
natural_language_inference,51,"Excluding baselines under different setups , our result is the best reported mean result on bAbI that we are aware of .",model,Text Question Answering,0,217,14,11,0,model : Text Question Answering,0.8188679245283019,0.3333333333333333,0.28205128205128205,Excluding baselines under different setups our result is the best reported mean result on bAbI that we are aware of ,21,The mean and s.d. across 10 runs are also compared with other results reported by recent works ( see ) .,More details are described in App.,method
text-classification,6,Many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or Glo Ve .,introduction,introduction,0,17,5,5,0,introduction : introduction,0.11486486486486487,0.3125,0.3125,Many models address the problem by implicitly performing limited transfer learning through the use of pre trained word embeddings such as those produced by word2vec or Glo Ve ,29,"Given the high cost of annotating supervised training data , very large training sets are usually not available for most research or industry NLP tasks .","However , recent work has demonstrated strong transfer task performance using pre-trained sentence level embeddings .",introduction
part-of-speech_tagging,2,"In these cases , transfer learning can improve performance by taking advantage of more plentiful labels from related tasks .",introduction,introduction,0,17,6,6,0,introduction : introduction,0.09550561797752807,0.2857142857142857,0.2857142857142857,In these cases transfer learning can improve performance by taking advantage of more plentiful labels from related tasks ,19,"Transfer learning can be used in several settings , notably for low - resource languages and low - resource domains such as biomedical corpora and Twitter corpora ) .","Even on datasets with relatively abundant labels , multi-task transfer can sometimes achieve improvement over state - of - the - art results .",introduction
natural_language_inference,80,"Finally , high attention between { decides , lay } and { rolling } , and { Male } and { guy } leads the model to correctly classify the sentence pair as contradiction ( for more samples with attention visualizations , see Section D of the Appendix ) .",analysis,analysis,0,234,28,28,0,analysis : analysis,0.8068965517241379,1.0,1.0,Finally high attention between decides lay and rolling and Male and guy leads the model to correctly classify the sentence pair as contradiction for more samples with attention visualizations see Section D of the Appendix ,36,"indicates the model 's ability in attending to critical pairs of words like < Male , guy > , < decides , rolling > , and < lay , rolling > .", ,result
natural_language_inference,84,"These sources of auxiliary information were available for 63.35 % , 97.43 % and 100 % of the rest of occurrences respectively .",model,LANGUAGE MODELLING,0,188,10,10,0,model : LANGUAGE MODELLING,0.8173913043478261,0.3125,0.3125,These sources of auxiliary information were available for 63 35 97 43 and 100 of the rest of occurrences respectively ,21,"We consider computing embeddings of the less frequent input words from their dictionary definitions , GloVe vectors and spellings .","In order to compare how helpful these sources are when they are available , we run additional set of experiments with "" restricted "" inputs .",method
natural_language_inference,58,"The learnable parameters ? of the ReasoNet are the embedding matrices ? W , attention network ? x , the state RNN network ? s , the answer action network ? a , and the termination gate network ? t .",system description,REASONING NETWORKS,0,111,31,31,0,system description : REASONING NETWORKS,0.33134328358208953,0.6078431372549019,0.6078431372549019,The learnable parameters of the ReasoNet are the embedding matrices W attention network x the state RNN network s the answer action network a and the termination gate network t ,31,The termination step T varies from instance to instance .,"The learnable parameters ? of the ReasoNet are the embedding matrices ? W , attention network ? x , the state RNN network ? s , the answer action network ? a , and the termination gate network ? t .",method
question-answering,3,"The local - 3 function worked better than the max function in term of the MAP , and also got a comparable MRR .",model,Model Properties,0,182,12,12,0,model : Model Properties,0.7165354330708661,0.17142857142857146,0.4137931034482759,The local 3 function worked better than the max function in term of the MAP and also got a comparable MRR ,22,"But after we enlarged the window size to 4 , the performance dropped .","Therefore , we use the local - 3 function in the following experiments .",method
question_similarity,0,This is probably because a lot of information is lost during the translation process .,experiment,Other Attempts,0,118,38,7,0,experiment : Other Attempts,0.855072463768116,1.0,1.0,This is probably because a lot of information is lost during the translation process ,15,"Moreover , an attempt to overcome the weakness of the Arabic ELMo model is done by translating the data to English using Google Translate 8 and treating the problem as an English SQS problem instead , but the results are much worse with 88.868 and 87.504 F1 - scores on public and private leaderboards , respectively .", ,experiment
text-classification,2,"For a set of N documents , this leads to minimizing the negative loglikelihood over the classes :",model,Model architecture,0,28,12,12,0,model : Model architecture,0.3010752688172043,0.3870967741935484,0.8571428571428571,For a set of N documents this leads to minimizing the negative loglikelihood over the classes ,17,We use the softmax function f to compute the probability distribution over the predefined classes .,"where x n is the normalized bag of features of the nth document , y n the label , A and B the weight matrices .",method
sentiment_analysis,9,"The CDM layer works better on twitter dataset because there are a lot of non-standard grammar usage and language abbreviations within it , and the local context focus techniques can promote to infer the polarity of terms .",analysis,Overall Performance Analysis,1,238,4,4,0,analysis : Overall Performance Analysis,0.8592057761732852,0.11428571428571427,0.26666666666666666,The CDM layer works better on twitter dataset because there are a lot of non standard grammar usage and language abbreviations within it and the local context focus techniques can promote to infer the polarity of terms ,38,"Benefit from the joint training process , the two ABSA subtasks of APC and ATE can promote each other and improve the performance .","Surprisingly , for the Laptop and Restaurant datasets , guests occasionally have a unified "" global "" view in a specific review .",result
natural_language_inference,7,"Formally , the passage - independent question representation q indep is computed as follows :",model,QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0,99,48,20,0,model : QUESTION-FOCUSED PASSAGE WORD EMBEDDING,0.5561797752808989,0.9056603773584906,0.9090909090909092,Formally the passage independent question representation q indep is computed as follows ,13,The goal is to generate a coarse - grained summary of the question that depends on word order .,This representation is a bidirectional generalization of the question representation recently proposed by for a different question - answering task .,method
natural_language_inference,44,"In this case , our selector chooses the second and the third sentences instead of the oracle sentence because the former contains more information relevant to question .",system description,SQuAD and NewsQA,0,146,53,43,0,system description : SQuAD and NewsQA,0.5104895104895105,0.5824175824175825,0.5308641975308642,In this case our selector chooses the second and the third sentences instead of the oracle sentence because the former contains more information relevant to question ,27,"In the last example , our sentence selector fails to choose the oracle sentence , so the QA model can not predict the correct answer .","In fact , the context over the first and the second sentences is required to correctly answer the question .",method
sarcasm_detection,0,"Either a user does not know of the "" / s "" convention , or the user believes their use of sarcasm is obvious enough to warrant not including the tag .",approach,Limitations of Our Approach,0,103,21,21,0,approach : Limitations of Our Approach,0.5852272727272727,0.65625,0.65625,Either a user does not know of the s convention or the user believes their use of sarcasm is obvious enough to warrant not including the tag ,28,There are two primary ways a false negative can arise :,"Notably , such a belief depends on what community the user is communicating in , who the user is communicating with ( another user they routinely have arguments with , or a stranger ) , and also on prior comments on the thread .",method
text-classification,7,"From singlelabel to multi-label ( with n category labels ) text classification , the label space is expanded from n to 2 n , thus more training is required to cover the whole label space .",ablation,Single-Label to Multi-Label Text Classification,0,159,9,4,0,ablation : Single-Label to Multi-Label Text Classification,0.654320987654321,0.16071428571428573,0.16666666666666666,From singlelabel to multi label with n category labels text classification the label space is expanded from n to 2 n thus more training is required to cover the whole label space ,33,"Multi-label text classification is , however , a more challenging practical problem .","For single - label texts , it is practically easy to collect and annotate the samples .",result
natural_language_inference,96,"one - layer CNN , with filter sizes ranging from 2 - 5 , over the second sentence .",system description,Stylistic models for adversarial filtering,0,150,92,10,0,system description : Stylistic models for adversarial filtering,0.38461538461538464,0.7666666666666667,0.38461538461538464,one layer CNN with filter sizes ranging from 2 5 over the second sentence ,15,bag - of - words model that averages the word embeddings of the second sentence as features .,. A bidirectional LSTM over the 100 most common words in the second sentence ; uncommon words are replaced by their POS tags .,method
question_answering,0,Each question and semantic graph are encoded into fixed - size vector representations and the reward function ? is used to score the graphs .,training,training,0,207,24,24,0,training : training,0.7016949152542373,0.8571428571428571,0.8571428571428571,Each question and semantic graph are encoded into fixed size vector representations and the reward function is used to score the graphs ,23,"On QALD - 7 , we use the annotated entities provided by the Shared Task organizers .",Each question and semantic graph are encoded into fixed - size vector representations and the reward function ? is used to score the graphs .,experiment
text_summarization,14,"shows that the seq2seq model produces more novel words ( i.e. , words that do not appear in the article ) than our model , indicating a lower degree of abstraction for our model .",analysis,Does our summarization model learn entailment knowledge?,1,202,16,14,0,analysis : Does our summarization model learn entailment knowledge?,0.8859649122807017,0.4571428571428571,0.6666666666666666,shows that the seq2seq model produces more novel words i e words that do not appear in the article than our model indicating a lower degree of abstraction for our model ,32,"Thus , readers may wonder whether our model is less abstractive .","However , when we exclude all the words not in the reference ( these words may lead to wrong information ) , our model generates more novel words , suggesting that our model provides a compromise solution for informativeness and correctness .",result
text_generation,3,"First , motivated by , we use two auto- encoders to learn the semantic representations of inputs and responses in an unsupervised style .",introduction,introduction,1,25,15,15,0,introduction : introduction,0.1773049645390071,0.6818181818181818,0.6818181818181818,First motivated by we use two auto encoders to learn the semantic representations of inputs and responses in an unsupervised style ,22,"To address this problem , we propose a novel Auto - Encoder Matching model to learn utterance - level dependency .","Second , given the utterance - level representations , the mapping module is taught to learn the utterance - level dependency .",introduction
sentiment_analysis,13,Research also shows that yes / no questions are very frequent for products with complicated specifications .,introduction,introduction,0,38,27,27,0,introduction : introduction,0.1366906474820144,0.6136363636363636,0.6136363636363636,Research also shows that yes no questions are very frequent for products with complicated specifications ,16,"RRC poses some domain challenges compared to the traditional MRC on Wikipedia , such as the need for rich product knowledge , informal text , and fine - grained opinions ( there is almost no subjective content in Wikipedia articles ) .","To the best of our knowledge , no existing work has been done in RRC .",introduction
natural_language_inference,28,This task requires RNN to remember the whole sequence of the data and perform extra logic on the sequence .,experiment,ASSOCIATIVE RECALL TASK,0,167,28,3,0,experiment : ASSOCIATIVE RECALL TASK,0.6162361623616236,0.5490196078431373,0.2,This task requires RNN to remember the whole sequence of the data and perform extra logic on the sequence ,20,Another important synthetic task to test the memory ability of recurrent neural network is the Associative Recall .,We follow the same setting as in and and modify the original task so that it can test for longer sequences .,experiment
natural_language_inference,100,Larger corpus requires higher dimension of word vectors to embed terms in vocabulary .,analysis,Parameter Sensitivity Analysis,0,353,10,10,0,analysis : Parameter Sensitivity Analysis,0.96448087431694,0.7692307692307693,0.7692307692307693,Larger corpus requires higher dimension of word vectors to embed terms in vocabulary ,14,The choice of word vector dimension also depends on the size of training corpus .,"2 ) For the number of bins , we can see that MAP and MRR will decrease as the bin number increase .",result
sentiment_analysis,37,Such complex relation between the aspects and the corresponding sentiment - bearing word is grasped by IARM as shown in .,result,Case Study,0,229,38,30,0,result : Case Study,0.9051383399209486,0.9047619047619048,0.8823529411764706,Such complex relation between the aspects and the corresponding sentiment bearing word is grasped by IARM as shown in ,20,"For example , "" my favs here are the tacos pastor and the tostada de tinga "" where the aspects "" tacos pastor "" and "" tostada de tinga "" are connected using conjunction "" and "" and both rely on the sentiment bearing word favs .","Another example where the inter-aspect relation is necessary for the correct classification is shown in , where the aspects "" atmosphere "" and "" service "" both rely on the sentiment bearing word "" good "" , due to the conjunction "" and "" .",result
relation-classification,1,Type end model with biased loss function to suit for the novel tags .,introduction,introduction,0,48,40,40,0,introduction : introduction,0.1951219512195122,0.975609756097561,0.975609756097561,Type end model with biased loss function to suit for the novel tags ,14,"3 ) Furthermore , we also develop an end - to - 1 https://github.com/shanzhenren/Co",It can enhance the association between related entities .,introduction
text_summarization,0,"Meanwhile , the model should not be sensitive to the diverse unimportant aspects introduced by some reader comments .",introduction,introduction,0,49,34,34,0,introduction : introduction,0.16279069767441862,0.6666666666666666,0.6666666666666666,Meanwhile the model should not be sensitive to the diverse unimportant aspects introduced by some reader comments ,18,The second challenge is how to generate summaries by jointly modeling the main aspect of document and the reader focused aspect revealed by comments .,"Thus , simply absorbing all the reader aspect information to directly guide the model to generate summary is not feasible , as it will make the generator lose the ability of modeling the main aspect .",introduction
text-classification,7,"They said this had come as a surprise and expected the targets , 2.90 marks and 1.60 dlrs , to be promptly tested in the foreign exchange markets .",ablation,Interest Rates,0,196,46,4,0,ablation : Interest Rates,0.8065843621399177,0.8214285714285714,0.3333333333333333,They said this had come as a surprise and expected the targets 2 90 marks and 1 60 dlrs to be promptly tested in the foreign exchange markets ,29,"Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling against the dollar and mark , dealers said .",Sterling opened 0.3 points lower in trade weighted terms at 71.3 .,result
natural_language_inference,43,"QA can be coarsely divided into semantic parsingbased QA , where a question is translated into a logical form that is executed against a knowledgebase , and unstructured QA , where a question is answered directly from some relevant text .",introduction,introduction,0,11,3,3,0,introduction : introduction,0.07096774193548387,0.13043478260869565,0.13043478260869565,QA can be coarsely divided into semantic parsingbased QA where a question is translated into a logical form that is executed against a knowledgebase and unstructured QA where a question is answered directly from some relevant text ,38,"Question answering ( QA ) has witnessed a surge of interest in recent years , as it is one of the prominent tests for natural language understanding .","In semantic parsing , background knowledge has already been compiled into a knowledge - base ( KB ) , and thus the challenge is in interpreting the question , which may contain compositional constructions ( "" What is the second - highest mountain in Europe ? "" ) or computations ( "" What is the difference in population between France and Germany ? "" ) .",introduction
natural_language_inference,19,case study was conducted to investigate the role of each structure of the Distance - based Self - Attention Network .,result,Case Study,0,197,25,2,0,result : Case Study,0.7725490196078432,0.3012048192771085,0.6666666666666666,case study was conducted to investigate the role of each structure of the Distance based Self Attention Network ,19, ,"For this , a sentence "" A lady stands outside of a Mexican market . "" is picked Model Name",result
sentiment_analysis,26,"Hence , the figure confirms that our DM - MCNN approach is able to exploit and customize the provided sentiment weights for the target domain .",analysis,Inspection of the DM-MCNN-learned Deep Sentiment Information.,0,223,77,12,0,analysis : Inspection of the DM-MCNN-learned Deep Sentiment Information.,0.9102040816326532,0.9871794871794872,0.9230769230769232,Hence the figure confirms that our DM MCNN approach is able to exploit and customize the provided sentiment weights for the target domain ,24,"Here , it is worth noting that the weight magnitudes of positive words such as "" laugh "" , "" appealing "" and negative words such as "" lack "" , "" missing "" increase further , while words such as "" damn "" , "" interest "" , "" war "" see decreases in magnitude , presumably due to their ambiguity and context ( e.g. , "" damn good "" , "" lost the interest "" , descriptions of war movies ) .","However , unlike the VADER data , our transfer learning approach results in multi-dimensional sentiment embeddings that can more easily capture multiple domains right from the start , thus making it possible to use them even without further fine - tuning .",result
natural_language_inference,79,Every sentence in the dataset has a label which goes from very negative to very positive in the scale from 0.0 to 1.0 .,experiment,Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0,148,13,6,0,experiment : Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0.5522388059701493,0.26,0.3157894736842105,Every sentence in the dataset has a label which goes from very negative to very positive in the scale from 0 0 to 1 0 ,26,"The dataset consists of three sets : 8544 sentences for training , 2210 sentences for test and 1101 sentences for validation ( or development ) .",The labels are generated by human annotators using Amazon Mechanical Turk .,experiment
text-classification,8,We choose the better strategy based on their corresponding performances on the validation set .,experiment,experiment,0,122,9,9,0,experiment : experiment,0.4535315985130112,0.5294117647058824,0.5294117647058824,We choose the better strategy based on their corresponding performances on the validation set ,15,The advantages of these two methods differ from dataset to dataset .,"The final classifier is implemented as an MLP layer with dimension selected from the set [ 100 , 300 , 500 , 1000 ] , followed by a sigmoid or softmax function , depending on the specific task .",experiment
relation_extraction,7,Word and Position Embeddings,methodology,Word and Position Embeddings,0,87,26,1,0,methodology : Word and Position Embeddings,0.3359073359073359,0.2280701754385965,0.09090909090909093,Word and Position Embeddings,4, , ,method
natural_language_inference,78,We compare against the model outputs of the ESIM model across 13 categories of linguistic phenenoma .,analysis,Linguistic Error Analysis,0,253,3,3,0,analysis : Linguistic Error Analysis,0.9166666666666666,0.14285714285714285,0.3,We compare against the model outputs of the ESIM model across 13 categories of linguistic phenenoma ,17,We perform a linguistic error analysis using the supplementary annotations provided by the MultiNLI dataset .,Table 4 reports the result of our error analysis .,result
natural_language_inference,64,ASNQ is an important contribution of our work to the research community .,system description,Related Work,0,38,19,19,0,system description : Related Work,0.15139442231075698,0.5757575757575758,0.5757575757575758,ASNQ is an important contribution of our work to the research community ,13,This was essential as our transfer step requires a large and accurate dataset .,"Finally , the generality of our approach and empirical investigation suggest that our TANDA findings also apply to other NLP tasks , especially , textual inference , although empirical analysis is essential to confirm these claims .",method
natural_language_inference,39,"Due to sentence length variations , for the SICK and MSRVID data we padded the sentences to 32 words ; for the STS2014 , WikiQA , and TrecQA data , we padded the sentences to 48 words ..",experiment,Experimental Setup,1,162,32,32,0,experiment : Experimental Setup,0.7864077669902912,0.8,0.8,Due to sentence length variations for the SICK and MSRVID data we padded the sentences to 32 words for the STS2014 WikiQA and TrecQA data we padded the sentences to 48 words ,33,Our timing experiments were conducted on an Intel Xeon E5 - 2680 CPU .,"Our model outperforms previous neural network models , most of which are based on sentence modeling .",experiment
sentiment_analysis,14,"Thus , to detect the corresponding emotion , more attention needs to be paid to words .",result,Results,1,136,25,25,0,result : Results,0.8947368421052632,1.0,1.0,Thus to detect the corresponding emotion more attention needs to be paid to words ,15,We believe the reason why we achieve a much better performance than SOTA on the SE0714 is that headlines are usually short and emotional words exist more commonly in headlines ., ,result
question-answering,0,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2.4 M entities and 600 k relationships .,introduction,introduction,1,35,25,25,0,introduction : introduction,0.13565891472868216,0.8064516129032258,0.8064516129032258,We end up learning meaningful vectorial representations for questions involving up to 800 k words and for triples of an mostly automatically created KB with 2 4 M entities and 600 k relationships ,34,We show empirically that our model is able to take advantage of noisy and indirect supervision by ( i ) automatically generating questions from KB triples and treating this as training data ; and ( ii ) supplementing this with a data set of questions collaboratively marked as paraphrases but with no associated answers .,Our method strongly outperforms previous results on the WikiAnswers + ReVerb evaluation data set introduced by .,introduction
sentiment_analysis,6,"On the MOUD dataset , the textual modality performs worse than audio modality due to the noise introduced in translating Spanish utterances to English .",baseline,baseline,0,264,18,18,0,baseline : baseline,0.9134948096885812,0.9,0.9,On the MOUD dataset the textual modality performs worse than audio modality due to the noise introduced in translating Spanish utterances to English ,24,"However , the margin is less in the other datasets .",Using Spanish word vectors 3 in text - CNN results in an improvement of 10 % .,result
sentence_classification,0,This suggests that the our model is more successful in learning optimal parameters for representing the citation text and classifying its respective intent compared with the baseline .,analysis,Analysis,0,211,11,11,0,analysis : Analysis,0.7902621722846442,0.55,0.6111111111111112,This suggests that the our model is more successful in learning optimal parameters for representing the citation text and classifying its respective intent compared with the baseline ,28,Our model correctly classifies this instance by putting more attention weights on words that relate to comparison of the results .,Note that the only difference between our model and the neural baseline is inclusion of the structural scaffolds .,result
natural_language_inference,0,https://github.com/deepmind/rc-data,dataset,Datasets,0,120,10,10,0,dataset : Datasets,0.6091370558375635,0.9090909090909092,0.9090909090909092,https github com deepmind rc data,6,We only focus on subsets where the deleted token is either a common noun ( CN ) or named entity ( NE ) since simple language models already give human - level performance on the other types ( cf. ) .,http://www.thespermwhale.com/jaseweston/babi/,experiment
natural_language_inference,55,"We do so by alternating the training of S with that of a scoring function S prp ( q 1 , q 2 ) = f ( q 1 ) f ( q 2 ) , which uses the same embedding matrix W and makes the embeddings of a pair of questions ( q 1 , q 2 ) similar to each other if they are paraphrases ( i.e. if they belong to the same paraphrase cluster ) , and make them different otherwise .",training,Multitask Training of Embeddings,0,105,14,4,0,training : Multitask Training of Embeddings,0.7142857142857143,1.0,1.0,We do so by alternating the training of S with that of a scoring function S prp q 1 q 2 f q 1 f q 2 which uses the same embedding matrix W and makes the embeddings of a pair of questions q 1 q 2 similar to each other if they are paraphrases i e if they belong to the same paraphrase cluster and make them different otherwise ,71,"Hence , we also multi-task the training of our model with the task of paraphrase prediction .", ,experiment
relation_extraction,12,"Most instances contain multiple sentences and each instance is assigned with one of the five labels , including : "" resistance or nonresponse "" , "" sensitivity "" , "" response "" , "" resistance "" and "" None "" .",experiment,Data,0,160,6,5,0,experiment : Data,0.486322188449848,0.18181818181818185,0.3333333333333333,Most instances contain multiple sentences and each instance is assigned with one of the five labels including resistance or nonresponse sensitivity response resistance and None ,26,"For the cross - sentence n-ary relation extraction task , we use the dataset introduced in , which contains 6,987 ternary relation instances and 6,087 binary relation instances extracted from PubMed .","We consider two specific tasks for evaluation , i , e. , binary - class n-ary relation extraction and multi-class n-ary relation extraction .",experiment
machine-translation,3,". Half ( f ) denotes the first half of the elements off , and Dr ( h ) is the dropout operation which randomly sets an element of h to zero with a certain probability .",system description,Network,0,131,89,52,0,system description : Network,0.4185303514376997,0.7416666666666667,0.6265060240963856, Half f denotes the first half of the elements off and Dr h is the dropout operation which randomly sets an element of h to zero with a certain probability ,32,"In our final model two additional operations are used with Eq. 5 , which is shown in Eq .",The use of Half ( ) is to reduce the parameter size and does not affect the performance .,method
natural_language_inference,99,We also add a checking layer on the answer refining in order to ensure the accuracy .,introduction,introduction,1,32,22,22,0,introduction : introduction,0.12698412698412698,0.6875,0.6875,We also add a checking layer on the answer refining in order to ensure the accuracy ,17,We then develop the interactive attention with memory network to mimic human reading procedure .,The main contributions of this paper are as follows : :,introduction
natural_language_inference,80,") sentence encoding based models ( rows 2 - 7 ) , 2 ) single inter-sentence attention - based models ( rows 8 - 16 ) , and 3 ) ensemble inter-sentence attention - based models ( rows 17 - 19 ) .",experiment,experiment,0,166,40,40,0,experiment : experiment,0.5724137931034483,0.9302325581395348,0.9302325581395348, sentence encoding based models rows 2 7 2 single inter sentence attention based models rows 8 16 and 3 ensemble inter sentence attention based models rows 17 19 ,30,"It is noteworthy that recent deep learning models surpass the human performance in the NLI task. , previous deep learning models ( rows 2 - 19 ) can be divided into three categories :","We can see that inter-sentence attention - based models perform better than sentence encoding based models , which supports our intuition .",experiment
sentiment_analysis,42,"However , the model mentioned above ignores the location information between context word and aspect .",system description,Location Attention,0,110,58,3,0,system description : Location Attention,0.4365079365079365,0.935483870967742,0.42857142857142855,However the model mentioned above ignores the location information between context word and aspect ,15,We have described our neural attention framework and a content - based model in previous subsection .,Such location information is helpful for an attention model because intuitively a context word closer to the aspect should be more important than a farther one .,method
natural_language_inference,69,"However , both models struggle to select relevant information ; and providing documents guaranteed to be relevant greatly improves their performance .",abstract,abstract,0,11,9,9,0,abstract : abstract,0.031884057971014484,0.9,0.9,However both models struggle to select relevant information and providing documents guaranteed to be relevant greatly improves their performance ,20,We evaluate two previously proposed competitive models and find that one can integrate information across documents .,"While the models outperform several strong baselines , their best accuracy reaches 54.5 % on an annotated test set , compared to human performance at 85.0 % , leaving ample room for improvement .",abstract
sentiment_analysis,21,For any java implementations of the LR classifier we use the LIBLINEAR library while for python implementations we use Sci-kit learn .,experiment,Experiments,0,91,10,10,0,experiment : Experiments,0.65,0.9090909090909092,0.9090909090909092,For any java implementations of the LR classifier we use the LIBLINEAR library while for python implementations we use Sci kit learn ,23,"However , the results reported in this paper include only the accuracy obtained from classifying documents in the final epoch .",Code to reproduce all experiments is available at https://github.com/tanthongtan/dv-cosine.,experiment
sentiment_analysis,48,Aspect - term sentiment analysis ( ATSA ) is a long - standing challenge in natural language processing .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.01694915254237288,0.2222222222222222,0.2222222222222222,Aspect term sentiment analysis ATSA is a long standing challenge in natural language processing ,15, ,It requires fine - grained semantical reasoning about a target entity appeared in the text .,abstract
sentiment_analysis,49,We analyze the attention values to understand the learning behavior of the proposed architecture .,analysis,Analysis of Attention Mechanism,0,167,2,2,0,analysis : Analysis of Attention Mechanism,0.6600790513833992,0.028571428571428567,0.060606060606060615,We analyze the attention values to understand the learning behavior of the proposed architecture ,15, ,"To illustrate , we take an example video from the CMU - MOSI test dataset .",result
natural_language_inference,56,We run our recurrent relational network for 4 steps .,ablation,Pretty-CLEVR experimental details,0,259,25,14,0,ablation : Pretty-CLEVR experimental details,0.7708333333333334,0.7352941176470589,0.6086956521739131,We run our recurrent relational network for 4 steps ,10,The last layer has 16 hidden linear units .,"We train on the 12.8 M training questions , and augment the data by scaling and rotating the scenes randomly .",result
relation_extraction,11,"In RESIDE , we define a k r -dimensional embedding for each relation which we call as matched relation embedding ( h rel ) .",system description,Relation Alias Side Information,0,166,99,17,0,system description : Relation Alias Side Information,0.6693548387096774,0.7734375,0.8095238095238095,In RESIDE we define a k r dimensional embedding for each relation which we call as matched relation embedding h rel ,22,We use a threshold on cosine distance to remove noisy aliases .,"For a given sentence , h rel is concatenated with its representa-tion s , obtained from syntactic sentence encoder ( Section 5.1 ) as shown in .",method
sentiment_analysis,14,We collect a larger dataset from Twitter with hashtags as distant supervision .,system description,Larger dataset,0,70,2,2,0,system description : Larger dataset,0.4605263157894737,0.2222222222222222,0.2222222222222222,We collect a larger dataset from Twitter with hashtags as distant supervision ,13, ,Such distant supervision method using hashtags has already been proved to provide reasonably relevant emotion labels by previous works .,method
sentiment_analysis,16,Note that ? is still needed to control the importance of different contexts .,approach,approach,0,162,40,40,0,approach : approach,0.5094339622641509,0.4878048780487805,0.4878048780487805,Note that is still needed to control the importance of different contexts ,13,"For example , d h , d p can help shift the final sentiment to negative and d h , d r can help shift it to positive .",Note that ? is still needed to control the importance of different contexts .,method
relation_extraction,8,The objective of multi-instance learning is to predict the labels of the unseen bags .,methodology,Multi-instance Learning,0,174,90,6,0,methodology : Multi-instance Learning,0.6468401486988847,0.8737864077669902,0.3157894736842105,The objective of multi instance learning is to predict the labels of the unseen bags ,16,"Suppose that there are T bags { M 1 , M 2 , , MT } and that the i - th bag contains q i instances M i = {m 1 i , m 2 i , , m q ii }.","In this paper , all instances in a bag are considered independently .",method
natural_language_inference,2,The number of hidden units in all the LSTMs is 150 .,experiment,Experimental Settings,1,178,6,6,0,experiment : Experimental Settings,0.6641791044776121,0.3157894736842105,0.3157894736842105,The number of hidden units in all the LSTMs is 150 ,12,We use 50 - dimension character - level embedding vectors .,We use dropout ) with probability 0.3 for every learnable layer .,experiment
question-answering,3,We implement the mathematical expressions with Theano and use Adam for optimization .,training,Training,0,159,6,6,0,training : Training,0.6259842519685039,1.0,1.0,We implement the mathematical expressions with Theano and use Adam for optimization ,13,"Otherwise , we assign Li = 0 .", ,experiment
natural_language_inference,16,We set the hidden size as 100 for all BiLSTM layers .,experiment,experiment,1,133,5,5,0,experiment : experiment,0.6244131455399061,0.5,0.5,We set the hidden size as 100 for all BiLSTM layers ,12,"For the charactercomposed embeddings , we initialize each character as a 20 - dimensional vector , and compose each word into a 50 dimensional vector with a LSTM layer .","We apply dropout to every layers in , and set the dropout ratio as 0.1 .",experiment
natural_language_inference,76,"By incorporating an attention mechanism we found a 0.9 percentage point improvement over a single LSTM with a hidden size of 159 , and a 1.4 percentage point increase over a benchmark model that uses two LSTMs for conditional encoding ( one for the premise and one for the hypothesis conditioned on the representation of the premise ) .",experiment,EXPERIMENTS,1,108,23,23,0,experiment : EXPERIMENTS,0.7397260273972602,0.6571428571428571,0.6571428571428571,By incorporating an attention mechanism we found a 0 9 percentage point improvement over a single LSTM with a hidden size of 159 and a 1 4 percentage point increase over a benchmark model that uses two LSTMs for conditional encoding one for the premise and one for the hypothesis conditioned on the representation of the premise ,58,"To the best of our knowledge , this is the first instance of a neural end - to - end differentiable model to achieve state - of the - art performance on a textual entailment dataset .",The attention model produces output vectors summarizing contextual information of the premise that is useful to attend over later when reading the hypothesis .,experiment
sentiment_analysis,39,Early research in the field of sentiment analysis only focused on identifying the over all sentiment or polarity of a given text .,introduction,introduction,0,18,4,4,0,introduction : introduction,0.07346938775510205,0.0975609756097561,0.0975609756097561,Early research in the field of sentiment analysis only focused on identifying the over all sentiment or polarity of a given text ,23,"It has received not only a lot of interest in academia but also in industry , in particular for identifying customer satisfaction on products and services .",The underlying assumption of this work was that there is one over all polarity in the whole text .,introduction
sentiment_analysis,12,"Aspect - level sentiment classification ( ASC ) , as an indispensable task in sentiment analysis , aims at inferring the sentiment polarity of an input sentence in a certain aspect .",introduction,introduction,1,13,2,2,0,introduction : introduction,0.05803571428571429,0.060606060606060615,0.060606060606060615,Aspect level sentiment classification ASC as an indispensable task in sentiment analysis aims at inferring the sentiment polarity of an input sentence in a certain aspect ,27, ,"In this regard , pre-vious representative models are mostly discriminative classifiers based on manual feature engineering , such as Support Vector Machine .",introduction
natural_language_inference,42,"Although both models required similar training times to reach these scores , the time for training one epoch in FABIR was more than four times shorter , which could be useful for tackling larger data sets .",evaluation,FABIR vs BiDAF,0,247,22,10,0,evaluation : FABIR vs BiDAF,0.8546712802768166,0.4150943396226415,0.625,Although both models required similar training times to reach these scores the time for training one epoch in FABIR was more than four times shorter which could be useful for tackling larger data sets ,35,"Their similar scores render further comparisons even more telling , because their differences can not be explained by their over all performances , but exclusively by their architectures .","Concerning inference time , FABIR was more than five times faster in processing the 10,570 question - passage pairs in the development data set .",result
part-of-speech_tagging,6,This is because other systems use various external resources and / or better pre-processing modules and / or construct ensemble models for dependency parsing .,result,MQuni at the CoNLL 2017 shared task,0,125,37,27,0,result : MQuni at the CoNLL 2017 shared task,0.8389261744966443,0.8222222222222222,0.7714285714285715,This is because other systems use various external resources and or better pre processing modules and or construct ensemble models for dependency parsing ,24,"In fact , it is hard to make a clear comparison between our jPTDP and the parsing models used in other top participating systems .","For example , UDPipe 1.2 only extends the word and sentence segmenters of the baseline UDPipe 1.1 .",result
natural_language_inference,51,"On the contrary , we explicitly model the working weight as an interpolation of multiple programs and use a meta-network to generate the coefficients .",model,Text Question Answering,0,243,40,37,0,model : Text Question Answering,0.9169811320754716,0.9523809523809524,0.9487179487179488,On the contrary we explicitly model the working weight as an interpolation of multiple programs and use a meta network to generate the coefficients ,25,"Instead , they propose Multiplicative RNN that factorizes the working weight to product of three matrices , which looses modularity .",This design facilitates modularity because each program is trained towards some functionality and can be switched or combined with each other to perform the current task .,method
sentiment_analysis,5,"Although it looks quite similar with , i.e. , the asymmetry on frontal and temporal lobes make more contribution to discriminate different emotions , we can observe some delicate distinctions from these maps of different emotions :",The activity maps of the paired EEG electrodes,The activity maps of the paired EEG electrodes,0,234,10,10,0,The activity maps of the paired EEG electrodes : The activity maps of the paired EEG electrodes,0.8830188679245283,0.2777777777777778,0.7692307692307693,Although it looks quite similar with i e the asymmetry on frontal and temporal lobes make more contribution to discriminate different emotions we can observe some delicate distinctions from these maps of different emotions ,35,"Specifically , to explore where the differential information coming from in terms of the emotion expressed , we separately depict the electrode activity maps corresponding to each emotion in .","1 ) For the positive emotions ( happy in SEED and SEED - IV , joy and funny in MPED ) , we can see that the asymmetry on temporal lobe actives as same as ( or even more than ) the frontal lobe ; ( 2 ) On the contrary , for the neutral emotion ( , ( e ) and ) , the asymmetry on frontal lobe contributes more than temporal lobe ; ( 3 ) For the sad emotion ( , ( f ) and ) , the asymmetry on frontal lobe basically dominates this emotion expression .",others
sentiment_analysis,20,"Also , we stop training after the validation loss has stopped decreasing ( early - stopping ) .",model,Regularization,0,134,58,7,0,model : Regularization,0.7204301075268817,0.8405797101449275,0.7,Also we stop training after the validation loss has stopped decreasing early stopping ,14,Furthermore we add L 2 regularization penalty ( weight decay ) to the loss function to discourage large weights .,"Finally , we do not fine - tune the embedding layers .",method
natural_language_inference,71,list of data points are located randomly in along noisy sequence .,experiment,Denoise Task,0,123,24,3,0,experiment : Denoise Task,0.5720930232558139,0.4,0.2,list of data points are located randomly in along noisy sequence ,12,"We evaluate the forgetting ability of each RNN architecture on a synthetic "" denoise "" task .","The RNN model is supposed to filter out the useless part ( "" noise "" ) and output the remaining sequential labels .",experiment
sentiment_analysis,4,"For text , we find the single layer CNN to perform at par with deeper variants .",training,Training Details,1,226,9,9,0,training : Training Details,0.6932515337423313,0.6923076923076923,0.6923076923076923,For text we find the single layer CNN to perform at par with deeper variants ,16,"For multimodal feature extraction , we explore different designs for the employed CNNs .","For visual features , however , a deeper CNN provides better representations .",experiment
natural_language_inference,66,This h s 0 represents NULL and is used with other h s j to derive the attention vectors {a k } N k=1 .,implementation,Implementation Details,0,127,6,6,0,implementation : Implementation Details,0.4551971326164875,0.5454545454545454,0.5454545454545454,This h s 0 represents NULL and is used with other h s j to derive the attention vectors a k N k 1 ,25,"Specifically , we introduce a vector h s 0 , which is fixed to be a vector of 0s of dimension d .","Second , we use word embeddings trained from GloVe ( Pennington et al. , 2014 ) instead of word2vec vectors .",experiment
sentiment_analysis,30,"The task requires classification of opinions on different entities across a range of different attributes , with the expectation that there will be no overt opinion expressed on a given entity for many attributes .",introduction,introduction,0,10,3,3,0,introduction : introduction,0.078125,0.15,0.15,The task requires classification of opinions on different entities across a range of different attributes with the expectation that there will be no overt opinion expressed on a given entity for many attributes ,34,Targeted aspect - based sentiment analysis ( TABSA ) is the task of identifying fine - grained opinion polarity towards a specific aspect associated with a given target .,"This can be seen in Example ( 1 ) , e.g. , where opinions on the aspects SAFETY and PRICE are expressed for entity LOC1 but not entity LOC2 : 2 ( 1 ) LOC1 is your best bet for secure although expensive and LOC2 is too far .",introduction
text-classification,0,The bag - of - means features are computed the same way as in the bag - of - words model .,method,Traditional Methods,0,119,17,17,0,method : Traditional Methods,0.5242290748898678,0.14285714285714285,0.9444444444444444,The bag of means features are computed the same way as in the bag of words model ,18,The dimension of the embedding is 300 .,The number of means is 5000 .,method
natural_language_inference,81,"The countrys capital , Honiara , is located on the island of Guadalcanal .",APPENDIX,Answer town,0,377,169,89,0,APPENDIX : Answer town,0.8954869358669834,0.7934272300469484,0.6691729323308271,The countrys capital Honiara is located on the island of Guadalcanal ,12,Solomon Islands is a sovereign country consisting of six major islands and over 900 smaller islands in Oceania lying to the east of Papua New Guinea and northwest of Vanuatu and covering a land are a of .,"The country takes its name from the Solomon Islands archipelago , which is a collection of Melanesian islands that also includes the North Solomon Islands ( part of Papua New Guinea ) , but excludes outlying islands , such as Rennell and Bellona , and the Santa Cruz Islands .",others
sarcasm_detection,1,"Below , we present a couple of cases from the Pol dataset where our model correctly identifies the sarcasm which is evident only with the neighboring comments .",analysis,Case Studies,0,312,15,4,0,analysis : Case Studies,0.934131736526946,0.46875,0.2352941176470588,Below we present a couple of cases from the Pol dataset where our model correctly identifies the sarcasm which is evident only with the neighboring comments ,27,This signifies the greater role of the contextual cues for classifying comments in this dataset over the other dataset variants used in our experiment .,"The previous state - of - the - art CUE - CNN , however , misclassifies them .",result
natural_language_inference,30,"However , open question answering remains challenging because of the scale of these KBs ( billions of triples , millions of entities and relationships ) and of the difficulty for machines to interpret natural language .",introduction,introduction,0,18,8,8,0,introduction : introduction,0.06976744186046513,0.25806451612903225,0.25806451612903225,However open question answering remains challenging because of the scale of these KBs billions of triples millions of entities and relationships and of the difficulty for machines to interpret natural language ,32,The use of KBs simplifies the problem by separating the issue of collecting and organizing information ( i.e. information extraction ) from the one of searching through it ( i.e. question answering or natural language interfacing ) .,Recent progress has been made by tackling this problem with semantic parsers .,introduction
natural_language_inference,64,"We evaluate TANDA on well - known academic benchmarks , i.e. , TREC - QA and WikiQA , as well as three different industrial datasets , where questions are derived from Alexa Traffic and candidate sentences are selected from web data .",system description,Related Work,0,41,22,22,0,system description : Related Work,0.16334661354581673,0.6666666666666666,0.6666666666666666,We evaluate TANDA on well known academic benchmarks i e TREC QA and WikiQA as well as three different industrial datasets where questions are derived from Alexa Traffic and candidate sentences are selected from web data ,37,"Finally , the generality of our approach and empirical investigation suggest that our TANDA findings also apply to other NLP tasks , especially , textual inference , although empirical analysis is essential to confirm these claims .",The results show that :,method
natural_language_inference,23,Prediction : his doubts about the identity of the Black Death ID : 5730cb8df6cb411900e244c6-high-conf- turk0,model,FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0,444,51,28,0,model : FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0.8654970760233918,0.425,0.32941176470588235,Prediction his doubts about the identity of the Black Death ID 5730cb8df6cb411900e244c6 high conf turk0,15,Question : What was Shrewsbury 's conclusion ? Answer : contemporary accounts were exaggerations FusionNet Prediction : contemporary accounts were exaggerations BiDAF,The Book of Discipline is the guidebook for local churches and pastors and describes in considerable detail the organizational structure of local United Methodist churches .,method
sentiment_analysis,51,We explain our model architecture and methodology in detail in Section V .,introduction,introduction,0,27,18,18,0,introduction : introduction,0.18,0.9,0.9,We explain our model architecture and methodology in detail in Section V ,13,"In Section IV , we describe the dataset we performed our experiments on .",Then we present and analyze our results in Section VI .,introduction
natural_language_inference,33,Sentences with some lexical overlap and similar discourse structure appear to be clustered together .,evaluation,EXPERIMENTAL RESULTS & DISCUSSION,0,163,39,29,0,evaluation : EXPERIMENTAL RESULTS & DISCUSSION,0.6245210727969349,0.2846715328467153,0.935483870967742,Sentences with some lexical overlap and similar discourse structure appear to be clustered together ,15,Appendix contains sentences from the BookCorpus and their nearest neighbors .,"We also report QVEC benchmarks Model Accuracy 1 k 5 k 10 k 25 k All ( 400 k ) , the next 5 from and The last 4 rows are our experiments using Infersent and our models .",result
natural_language_inference,58,"The ReasoNet learns a stochastic policy ? ( ( t t , at ) |s t ; ? ) with parameters ? to get a distribution of termination actions , to continue reading or to stop , and of answer actions if the model decides to stop at the current step .",system description,REASONING NETWORKS,0,108,28,28,0,system description : REASONING NETWORKS,0.3223880597014925,0.5490196078431373,0.5490196078431373,The ReasoNet learns a stochastic policy t t at s t with parameters to get a distribution of termination actions to continue reading or to stop and of answer actions if the model decides to stop at the current step ,41,"The ReasoNet performs an answer action a T at the T - th step , which implies that the termination gate variables t 1:T = ( t 1 = 0 , t 2 = 0 , ... , t T ?1 = 0 , t T = 1 ) .","The ReasoNet learns a stochastic policy ? ( ( t t , at ) |s t ; ? ) with parameters ? to get a distribution of termination actions , to continue reading or to stop , and of answer actions if the model decides to stop at the current step .",method
natural_language_inference,72,"In the empirical part , we use for training only the instances for which at least one answer occurs in the passage , but we evaluate on all instances in the validation and test sets , including those for which A ? E = ? , where E is the set of all entities in the passage .",system description,Answer set,0,107,31,6,0,system description : Answer set,0.34294871794871795,0.7045454545454546,0.8571428571428571,In the empirical part we use for training only the instances for which at least one answer occurs in the passage but we evaluate on all instances in the validation and test sets including those for which A E where E is the set of all entities in the passage ,51,"We have found upon manual inspection that this is mostly due to lexical variation that is not captured by answer extension , and to a lesser degree , due to the introduction of entirely new information in the learning point and the entity recognition errors .",This mimics a likely real - life scenario where the set of ground - truth answers is a priori unknown .,method
sentiment_analysis,12,"Ds ? Ds ? ( x , t , y , sa ( x ) , s m ( x ) ) 29 : end for 30 : ? ? Train ( Ds ) Return : ? ; effects , whose attention weights are expected to be decreased .",approach,Details of Our Approach,0,98,33,19,0,approach : Details of Our Approach,0.4375,0.4583333333333333,0.40425531914893614,Ds Ds x t y sa x s m x 29 end for 30 Train Ds Return effects whose attention weights are expected to be decreased ,27,"end for 24 : ? ( k ) ? Train ( D ( k ) ; ? ( k?1 ) ) 25 : end for 26 : Ds ? ? 27 : for ( x , t , y ) ? D do 28:","Ds ? Ds ? ( x , t , y , sa ( x ) , s m ( x ) ) 29 : end for 30 : ? ? Train ( Ds ) Return : ? ; effects , whose attention weights are expected to be decreased .",method
topic_models,0,"Finally , we conclude and discuss directions for future research in Section VIII",introduction,introduction,0,54,32,32,0,introduction : introduction,0.13106796116504854,0.9696969696969696,0.9696969696969696,Finally we conclude and discuss directions for future research in Section VIII,12,"Experimental details are given in Section VI , followed by results and analysis in Section VII .", ,introduction
sentiment_analysis,11,The dataset provides rich video and audio samples for all the utterances along with transcriptions .,system description,Multiple Layers,0,215,71,15,0,system description : Multiple Layers,0.625,0.7171717171717171,0.625,The dataset provides rich video and audio samples for all the utterances along with transcriptions ,16,This is done to compare our method with state - of - the - art frameworks .,"Apart from these emotional states , we also investigate the valence and arousal degrees of each utterance .",method
natural_language_inference,48,"For both datasets , the training and evaluation data consist of tuples ( Q , D , A , a ) , where Q is the query ( represented as a sequence of words ) , Dis the document , A is the set of possible answers , and a ? A is",system description,Task Description,0,42,13,13,0,system description : Task Description,0.19004524886877827,0.18840579710144928,0.9285714285714286,For both datasets the training and evaluation data consist of tuples Q D A a where Q is the query represented as a sequence of words Dis the document A is the set of possible answers and a A is,40,"Instead of extracting a query from the articles themselves , the authors replace a named entity within each article summary with an anonymous placeholder token .","For both datasets , the training and evaluation data consist of tuples ( Q , D , A , a ) , where Q is the query ( represented as a sequence of words ) , Dis the document , A is the set of possible answers , and a ? A is",method
text_summarization,14,"To verify the generalization of our entailment - based strategies , we adopt selective encoding mechanism to our seq2seq model and apply MTL and RAML to Seq2seq + selective model , which is denoted as the Seq2seq + selective + MTL + RAML model .",method,Comparative Methods,0,168,20,20,0,method : Comparative Methods,0.7368421052631579,1.0,1.0,To verify the generalization of our entailment based strategies we adopt selective encoding mechanism to our seq2seq model and apply MTL and RAML to Seq2seq selective model which is denoted as the Seq2seq selective MTL RAML model ,38,employ a selective encoding model to control the information flow from encoder to decoder ., ,method
text_summarization,0,Analysis of goal tracker,analysis,analysis,0,263,1,1,0,analysis : analysis,0.8737541528239202,0.09090909090909093,0.09090909090909093,Analysis of goal tracker,4, , ,result
semantic_role_labeling,1,"In we present predicate detection precision , recall and F1 on the CoNLL - 2005 and 2012 test sets .",experiment,"Parsing, POS and predicate detection",0,186,37,4,0,"experiment : Parsing, POS and predicate detection",0.8532110091743119,0.925,0.5714285714285714,In we present predicate detection precision recall and F1 on the CoNLL 2005 and 2012 test sets ,18,The difference in parse accuracy between LISA G and D&M likely explains the large increase in SRL performance we see from decoding with D&M parses in that setting .,SA and LISA with and without ELMo attain comparable scores so we report only LISA + Glo Ve .,experiment
text-classification,1,"Only on RCV1 , n-gram SVM is no better than bag - of - word SVM , and only on RCV1 , bow - CNN outperforms seq-CNN .",system description,Experiments (supervised),0,147,101,24,0,system description : Experiments (supervised),0.57421875,0.505,0.4,Only on RCV1 n gram SVM is no better than bag of word SVM and only on RCV1 bow CNN outperforms seq CNN ,24,This point can also be observed in the SVM and CNN performances .,"That is , on RCV1 , bags of words in a window of 20 at every location are more useful than words in strict order .",method
natural_language_inference,53,"We obtain a pearson score of 0.885 on SICK - R while obtained 0.868 , and we obtain 86.3 % test accuracy on SICK - E while previous best handengineered models obtained 84.5 % .",model,Task transfer,1,185,44,24,0,model : Task transfer,0.8894230769230769,0.7213114754098361,0.5853658536585366,We obtain a pearson score of 0 885 on SICK R while obtained 0 868 and we obtain 86 3 test accuracy on SICK E while previous best handengineered models obtained 84 5 ,34,Our transfer learning approach obtains better results than previous state - of - the - art on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,"We also significantly outperformed previous transfer learning approaches on SICK - E ( Bowman et al. , 2015 ) that used the parameters of an LSTM model trained on SNLI to fine - tune on SICK ( 80.8 % accuracy ) .",method
text_summarization,4,This dataset receives the first sentence of a news article as input and use the headline title as the gold standard summary .,dataset,dataset,0,167,4,4,0,dataset : dataset,0.6472868217054264,0.3333333333333333,0.3333333333333333,This dataset receives the first sentence of a news article as input and use the headline title as the gold standard summary ,23,"First , we use the Annotated English Gigaword dataset as used in .","Since the development dataset is large , we randomly selected 2000 pairs as our development dataset .",experiment
natural_language_inference,2,"We evaluated AMANDA on three challenging QA datasets : NewsQA , TriviaQA , and Search QA .",experiment,Experiments,0,156,2,2,0,experiment : Experiments,0.582089552238806,0.6666666666666666,0.6666666666666666,We evaluated AMANDA on three challenging QA datasets NewsQA TriviaQA and Search QA ,14, ,"Using the News QA development set as a benchmark , we perform rigorous analysis for better understanding of how our proposed model works .",experiment
text_generation,1,"However , in text generation , the generator G ? obtains the reward if and only if one sequence has been completely generated , which means no intermediate reward is gained before the sequence hits the end symbol .",training,Training,0,127,15,15,0,training : Training,0.4635036496350365,0.3333333333333333,0.3333333333333333,However in text generation the generator G obtains the reward if and only if one sequence has been completely generated which means no intermediate reward is gained before the sequence hits the end symbol ,35,"Note that in reinforcement learning , the current reward is compromised by the rewards from intermediate states and future states .","However , in text generation , the generator G ? obtains the reward if and only if one sequence has been completely generated , which means no intermediate reward is gained before the sequence hits the end symbol .",experiment
sentiment_analysis,36,"SVM : It is a traditional support vector machine based model with extensive feature engineering ; AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ; AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ; IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ; CNN - ASP :",experiment,Experimental Setup,1,151,10,9,0,experiment : Experimental Setup,0.604,0.2857142857142857,0.2647058823529412,SVM It is a traditional support vector machine based model with extensive feature engineering AdaRNN It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree AE LSTM and ATAE LSTM AE LSTM is a simple LSTM model incorporating the target embedding as input while ATAE LSTM extends AE LSTM with attention IAN IAN employs two LSTMs to learn the representations of the context and the target phrase interactively CNN ASP ,77,"SVM : It is a traditional support vector machine based model with extensive feature engineering ; AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ; AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ; IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ; CNN - ASP :","SVM : It is a traditional support vector machine based model with extensive feature engineering ; AdaRNN : It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree ; AE - LSTM , and ATAE - LSTM : AE - LSTM is a simple LSTM model incorporating the target embedding as input , while ATAE - LSTM extends AE - LSTM with attention ; IAN : IAN employs two LSTMs to learn the representations of the context and the target phrase interactively ; CNN - ASP :",experiment
sarcasm_detection,1,There are also other cases where our model fails despite the presence of contextual information from the previous comments .,analysis,Case Studies,0,321,24,13,0,analysis : Case Studies,0.9610778443113772,0.75,0.7647058823529411,There are also other cases where our model fails despite the presence of contextual information from the previous comments ,20,"However , sometimes contextual cues from the previous comments are not enough and misclassifications are observed due to lack of necessary commonsense and background knowledge about the topic of discussion .","During exploration , this is primarily observed for contextual comments which are very long .",result
natural_language_inference,70,IR is the baseline system based on the search engine results PTK uses the default parameters .,result,Subtask C Model:,0,163,41,20,0,result : Subtask C Model:,0.9314285714285714,0.7735849056603774,0.625,IR is the baseline system based on the search engine results PTK uses the default parameters ,17,"KeLP is our primary submission , while KC1 and KC2 are the contrastive ones .","The subtask data is rather imbalanced , as the number of negative examples is about 10 times the positive ones .",result
negation_scope_resolution,0,"Section 3 contains the details of the methodology used for Neg - BERT , while 4 includes experimental details for our experimentation .",introduction,introduction,0,39,28,28,0,introduction : introduction,0.16956521739130434,0.9333333333333332,0.9333333333333332,Section 3 contains the details of the methodology used for Neg BERT while 4 includes experimental details for our experimentation ,21,"This paper is organized as follows : In Section 2 , we extensively review available literature on the subject .","In Section 5 , we report the results and analyze them .",introduction
sentiment_analysis,4,Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance - videos .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.02147239263803681,0.7142857142857143,0.7142857142857143,Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance videos ,16,"To this end , we propose Interactive COnversational memory Network ( ICON ) , a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the selfand interspeaker emotional influences into global memories .",Our model outperforms state - of - the - art networks on multiple classification and regression tasks in two benchmark datasets .,abstract
relation_extraction,12,ON PREP_OF NN NSUBJ COP NUM PREP_IN NN NN DET MARK PREP_ ON AUXPASS NSUBJPASS ADVCL DET NSUBJPASS PREP_WITH CONJ_AND DOBJ DET AMOD NEXT ROOT,abstract,abstract,0,15,13,13,0,abstract : abstract,0.04559270516717325,1.0,1.0,ON PREP_OF NN NSUBJ COP NUM PREP_IN NN NN DET MARK PREP_ ON AUXPASS NSUBJPASS ADVCL DET NSUBJPASS PREP_WITH CONJ_AND DOBJ DET AMOD NEXT ROOT,25,with gefitinib and showed a partial ROOT DET NN PREP _, ,abstract
text_summarization,8,"For this setup , we use a shared encoder for both abstractive summarization and content selection .",system description,End-to-End Alternatives,0,139,69,13,0,system description : End-to-End Alternatives,0.4860139860139861,0.7582417582417582,0.6842105263157895,For this setup we use a shared encoder for both abstractive summarization and content selection ,16,We first test this hypothesis by posing summarization as a multi-task problem and training the tagger and summarization model with the same features .,"At test time , we apply the same masking method as bottom - up attention .",method
relation_extraction,8,The relation instances discovered from the test articles are automatically compared with those in Freebase .,approach,Held-out Evaluation,0,224,5,4,0,approach : Held-out Evaluation,0.8327137546468402,0.29411764705882354,0.25,The relation instances discovered from the test articles are automatically compared with those in Freebase ,16,Half of the Freebase relations are used for testing .,"To evaluate the proposed method , we select the following three traditional methods for comparison .",method
relation-classification,3,"In the case of multi-token entities , only the last token of the entity can serve as head of another token , to eliminate redundant relations .",model,Joint learning as head selection,0,65,32,31,0,model : Joint learning as head selection,0.4744525547445255,0.6956521739130435,0.96875,In the case of multi token entities only the last token of the entity can serve as head of another token to eliminate redundant relations ,26,The final objective for the joint task is computed as L JOINT ( w ; ? ) = L NER + L rel where ? is a set of parameters .,"If an entity is not involved in any relation , we predict the auxiliary "" N "" relation label and the token itself as head .",method
sentence_classification,2,"Thus , the Korean attention weight is larger .",model,NN (altered),0,220,29,9,0,model : NN (altered),0.873015873015873,0.6904761904761905,0.45,Thus the Korean attention weight is larger ,8,"However , this phrase is not necessary to classify the sentence correctly , and may induce possible vagueness because of the word trouble .",shows the PCA visualization of the unaltered and the altered vectors of four different languages .,method
natural_language_inference,95,"With the release of various datasets , the MRC task has evolved from the early cloze - style test to answer extraction from a single passage and to the latest more complex question answering on web data .",introduction,introduction,0,12,4,4,0,introduction : introduction,0.05128205128205128,0.06896551724137931,0.06896551724137931,With the release of various datasets the MRC task has evolved from the early cloze style test to answer extraction from a single passage and to the latest more complex question answering on web data ,36,Recent years have seen rapid growth in the MRC community .,"Great efforts have also been made to develop models for these MRC tasks , especially for the answer extraction on single passage .",introduction
sentiment_analysis,6,"In our case , the LSTM network serves the purpose of context - dependent feature extraction by modeling relations among utterances .",method,Long Short-Term Memory,0,121,63,16,0,method : Long Short-Term Memory,0.4186851211072664,0.9545454545454546,0.8888888888888888,In our case the LSTM network serves the purpose of context dependent feature extraction by modeling relations among utterances ,20,Current research indicates the benefit of using such networks to incorporate contextual information in the classification process .,We term our architecture ' contextual LSTM ' .,method
natural_language_inference,64,We apply early stopping on the dev. set of the target corpus for both steps based on the highest MAP score .,training,training,1,152,3,3,0,training : training,0.6055776892430279,0.2307692307692308,0.2307692307692308,We apply early stopping on the dev set of the target corpus for both steps based on the highest MAP score ,22,We adopt Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 2e - 5 for the transfer step on the ASNQ dataset and a learning rate of 1e - 6 for the adapt step on the target dataset .,"We set the max number of epochs equal to 3 and 9 for adapt and transfer steps , respectively .",experiment
natural_language_inference,34,We provide an experimental study on a public dataset ( Hotpot QA ) to demonstrate that our proposed DFGN is competitive against stateof - the - art unpublished work .,system description,Original Entity Graph,0,67,43,40,0,system description : Original Entity Graph,0.2271186440677966,1.0,1.0,We provide an experimental study on a public dataset Hotpot QA to demonstrate that our proposed DFGN is competitive against stateof the art unpublished work ,26,The mask prediction module is additionally weakly trained ., ,method
natural_language_inference,72,"We find that higher embedding dimensionality works better , that CBOW obtains somewhat better scores than Skipgram , and that medium - sized word windows work best .",Training details and hyper-parameter optimization,Training details and hyper-parameter optimization,0,264,4,4,0,Training details and hyper-parameter optimization : Training details and hyper-parameter optimization,0.8461538461538461,0.07692307692307693,0.26666666666666666,We find that higher embedding dimensionality works better that CBOW obtains somewhat better scores than Skipgram and that medium sized word windows work best ,25,"For the embedding baseline sim-entity , the evaluation was carried out 20 times on the validation part of our dataset , and we chose the parameter configuration that led to the highest - performing embedding model as measured by F1 .","The best configuration : ' win size ' : 5 , ' min freq ' : 200 , ' model ' : ' cbow ' , ' dimension ' : 750 , ' neg samples ' :",experiment
sentiment_analysis,49,We hypothesize that applying attention to contributing neighboring utterances and / or multi-modal representations may assist the network to learn in a better way .,introduction,introduction,0,27,17,17,0,introduction : introduction,0.1067193675889328,0.5151515151515151,0.5151515151515151,We hypothesize that applying attention to contributing neighboring utterances and or multi modal representations may assist the network to learn in a better way ,25,"In this paper , we propose a novel method that employs a recurrent neural network based multimodal multi-utterance attention framework for sentiment prediction .",The main challenge in multi-modal sentiment analysis lies in the proper utilization of the information extracted from multiple modalities .,introduction
text_summarization,1,This module can be used as a plug - and - play to an arbitrary encoder - decoder model for generation without architecture change .,introduction,introduction,1,38,28,28,0,introduction : introduction,0.15833333333333333,0.8,0.8,This module can be used as a plug and play to an arbitrary encoder decoder model for generation without architecture change ,22,We present a generic module called SELECTOR that is specialized for diversification .,The SELECTOR module leverages a mixture of experts to identify diverse key contents to focus on during generation .,introduction
sentiment_analysis,42,"An example is "" but dinner here is never dis appointing , even if the prices are a bit over the top "" .",analysis,Error Analysis,0,229,10,10,0,analysis : Error Analysis,0.9087301587301588,0.8333333333333334,0.8333333333333334,An example is but dinner here is never dis appointing even if the prices are a bit over the top ,21,"The third factor is sentimental relation between context words such as negation , comparison and condition .",We believe that this is caused by the weakness of weighted average compositional function in each hop .,result
natural_language_inference,72,"We take this into account by using an embedding metric ( Emb ) , in which we construct mean vectors for both ground - truth and system answer sequences , and then compare them with the cosine similarity .",evaluation,Evaluation,0,204,23,23,0,evaluation : Evaluation,0.6538461538461539,0.9583333333333334,0.9583333333333334,We take this into account by using an embedding metric Emb in which we construct mean vectors for both ground truth and system answer sequences and then compare them with the cosine similarity ,34,"Second , it may occur that answers contain no word overlap yet still be good candidates because of their semantical relatedness , as in "" renal failure "" - "" kidney breakdown "" .",This and other embedding metrics for evaluation were previously studied in dialog - system research .,result
machine-translation,4,"We propose the weight - sharing constraint to unsupervised NMT , enabling the model to utilize an independent encoder for each language .",introduction,introduction,0,38,30,30,0,introduction : introduction,0.1589958158995816,0.8333333333333334,0.8333333333333334,We propose the weight sharing constraint to unsupervised NMT enabling the model to utilize an independent encoder for each language ,21,"In summary , we mainly make the following contributions :","To enforce the shared - latent space , we also propose the embedding - reinforced encoders and two different GANs for our model .",introduction
natural_language_inference,91,"It stores each element involved in the feedforward computation exactly once , meaning that this representation can still support efficient backpropagation .",implementation,Implementation issues,0,113,19,19,0,implementation : Implementation issues,0.4849785407725322,0.3392857142857143,0.5135135135135135,It stores each element involved in the feedforward computation exactly once meaning that this representation can still support efficient backpropagation ,21,This stack representation requires substantially less space .,"Furthermore , all of the updates to Sand Q can be performed batched and in - place on a GPU , yielding substantial speed gains over both a more nave SPINN implementation and a standard TreeRNN implementation .",experiment
sentiment_analysis,34,"Word embeddings were created using the word2vec , the skip - gram architecture was used .",methodology,Text representation,0,83,15,5,0,methodology : Text representation,0.5608108108108109,0.7894736842105263,0.5555555555555556,Word embeddings were created using the word2vec the skip gram architecture was used ,14,"In our work , short sentences are padded to match the longest sentence in the training set .","The embeddings were built using a corpus of 250M unique Arabic tweets ; this makes it the largest Arabic word embeddings set when compared to the available AraVec , which is currently the largest set , built using a corpus of 67M tweets .",method
machine-translation,9,The training is run for 200K iterations .,experiment,CODE LEARNING,1,175,19,11,0,experiment : CODE LEARNING,0.6097560975609756,0.9047619047619048,0.8461538461538461,The training is run for 200K iterations ,8,We use Adam optimizer with a fixed learning rate of 0.0001 .,"Every 1,000 iterations , we examine the loss on a fixed validation set and save the parameters if the loss decreases .",experiment
sentence_classification,0,The overview of our model is illustrated in .,model,model,0,33,6,6,0,model : model,0.12359550561797752,0.12,0.3157894736842105,The overview of our model is illustrated in ,9,Our model uses a large auxiliary dataset to incorporate this structural information available in scientific documents into the citation intents .,Let C denote the citation and x denote the ci-tation context relevant to C .,method
natural_language_inference,54,We evaluate our approach using a state - of - the - art neural attention model on the SQuAD dataset .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.026200873362445413,0.8,0.8,We evaluate our approach using a state of the art neural attention model on the SQuAD dataset ,18,"In this paper , we propose structural embedding of syntactic trees ( SEST ) , an algorithm framework to utilize structured information and encode them into vector representations that can boost the performance of algorithms for the machine comprehension .",Experimental results demonstrate that our model can accurately identify the syntactic boundaries of the sentences and extract answers thatare syntactically coherent over the baseline methods .,abstract
natural_language_inference,74,Discourse Marker Augmented Network,system description,Discourse Marker Augmented Network,0,63,1,1,0,system description : Discourse Marker Augmented Network,0.28125,0.02127659574468085,0.5,Discourse Marker Augmented Network,4, , ,method
sentiment_analysis,38,One exception is that we use an L1 penalty for text classification results instead of L2 as we found this performed better in the very low data regime .,experiment,Experimental Setup and Results,0,92,16,16,0,experiment : Experimental Setup and Results,0.5476190476190477,0.7272727272727273,0.7272727272727273,One exception is that we use an L1 penalty for text classification results instead of L2 as we found this performed better in the very low data regime ,29,"For the details on these comparison experiments , we refer the reader to their work .",shows the results of our model on 4 standard text classification datasets .,experiment
natural_language_inference,23,The final formulation for attention score is,architecture,FULLY-AWARE FUSION NETWORK,0,140,68,16,0,architecture : FULLY-AWARE FUSION NETWORK,0.2729044834307992,0.4788732394366197,0.8,The final formulation for attention score is,7,"Additionally , we marry nonlinearity with the symmetric form to provide richer interaction among different parts of the history - of - word .",is an activation function applied element - wise .,method
natural_language_inference,82,"This helps Iron Man surpass @entity26 ( Transformers ) , which is the name of a different movie series in which robots appear but Downey does n't .",analysis,analysis,0,107,4,4,0,analysis : analysis,0.9385964912280702,0.6666666666666666,0.6666666666666666,This helps Iron Man surpass entity26 Transformers which is the name of a different movie series in which robots appear but Downey does n t ,26,"In contrast , maxpooling of @entity2 ( Iron Man ) draws attention to the second and third sentences because Iron Man is said related to Downey in the first sentence .","Quantitatively , in the 479 samples in test set correctly answered by max - pooling but missed by basic model , the average occurrences of answer entities ( 8.0 ) is higher than the one ( 7.2 ) in the 1782 samples correctly answered by both models .",result
sentence_compression,1,The vectors are pre-trained using the Skipgram model,baseline,Baseline,0,54,17,17,0,baseline : Baseline,0.2583732057416268,0.19767441860465115,0.19767441860465115,The vectors are pre trained using the Skipgram model,9,"Additionally , and to better compare the baseline with the LSTM models , we have included as an optional feature a 256 - dimension embedding - vector representation of each input word and its syntactic parent .","Ultimately , our implementation of McDonald 's model contained 463,614 individual features , summarized in three categories :",result
sentiment_analysis,28,"To address this issue , this paper proposes an Attentional Encoder Network ( AEN ) which eschews recurrence and employs attention based encoders for the modeling between context and target .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03888888888888889,0.625,0.625,To address this issue this paper proposes an Attentional Encoder Network AEN which eschews recurrence and employs attention based encoders for the modeling between context and target ,28,"However , RNNs are difficult to parallelize and truncated backpropagation through time brings difficulty in remembering long - term patterns .",We raise the label unreliability issue and introduce label smoothing regularization .,abstract
sentiment_analysis,26,"From these , we then extract token - level scores thatare tied to specific prediction outcomes .",approach,Sentiment Embedding Computation,0,34,13,12,0,approach : Sentiment Embedding Computation,0.13877551020408166,0.15476190476190474,0.32432432432432434,From these we then extract token level scores thatare tied to specific prediction outcomes ,15,"Given a training collection consisting of n binary classification tasks ( e.g. , with documents inn different domains ) , we learn n corresponding polarity prediction models .","Specifically , we train n linear models f i ( x ) = w ix + bi for tasks i = 1 , . . . , n.",method
sentiment_analysis,36,The hyper -parameters of TNet - LF and TNet - AS are listed in .,experiment,Experimental Setup,0,169,28,27,0,experiment : Experimental Setup,0.6759999999999999,0.8,0.7941176470588235,The hyper parameters of TNet LF and TNet AS are listed in ,13,"The training objective is cross - entropy , and Adam is adopted as the optimizer by following the learning rate and the decay rates in the original paper .","Specifically , all hyperparameters are tuned on 20 % randomly held - out training data and the hyper - parameter collection producing the highest accuracy score is used for testing .",experiment
named-entity-recognition,4,"To fine tune on a given task , the supervised labels were temporarily ignored , the biLM fine tuned for one epoch on the training split and evaluated on the development split .",system description,system description,0,195,7,7,0,system description : system description,0.7169117647058824,0.21212121212121213,0.21212121212121213,To fine tune on a given task the supervised labels were temporarily ignored the biLM fine tuned for one epoch on the training split and evaluated on the development split ,31,"As noted in Sec. 3.4 , fine tuning the biLM on task specific data typically resulted in significant drops in perplexity .","Once fine tuned , the biLM weights were fixed during task training .",method
natural_language_inference,11,An illustration of our incremental refinement strategy can be found in .,system description,Refining Word Embeddings by Reading,0,57,23,12,0,system description : Refining Word Embeddings by Reading,0.2065217391304348,0.4107142857142857,0.631578947368421,An illustration of our incremental refinement strategy can be found in ,12,"is constructed by refining the embeddings from the previous step E ? 1 using ( userspecified ) contextual information X for reading step , which is a set of natural language sequences ( i.e. , texts ) .","In the following , we define this procedure formally .",method
sentiment_analysis,9,"In LCF - ATEPC , BERT - SPC only refactored the input sequence form compared with BERT - BASE model .",training,Training Details,0,162,4,4,0,training : Training Details,0.5848375451263538,0.2857142857142857,0.2857142857142857,In LCF ATEPC BERT SPC only refactored the input sequence form compared with BERT BASE model ,17,And the BERT - SPC significantly improved the performance of APC tasks .,"The input sequence of BERT - BASE is formed in "" [ CLS ] "" + sequence + "" [ SEP ] "" , while it is formed in "" [ CLS ] "" + sequence + "" [ SEP ] "" + aspect + "" [ SEP ] "" for BERT - SPC .",experiment
natural_language_inference,79,"Following these successful techniques , researchers have tried to extend the models to go beyond word level to achieve phrase - level or sentence - level representations .",introduction,introduction,0,35,23,23,0,introduction : introduction,0.13059701492537312,0.6764705882352942,0.6764705882352942,Following these successful techniques researchers have tried to extend the models to go beyond word level to achieve phrase level or sentence level representations ,25,"The outcome is that after the model is trained , the word vectors are mapped into a vector space such that semantically similar words have similar vector representations ( e.g. , "" strong "" is close to "" powerful "" ) .","For instance , a simple approach is using a weighted average of all the words in the document .",introduction
sentiment_analysis,50,The sentence representation z is fed into an output layer to predict the probability distribution p over sentiment labels on the target :,model,Attention-based LSTM,0,53,18,17,0,model : Attention-based LSTM,0.3271604938271605,0.8571428571428571,0.85,The sentence representation z is fed into an output layer to predict the probability distribution p over sentiment labels on the target ,23,"score is a content - based function that captures the semantic association between a word and the target , for which we adopt the formulation used in with parameter matrix W a ? R dd .",We refer to this baseline model as LSTM + ATT .,method
text-classification,8,"Inspired by a case study on sentiment analysis tasks , we further propose a hierarchical pooling strategy to abstract and preserve the spatial information in the final representations .",introduction,introduction,0,29,19,19,0,introduction : introduction,0.10780669144981413,0.8260869565217391,0.8260869565217391,Inspired by a case study on sentiment analysis tasks we further propose a hierarchical pooling strategy to abstract and preserve the spatial information in the final representations ,28,"This strategy is demonstrated to extract complementary features relative to the standard averaging operation , while resulting in a more interpretable model .","This strategy is demonstrated to exhibit comparable empirical results to LSTM and CNN on tasks thatare sensitive to word - order features , while maintaining the favorable properties of not having compositional parameters , thus fast training .",introduction
part-of-speech_tagging,1,We employ mini-batch stochastic gradient descent with momentum .,training,training,1,186,12,12,0,training : training,0.7440000000000001,0.7058823529411765,0.7058823529411765,We employ mini batch stochastic gradient descent with momentum ,10,We adopt standard BIOES tagging scheme for NER and Chunking .,"The batch size , momentum and learning rate are set to 10 , 0.9 and ? t = ? 0 1+?t , where ? 0 is the initial learning rate 0.01 and ? =",experiment
sentiment_analysis,20,"It performs the same operation , The * new * season of # TwinPeaks is coming on May 21 , 2017 .",system description,Recurrent Neural Networks,0,58,33,4,0,system description : Recurrent Neural Networks,0.3118279569892473,0.6470588235294118,0.2352941176470588,It performs the same operation The new season of TwinPeaks is coming on May 21 2017 ,17,"An RNN processes an input sequentially , in a way that resembles how humans do it .","It performs the same operation , The * new * season of # TwinPeaks is coming on May 21 , 2017 .",method
natural_language_inference,88,The average sentence length is 21 .,model,Language Modeling,0,150,5,5,0,model : Language Modeling,0.6072874493927125,0.15151515151515152,0.15151515151515152,The average sentence length is 21 ,7,The dataset contains approximately 1 million tokens and a vocabulary size of 10K .,"We use perplexity as our evaluation metric : PPL = exp ( NLL / T ) , where NLL denotes the negative log likelihood of the entire test set and T the corresponding number of tokens .",method
natural_language_inference,57,"Having fixed the embedding space and order , we now consider the problem of finding an orderembedding into this space .",system description,PENALIZING ORDER VIOLATIONS,0,63,29,2,0,system description : PENALIZING ORDER VIOLATIONS,0.3662790697674417,0.5087719298245614,0.13333333333333333,Having fixed the embedding space and order we now consider the problem of finding an orderembedding into this space ,20, ,"In practice , the order embedding condition ( Definition 1 ) is too restrictive to impose as a hard constraint .",method
text-to-speech_synthesis,1,Corresponding author 3 Synthesized speech samples can be found in https://speechresearch.github.io/fastspeech/.,abstract,abstract,0,13,11,11,0,abstract : abstract,0.0593607305936073,1.0,1.0,Corresponding author 3 Synthesized speech samples can be found in https speechresearch github io fastspeech ,16,* Equal contribution ., ,abstract
relation-classification,0,We represent the word sequence in a sentence with bidirectional LSTM - RNNs .,model,Sequence Layer,0,62,14,4,0,model : Sequence Layer,0.2743362831858407,0.18181818181818185,0.4,We represent the word sequence in a sentence with bidirectional LSTM RNNs ,13,"This layer represents sentential context information and maintains entities , as shown in bottom - left part of .","The LSTM unit at t-th word consists of a collection of n ls - dimensional vectors : an input gate it , a forget gate ft , an output gate o t , a memory cell ct , and a hidden state ht .",method
natural_language_inference,26,"Most of the top results from the SQuAD leaderboard do not have public model descriptions available , and it is allowed to use any public data for system training .",dataset,Tasks and Datasets,0,149,18,18,0,dataset : Tasks and Datasets,0.7028301886792453,0.5294117647058824,0.5294117647058824,Most of the top results from the SQuAD leaderboard do not have public model descriptions available and it is allowed to use any public data for system training ,29,"denotes the top 3 single submissions from the leaderboard at the time of submitting SemBERT ( 11 April , 2019 ) .","We therefore further adopt synthetic self training 7 for data augmentation , denoted as SemBERT * LARGE . shows results on the GLUE benchmark datasets , showing SemBERT gives substantial gains over BERT and outperforms all the previous state - of - the - art models in literature 7 .",experiment
natural_language_inference,40,"To test our contribution , we replace the pair of bi - GRUs with the single MAGE - GRU model described for multiple sequences for computing the document and query representations , and compare the final performance .",experiment,Text Comprehension with Coreference,0,162,20,19,0,experiment : Text Comprehension with Coreference,0.5848375451263538,0.7407407407407407,0.7307692307692307,To test our contribution we replace the pair of bi GRUs with the single MAGE GRU model described for multiple sequences for computing the document and query representations and compare the final performance ,34,"where WC is a lookup table of output embeddings , one for each candidate .",As a baseline we also compare to the setting where coreference information is added as extra features at the input of the GRU .,experiment
question-answering,3,"Experimental results on two tasks show that our model gets the state - of - the - art performance on the answer sentence selection task , and achieves a comparable result on the paraphrase identification task .",abstract,abstract,0,51,49,49,0,abstract : abstract,0.20078740157480315,0.9607843137254902,0.9607843137254902,Experimental results on two tasks show that our model gets the state of the art performance on the answer sentence selection task and achieves a comparable result on the paraphrase identification task ,33,"Finally , the composed feature vector is utilized to predict the sentence similarity .","In following parts , we start with a brief overview of our model ( Section 2 ) , followed by the details of our end - to - end implementation ( Section 3 ) .",abstract
relation_extraction,10,"It has 351 documents for train , 80 for validation and 80 for test .",dataset,dataset,0,140,3,3,0,dataset : dataset,0.7291666666666666,0.42857142857142855,0.42857142857142855,It has 351 documents for train 80 for validation and 80 for test ,14,We use the ACE2005 dataset .,There are seven span - level entity types and six ordered span relation types .,experiment
machine-translation,8,"Most of the proposed neural machine translation models belong to a family of encoderdecoders , with an encoder and a decoder for each language , or involve a language - specific encoder applied to each sentence whose outputs are then compared ) .",introduction,introduction,0,13,4,4,0,introduction : introduction,0.039274924471299086,0.19047619047619047,0.19047619047619047,Most of the proposed neural machine translation models belong to a family of encoderdecoders with an encoder and a decoder for each language or involve a language specific encoder applied to each sentence whose outputs are then compared ,39,"Unlike the traditional phrase - based translation system ( see , e.g. , which consists of many small sub-components thatare tuned separately , neural machine translation attempts to build and train a single , large neural network that reads a sentence and outputs a correct translation .",An encoder neural network reads and encodes a source sentence into a fixed - length vector .,introduction
natural_language_inference,17,"Aligning Rounds Attention Interactive Self Type and U = {u j } m j=1 , representing question and context respectively , a similarity matrix E ? R nm is computed as",architecture,Alignment Architecture for MRC,0,52,9,9,0,architecture : Alignment Architecture for MRC,0.2,0.06569343065693431,0.06923076923076922,Aligning Rounds Attention Interactive Self Type and U u j m j 1 representing question and context respectively a similarity matrix E R nm is computed as,27,"Concretely , given two sets of hidden vectors , Model","Aligning Rounds Attention Interactive Self Type and U = {u j } m j=1 , representing question and context respectively , a similarity matrix E ? R nm is computed as",method
natural_language_inference,82,"The training set has 90 k articles and 380 k queries , and both validation and test sets have 1 k articles and 3 k queries .",evaluation,Evaluation,0,71,4,4,0,evaluation : Evaluation,0.6228070175438597,0.3636363636363637,0.3636363636363637,The training set has 90 k articles and 380 k queries and both validation and test sets have 1 k articles and 3 k queries ,26,"The dataset consists of ( D , q , e ) - triples , where the document Dis taken from online news articles , and the query q is formed by hiding a named entity e in a summarizing bullet point of the document .",An average article has about 25 entities and 700 word tokens .,result
sentiment_analysis,43,"2 ) MGAN - CF is better than both MGAN - C and MGAN - F , which demonstrates the coarsegrained attentions and fine - grained attentions could improve the performance from different perspectives .",analysis,Analysis of MGAN model,0,217,6,6,0,analysis : Analysis of MGAN model,0.9041666666666668,0.3,0.5454545454545454,2 MGAN CF is better than both MGAN C and MGAN F which demonstrates the coarsegrained attentions and fine grained attentions could improve the performance from different perspectives ,29,"It demonstrates MGAN - F has better performance on aspects with more words , and make use of the word - level interactions to relieve the information loss occurred in coarsegrained attention mechanism .","Compared with MGAN - CF , the complete MGAN model gains further improvement by bringing the aspect alignment loss , which is designed to capture the aspect level interactions .",result
sentiment_analysis,50,Significance tests are conducted for testing the robustness of methods under random parameter initialization .,dataset,Datasets and Experimental Settings,0,96,20,20,0,dataset : Datasets and Experimental Settings,0.5925925925925926,0.9090909090909092,0.9090909090909092,Significance tests are conducted for testing the robustness of methods under random parameter initialization ,15,"shows the results of LSTM , LSTM + ATT , PRET , MULT , PRET + MULT , and four representative prior works .",Both accuracy and macro - F1 are used for evaluation as label distribution is unbalanced .,experiment
sentence_classification,2,"Finally , we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below , where V k ? R 2dd is a trainable parameter and ? is the element - wise multiplication procedure .",model,Vector Fixing Module,0,130,45,26,0,model : Vector Fixing Module,0.5158730158730159,0.8490566037735849,0.7647058823529411,Finally we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below where V k R 2dd is a trainable parameter and is the element wise multiplication procedure ,38,The integrated context vector c i is then calculated as,"Finally , we construct a weighted gate vector wk and use it to fix the source sentence vectors using the equations below , where V k ? R 2dd is a trainable parameter and ? is the element - wise multiplication procedure .",method
sentiment_analysis,4,"For multimodal feature extraction , we explore different designs for the employed CNNs .",training,Training Details,1,225,8,8,0,training : Training Details,0.6901840490797546,0.6153846153846154,0.6153846153846154,For multimodal feature extraction we explore different designs for the employed CNNs ,13,Their values are summarized in .,"For text , we find the single layer CNN to perform at par with deeper variants .",experiment
sentiment_analysis,35,"Thus , inconsistent granularity in aspects causes the discrepancy between tasks ; ( 2 ) feature distribution discrepancy : generally the domains in the two tasks are different , which causes the distribution shift for both the aspects and its context between domains .",introduction,introduction,0,36,22,22,0,introduction : introduction,0.14516129032258066,0.6285714285714286,0.6285714285714286,Thus inconsistent granularity in aspects causes the discrepancy between tasks 2 feature distribution discrepancy generally the domains in the two tasks are different which causes the distribution shift for both the aspects and its context between domains ,38,"However , target aspects are fine - grained aspect terms , which have accurate position information .","For example , in the source Restaurant domain , tasty and delicious are used to express positive sentiment towards the aspect category "" food "" , while lightweight and responsive often indicate positive sentiment towards the aspect term "" mouse "" in the target Laptop domain .",introduction
natural_language_inference,92,The last example shows the cases that the annotated query makes more sense than the model prediction .,model,Examples,0,288,70,26,0,model : Examples,1.0,1.0,1.0,The last example shows the cases that the annotated query makes more sense than the model prediction ,18,Next example shows the case that the model prediction makes more sense than the annotated query ., ,method
sentiment_analysis,2,"From , we can find that PBAN achieves the best performance among these models .",analysis,Analysis of PBAN Model,0,170,10,10,0,analysis : Analysis of PBAN Model,0.748898678414097,1.0,1.0,From we can find that PBAN achieves the best performance among these models ,14,"PAN takes the concatenation of the aspect term embedding and the word embedding as the inputs of the Bi - GRU structure to obtain the hidden contextual representation , and then PAN utilizes this representation and the position embedding of the aspect term to calculate the attention weights , so as to effectively judge the sentiment polarity of an aspect term .", ,result
text_summarization,8,"The contextual embeddings are fine - tuned to learn a task - specific embedding e ( c ) i as a linear combination of the states of each LSTM layer and the token embedding , with ? and s 0 , 1 , 2 as trainable parameters .",system description,Content Selection,0,108,38,16,0,system description : Content Selection,0.3776223776223776,0.4175824175824176,0.7619047619047619,The contextual embeddings are fine tuned to learn a task specific embedding e c i as a linear combination of the states of each LSTM layer and the token embedding with and s 0 1 2 as trainable parameters ,40,1 ) i and hi .,"The contextual embeddings are fine - tuned to learn a task - specific embedding e ( c ) i as a linear combination of the states of each LSTM layer and the token embedding , with ? and s 0 , 1 , 2 as trainable parameters .",method
natural_language_inference,17,is obtained by greedily maximizing the model distribution :,architecture,Alignment Architecture for MRC,0,105,62,62,0,architecture : Alignment Architecture for MRC,0.40384615384615385,0.4525547445255474,0.476923076923077,is obtained by greedily maximizing the model distribution ,9,"where we abbreviate the model distribution p ( A |C , Q ; ? ) asp ? ( A ) , and the reward function R ( A s , A * ) as R ( A s ) .",The expected gradient ? ? L SCST ( ? ) can be computed according to the REINFORCE algorithm [ Sutton and as,method
natural_language_inference,85,"In our sequential inference model , we keep using BiLSTM to compose local inference information sequentially .",model,Inference Composition,0,131,86,5,0,model : Inference Composition,0.5574468085106383,0.8269230769230769,0.21739130434782608,In our sequential inference model we keep using BiLSTM to compose local inference information sequentially ,16,"We perform the composition sequentially or in its parse context using BiLSTM and tree - LSTM , respectively .","The formulas for BiLSTM are similar to those in Equations and in their forms so we skip the details , but the aim is very different here - they are used to capture local inference information ma and m band their context here for inference composition .",method
sentiment_analysis,1,"few features are computed on multiple channels to capture the inter-channel relations , e.g. , the asymmetry features of PSD and functional connectivity , , where common indices such as correlation , coherence and phase synchronization were used estimate brain functional connectivity between channels .",system description,EEG-Based Emotion Recognition,0,77,5,5,0,system description : EEG-Based Emotion Recognition,0.19444444444444445,0.024154589371980683,0.3333333333333333,few features are computed on multiple channels to capture the inter channel relations e g the asymmetry features of PSD and functional connectivity where common indices such as correlation coherence and phase synchronization were used estimate brain functional connectivity between channels ,42,"The majority of existing features are single - channel features such as statistical features , , fractal dimension ( FD ) , PSD , differential entropy ( DE ) , and wavelet features .","However , leveraging functional connectivity require labor - intensive manual connectivity analysis for each subject and may not be ideal for real - time applications .",method
natural_language_inference,10,"First , our model introduces the trainable composition query vector q ? RD h .",model,Gumbel Tree-LSTM,0,119,63,24,0,model : Gumbel Tree-LSTM,0.5021097046413502,0.8076923076923077,0.6153846153846154,First our model introduces the trainable composition query vector q RD h ,13,We now describe the mechanism for building up the tree structure from an unstructured sentence .,The composition query vector measures how valid a representation is .,method
natural_language_inference,56,task is considered solved if a model achieves greater than 95 % accuracy .,experiment,Experiments,0,88,9,9,0,experiment : Experiments,0.2619047619047619,0.08411214953271028,0.9,task is considered solved if a model achieves greater than 95 accuracy ,13,"The target is a single word , in this case "" garden "" , one - hot encoded over the full bAbI vocabulary of 177 words .",The most difficult tasks require reasoning about three facts .,experiment
natural_language_inference,85,"The original SNLI corpus contains also "" the other "" category , which includes the sentence pairs lacking consensus among multiple human annotators .",experiment,Experimental Setup,0,158,3,3,0,experiment : Experimental Setup,0.6723404255319149,0.42857142857142855,0.42857142857142855,The original SNLI corpus contains also the other category which includes the sentence pairs lacking consensus among multiple human annotators ,21,"Data The Stanford Natural Language Inference ( SNLI ) corpus focuses on three basic relationships between a premise and a potential hypothesis : the premise entails the hypothesis ( entailment ) , they contradict each other ( contradiction ) , or they are not related ( neutral ) .","As in the related work , we remove this category .",experiment
natural_language_inference,28,"In particular , At is determined by A t?1 , h t?1 and x t which can be apart of a multi - layer or a Hopfiled net .",approach,ASSOCIATIVE MEMORY APPROACH,0,73,33,11,0,approach : ASSOCIATIVE MEMORY APPROACH,0.2693726937269373,0.9166666666666666,0.7857142857142857,In particular At is determined by A t 1 h t 1 and x t which can be apart of a multi layer or a Hopfiled net ,28,"Thus , the memory size increases from O(N h ) to O (N 2 h ) , where N h is the hidden size .","By treating the RNN weights as memory determined by the current input data , a larger memory size is provided and less trainable parameters are required .",method
sentiment_analysis,44,"Initially , we considered directly applying the Skip - to each text span to get a generic vector representations for them , since the original Skip - thought vectors were shown in to be useful for many NLP tasks .",model,Learning Text Embeddings,0,115,10,4,0,model : Learning Text Embeddings,0.5227272727272727,0.12345679012345676,0.2352941176470588,Initially we considered directly applying the Skip to each text span to get a generic vector representations for them since the original Skip thought vectors were shown in to be useful for many NLP tasks ,36,Our inputs to the Neural Nets are text spans consisting of multiple words .,"However , given the size of our datasets ( only in the thousands of instances ) , it was clear that using 4800 - dimensional Skip - thought would have created an over-parametrized network prone to over-fitting .",method
sentence_compression,0,"performance of LSTM + trained on 2 million sentence pairs , our method trained on 8,000 sentence pairs does not perform substantially worse .",evaluation,Automatic Evaluation,0,201,17,17,0,evaluation : Automatic Evaluation,0.7204301075268817,0.3863636363636364,0.5151515151515151,performance of LSTM trained on 2 million sentence pairs our method trained on 8 000 sentence pairs does not perform substantially worse ,23,"In the in - domain setting , even compared with the We use an open source implementation : https:// github.com/cnap/sentence-compression .","4 ) In the out - of - domain setting , our BiLSTM + SynFeat and BiLSTM+SynFeat+ILP methods clearly outperform the LSTM and LSTM + methods .",result
machine-translation,6,"Let L D ( V ; ? D , ? emb ) denote the loss of the discriminator :",method,Our Method,0,136,28,28,0,method : Our Method,0.4673539518900344,0.5833333333333334,0.5833333333333334,Let L D V D emb denote the loss of the discriminator ,13,"Let f ? D denote a discriminator with parameters ? D , which takes a word embedding as input and outputs a confidence score between 0 and 1 indicating how likely the word is a rare word .","Following the principle of adversarial training , we develop a minimax objective to train the taskspecific model ( ? model and ? emb ) and the discriminator ( ? D ) as below :",method
relation_extraction,8,The size of the convolution output matrix C ? R n ( s+w?1 ) depends on the number of tokens sin the sentence that is fed into the network .,methodology,Piecewise Max Pooling,0,139,55,2,0,methodology : Piecewise Max Pooling,0.516728624535316,0.5339805825242718,0.1,The size of the convolution output matrix C R n s w 1 depends on the number of tokens sin the sentence that is fed into the network ,29, ,The size of the convolution output matrix C ? R n ( s+w?1 ) depends on the number of tokens sin the sentence that is fed into the network .,method
machine-translation,3,The SMT result from is also listed and falls behind our model by 2.6 BLEU points .,model,Single models,0,251,15,15,0,model : Single models,0.8019169329073482,1.0,1.0,The SMT result from is also listed and falls behind our model by 2 6 BLEU points ,18,We obtain BLEU = 35.9 which outperforms its corresponding shallow model RNNsearch by 7.4 BLEU points ., ,method
natural_language_inference,54,"Take the word "" unit "" as an example , we encode the dependency sub - tree using a Bi-directional LSTM , as indicated in .",system description,Structural Embedding of Dependency Trees (SEDT),0,91,36,8,0,system description : Structural Embedding of Dependency Trees (SEDT),0.3973799126637553,0.631578947368421,0.7272727272727273,Take the word unit as an example we encode the dependency sub tree using a Bi directional LSTM as indicated in ,22,The processing order A ( p ) for dependency tree is then defined to be the dependent 's original order in the sentence .,"In such as a sub-tree , since children are directly linked to the root , they are position according to the original sequence in the sentence .",method
natural_language_inference,23,"Altogether , they form the history of each word in our mental flow .",architecture,FULLY-AWARE ATTENTION ON HISTORY OF WORD,0,110,38,4,0,architecture : FULLY-AWARE ATTENTION ON HISTORY OF WORD,0.21442495126705646,0.2676056338028169,0.25,Altogether they form the history of each word in our mental flow ,13,"As we read through the context , each input word will gradually transform into a more abstract representation , e.g. , from low - level to high - level concepts .","For a human , we utilize the history - of - word so frequently but we often neglect its importance .",method
natural_language_inference,63,"We consider it as a concatenation of various functional vectors which determine the basic operation of the memory such as memory addressing , read and write .",model,Memory Controller,0,89,49,19,0,model : Memory Controller,0.4944444444444445,0.6447368421052632,0.76,We consider it as a concatenation of various functional vectors which determine the basic operation of the memory such as memory addressing read and write ,26,"At the same time , the controller generates interface vector it for the memory interaction based on h mt as follows :",The complete list of functional vectors is described in .,method
natural_language_inference,78,This paper presents a new neural model for NLI .,introduction,introduction,0,25,14,14,0,introduction : introduction,0.09057971014492754,0.35897435897435903,0.35897435897435903,This paper presents a new neural model for NLI ,10,bidirectional LSTM is then used to aggregate the compared alignment vectors .,There are several new novel components in our work .,introduction
sentiment_analysis,21,The weights learned should be smaller over all .,introduction,introduction,0,17,9,9,0,introduction : introduction,0.12142857142857146,0.75,0.75,The weights learned should be smaller over all ,9,"Firstly , cosine similarity serves as a regularization mechanism ; by ignoring vector magnitudes , there is less incentive to increase the magnitudes of the input and output vectors , whereas in the case of dot product , vectors of frequent document - n - gram pairs can be made to have a high dot product simply by increasing the magnitudes of each vector .","Secondly , as cosine similarity is widely used to measure document similarity , we believe our method should more directly maximize the cosine similarity between similar document vectors .",introduction
natural_language_inference,51,We design an experiment similar to the Split MNIST to investigate whether NSM can improve NTM 's performance .,result,Few-shot Learning,0,184,51,4,0,result : Few-shot Learning,0.6943396226415094,0.7285714285714285,0.17391304347826084,We design an experiment similar to the Split MNIST to investigate whether NSM can improve NTM s performance ,19,"In this section , we prove the versatility of NSM by showing that a naive application of NSM without much modification can help NTM to mitigate catastrophic forgetting .","In our experiment , we let the models see the training data from the while freezing others , we force "" hard "" attention over the programs by replacing the softmax function in Eq. 5 with the Gumbel - softmax .",result
text-classification,3,Experiment 1 : Preprocessing effect,experiment,Experiment 1: Preprocessing effect,1,94,33,20,0,experiment : Experiment 1: Preprocessing effect,0.7642276422764228,0.6470588235294118,0.9090909090909092,Experiment 1 Preprocessing effect,4,"In fact , lowercasing and lemmatizing , which are mainly aimed at reducing sparsity , outperform the vanilla setting by over six points in the CNN + LSTM setting and clearly outperform the other preprocessing techniques on the single CNN model as well .","Nevertheless , the use of more complex preprocessing techniques such as lemmatization and multiword grouping does not help in general .",experiment
natural_language_inference,20,"Note , that the same specifications also apply to the experiments that we already discussed above .",model,Fig. 1. Overall NLI Architecture,0,69,34,27,0,model : Fig. 1. Overall NLI Architecture,0.2839506172839506,1.0,1.0,Note that the same specifications also apply to the experiments that we already discussed above ,16,"But before , we first give some more details about the implementation of the model and the training procedures we use .", ,method
natural_language_inference,25,Supplementing the calculation of token reembeddings with the hidden states of a strong language model proves to be highly effective .,model,Incorporating language model representations,1,80,2,2,0,model : Incorporating language model representations,0.7692307692307693,0.2,0.2,Supplementing the calculation of token reembeddings with the hidden states of a strong language model proves to be highly effective ,21, ,In we list development set results for using either the LM hidden states of the first stacked LSTM layer or those of the second one .,method
sentiment_analysis,16,Each of them has its characteristics .,approach,approach,0,125,3,3,0,approach : approach,0.3930817610062893,0.03658536585365853,0.03658536585365853,Each of them has its characteristics ,7,"This section introduces six ( 6 ) alternative targetsensitive memory networks ( TMNs ) , which all can deal with the target - sensitive sentiment problem .",Non- linear Projection ( NP ) :,method
natural_language_inference,32,"Humans seek information in a conversational manner , by asking follow - up questions for additional information based on what they have already learned .",system description,BACKGROUND: MACHINE COMPREHENSION,0,28,19,19,0,system description : BACKGROUND: MACHINE COMPREHENSION,0.06407322654462243,0.5135135135135135,0.6333333333333333,Humans seek information in a conversational manner by asking follow up questions for additional information based on what they have already learned ,23,Context : : An illustration of conversational machine comprehension with an example from the Conversational Question Answering Challenge dataset ( CoQA ) .,Recently proposed conversational machine comprehension ( MC ) datasets aim to enable models to assist in such information seeking dialogs .,method
semantic_role_labeling,2,"With our best model ( L8 + PoE ) , 94.3 % of the predicted arguments spans are part of the gold parse tree .",analysis,BIO Violations,0,187,70,32,0,analysis : BIO Violations,0.8348214285714286,0.813953488372093,0.6666666666666666,With our best model L8 PoE 94 3 of the predicted arguments spans are part of the gold parse tree ,21,natural question follows : are neural SRL models implicitly learning syntax ? shows the trend of deeper models making predictions that are more consistent with the gold syntax in terms of span boundaries .,This consistency is on par with previous CoNLL 2005 systems that directly model constituency and use predicted parse trees as features .,result
natural_language_inference,73,"As a result , training a hard attention model is usually an inefficient process - some even find convergence difficult - and combining them with other neural nets in an end - to - end manner is problematic .",introduction,introduction,0,29,17,17,0,introduction : introduction,0.11068702290076336,0.5,0.5,As a result training a hard attention model is usually an inefficient process some even find convergence difficult and combining them with other neural nets in an end to end manner is problematic ,34,"Thus , it can not be optimized through back - propagation and more typically rely on policy gradient , e.g. , REINFORCE .","However , soft and hard attention mechanisms might be integrated into a single model to benefit each other in overcoming their inherent dis advantages , and this notion motivates our study .",introduction
natural_language_inference,9,"However , a significant difference is that the update gate is computed using sigmoid ( ? ) function on the current memory slot only ( hence internally embedded within the unit ) , whereas the global attention is computed using softmax function over the entire memory ( hence globally defined ) .",model,QRN UNIT,0,84,44,18,0,model : QRN UNIT,0.2545454545454545,0.4583333333333333,0.9473684210526316,However a significant difference is that the update gate is computed using sigmoid function on the current memory slot only hence internally embedded within the unit whereas the global attention is computed using softmax function over the entire memory hence globally defined ,43,The update gate is similar to the global attention mechanism in that it measures the similarity between the sentence ( a memory slot ) and the query .,The update gate can be rather considered as local sigmoid attention .,method
semantic_parsing,1,We now explain the transition system using our running example .,system,Transition System,0,69,3,3,0,system : Transition System,0.4825174825174825,0.08108108108108109,0.17647058823529413,We now explain the transition system using our running example ,11,"Inspired by ( hereafter YN17 ) , we develop a transition system that decomposes the generation procedure of an AST into a sequence of tree - constructing actions .",Right lists the sequence of actions used to construct the example AST .,method
part-of-speech_tagging,0,"Nonetheless , parsing with gold POS tags still yields better results , bolstering the view that POS tagging is an essential task in NLP that needs further development .",system description,POS Tagging,0,57,11,11,0,system description : POS Tagging,0.23170731707317074,1.0,1.0,Nonetheless parsing with gold POS tags still yields better results bolstering the view that POS tagging is an essential task in NLP that needs further development ,27,"In this work , we also demonstrate that the improvements obtained from our AT POS tagger actually contribute to dependency parsing .", ,method
natural_language_inference,2,"Given a pair of passage and question , an MC system needs to extract a text span from the passage as the answer .",system description,Problem Definition,0,41,2,2,0,system description : Problem Definition,0.15298507462686567,0.3333333333333333,0.3333333333333333,Given a pair of passage and question an MC system needs to extract a text span from the passage as the answer ,23, ,"We formulate the answer as two pointers in the passage , which represent the beginning and ending tokens of the answer .",method
text-to-speech_synthesis,1,Synthesized speech is lack of controllability .,introduction,introduction,0,25,12,12,0,introduction : introduction,0.1141552511415525,0.4444444444444444,0.4444444444444444,Synthesized speech is lack of controllability ,7,"Due to error propagation and the wrong attention alignments between text and speech in the autoregressive generation , the generated mel-spectrogram is usually deficient with the problem of words skipping and repeating .","Previous autoregressive models generate mel-spectrograms one by one automatically , without explicitly leveraging the alignments between text and speech .",introduction
sentiment_analysis,5,"Again , the mean accuracy ( ACC ) and standard deviation ( STD ) are used as the evaluation metrics .",experiment,The EEG emotion recognition experiments,0,192,57,27,0,experiment : The EEG emotion recognition experiments,0.7245283018867924,0.6551724137931034,0.7714285714285715,Again the mean accuracy ACC and standard deviation STD are used as the evaluation metrics ,16,This procedure is repeated such that the EEG signals of each subject will be used as testing data once .,"In addition , for comparison purpose , we use twelve methods including Kullback - Leibler importance estimation procedure ( KLIEP ) , unconstrained least - squares importance fitting ( ULSIF ) , selective transfer machine ( STM ) , linear SVM , transfer component analysis ( TCA ) , transfer component analysis ( TCA ) , geodesic flow kernel ( GFK ) , DANN , DGCNN , deep adaptation network ( DAN ) , BiDANN , and A - LSTM , to conduct the same experiments .",experiment
natural_language_inference,20,We further perform additional linguistic error analyses using the MultiNLI Annotation Dataset and the Breaking NLI dataset .,evaluation,Evaluation Benchmarks,0,87,5,5,0,evaluation : Evaluation Benchmarks,0.3580246913580247,0.05319148936170213,0.625,We further perform additional linguistic error analyses using the MultiNLI Annotation Dataset and the Breaking NLI dataset ,18,"Note that we treat them as separate tasks and do not mix any of the training , development and test data in our NLI experiments .","Finally , in order to test the ability of the model to learn generalpurpose representations , we apply the downstream tasks thatare bundled in the SentEval package for sentence embedding evaluation .",result
natural_language_inference,100,We use the term bin to denote a specific range of matching signals .,model,Value-shared Weighting,0,162,33,22,0,model : Value-shared Weighting,0.4426229508196721,0.5892857142857143,0.7333333333333333,We use the term bin to denote a specific range of matching signals ,14,"After this step , the size of the hidden representation becomes fixed and we can use normal fully connected layers to learn higher level representations .","since Pj , i ? [ ? 1 , 1 ] , if we set the size of bins as 0.1 , then we have 21 bins where there is a separate bin for Pj , i = 1 to denote exact match of terms .",method
named-entity-recognition,3,There are alternate possibilities for adding the LM embeddings to the sequence model .,model,Baseline sequence tagging model,0,65,39,39,0,model : Baseline sequence tagging model,0.35135135135135137,0.9069767441860463,0.9069767441860463,There are alternate possibilities for adding the LM embeddings to the sequence model ,14,"More formally , we simply replace ( 2 ) with",One pos-sibility adds a non-linear mapping after the concatenation and before the second RNN ( e.g. re -,method
sentiment_analysis,26,We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the sentiment embeddings .,introduction,introduction,1,20,13,13,0,introduction : introduction,0.08163265306122447,0.9285714285714286,0.9285714285714286,We instead propose a bespoke convolutional neural network architecture with a separate memory module dedicated to the sentiment embeddings ,20,"An intuitive solution would be to concatenate regular embeddings , which provide semantic relatedness cues , with sentiment polarity cues thatare captured in additional dimensions .",Our empirical study shows that the sentiment embeddings can lead to consistent gains across different datasets in a diverse set of domains and languages if a suitable neural network architecture is used .,introduction
natural_language_inference,37,This optimizes the negative log -likelihood of selecting any correct start token .,method,Handling Noisy Labels,0,66,28,17,0,method : Handling Noisy Labels,0.25680933852140075,0.9333333333333332,0.8947368421052632,This optimizes the negative log likelihood of selecting any correct start token ,13,"where A is the set of tokens that start an answer span , n is the number of context tokens , and s i is a scalar score computed by the model for span i .","This objective is agnostic to how the model distributes probability mass across the possible answer spans , thus the model can "" choose "" to focus on only the more relevant spans .",method
sentiment_analysis,29,"In this paper , we introduce an attention - over- attention ( AOA ) neural network for aspect level sentiment classification .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02873563218390805,0.5,0.5,In this paper we introduce an attention over attention AOA neural network for aspect level sentiment classification ,18,Aspect - level sentiment classification aims to identify the sentiment expressed towards some aspects given context sentences .,Our approach models aspects and sentences in a joint way and explicitly captures the interaction between aspects and context sentences .,abstract
machine-translation,0,"where x <t is a shorthand notation for xt ? 1 , . . . , x 1 .",analysis,Quantitative Analysis,0,170,20,20,0,analysis : Quantitative Analysis,0.7762557077625569,0.35714285714285715,0.9523809523809524,where x t is a shorthand notation for xt 1 x 1 ,13,"Hence , the conditional probability of any xi t / ? SL is actually given by the model as","were notable to achieve better performance on the test set , but only on the development set .",result
sentiment_analysis,0,"However , none of these studies have utilized information from speech signals and text sequences simultaneously in an end - to - end learning neural network - based model to classify emotions .",introduction,introduction,0,41,32,32,0,introduction : introduction,0.2303370786516854,1.0,1.0,However none of these studies have utilized information from speech signals and text sequences simultaneously in an end to end learning neural network based model to classify emotions ,29,"As emotional dialogue is composed of sound and spoken content , researchers have also investigated the combination of acoustic features and language information , built belief network - based methods of identifying emotional key phrases , and assessed the emotional salience of verbal cues from both phoneme sequences and words .", ,introduction
question-answering,4,To determine the best hyperparameters we performed a grid search over 150 settings based on validation - set accuracy .,training,Training and Model Details,1,236,22,22,0,training : Training and Model Details,0.8109965635738832,0.8461538461538461,0.8461538461538461,To determine the best hyperparameters we performed a grid search over 150 settings based on validation set accuracy ,19,"We used the Adam optimizer with the standard settings ( Kingma and Ba , 2014 ) and a learning rate of 0.003 .","MCTest 's original validation set is too small for reliable hyperparameter tuning , so , following , we merged the training and validation sets of MCTest - 160 and MCTest - 500 , then split them randomly into a 250 - story training set and a 200 - story validation set .",experiment
natural_language_inference,64,"However , finding the right weights is complex as neural models do not converge to a global optimum : thereby leading to very different outcomes for different weights .",system description,Transformers for AS2,0,106,35,28,0,system description : Transformers for AS2,0.4223107569721116,0.5384615384615384,1.0,However finding the right weights is complex as neural models do not converge to a global optimum thereby leading to very different outcomes for different weights ,27,"In principle when using a combination of general and domain specific data , instance weighting can be used by giving more importance to the target data .", ,method
sentiment_analysis,6,"In this section , we present unimodal and multimodal sentiment analysis performance of different LSTM network variants as explained in Section 3.2.3 and comparison with the state of the art .",model,Performance of Different Models,0,235,2,2,0,model : Performance of Different Models,0.8131487889273357,0.15384615384615385,0.15384615384615385,In this section we present unimodal and multimodal sentiment analysis performance of different LSTM network variants as explained in Section 3 2 3 and comparison with the state of the art ,32, ,Hierarchical vs Non-hierarchical Fusion Framework,method
part-of-speech_tagging,5,We compare our results against the top systems of the CoNLL 2017 Shared Task .,system description,Part-of-Speech Tagging Results,0,138,4,4,0,system description : Part-of-Speech Tagging Results,0.6831683168316832,0.4444444444444444,0.4444444444444444,We compare our results against the top systems of the CoNLL 2017 Shared Task ,15,"In our first experiment , we used our model in the setting of the CoNLL 2017 Shared Task to annotate words with XPOS 3 tags .",contains the results of this task for the large treebanks .,method
machine-translation,8,"In order to reduce computation , we use a singlelayer multilayer perceptron such that",model,ALIGNMENT MODEL,0,241,28,3,0,model : ALIGNMENT MODEL,0.7280966767371602,0.23728813559322035,0.3333333333333333,In order to reduce computation we use a singlelayer multilayer perceptron such that,13,The alignment model should be designed considering that the model needs to be evaluated T x Ty times for each sentence pair of lengths T x and Ty .,"where W a ? R nn , U a ? R n 2n and v a ? Rn are the weight matrices .",method
sentiment_analysis,49,"Additionally , a speaker can utter multiple utterances in a single video and these utterances can have different sentiments .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.09090909090909093,0.3939393939393939,0.3939393939393939,Additionally a speaker can utter multiple utterances in a single video and these utterances can have different sentiments ,19,"In addition to the visual frames , it also provides information such as acoustic and textual representation of spoken language .",The sentiment information of an utterance often has inter-dependence on other contextual utterances .,introduction
sentiment_analysis,4,"Pb , however , is pre-occupied and replies sarcastically ( u4 ) .",introduction,introduction,0,49,40,40,0,introduction : introduction,0.15030674846625766,0.9090909090909092,0.9090909090909092,Pb however is pre occupied and replies sarcastically u4 ,10,"Pa is frustrated over her long term unemployment and seeks encouragement ( u1 , u 3 ) .",This enrages Pa to appropriate an angry response ( u6 ) .,introduction
text_generation,5,"If only the first term of the VAE variational bound E q ? ( z |x ) [ log p ? ( x |z ) ] is used as an objective , the variance of the posterior probability q ? ( z|x ) will become small and the training procedure reduces to an autoencoder .",system description,Training Collapse with Textual VAEs,0,73,5,5,0,system description : Training Collapse with Textual VAEs,0.2474576271186441,0.06578947368421052,0.29411764705882354,If only the first term of the VAE variational bound E q z x log p x z is used as an objective the variance of the posterior probability q z x will become small and the training procedure reduces to an autoencoder ,44,"Note , however , that while VAEs are valid probabilistic models whose likelihood can be evaluated on held - out data , autoencoders are not valid models .","It is the KL - divergence term , KL ( q ? ( z|x ) ||p ? ( z ) ) , that discourages the VAE memorizing each x as a single latent point .",method
sentiment_analysis,33,"Two answer sentence selection datasets , QASent and Wik - iQA , are adopted in our sentence matching experiments .",experiment,Experimental settings,0,72,8,5,0,experiment : Experimental settings,0.5760000000000001,0.8888888888888888,0.8333333333333334,Two answer sentence selection datasets QASent and Wik iQA are adopted in our sentence matching experiments ,17,We report accuracy results for both binary classification and fine - grained classification settings .,We use MAP and MRR to evaluate the performance of answer sentence selection models .,experiment
natural_language_inference,82,"For example , in , the first sentence mentioning "" Robert Downey Jr. "" relates Downey to Iron Man , whereas a subsequent mention of "" Downey "" also relates him to a robotic arm .",system description,Max-pooling,0,56,19,4,0,system description : Max-pooling,0.4912280701754386,0.6333333333333333,0.2857142857142857,For example in the first sentence mentioning Robert Downey Jr relates Downey to Iron Man whereas a subsequent mention of Downey also relates him to a robotic arm ,29,"However , as an entity occurs multiple times in a document , information is accumulated as subsequent occurrences of the entity draw information from previous mentions .","Both of the two pieces of information are necessary to answer the query "" Iron Man star [ X ] presents . . . with a bionic arm "" .",method
natural_language_inference,33,We present a multi-task framework for learning general - purpose fixed - length sentence representations .,evaluation,CONCLUSION & FUTURE WORK,0,167,43,2,0,evaluation : CONCLUSION & FUTURE WORK,0.6398467432950191,0.3138686131386861,0.2,We present a multi task framework for learning general purpose fixed length sentence representations ,15, ,Our primary motivation is to encapsulate the inductive biases of several diverse training signals used to learn sentence representations into a single model .,result
natural_language_inference,58,It is hard for these baselines to capture the dynamic termination behavior of ReasoNets .,system description,REASONING NETWORKS,0,128,48,48,0,system description : REASONING NETWORKS,0.38208955223880603,0.9411764705882352,0.9411764705882352,It is hard for these baselines to capture the dynamic termination behavior of ReasoNets ,15,"Intuitively , the average baselines {b T ;T = 1.. T max } are global variables independent of instances .","Since ReasoNets may stop at di erent time steps for di erent instances , the adoption of a global variable without considering the dynamic variance in each instance is inappropriate .",method
named-entity-recognition,8,The code and pre-trained models are available at https://github.com/,introduction,introduction,0,36,24,24,0,introduction : introduction,0.09302325581395347,0.96,0.96,The code and pre trained models are available at https github com ,13,BERT advances the state of the art for eleven NLP tasks .,google-research/bert .,introduction
natural_language_inference,51,"These findings have sparked a new research direction called Memory Augmented Neural Networks ( MANNs ) that emulate modern computer behavior by detaching memorization from computation via memory and controller network , respectively .",introduction,introduction,0,12,4,4,0,introduction : introduction,0.045283018867924525,0.17391304347826084,0.17391304347826084,These findings have sparked a new research direction called Memory Augmented Neural Networks MANNs that emulate modern computer behavior by detaching memorization from computation via memory and controller network respectively ,31,"However , in practice RNNs struggle to learn simple procedures as they lack explicit memory .",MANNs have demonstrated significant improvements over memory - less RNNs in various sequential learning tasks .,introduction
machine-translation,1,and contain the results of the experiments .,model,model,0,185,41,41,0,model : model,0.9203980099502488,0.8367346938775511,0.8367346938775511,and contain the results of the experiments ,8,"We do not use length normalization , nor do we keep score of which parts of the source sentence have been translated .","On NewsTest 2014 the ByteNet achieves the highest performance in character - level and subword - level neural machine translation , and compared to the word - level systems it is second only to the version of GNMT that uses word - pieces .",method
question-answering,1,"Moreover , the three tasks involve two languages , different types of matching , and distinctive writing styles , proving the broad applicability of the proposed models .",experiment,Experiments,0,140,4,4,0,experiment : Experiments,0.7253886010362695,1.0,1.0,Moreover the three tasks involve two languages different types of matching and distinctive writing styles proving the broad applicability of the proposed models ,24,"Among them , the first two tasks ( namely , Sentence Completion and Tweet - Response Matching ) are about matching of language objects of heterogenous natures , while the third one ( paraphrase identification ) is a natural example of matching homogeneous objects .", ,experiment
natural_language_inference,49,"Take premise as example , we model self - attention by",model,DENSELY INTERACTIVE INFERENCE NETWORK,0,92,46,22,0,model : DENSELY INTERACTIVE INFERENCE NETWORK,0.3622047244094488,0.6764705882352942,0.5,Take premise as example we model self attention by,9,These new representation are then passed to self - attention layer to take into account the word order and context information .,"is vector concatenation across row , and the implicit multiplication is matrix multiplication .",method
sentiment_analysis,39,"We develop several strong baselines , relying on logistic regression and state - of - the - art recurrent neural networks .",abstract,abstract,0,12,10,10,0,abstract : abstract,0.048979591836734684,0.8333333333333334,0.8333333333333334,We develop several strong baselines relying on logistic regression and state of the art recurrent neural networks ,18,Text coming from QA platforms is far less constrained compared to text from review specific platforms which current datasets are based on .,This work is licensed under a Creative Commons Attribution 4.0 International Licence .,abstract
phrase_grounding,0,"In contrast to many works in the literature , we do n't use a pre-defined set of image concepts or words in our method .",system description,Grounding natural language in images,0,47,14,14,0,system description : Grounding natural language in images,0.2088888888888889,0.35,0.9333333333333332,In contrast to many works in the literature we do n t use a pre defined set of image concepts or words in our method ,26,"Several grounding works have also explored the use of additional knowledge , such as image and linguistic structures , phrase context and exploiting pre-trained visual models predictions .",We instead rely on visual feature maps and a character - based language model with contextualized embeddings which could handle any unseen word considering the context in the sentence .,method
relation-classification,1,"Besides , I ( O ) is a switching function to distinguish the loss of tag ' O ' and relational tags that can indicate the results .",method,The End-to-end Model,0,130,65,34,0,method : The End-to-end Model,0.5284552845528455,0.9285714285714286,0.8717948717948718,Besides I O is a switching function to distinguish the loss of tag O and relational tags that can indicate the results ,23,"where | D| is the size of training set , L j is the length of sentence x j , y ( j ) t is the label of word tin sentence x j and p ( j ) t is the normalized probabilities of tags which defined in Formula 15 .",It is defined as follows :,method
sentiment_analysis,9,Suppose is the input features learned by the LCFG .,model,Multi-Head Self-Attention,0,96,26,9,0,model : Multi-Head Self-Attention,0.3465703971119133,0.325,0.5625,Suppose is the input features learned by the LCFG ,10,The MHSA can avoids the negative influence caused by the long distance dependence of the context when learning the features .,The scale - dot attention is calculate as follows :,method
named-entity-recognition,1,"From the formulation above , it is evident that we encourage our network to produce a valid sequence of output labels .",model,model,0,65,16,16,0,model : model,0.3140096618357488,0.8421052631578947,0.8421052631578947,From the formulation above it is evident that we encourage our network to produce a valid sequence of output labels ,21,where Y X represents all possible tag sequences ( even those that do not verify the IOB format ) for a sentence X .,"While decoding , we predict the output sequence that obtains the maximum score given by :",method
natural_language_inference,10,"In this paper , we propose Gumbel Tree - LSTM , which is a novel RvNN architecture that does not require structured data and learns to compose task - specific tree structures without explicit guidance .",introduction,introduction,1,23,14,14,0,introduction : introduction,0.0970464135021097,0.4827586206896552,0.4827586206896552,In this paper we propose Gumbel Tree LSTM which is a novel RvNN architecture that does not require structured data and learns to compose task specific tree structures without explicit guidance ,32,"Furthermore , the optimal hierarchical composition of words might differ depending on the properties of a task .","Our Gumbel Tree - LSTM model is based on tree - structured long short - term memory ( Tree - LSTM ) architecture , which is one of the most renowned variants of RvNN .",introduction
sentiment_analysis,48,The model learns the latent distribution via variational inference .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.03389830508474576,0.6666666666666666,0.6666666666666666,The model learns the latent distribution via variational inference ,10,This paper proposes a semisupervised method for the ATSA problem by using the Variational Autoencoder based on Transformer .,"By disentangling the latent representation into the aspect - specific sentiment and the lexical context , our method induces the underlying sentiment prediction for the unlabeled data , which then benefits the ATSA classifier .",abstract
text_summarization,11,The learning process is to minimize the negative log -likelihood between the generated summary ? and reference y:,training,Training,0,75,5,5,0,training : Training,0.5208333333333334,0.625,0.625,The learning process is to minimize the negative log likelihood between the generated summary and reference y ,18,"Given the parameters ? and source text x , the models generates a summary ?.",The learning process is to minimize the negative log -likelihood between the generated summary ? and reference y:,experiment
natural_language_inference,73,"Such method would improve both the prediction quality of the soft attention mechanism and the trainability of the hard attention mechanism , while boosting the ability to model contextual dependencies .",introduction,introduction,0,33,21,21,0,introduction : introduction,0.12595419847328246,0.6176470588235294,0.6176470588235294,Such method would improve both the prediction quality of the soft attention mechanism and the trainability of the hard attention mechanism while boosting the ability to model contextual dependencies ,30,"Conversely , the soft one is used to provide a stable environment and strong reward signals to help in training the hard one .","To the best of our knowledge , the idea of combining hard and soft attention within a model has not yet been studied .",introduction
natural_language_inference,96,"We found after 1000 examples that the annotators tended to have high agreement , also generally choosing found endings over generations ( see ) .",system description,Human verification,0,177,119,11,0,system description : Human verification,0.4538461538461538,0.9916666666666668,0.9166666666666666,We found after 1000 examples that the annotators tended to have high agreement also generally choosing found endings over generations see ,22,Examples with ? 3 nongibberish endings were filtered out .,"Thus , we collected the remaining 112 k examples with one annotator each , periodically verifying that annotators preferred the found endings .",method
question_answering,0,To encode the question we use the same DCNN model ( see Section 3.1 ) .,model,Graph Neural Network (GNN) -,0,178,16,5,0,model : Graph Neural Network (GNN) -,0.6033898305084746,0.7619047619047619,0.5,To encode the question we use the same DCNN model see Section 3 1 ,15,"This model encodes all information from semantic graphs , including their structure , into a vector representation .","The defined baselines use either manual features to capture the structure of the semantic graph ( STAGG ) , a simple pooling mechanism ( Pooled Edges ) or disregard the structure completely ( Single Edge ) .",method
temporal_information_extraction,1,"However , we propose to exclude both the unknown pairs and the vague classifier from the training process - by changing the structured loss function to ignore the inference feedback on vague TLINK s ( see Line 9 in Algorithm 1 and Line 9 in Algorithm 2 ) .",training,Missing Annotations,0,158,83,7,0,training : Missing Annotations,0.6147859922178989,0.7830188679245284,0.2333333333333333,However we propose to exclude both the unknown pairs and the vague classifier from the training process by changing the structured loss function to ignore the inference feedback on vague TLINK s see Line 9 in Algorithm 1 and Line 9 in Algorithm 2 ,45,We could simply use these unknown pairs ( or some filtered version of them ) to design rules or train classifiers to identify whether a TLINK is vague or not .,The reasons are discussed below .,experiment
natural_language_inference,31,"The input of the first block x ( 1 ) , as mentioned before , is the output of the embedding layer ( denoted by blank rectangles in ) .",approach,Augmented Residual Connections,0,65,29,7,0,approach : Augmented Residual Connections,0.2355072463768116,0.453125,0.6363636363636364,The input of the first block x 1 as mentioned before is the output of the embedding layer denoted by blank rectangles in ,24,Let o ( 0 ) be a sequence of zero vectors .,"The input of the n - th block x ( n ) ( n ? 2 ) , is the concatenation of the input of the first block x ( 1 ) and the summation of the output of previous two blocks ( denoted by rectangles with diagonal stripes in ) :",method
natural_language_inference,97,"In this section we borrow from , who laid out the MC problem nicely .",system description,The Problem,0,50,2,2,0,system description : The Problem,0.17123287671232876,0.2,0.2,In this section we borrow from who laid out the MC problem nicely ,14, ,Machine comprehension requires machines to answer questions based on unstructured text .,method
sentiment_analysis,27,"Based on the intuition that the polarity of a given aspect is easier to be influenced by the context words with closer distance to the aspect , we introduce position encoding to simulate this normal rules in natural language .",methodology,Position encoding,0,106,29,2,0,methodology : Position encoding,0.3840579710144928,0.3411764705882353,0.3333333333333333,Based on the intuition that the polarity of a given aspect is easier to be influenced by the context words with closer distance to the aspect we introduce position encoding to simulate this normal rules in natural language ,39, ,"Formally , given an aspect W a i that is one of the K aspects , where i ? [ 1 , K ] is the index of aspects , the relative distance d a it between the t- th word and the i - th aspect is defined as follows :",method
sentiment_analysis,36,"Then , we use v to help CNN locate the correct opinion w.r.t. the given target :",model,Convolutional Feature Extractor,0,125,78,8,0,model : Convolutional Feature Extractor,0.5,0.8297872340425532,0.3333333333333333,Then we use v to help CNN locate the correct opinion w r t the given target ,18,"where k is the index of the first target word , C is a pre-specified constant , and m is the length of the target w ? .","Based on Eq. 10 and Eq. 11 , the words close to the target will be highlighted and those faraway will be downgraded .",method
natural_language_inference,14,The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text .,introduction,introduction,1,16,3,3,0,introduction : introduction,0.0935672514619883,0.15789473684210525,0.15789473684210525,The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text ,19,Enabling commonsense reasoning in machines is a longstanding challenge in AI .,The Situations With Adversarial Generations ( SWAG ) dataset introduced a large - scale benchmark for commonsense question answering in the form of multiple choice sentence completion questions describing situations as observed in video .,introduction
natural_language_inference,97,"The Fiedler vector maps a weighted graph onto a line such that connected nodes stay close , modulated by the connection weights .",model,Dependency Sliding Window,0,162,80,16,0,model : Dependency Sliding Window,0.5547945205479452,0.7142857142857143,0.64,The Fiedler vector maps a weighted graph onto a line such that connected nodes stay close modulated by the connection weights ,22,"where vi are the vertices of the graph , and ? ij is the weight of the edge from vertex i to vertex j.",This enables us to reorder the words of a sentence based on their proximity in the dependency graph .,method
named-entity-recognition,7,"We use neural networks to learn the representation of the parser state , which is pk in ( 1 ) .",model,Neural Transition-based Model,0,86,43,2,0,model : Neural Transition-based Model,0.5443037974683544,0.6323529411764706,1.0,We use neural networks to learn the representation of the parser state which is pk in 1 ,18, , ,method
relation_extraction,9,"Recall the definition of ? ? ( x , y ) in Eq. ( 1 ) .",architecture,Our Architectures,0,183,12,12,0,architecture : Our Architectures,0.8755980861244019,0.35294117647058826,0.35294117647058826,Recall the definition of x y in Eq 1 ,10,Table 5 lists the top - ranked trigrams for each relation classy in terms of their contribution to the score for determining the relation classification .,"In the network , we trace back the trigram that contributed most to the correct classification in terms of ? ? ( S i , y ) for each sentence Si .",method
text_summarization,8,This improvement increases with the larger subsets to up to 7 points .,result,Results,0,224,30,30,0,result : Results,0.7832167832167832,0.9090909090909092,0.9090909090909092,This improvement increases with the larger subsets to up to 7 points ,13,"The results , shown in , demonstrates that even a model trained on the smallest subset leads to an improvement of almost 5 points over the model without bottom - up attention .","While this approach does not reach a comparable performance to models trained directly on the NYT dataset , it still represents a significant increase over the not-augmented CNN - DM model and produces summaries that are quite readable .",result
text_summarization,9,In this paper we propose a novel recurrent neural network for the problem of abstractive sentence summarization .,introduction,introduction,0,15,7,7,0,introduction : introduction,0.09803921568627452,0.35,0.35,In this paper we propose a novel recurrent neural network for the problem of abstractive sentence summarization ,18,Abstractive models generate summaries from scratch without being constrained to reuse phrases from the original text .,"Inspired by the recently proposed architectures for machine translation , our model consists of a conditional recurrent neural network , which acts as a decoder to generate the summary of an input sentence , much like a standard recurrent language model .",introduction
natural_language_inference,34,"In graph construction stage , we use a pretrained NER model from Stanford CoreNLP Toolkits 1 to extract named entities .",implementation,Implementation Details,1,212,5,5,0,implementation : Implementation Details,0.7186440677966102,0.38461538461538464,0.38461538461538464,In graph construction stage we use a pretrained NER model from Stanford CoreNLP Toolkits 1 to extract named entities ,20,We set a relatively low threshold during selection to keep a high recall ( 97 % ) and a reasonable precision ( 69 % ) on supporting facts .,The maximum number of entities in a graph is set to be 40 .,experiment
text_generation,5,We select the dimension of z from .,system description,Model configurations and Training details,0,171,19,19,0,system description : Model configurations and Training details,0.5796610169491525,0.59375,0.59375,We select the dimension of z from ,8,"The number of channels for convolutions in CNN decoders is 512 internally and 1024 externally , as shown in Section 2.3 .",We find our model is not sensitive to this parameter .,method
paraphrase_generation,0,historical epic with the courage of its convictions about both scope and detail .,baseline,Very Positive 160925,0,217,39,2,0,baseline : Very Positive 160925,0.9156118143459916,0.9285714285714286,1.0,historical epic with the courage of its convictions about both scope and detail ,14, , ,result
text_summarization,12,"If | y | | x | , which means not all words in summary come from input sentence , we denote this as abstractive sentence summarization .",system description,Problem Formulation,0,73,7,7,0,system description : Problem Formulation,0.3201754385964912,0.5384615384615384,0.7777777777777778,If y x which means not all words in summary come from input sentence we denote this as abstractive sentence summarization ,22,"If | y | ? | x | , which means all words in summary y must appear in given input , we denote this as extractive sentence summarization .",provides an example .,method
natural_language_inference,37,"The British public reacted to his death by acclaiming ' Gordon of Khartoum ' , a saint .",method,Handling Noisy Labels,0,55,17,6,0,method : Handling Noisy Labels,0.2140077821011673,0.5666666666666667,0.3157894736842105,The British public reacted to his death by acclaiming Gordon of Khartoum a saint ,15,"Khartoum came under siege the next month and rebels broke into the city , killing Gordon and the other defenders .","However , historians have suggested that Gordon defied orders and refused to evacuate ... :",method
natural_language_inference,81,"Nice ( ; Niard , classical norm , or "" "" , nonstandard , ) is the fifth most populous city in France and the capital of the Alpes - Maritimes "" dpartement "" .",APPENDIX,ATTENTION MAPS,0,252,44,29,0,APPENDIX : ATTENTION MAPS,0.5985748218527316,0.20657276995305165,0.4833333333333333,Nice Niard classical norm or nonstandard is the fifth most populous city in France and the capital of the Alpes Maritimes dpartement ,23,"His trademark attire was a boater hat , which he always wore onstage with a tuxedo .","The urban are a of Nice extends beyond the administrative city limits , with a population of about 1 million on an are a of .",others
natural_language_inference,42,"This type of attention is often called "" self - attention "" or "" self - alignment "" , .",system description,Google's Transformer,0,72,32,15,0,system description : Google's Transformer,0.2491349480968858,0.18285714285714288,0.8823529411764706,This type of attention is often called self attention or self alignment ,13,"If one wants to model the interdependence of words within a single piece of text , U , K and V are all equal and consist of the text of interest embedded in some vectorial space .","Conversely , if one seeks the relationship between words from two different passages , then U represents one , while K and V represent the other .",method
temporal_information_extraction,0,"We merge all rule - based classifiers into one sieve component ( rule - based sieve ) , and all Support Vector Machine ( SVM ) classifiers in the machine - learned sieve .",system,Temporal Relation Type Classification,0,38,11,3,0,system : Temporal Relation Type Classification,0.20105820105820105,0.16923076923076924,0.6,We merge all rule based classifiers into one sieve component rule based sieve and all Support Vector Machine SVM classifiers in the machine learned sieve ,26,"Our sieve - based architecture is inspired by CAEVO , although we significantly reduce the system complexity as follows :","Instead of running transitive inference after each classifier , we run our temporal reasoner module on the output of the rule - based sieve , only once .",method
sentiment_analysis,43,"Hence , we propose an aspect alignment loss , which is designed to strengthen the at -",approach,Multi-grained Attention Layer,0,90,35,9,0,approach : Multi-grained Attention Layer,0.375,0.4022988505747127,0.8181818181818182,Hence we propose an aspect alignment loss which is designed to strengthen the at ,15,"From other perspective , we observe the relationship among aspects can introduce extra valuable information .",Loss tention difference among aspects with same context and different sentiment polarities .,method
natural_language_inference,30,We first use 4 M examples to train and 6M as validation set to determine the value of the regularization parameter ?.,architecture,Fine-tuning the Similarity between Embeddings,0,177,58,16,0,architecture : Fine-tuning the Similarity between Embeddings,0.686046511627907,0.935483870967742,0.8,We first use 4 M examples to train and 6M as validation set to determine the value of the regularization parameter ,22,We solve this problem in a few minutes using L - BFGS on a subset of m = 10 M examples from D .,"We then retrain the model on the whole 10M examples using the selected value , which happened to be ? = 1.7 10 ?5 .",method
text_summarization,2,"Without explicitly modeling the two factors , ' semantics ' can outweigh ' structure , ' resulting in summaries that fail to keep the original meaning intact .",approach,Structural info,0,107,56,15,0,approach : Structural info,0.3848920863309353,0.4274809160305344,0.9375,Without explicitly modeling the two factors semantics can outweigh structure resulting in summaries that fail to keep the original meaning intact ,22,"Intuitively , a source word is copied to the summary for two reasons : it contains salient semantic content , or it serves a critical syntactic role in the source sentence .","In the following we propose a two - way mechanism to separately model the "" semantic "" and "" structural "" importance of source words .",method
relation-classification,6,task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence .,introduction,introduction,1,12,3,3,0,introduction : introduction,0.06629834254143646,0.17647058823529413,0.17647058823529413,task of relation classification is defined as predicting a semantic relationship between two tagged entities in a sentence ,19,"Classifying semantic relations between entity pairs in sentences plays a vital role in various NLP tasks , such as information extraction , question answering and knowledge base population .","For example , given a sentence with tagged entity pair , crash and attack , this sentence is classified into the re-lation Cause - Effect ( e1 , e2 ) 1 between the entity pair like .",introduction
natural_language_inference,7,"When computing ha , we assume access to representations of individual passage words that have been augmented with a representation of the question .",model,RASOR: RECURRENT SPAN REPRESENTATION,0,73,22,3,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.4101123595505618,0.4150943396226415,0.3333333333333333,When computing ha we assume access to representations of individual passage words that have been augmented with a representation of the question ,23,"The previously defined probability distribution depends on the answer span representations , ha .","We denote these question - focused passage word embeddings as {p * 1 , . . . , p * m } and describe their creation in Section 3.3 .",method
named-entity-recognition,7,"Since the shift - reduce system assumes unary and binary branching , we binarize the trees in each forest in a left - branching manner .",model,Shift-Reduce System,0,59,16,11,0,model : Shift-Reduce System,0.3734177215189873,0.2352941176470588,0.44,Since the shift reduce system assumes unary and binary branching we binarize the trees in each forest in a left branching manner ,23,UNARY - X pops the top item t 0 from the stack and constructs a new tree element { X ? t 0 } which is pushed back to the stack .,"For example , if three consecutive words A , B , C are annotated as Person , we convert it into a binary tree { P erson ? { P erson * ? A , B} , C} where P erson * is a temporary label for P erson .",method
natural_language_inference,80,"Hence , we use 450 - dimensional BiLSTM in our proposed model .",ablation,ablation,0,206,17,17,0,ablation : ablation,0.7103448275862069,1.0,1.0,Hence we use 450 dimensional BiLSTM in our proposed model ,11,"On the other hand , using higher dimensionality leads to overfitting which hurts the performance on the development set .", ,result
natural_language_inference,100,"We make the following observations : ( 1 ) The exact match signal which is corresponding to 1 in the last bin is tied with a very large weight , which shows that exact match information is very important .",result,Value-shared Weight,0,273,12,9,0,result : Value-shared Weight,0.7459016393442623,0.14634146341463414,0.5625,We make the following observations 1 The exact match signal which is corresponding to 1 in the last bin is tied with a very large weight which shows that exact match information is very important ,36,"The range of match signals is [ - 1 , 1 ] from the left to the right .","2 ) For positive matching score from ( 0 , 1 ) , which is corresponding to bin index ( 300 , 600 ) , the learned value - shared weights are different for matching score range ( 0.5 , 1 ) ( bin index ( 450 , 600 ) ) and matching score range ( 0 , 0.5 ) ( bin index ( 300 , 450 ) ) .",result
natural_language_inference,27,The reported results are obtained on the development set .,analysis,ROBUSTNESS STUDY,0,277,34,12,0,analysis : ROBUSTNESS STUDY,0.8195266272189349,0.9189189189189192,0.8,The reported results are obtained on the development set ,10,From 74.9 / 83.6 + 1.3 / + 0.9 + data augmentation 3 ( 2:1:1 ) 75.0 / 83.6 + 1.4 / + 0.9 + data augmentation 3 ( 3:1:1 ) 75.1 / 83.8 + 1.5 / + 1.1 + data augmentation 3 ( 4:1:1 ) 75.0 / 83.6 + 1.4 / + 0.9 + data augmentation 3 ( 5:1:1 ) 74.9 / 83.5 + 1.3 / + 0.8 : An ablation study of data augmentation and other aspects of our model .,"For rows containing entry "" data augmentation "" , "" N "" means the data is enhanced to N times as large as the original size , while the ratio in the bracket indicates the sampling ratio among the original , English - French - English and English - German - English data during training .",result
sentiment_analysis,19,"The relative performance of SuBiL - STM and SuBiLSTM - Tied are fairly close to each other , as shown by the relative gains in .",model,Comparison of SuBiLSTM and SuBiLSTM-Tied,1,125,5,3,0,model : Comparison of SuBiLSTM and SuBiLSTM-Tied,0.8169934640522876,0.625,0.5,The relative performance of SuBiL STM and SuBiLSTM Tied are fairly close to each other as shown by the relative gains in ,23,The results shown above clearly show the efficacy of using SuBiLSTMs in existing models geared towards four different sentence modeling tasks .,"SuBiLSTM - Tied works better on small datasets ( SST and TREC ) , probably owing to the regularizing effect of using the same LSTM to encode both suffixes and prefixes .",method
natural_language_inference,69,"For 45 % , the true answer either uniquely follows from multiple texts directly or is suggested as likely .",analysis,Qualitative Analysis,0,157,4,4,0,analysis : Qualitative Analysis,0.455072463768116,0.07407407407407407,0.4,For 45 the true answer either uniquely follows from multiple texts directly or is suggested as likely ,18,WIKIHOP lists characteristics along with the proportion of samples that exhibit them .,"For 26 % , more than one candidate is plausibly supported by the documents , including the correct answer .",result
text-classification,7,"It could be as simple as a keyword / phrase matching problem , but it could also be a nontrivial problem if compositions , hierarchies , and structures of texts are considered .",introduction,introduction,0,13,3,3,0,introduction : introduction,0.05349794238683128,0.10714285714285714,0.10714285714285714,It could be as simple as a keyword phrase matching problem but it could also be a nontrivial problem if compositions hierarchies and structures of texts are considered ,29,Modeling articles or sentences computationally is a fundamental topic in natural language processing .,"For example , a news article which mentions a single phrase "" US election "" maybe categorized into the political news with high probability .",introduction
natural_language_inference,39,"We use a mask matrix , mask , to hold the weights of each .",system description,Similarity Focus Layer,0,108,45,24,0,system description : Similarity Focus Layer,0.5242718446601942,0.6716417910447762,0.8571428571428571,We use a mask matrix mask to hold the weights of each ,13,"We increase weights of important word interactions to 1 ( in Line 11 based on cosine and Line 21 based on L2 ) , while unimportant word interactions receive weights of 0.1 ( in Line 2 ) .","The final output of the similarity focus layer is a focus - weighted similarity cube f ocusCube , which is the element - wise multiplication ( Line 25 ) of the matrix mask and the input simCube .",method
topic_models,0,"They were then used for training linear classifiers for topic ID from spoken and textual documents , .",introduction,introduction,0,43,21,21,0,introduction : introduction,0.10436893203883496,0.6363636363636364,0.6363636363636364,They were then used for training linear classifiers for topic ID from spoken and textual documents ,17,"Earlier , ( non-Bayesian ) SMM was used for learning document embeddings in an unsupervised fashion .","However , one of the limitations was that the learned document embeddings ( also termed as document i-vectors ) were only point - estimates and were prone to over-fitting , especially for shorter documents .",introduction
text_generation,3,"Due to the lack of utterance - level semantic dependency , the conventional attention - based methods that simply capture the word - level dependency achieve less satisfying performance in generating high - quality responses .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.16312056737588654,0.5909090909090909,0.5909090909090909,Due to the lack of utterance level semantic dependency the conventional attention based methods that simply capture the word level dependency achieve less satisfying performance in generating high quality responses ,31,"In fact , this task requires the model to understand the utterance - level dependency , a relation between the whole meanings of inputs and outputs .","To address this problem , we propose a novel Auto - Encoder Matching model to learn utterance - level dependency .",introduction
semantic_parsing,0,Data base Collection and Creation,system description,Data base Collection and Creation,0,83,40,1,0,system description : Data base Collection and Creation,0.2985611510791367,0.3809523809523809,0.05555555555555555,Data base Collection and Creation,5, , ,method
natural_language_inference,29,"Up to now , we can use loss ? to compute gradients for all the parameters in answer generator and use gradient descent method to update these parameters .",system description,Facts decoder,0,185,90,24,0,system description : Facts decoder,0.5068493150684932,0.5625,0.8275862068965517,Up to now we can use loss to compute gradients for all the parameters in answer generator and use gradient descent method to update these parameters ,27,"The procedure of pointer network is the same as the model proposed by See et al. , and we omit this procedure in our paper due to the limited space .","Up to now , we can use loss ? to compute gradients for all the parameters in answer generator and use gradient descent method to update these parameters .",method
natural_language_inference,28,Numerous works ; ) use associative memory to span a large memory space .,introduction,introduction,0,24,9,9,0,introduction : introduction,0.08856088560885607,0.375,0.375,Numerous works use associative memory to span a large memory space ,12,"Nevertheless , researchers are still interested in improving basic RNN cell models to process sequential data better .","For example , a practical way to implement associative memory is to set weight matrices as trainable structures that change according to input instances for training .",introduction
natural_language_inference,59,Neural Paraphrase Identification of Questions with Noisy Pretraining,title,title,1,2,1,1,0,title : title,0.014814814814814815,1.0,1.0,Neural Paraphrase Identification of Questions with Noisy Pretraining,8, , ,title
sentiment_analysis,19,"The 2 - layer BiLSTM based model performs comparably to the single layer BiLSTM , despite using a much larger number of parameters .",system description,General Sentence Representation,0,82,30,8,0,system description : General Sentence Representation,0.5359477124183006,0.4918032786885246,0.5714285714285714,The 2 layer BiLSTM based model performs comparably to the single layer BiLSTM despite using a much larger number of parameters ,22,The better performance on STSB is noteworthy as the sentence representations derived from a SuBiLSTM can take advantage of the long range dependencies it encodes .,In we plot the absolute gains made by SuBiLSTM and SuBiLSTM - Tied over BiLSTM for all the 10 tasks .,method
natural_language_inference,65,"In Section 4 , we discuss some related work .",introduction,introduction,0,39,31,31,0,introduction : introduction,0.16956521739130434,0.9393939393939394,0.9393939393939394,In Section 4 we discuss some related work ,9,"In Section 3 , we detail the attentive pooling approach .","Sections 5 and 6 detail our experimental setup and results , respectively .",introduction
natural_language_inference,27,We employ two types of standard regularizations .,dataset,DATASET AND EXPERIMENTAL SETTINGS,1,201,19,19,0,dataset : DATASET AND EXPERIMENTAL SETTINGS,0.5946745562130178,0.6551724137931034,0.6551724137931034,We employ two types of standard regularizations ,8,"We generate two additional augmented datasets obtained from Section 3 , which contain 140K and 240K examples and are denoted as "" data augmentation 2 "" and "" data augmentation 3 "" respectively , including the original data .","First , we use L2 weight decay on all the trainable variables , with parameter ? = 3 10 ?7 .",experiment
natural_language_inference,31,The architecture on the right is the same as the one on the left so it 's omitted for conciseness .,approach,Our Approach,0,44,8,8,0,approach : Our Approach,0.15942028985507245,0.125,0.3636363636363637,The architecture on the right is the same as the one on the left so it s omitted for conciseness ,21,"There are three parts in the input of alignment and fusion layers : original pointwise features ( Embedding vectors , denoted by blank rectangles ) , previous aligned features ( Residual vectors , denoted by rectangles with diagonal stripes ) , and contextual features ( Encoded vectors , denoted by solid rectangles ) .",) connected by augmented residual connections .,method
sentiment_analysis,11,"Here , K serves as the length of the context window for history of u i .",system description,Task Definition,0,89,13,13,0,system description : Task Definition,0.25872093023255816,0.6842105263157895,0.6842105263157895,Here K serves as the length of the context window for history of u i ,16,"To get its history , preceding K utterances of each person are separately collected as hist a and hist b .","Thus , for ? ? {a , b}:",method
natural_language_inference,50,"Take the first sample in for example , it begins at atop level with ' burger king ' and then drills down progressively to ' what is gross sales ? ' .",analysis,DISCUSSION AND ANALYSIS,0,296,73,29,0,analysis : DISCUSSION AND ANALYSIS,0.9337539432176656,0.9240506329113924,0.8787878787878788,Take the first sample in for example it begins at atop level with burger king and then drills down progressively to what is gross sales ,26,"First , we find that questions often start with the over all context and drill down into more specific query words .","Similarly in the second example , it begins with ' florence nightingale ' and drills down to ' famous ' at H3 in which a match is being found with ' nursing ' in the same hierarchical level .",result
sentiment_analysis,9,"Suppose that "" The price is reasonable although the service is poor . "" is the input for APC task , consistently with ATE task , = { 1 , 2 ? } stands for all the token of the review , and =",system description,Aspect Polarity Classification,0,63,8,3,0,system description : Aspect Polarity Classification,0.22743682310469315,1.0,1.0,Suppose that The price is reasonable although the service is poor is the input for APC task consistently with ATE task 1 2 stands for all the token of the review and ,33,"Aspect polarity classification is a multi -grained sub - task of sentiment analysis , aiming at predicting the aspect polarity for targeted aspects .", ,method
relation-classification,2,We measure the performance by computing the F 1 score on the test set .,dataset,Datasets and evaluation metrics,0,185,14,14,0,dataset : Datasets and evaluation metrics,0.6271186440677966,0.4516129032258064,0.4516129032258064,We measure the performance by computing the F 1 score on the test set ,15,"The dataset consists of 910 training instances , 243 for validation and 288 for testing .",We adopt two evaluation settings to compare to previous work .,experiment
natural_language_inference,29,1 ) Reviews are informal and noisy ; ( 2 ) joint modeling of reviews and key - value product attributes is challenging ; ( 3 ) traditional methods easily generate meaningless answers .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.019178082191780826,0.5,0.5,1 Reviews are informal and noisy 2 joint modeling of reviews and key value product attributes is challenging 3 traditional methods easily generate meaningless answers ,26,"Unlike existing question - answering problems , answer generation in e-commerce confronts three main challenges :","To tackle above challenges , we propose an adversarial learning based model , named PAAG , which is composed of three components : a questionaware review representation module , a key - value memory network encoding attributes , and a recurrent neural network as a sequence generator .",abstract
question-answering,7,NSE is capabable of performing multiscale composition by retrieving associative slots for a particular input at a time step .,analysis,Memory Access and Compositionality,0,246,47,2,0,analysis : Memory Access and Compositionality,0.8945454545454545,0.734375,0.10526315789473684,NSE is capabable of performing multiscale composition by retrieving associative slots for a particular input at a time step ,20, ,We analyzed the memory access order and the compositionality of memory slot and the input word in the NSE model trained on the SNLI data .,result
natural_language_inference,24,BiDAF + Self Attention + ELMo - -/-75.789/83.261 DCN +  74.5/83.1 75.087/83.081 r- net 72.3/80.6 72.3/80.7 ReasoNet ++ 70,result,SAN:,0,162,30,20,0,result : SAN:,0.6923076923076923,0.375,0.2857142857142857,BiDAF Self Attention ELMo 75 789 83 261 DCN 74 5 83 1 75 087 83 081 r net 72 3 80 6 72 3 80 7 ReasoNet 70,29,Individual model results :,We are interested in whether the proposed model is sensitive to different random initial conditions .,result
natural_language_inference,40,"one - hot "" refers to a model where coreference ids are appended to the input word embeddings .",model,Model,0,215,16,16,0,model : Model,0.7761732851985559,0.25806451612903225,0.26229508196721313,one hot refers to a model where coreference ids are appended to the input word embeddings ,17,"MAGE "" refers to our proposed model , where the number within parentheses denotes the number of hidden dimensions for coreference edges .",Results marked with are cf .,method
natural_language_inference,53,Our transfer learning approach obtains better results than previous state - of - the - art on the SICK task - can be seen as an out - domain version of SNLI - for both entailment and relatedness .,model,Task transfer,1,184,43,23,0,model : Task transfer,0.8846153846153846,0.7049180327868853,0.5609756097560976,Our transfer learning approach obtains better results than previous state of the art on the SICK task can be seen as an out domain version of SNLI for both entailment and relatedness ,33,Domain adaptation on SICK tasks,"We obtain a pearson score of 0.885 on SICK - R while obtained 0.868 , and we obtain 86.3 % test accuracy on SICK - E while previous best handengineered models obtained 84.5 % .",method
natural_language_inference,33,"We demonstrate substantial gains on TREC ( 6 % over Infersent and roughly 2 % over the CNN - LSTM ) , outperforming even a competitive supervised baseline .",evaluation,EXPERIMENTAL RESULTS & DISCUSSION,1,139,15,5,0,evaluation : EXPERIMENTAL RESULTS & DISCUSSION,0.5325670498084292,0.10948905109489053,0.16129032258064516,We demonstrate substantial gains on TREC 6 over Infersent and roughly 2 over the CNN LSTM outperforming even a competitive supervised baseline ,23,"We observe gains of 1.1 - 2.0 % on the sentiment classification tasks ( MR , CR , SUBJ & MPQA ) over Infersent .","We see similar gains ( 2.3 % ) on paraphrase identification ( MPRC ) , closing the gap on supervised approaches trained from scratch .",result
natural_language_inference,64,"Transformer - based models are neural networks designed to capture dependencies between words , i.e. , their interdependent contexts .",system description,Transformers for AS2,0,80,9,2,0,system description : Transformers for AS2,0.3187250996015936,0.13846153846153847,0.07142857142857142,Transformer based models are neural networks designed to capture dependencies between words i e their interdependent contexts ,18, ,shows the standard architecture for a text pair classification task .,method
natural_language_inference,74,"In , we also provide a visualized analysis of the hidden representation from similarity matrix A ( computed in the equation ) in the situations that whether we use the discourse markers or not .",analysis,Visualization,0,194,9,2,0,analysis : Visualization,0.8660714285714286,0.75,0.4,In we also provide a visualized analysis of the hidden representation from similarity matrix A computed in the equation in the situations that whether we use the discourse markers or not ,32, ,"We pick a sentence pair whose premise is "" 3 young man in hoods standing in the middle of a quiet street facing the camera . "" and hypothesis is "" Three people sit by a busy street bareheaded . """,result
text_summarization,9,However the improvements are smaller than for Gi-gaword which is likely due to two reasons :,result,Results,0,120,15,15,0,result : Results,0.7843137254901961,0.3488372093023256,0.3488372093023256,However the improvements are smaller than for Gi gaword which is likely due to two reasons ,17,The results ( Table 3 ) show that our models are better than ABS + .,"First , tokenization of DUC - 2004 differs slightly from our training corpus .",result
sentiment_analysis,46,Movie Review ( MR ) 2 and Stanford Sentiment Treebank ( SST ) 3 are used to evaluate our model .,dataset,Datasets and Sentiment Resources,0,81,2,2,0,dataset : Datasets and Sentiment Resources,0.6639344262295082,0.25,0.25,Movie Review MR 2 and Stanford Sentiment Treebank SST 3 are used to evaluate our model ,17, ,"MR dataset has 5,331 positive samples and 5,331 negative samples .",experiment
sentiment_analysis,5,Then there are totally about 3400 samples in one session ; ( 2 ) SEED - IV 2 .,experiment,experiment,0,148,13,13,0,experiment : experiment,0.5584905660377358,0.14942528735632185,0.43333333333333335,Then there are totally about 3400 samples in one session 2 SEED IV 2 ,15,"Consequently , there are totally 15 trails , and each trail has 185 - 238 samples for one session of each subject .","SEED - IV dataset also contains 15 subjects , and each subject has three sessions .",experiment
sentiment_analysis,47,"In recently years , more and more researchers have adopted more advanced deep learning algorithms .",introduction,introduction,0,20,6,6,0,introduction : introduction,0.09302325581395347,0.15384615384615385,0.15384615384615385,In recently years more and more researchers have adopted more advanced deep learning algorithms ,15,"However , such kind of feature engineering work was labor - intensive and almost reached its performance bottleneck .","By taking advantage of the powerful representation ability , well - designed neural networks can automatically generate meaningful low - dimensional representations for the targets and their contexts , and obtained the state - of the - art results in aspect - based sentiment classification task .",introduction
machine-translation,8,"We train each model twice : first with the sentences of length up to 30 words ( RNNencdec - 30 , RNNsearch - 30 ) and then with the sentences of length up to 50 word ( RNNencdec - 50 , RNNsearch - 50 ) .",model,MODELS,1,120,4,4,0,model : MODELS,0.3625377643504532,0.2857142857142857,0.2857142857142857,We train each model twice first with the sentences of length up to 30 words RNNencdec 30 RNNsearch 30 and then with the sentences of length up to 50 word RNNencdec 50 RNNsearch 50 ,35,"The first one is an RNN Encoder - Decoder ( RNNencdec , , and the other is the proposed model , to which we refer as RNNsearch .",The encoder and decoder of the RNNencdec have 1000 hidden units each .,method
machine-translation,9,"For our proposed methods , the maximum loss - free compression rate is achieved by a 16 32 coding scheme .",analysis,SENTIMENT ANALYSIS,1,206,29,29,0,analysis : SENTIMENT ANALYSIS,0.7177700348432056,0.3295454545454545,0.90625,For our proposed methods the maximum loss free compression rate is achieved by a 16 32 coding scheme ,19,"To make the results comparable , we report the codebook size in numpy format .","In this case , the total size of the embedding layer is 1.23 MB , which is equivalent to a compression rate of 98.4 % .",result
sentence_classification,0,"We consider three intent categories outlined in : BACK - GROUND , METHOD and RESULTCOMPARISON .",dataset,SciCite dataset,1,125,10,10,0,dataset : SciCite dataset,0.4681647940074906,0.9090909090909092,0.9090909090909092,We consider three intent categories outlined in BACK GROUND METHOD and RESULTCOMPARISON ,13,"Therefore , our dataset provides a concise annotation scheme that is useful for navigating research topics and machine reading of scientific papers .",Below we describe data collection and annotation details .,experiment
machine-translation,2,"An attention function can be described as mapping a query and a set of key - value pairs to an output , where the query , keys , values , and output are all vectors .",model,Attention,1,63,22,2,0,model : Attention,0.28125,0.2018348623853211,0.6666666666666666,An attention function can be described as mapping a query and a set of key value pairs to an output where the query keys values and output are all vectors ,31, ,"The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .",method
natural_language_inference,75,It was built with the following goals in mind : ( i ) machine learning techniques should have ample training examples for learning ; and ( ii ) one can analyze easily the performance of different representations of knowledge and breakdown the results by question type .,model,The WikiMovies Benchmark,0,122,73,3,0,model : The WikiMovies Benchmark,0.6009852216748769,0.47402597402597396,0.75,It was built with the following goals in mind i machine learning techniques should have ample training examples for learning and ii one can analyze easily the performance of different representations of knowledge and breakdown the results by question type ,41,The WIKIMOVIES benchmark consists of questionanswer pairs in the domain of movies .,The dataset can be downloaded from http://fb.ai/babi.,method
natural_language_inference,73,"Hard attention is well suited to these tasks , because it overcomes the weaknesses associated with soft attention in long sequences .",introduction,introduction,0,26,14,14,0,introduction : introduction,0.09923664122137403,0.4117647058823529,0.4117647058823529,Hard attention is well suited to these tasks because it overcomes the weaknesses associated with soft attention in long sequences ,21,"In fact , various NLP tasks solely rely on very sparse tokens from along text input .","However , hard attention mechanism is time - inefficient with sequential sampling and non-differentiable by virtue of their combinatorial nature .",introduction
sentiment_analysis,27,"GCN encodes relevant information about its neighborhood as a new representation vector , where each node in the graph indicates a representation of aspect .",methodology,Sentiment graph based GCN,0,142,65,2,0,methodology : Sentiment graph based GCN,0.5144927536231884,0.7647058823529411,0.14285714285714285,GCN encodes relevant information about its neighborhood as a new representation vector where each node in the graph indicates a representation of aspect ,24, ,"In addition , as Kipf et al . do , we assume all nodes contain self - loops .",method
text_summarization,1,Sample questions produced by our method from given passage - answer pair ( answer is underlined ) .,introduction,introduction,0,26,16,16,0,introduction : introduction,0.10833333333333334,0.4571428571428571,0.4571428571428571,Sample questions produced by our method from given passage answer pair answer is underlined ,15,Ours ) ? what did tesla do to hide he dropped out of school ? :,"Our method generates diverse questions , by selecting different tokens to focus ( colored ) in contrast to 3 mixture decoder that generates 3 identical questions : "" what did tesla do to hide the fact that he dropped out of school ? "" .",introduction
semantic_parsing,0,"To study this , in we compare model performances under the two settings .",performance,performance,0,261,15,15,0,performance : performance,0.9388489208633094,0.5357142857142857,0.5357142857142857,To study this in we compare model performances under the two settings ,13,"As discussed in Section 5 , another challenge of the dataset is to generalize to new data bases .","For all models , the performance under data base split is much lower than that under example split .",result
semantic_role_labeling,4,"As the loss function , we use the cross -entropy ( Eq. 3 ) .",architecture,architecture,0,146,58,58,0,architecture : architecture,0.4866666666666667,1.0,1.0,As the loss function we use the cross entropy Eq 3 ,12,"During training , we update only the parameters of the ensemble model , i.e. , That is , we fix the parameters of each trained model m .", ,method
text_summarization,9,"Our evaluation is based on three variants of ROUGE , namely , ROUGE - 1 ( unigrams ) , ROUGE - 2 ( bigrams ) , and ROUGE - L ( longest - common substring ) .",dataset,Datasets and Evaluation,0,94,7,7,0,dataset : Datasets and Evaluation,0.6143790849673203,1.0,1.0,Our evaluation is based on three variants of ROUGE namely ROUGE 1 unigrams ROUGE 2 bigrams and ROUGE L longest common substring ,23,We also evaluate our models on the DUC - 2004 evaluation data set comprising 500 pairs ., ,experiment
natural_language_inference,82,"Further , we note that initializing our model with pre-trained word vectors 10 is helpful , though world knowledge of entities has been prevented by the anonymization process .",model,Models,1,100,11,11,0,model : Models,0.8771929824561403,0.7857142857142857,0.7857142857142857,Further we note that initializing our model with pre trained word vectors 10 is helpful though world knowledge of entities has been prevented by the anonymization process ,28,Combining these two techniques helps more .,This suggests that pre-trained word vectors may still bring extra linguistic knowledge encoded in ordinary words .,method
named-entity-recognition,5,"Recurrent steps are used to perform local and global information exchange between words simultaneously , rather than incremental reading of a sequence of words .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.03827751196172249,0.8571428571428571,0.8571428571428571,Recurrent steps are used to perform local and global information exchange between words simultaneously rather than incremental reading of a sequence of words ,24,"We investigate an alternative LSTM structure for encoding text , which consists of a parallel state for each word .","Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power , giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers .",abstract
natural_language_inference,6,"This work was performed during an internship at Facebook AI Research . , but has recently been superseded by sentence - level representations .",introduction,introduction,0,19,5,5,0,introduction : introduction,0.07661290322580645,0.38461538461538464,0.38461538461538464,This work was performed during an internship at Facebook AI Research but has recently been superseded by sentence level representations ,21,This approach was first popularized byword embeddings,"Nevertheless , all these works learn a separate model for each language and are thus unable to leverage information across different languages , greatly limiting their potential performance for low - resource languages .",introduction
natural_language_inference,36,"As a challenging task in NLP , text comprehension is one of the key problems in artificial intelligence , which aims to read and comprehend a given text , and then answer questions or make inference based on it .",system description,Text Comprehension,0,38,2,2,0,system description : Text Comprehension,0.18095238095238086,0.03125,0.05555555555555555,As a challenging task in NLP text comprehension is one of the key problems in artificial intelligence which aims to read and comprehend a given text and then answer questions or make inference based on it ,37, ,These tasks require a comprehensive understanding of natural languages and the ability to do further inference and reasoning .,method
sentiment_analysis,1,"In recent years , due to the rapid development of noninvasive , easy - to - use and inexpensive EEG recording devices , EEG - based emotion recognition has received an increasing amount of attention in both research and applications .",introduction,introduction,0,17,4,4,0,introduction : introduction,0.04292929292929293,0.07017543859649122,0.07017543859649122,In recent years due to the rapid development of noninvasive easy to use and inexpensive EEG recording devices EEG based emotion recognition has received an increasing amount of attention in both research and applications ,35,"Compared to other modalities , physiological signals , such as electroencephalogram ( EEG ) , electrocardiogram ( ECG ) , electromyogram ( EMG ) , galvanic skin response ( GSR ) , etc. , have the advantage of being difficult to hide or disguise .",Emotion models can be broadly categorized into discrete models and dimensional models .,introduction
text_summarization,5,"Recently , the application of the attentional sequence - to - sequence ( seq2seq ) framework has attracted growing attention and achieved state - of - the - art performance on this task .",introduction,introduction,0,14,5,5,0,introduction : introduction,0.05555555555555555,0.1388888888888889,0.1388888888888889,Recently the application of the attentional sequence to sequence seq2seq framework has attracted growing attention and achieved state of the art performance on this task ,26,It can be used to design or refine appealing headlines .,Most previous seq2seq models purely depend on the source text to generate summaries .,introduction
sentiment_analysis,10,"We believe this to be due to the incorporation of party state and emotion GRU , which are missing from CMN .",result,Comparison with the State of the Art,0,184,17,14,0,result : Comparison with the State of the Art,0.7159533073929961,0.265625,1.0,We believe this to be due to the incorporation of party state and emotion GRU which are missing from CMN ,21,It yields significantly lower mean absolute error ( M AE ) and higher Pearson correlation coefficient ( r ) for all four attributes ., ,result
sentence_classification,2,"However , this phrase is not necessary to classify the sentence correctly , and may induce possible vagueness because of the word trouble .",model,NN (altered),0,219,28,8,0,model : NN (altered),0.8690476190476191,0.6666666666666666,0.4,However this phrase is not necessary to classify the sentence correctly and may induce possible vagueness because of the word trouble ,22,"In the second example , the Korean sentence correctly translates all parts of the English sentence , except for the phrase as it does in trouble .","Thus , the Korean attention weight is larger .",method
natural_language_inference,69,"Some drugs interact with more drugs than others - Aspirin for example interacts with 743 other drugs , but Isotretinoin with only 34 .",system description,Assembly,0,105,32,23,0,system description : Assembly,0.3043478260869565,0.7272727272727273,0.6571428571428571,Some drugs interact with more drugs than others Aspirin for example interacts with 743 other drugs but Isotretinoin with only 34 ,22,Mitigating Candidate Frequency Imbalance,This leads to similar candidate frequency imbalance issues as with WIKIHOP - but due to its smaller size MEDHOP is difficult to sub-sample .,method
natural_language_inference,47,"However , machine learning research in this are a has been dramatically limited by the lack of large - scale resources .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02325581395348837,0.5,0.5,However machine learning research in this are a has been dramatically limited by the lack of large scale resources ,20,"Understanding entailment and contradiction is fundamental to understanding natural language , and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations .","To address this , we introduce the Stanford Natural Language Inference corpus , a new , freely available collection of labeled sentence pairs , written by humans doing a novel grounded task based on image captioning .",abstract
natural_language_inference,46,The doctor steps away .,Figure 1:,Review of Reading Comprehension Datasets and Models,0,288,15,10,0,Figure 1: : Review of Reading Comprehension Datasets and Models,0.9696969696969696,0.625,0.5263157894736842,The doctor steps away ,5,Cut to Jacob Singer . . .,nurse rudely pulls a green sheet up over his head .,others
natural_language_inference,61,"Recently , end - to - end neural network - based models have drawn worldwide a ention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation , natural language inference , etc .",introduction,introduction,0,24,17,17,0,introduction : introduction,0.15384615384615385,0.3333333333333333,0.3333333333333333,Recently end to end neural network based models have drawn worldwide a ention since they have demonstrated excellent performance on quite a few NLP tasks including machine translation natural language inference etc ,33,"Feature - based models represent a premise and a hypothesis by their unlexicalized and lexicalized features , such as n-gram length and the real - valued feature of length di erence , then train a classi er to perform relation classi cation .","On the basis of their model structures , we can divide neural network - based models for NLI into two classes , sentence encoding models and sentence interaction - aggregation models .",introduction
machine-translation,1,The encoder processes the source string into a representation and is formed of one - dimensional convolutional layers that use dilation but are not masked .,model,Desiderata,0,76,26,15,0,model : Desiderata,0.3781094527363184,0.4262295081967213,0.3,The encoder processes the source string into a representation and is formed of one dimensional convolutional layers that use dilation but are not masked ,25,The decoder is a language model that is formed of one - dimensional convolutional layers that are masked ( Sect. 3.4 ) and use dilation ( Sect. 3.5 ) .,depicts the two networks and their combination .,method
natural_language_inference,79,"We use D to make a prediction about some particular labels using a standard classifier , e.g. , logistic regression .",model,model,0,114,34,34,0,model : model,0.4253731343283582,0.6181818181818182,1.0,We use D to make a prediction about some particular labels using a standard classifier e g logistic regression ,20,"Don already seen paragraphs ; and 2 ) "" the inference stage "" to get paragraph vectors D for new paragraphs ( never seen before ) by adding more columns in D and gradient descending on D while holding W , U , b fixed .", ,method
natural_language_inference,7,The best model uses 50d LSTM states ; two - layer BiLSTMs for the span encoder and the passage - independent question representation ; dropout of 0.1 throughout ; and a learning rate decay of 5 % every 10 k steps .,experiment,EXPERIMENTAL SETUP,1,112,8,8,0,experiment : EXPERIMENTAL SETUP,0.6292134831460674,0.8888888888888888,0.8888888888888888,The best model uses 50d LSTM states two layer BiLSTMs for the span encoder and the passage independent question representation dropout of 0 1 throughout and a learning rate decay of 5 every 10 k steps ,37,"To choose the final model configuration , we ran grid searches over : the dimensionality of the LSTM hidden states ; the width and depth of the feed forward neural networks ; dropout for the LSTMs ; the number of stacked LSTM layers ; and the decay multiplier [ 0.9 , 0.95 , 1.0 ] with which we multiply the learning rate every 10 k steps .",All models are implemented using TensorFlow 3 and trained on the SQUAD training set using the ADAM optimizer with a mini-batch size of 4 and trained using 10 asynchronous training threads on a single machine .,experiment
text_summarization,12,"We use the method in to visualize the contribution of the selective gate to the final output , which can be approximated by the first derivative .",Effectiveness of Selective Encoding,Effectiveness of Selective Encoding,0,216,10,10,0,Effectiveness of Selective Encoding : Effectiveness of Selective Encoding,0.9473684210526316,0.5882352941176471,0.5882352941176471,We use the method in to visualize the contribution of the selective gate to the final output which can be approximated by the first derivative ,26,"Since the output of the selective gate network is a high dimensional vector , it is hard to visualize all the gate values .","Given sentence words x with associated output summary y , the trained model associates the pair ( x , y) with a score S y ( x ) .",others
natural_language_inference,56,Alice is 20 years old .,experiment,Age arithmetic,0,182,103,4,0,experiment : Age arithmetic,0.5416666666666666,0.9626168224299064,0.5,Alice is 20 years old ,6,"The task is to infer the age of a person given a single absolute age and a set of age differences , e.g .",Alice is 4 years older than Bob.,experiment
natural_language_inference,56,This allows the network to learn what kind of messages to send .,system description,Recurrent Relational Networks,0,58,17,17,0,system description : Recurrent Relational Networks,0.17261904761904762,0.4473684210526316,0.4473684210526316,This allows the network to learn what kind of messages to send ,13,"We define the message mt ij from node i to node j at step t by where f , the message function , is a multi - layer perceptron .","In our experiments , MLPs with linear outputs were used .",method
sentiment_analysis,17,"We introduce the Tree - LSTM , a generalization of LSTMs to tree - structured network topologies .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.031111111111111117,0.8333333333333334,0.8333333333333334,We introduce the Tree LSTM a generalization of LSTMs to tree structured network topologies ,15,"However , natural language exhibits syntactic properties that would naturally combine words to phrases .","Tree - LSTMs outperform all existing systems and strong LSTM baselines on two tasks : predicting the semantic relatedness of two sentences ( Sem Eval 2014 , Task 1 ) and sentiment classification ( Stanford Sentiment Treebank ) .",abstract
relation-classification,1,The bias parameter ? corresponding to the results in is 10 .,hyperparameters,hyperparameters,1,158,7,7,0,hyperparameters : hyperparameters,0.6422764227642277,0.4375,0.4375,The bias parameter corresponding to the results in is 10 ,11,The number of lstm units in encoding layer is 300 and the number in decoding layer is 600 .,The bias parameter ? corresponding to the results in is 10 .,experiment
text-to-speech_synthesis,0,"On the one hand , we train deeper models to achieve higher accuracy .",system description,Ensemble Distillation with Diverse Models,0,78,43,8,0,system description : Ensemble Distillation with Diverse Models,0.4814814814814815,0.7543859649122807,0.8888888888888888,On the one hand we train deeper models to achieve higher accuracy ,13,The performance of the individual models and the diversity between them are essential for ensemble .,"On the other hand , we choose Transformer , Bi - LSTM , and convolutional sequence to sequence models to increase the diversity of ensemble models .",method
text-to-speech_synthesis,2,"However , synthetic utterances are still easily distinguishable from the real human speech as demonstrated by the t - SNE visualization ( right ) where utterances from each synthetic speaker form a distinct cluster adjacent to a cluster of real utterances from the corresponding speaker .",experiment,experiment,1,170,79,79,0,experiment : experiment,0.6938775510204082,0.7596153846153846,0.7596153846153846,However synthetic utterances are still easily distinguishable from the real human speech as demonstrated by the t SNE visualization right where utterances from each synthetic speaker form a distinct cluster adjacent to a cluster of real utterances from the corresponding speaker ,42,The PCA visualization ( left ) shows that synthesized utterances tend to lie very close to real speech from the same speaker in the embedding space .,"Speakers appear to be well separated by gender in both the PCA and t- SNE visualizations , with all female speakers appearing on the left , and all male speakers appearing on the right .",experiment
smile_recognition,0,Experiments have shown that the training time mostly depends on the number of convolutions .,experiment,experiment,0,83,23,23,0,experiment : experiment,0.9222222222222224,0.92,0.92,Experiments have shown that the training time mostly depends on the number of convolutions ,15,"Training time per epoch are 82 seconds and 41 seconds for the mouth and face input models , respectively .",Using the Tesla K40c GPU has allowed to speedup the training time by factor ten over the use of a CPU to execute the CPU code generated by the library .,experiment
natural_language_inference,81,The fine - grain - only model under-performs the coarse - grain - only model consistently across almost all length measures .,system description,RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA,1,149,16,16,0,system description : RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA,0.3539192399049881,0.8421052631578947,0.8421052631578947,The fine grain only model under performs the coarse grain only model consistently across almost all length measures ,19,shows the distribution of model prediction errors across various lengths of the dataset for the coarse - grain - only model ( - fine ) and the fine - grain - only model ( - coarse ) .,This is likely due to the difficulty of coreference resolution of candidates in the support documents - the technique we use of exact lexical matching tends to produce high precision and low recall .,method
relation_extraction,5,"Comparing the C - GCN model with the GCN model , we find that the gain mainly comes from improved recall .",experiment,Results on the TACRED Dataset,1,154,19,6,0,experiment : Results on the TACRED Dataset,0.5855513307984791,0.4418604651162791,0.42857142857142855,Comparing the C GCN model with the GCN model we find that the gain mainly comes from improved recall ,20,"In addition , we find our model improves upon other dependencybased models in both precision and recall .",We hypothesize that this is because the C - GCN is more robust to parse errors by capturing local word patterns ( see also Section 6.2 ) .,experiment
sentiment_analysis,16,"That is , their performance degrades when the sentiment of a context word is sensitive to the given target .",introduction,introduction,0,24,12,12,0,introduction : introduction,0.07547169811320754,0.3,0.3,That is their performance degrades when the sentiment of a context word is sensitive to the given target ,19,"However , we found that using existing MNs to deal with ASC has an important problem and simply relying on attention modeling can not solve it .",Let us consider the following sentences :,introduction
text_generation,1,"The discriminator aims to distinguish the synthetic from the real data , while the generator is trained to confuse the discriminator by generating high quality synthetic data .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.062043795620437964,0.25925925925925924,0.25925925925925924,The discriminator aims to distinguish the synthetic from the real data while the generator is trained to confuse the discriminator by generating high quality synthetic data ,27,"The main idea behind GANs is to have two neural network models , the discriminator and the generator , competing against each other during learning .","During learning , the gradient of the training loss from the discriminator is then used as the guidance for updating the parameters of the generator .",introduction
natural_language_inference,25,"The question is encoded via a BiLSTM {v 1 , . . . , v m } = BiLSTM ( q 1 , . . . , q m ) and the resulting hidden states are summarized via attention :",model,Base model,0,50,5,5,0,model : Base model,0.4807692307692308,0.3333333333333333,0.5,The question is encoded via a BiLSTM v 1 v m BiLSTM q 1 q m and the resulting hidden states are summarized via attention ,26,Passage - independent question representation,for a parameter vector w q ? Rd f and FF ( ) a single layer feed - forward network .,method
semantic_parsing,2,"As shown in , sketches are more compact and as a result easier to generate compared to decoding the entire meaning structure in one go .",introduction,introduction,0,21,13,13,0,introduction : introduction,0.07216494845360824,0.4482758620689655,0.4482758620689655,As shown in sketches are more compact and as a result easier to generate compared to decoding the entire meaning structure in one go ,25,"Firstly , the decomposition disentangles high - level from low - level semantic information , which enables the decoders to model meaning at different levels of granularity .","Secondly , the model can explicitly share knowledge of coarse structures for the examples that have the same sketch ( i.e. , basic meaning ) , even though their actual meaning representations are different ( e.g. , due to different details ) .",introduction
part-of-speech_tagging,0,"Following the recent top - performing models for sequence labeling tasks , we employ a Bi-directional LSTM - CRF model as our baseline ( see for an illustration ) .",model,model,0,101,2,2,0,model : model,0.4105691056910569,0.1,0.1,Following the recent top performing models for sequence labeling tasks we employ a Bi directional LSTM CRF model as our baseline see for an illustration ,26, ,Character - level BiLSTM .,method
paraphrase_generation,0,"Question sentence encoding feature f i is obtained after passing through an LSTM which is parameterized using the function F ( C i , W l ) where W l are the weights of the LSTM .",method,Encoder-LSTM,0,87,27,8,0,method : Encoder-LSTM,0.3670886075949367,0.4354838709677419,0.8888888888888888,Question sentence encoding feature f i is obtained after passing through an LSTM which is parameterized using the function F C i W l where W l are the weights of the LSTM ,34,"When the network reaches the last word ( L th word ) , the hidden state h L of the network provides a semantic representation of the whole sentence conditioned on all the previously generated words ( q 0 , q 1 ... , qt ) .",This is illustrated in part 1 of .,method
semantic_parsing,0,"For example , imagine a table with birth and death year columns .",system description,Task Definition,0,172,13,13,0,system description : Task Definition,0.6187050359712231,0.7647058823529411,0.7647058823529411,For example imagine a table with birth and death year columns ,12,"As mentioned in the previous sections , we exclude some queries that require outside knowledge such as commonsense inference and math calculation .","To answer the questions like "" How long is X 's life length ? "" , we use SELECT death year - birth year .",method
sentiment_analysis,6,The IEMOCAP contains the acts of 10 speakers in a twoway conversation segmented into utterances .,system description,Multimodal Emotion Recognition Datasets,0,218,3,3,0,system description : Multimodal Emotion Recognition Datasets,0.7543252595155708,0.3333333333333333,0.3333333333333333,The IEMOCAP contains the acts of 10 speakers in a twoway conversation segmented into utterances ,16,Multimodal Emotion Recognition Datasets,The medium of the conversations in all the videos is English .,method
part-of-speech_tagging,4,word is represented by concatenating it s word embedding and its character representation :,baseline,Word Representation Layer,0,70,9,7,0,baseline : Word Representation Layer,0.30042918454935624,0.3333333333333333,0.875,word is represented by concatenating it s word embedding and its character representation ,14,c i denotes the output of character - level encoding .,where e w denotes a word embedding lookup table .,result
natural_language_inference,51,"Reading comprehension typically involves an iterative process of multiple actions such as reading the story , reading the question , outputting the answers and other implicit reasoning steps .",model,Text Question Answering,0,208,5,2,0,model : Text Question Answering,0.7849056603773585,0.11904761904761905,0.05128205128205128,Reading comprehension typically involves an iterative process of multiple actions such as reading the story reading the question outputting the answers and other implicit reasoning steps ,27, ,We apply NUTM to the question answering domain by replacing the NTM core with DNC .,method
natural_language_inference,72,We investigate different ways of representing medical entities in the text and how this affects the neural readers .,introduction,introduction,0,44,36,36,0,introduction : introduction,0.14102564102564102,0.75,0.75,We investigate different ways of representing medical entities in the text and how this affects the neural readers ,19,"To enable a more flexible answer evaluation , we expand the answers with their respective synonyms from a medical knowledge base , and additionally supplement the standard evaluation metrics with BLEU and embedding - based methods .","We obtain the best results with a recurrent neural network ( RNN ) with gated attention , but a simple approach based on embedding similarity proves to be a strong baseline as well .",introduction
sentiment_analysis,45,Results on SemEval - 2014 are presented in and .,result,Results,0,119,2,2,0,result : Results,0.8263888888888888,0.2,0.4,Results on SemEval 2014 are presented in and ,9, ,"We find that BERT - single has achieved better results on these two subtasks , and BERT - pair has achieved further improvements over BERT - single .",result
natural_language_inference,25,"We denote these representations as {o 1 , . . . , o k } and set u t = [u t ; o t ] for {u 1 , . . . , u k } = BiLSTM ( x 1 , . . . , x k ) .",system description,LM-augmented token re-embedding (TR+LM),0,40,22,6,0,system description : LM-augmented token re-embedding (TR+LM),0.38461538461538464,0.8148148148148148,0.5454545454545454,We denote these representations as o 1 o k and set u t u t o t for u 1 u k BiLSTM x 1 x k ,28,"Therefore , we leverage a strong language model that was pre-trained on large corpora as a fixed encoder which supplies additional contextualized token representations .","The LM we use is from , 2 trained on the One Billion Words Benchmark dataset .",method
sentiment_analysis,41,"With the abundance of sentiment lexicons , the lexicon - based features were built for sentiment analysis .",system description,Sentiment Classification at Aspect-level,0,44,6,6,0,system description : Sentiment Classification at Aspect-level,0.1973094170403588,0.07792207792207792,0.6666666666666666,With the abundance of sentiment lexicons the lexicon based features were built for sentiment analysis ,16,Traditional approaches to solve those problems are to manually design a set of features .,"Most of these studies focus on building sentiment classifiers with features , which include bag - of - words and sentiment lexicons , using SVM .",method
natural_language_inference,23,"The context is described as a sequence of word tokens : C = {w C 1 , . . . , w Cm } , and the question as :",system description,TASK DESCRIPTION,0,58,8,3,0,system description : TASK DESCRIPTION,0.11306042884990253,0.3636363636363637,0.3333333333333333,The context is described as a sequence of word tokens C w C 1 w Cm and the question as ,21,"In machine comprehension , given a context and a question , the machine needs to read and understand the context , and then find the answer to the question .","where m is the number of words in the context , and n is the number of words in the question .",method
natural_language_inference,14,Three trials are conducted for all settings ; the mean and standard deviation of the model accuracy are reported in .,evaluation,evaluation,0,99,9,9,0,evaluation : evaluation,0.5789473684210527,0.3103448275862069,0.3103448275862069,Three trials are conducted for all settings the mean and standard deviation of the model accuracy are reported in ,20,This makes the results from the experiments more comparable with each other .,"SWAG + CODAH : Fine - tuned on SWAG first , then fine - tuned again in cross -validation on CODAH .",result
sentiment_analysis,17,is an integer .,model,Semantic Relatedness of Sentence Pairs,0,133,16,3,0,model : Semantic Relatedness of Sentence Pairs,0.5911111111111111,0.5333333333333333,0.17647058823529413,is an integer ,4,"Given a sentence pair , we wish to predict a real - valued similarity score in some range [ 1 , K ] , where K >","The sequence { 1 , 2 , . . . , K} is some ordinal scale of similarity , where higher scores indicate greater degrees of similarity , and we allow real - valued scores to account for ground - truth ratings thatare an average over the evaluations of several human annotators .",method
text_summarization,1,We experiment on question generation and abstractive summarization tasks and show that our method achieves the best trade - off between accuracy and diversity over previous models on SQuAD and datasets .,introduction,introduction,0,44,34,34,0,introduction : introduction,0.18333333333333326,0.9714285714285714,0.9714285714285714,We experiment on question generation and abstractive summarization tasks and show that our method achieves the best trade off between accuracy and diversity over previous models on SQuAD and datasets ,31,"To mitigate the lack of ground truth annotation for the mask ( content selection ) , we use the overlap between the source and target sequences as a simple proxy for the ground - truth mask .","In particular , compared to the recently - introduced mixture decoder that also aims to diversify outputs by creating multiple decoders , our modular method not only demonstrates better accuracy and diversity , but also trains 3.7 times faster .",introduction
natural_language_inference,26,We observe that the modest number of m would be better .,ablation,The influence of the number m,0,176,10,4,0,ablation : The influence of the number m,0.8301886792452831,0.8333333333333334,0.8,We observe that the modest number of m would be better ,12,shows the result .,"sources , we exclude them in our table to save space .",result
natural_language_inference,11,"Pooling over lemma-occurrences effectively connects different text passages ( even across texts ) thatare otherwise disconnected , mitigating the problems arising from long - distance dependencies .",system description,Unrefined Word Embeddings (E 0 ),0,88,54,24,0,system description : Unrefined Word Embeddings (E 0 ),0.3188405797101449,0.9642857142857144,0.9230769230769232,Pooling over lemma occurrences effectively connects different text passages even across texts thatare otherwise disconnected mitigating the problems arising from long distance dependencies ,24,"As a consequence , this minor linguistic preprocessing step allows for additional interaction between tokens of the same lemma .","This is reminiscent of the ( soft ) attention mechanism used in reading comprehension models ( e.g. , ; ) .",method
text-classification,6,The encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional vector as the sentence embedding .,model,Transformer,0,47,19,5,0,model : Transformer,0.31756756756756754,0.59375,0.4545454545454545,The encoder takes as input a lowercased PTB tokenized string and outputs a 512 dimensional vector as the sentence embedding ,21,The context aware word representations are converted to a fixed length sentence encoding vector by computing the element - wise sum of the representations at each word position .,The encoding model is designed to be as general purpose as possible .,method
topic_models,0,The embeddings obtained from SMM were then used to train GLC and LR classifiers .,baseline,baseline,0,295,12,12,0,baseline : baseline,0.7160194174757282,0.4615384615384616,0.4615384615384616,The embeddings obtained from SMM were then used to train GLC and LR classifiers ,15,"It was trained with hyper - parameters such as embedding dimension K = { 100 , . . . , 800 } , and regularization weight ? = { 0.0001 , . . . , 10.0 }.","Note that we can not use GLCU here , because SMM yields only point - estimates of embeddings .",result
relation_extraction,6,"Compared to a baseline system , our method results in an average error reduction of 24 % on a held - out set of relations .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.061538461538461535,0.75,0.75,Compared to a baseline system our method results in an average error reduction of 24 on a held out set of relations ,23,We use the Wikidata knowledge base to construct a dataset of multiple relations per sentence and to evaluate our approach .,The code and the dataset to replicate the experiments are made available at https://github.com/ukplab.,abstract
sentiment_analysis,1,"Data on V can be represented by a feature matrix X ? R nd , where n denotes the number of nodes and d denotes the input feature dimension .",system description,Simple Graph Convolution Network (SGC),0,124,52,3,0,system description : Simple Graph Convolution Network (SGC),0.31313131313131315,0.25120772946859904,0.0967741935483871,Data on V can be represented by a feature matrix X R nd where n denotes the number of nodes and d denotes the input feature dimension ,28,"Given a graph G = ( V , E ) , where V denotes a set of nodes and E denotes a set of edges between nodes in V .","Data on V can be represented by a feature matrix X ? R nd , where n denotes the number of nodes and d denotes the input feature dimension .",method
sentiment_analysis,4,ICON performs better than the compared models with significant performance increase in emotions ( ? 2.1 % acc. ) .,result,Results,1,246,4,4,0,result : Results,0.7546012269938649,0.16666666666666666,0.16666666666666666,ICON performs better than the compared models with significant performance increase in emotions 2 1 acc ,17,"In , we evaluate the mean classification performance using Weighted Accuracy ( acc. ) and F1 - Score ( F1 ) on the discrete emotion categories .","For each emotion , ICON outperforms all the compared models except for happiness emotion .",result
temporal_information_extraction,1,"For example , if a before TLINK can be established given a sentence , then it always holds as before regardless of other events around it , but if a TLINK is vague given a sentence , it may still change to other types afterwards if a connection can later be established through other nodes from the context .",training,Missing Annotations,0,165,90,14,0,training : Missing Annotations,0.6420233463035021,0.8490566037735849,0.4666666666666667,For example if a before TLINK can be established given a sentence then it always holds as before regardless of other events around it but if a TLINK is vague given a sentence it may still change to other types afterwards if a connection can later be established through other nodes from the context ,55,"Third , vague is fundamentally different from the other relation types .","This distinction emphasizes that vague is a consequence of lack of background / contextual information , rather than a concrete relation type to be trained on .",experiment
part-of-speech_tagging,5,The accuracy of the word - based character model joint with a word - based model were significantly lower than a sentence - based character model .,ablation,Impact of the Sentence-based Character Model,0,176,18,5,0,ablation : Impact of the Sentence-based Character Model,0.8712871287128713,0.4864864864864865,0.2083333333333333,The accuracy of the word based character model joint with a word based model were significantly lower than a sentence based character model ,24,The results are shown in .,We conclude Here we investigate the part - ofspeech tagging performance of the joint model compared with the word and character models on their own ( using hyperparameters from in 4.1 ) .,result
natural_language_inference,17,"where ? denotes the sigmoid activation function , denotes element - wise multiplication , and the bias term is omitted .",architecture,Alignment Architecture for MRC,0,157,114,114,0,architecture : Alignment Architecture for MRC,0.6038461538461538,0.8321167883211679,0.8769230769230769,where denotes the sigmoid activation function denotes element wise multiplication and the bias term is omitted ,17,"To efficiently fuse the attentive information into the context , an heuristic fusion function , denoted as o = fusion ( x , y ) , is proposed as","where ? denotes the sigmoid activation function , denotes element - wise multiplication , and the bias term is omitted .",method
natural_language_inference,44,We analyze 50 randomly sampled examples in which the model fails on exact match ( EM ) despite using the oracle sentence .,system description,Analyses on existing QA model,0,46,33,5,0,system description : Analyses on existing QA model,0.16083916083916086,0.7857142857142857,0.35714285714285715,We analyze 50 randomly sampled examples in which the model fails on exact match EM despite using the oracle sentence ,21,The model achieves 83.1 F1 when trained and evaluated using the full document and 85.1 F1 when trained and evaluated using the oracle sentence .,"We classify these errors into 4 categories , as shown in .",method
text_summarization,7,"Based on our motivation , we specifically selected the redundant repeating output that occurred in the baseline EncDec .",method,method,0,136,7,7,0,method : method,0.9006622516556292,0.4117647058823529,0.7777777777777778,Based on our motivation we specifically selected the redundant repeating output that occurred in the baseline EncDec ,18,shows actual generation examples .,It is clear that EncDec + WFE successfully reduced them .,method
natural_language_inference,0,"In there is a high attention of the correct answer on financial regulatory standards in the first layer , and onus president in the second layer .",model,Attention Visualization,0,187,12,4,0,model : Attention Visualization,0.949238578680203,0.8571428571428571,0.6666666666666666,In there is a high attention of the correct answer on financial regulatory standards in the first layer and onus president in the second layer ,26,"generic pattern observed in these examples is that in intermediate layers , candidates in the document ( shown along rows ) tend to pick out salient tokens in the query which provide clues about the cloze , and in the final layer the candidate with the highest match with these tokens is selected as the answer .","The incorrect answer , in contrast , only attends to one of these aspects , and hence receives a lower score in the final layer despite the n-gram overlap it has with the cloze token in the query .",method
natural_language_inference,38,"In this paper , we introduce the Multi - layer Embedding with Memory Networks ( MEMEN ) , an end - to - end neural network for machine comprehension task .",introduction,introduction,1,27,17,17,0,introduction : introduction,0.14285714285714285,0.5483870967741935,0.5483870967741935,In this paper we introduce the Multi layer Embedding with Memory Networks MEMEN an end to end neural network for machine comprehension task ,24,"On the contrary , every word in the query has its own embedding vector in the situation of two dimensional attention , but many words in the question sentence are useless even if disturbing , such as the stopwords .",Our model consists of three parts :,introduction
phrase_grounding,0,"On the visual side , most of the works exploit Deep Convolutional Neural Networks but often rely on bounding box 1 https://github.com/hassanhub/MultiGrounding",introduction,introduction,0,15,5,5,0,introduction : introduction,0.06666666666666668,0.2380952380952381,0.2380952380952381,On the visual side most of the works exploit Deep Convolutional Neural Networks but often rely on bounding box 1 https github com hassanhub MultiGrounding,25,It is especially challenging as it requires a good representation of both the visual and textual domain and an effective way of linking them .,crowd of onlookers on a tractor ride watch a farmer hard at work in the field .,introduction
natural_language_inference,60,Dynamic Meta - Embeddings for Improved Sentence Representations,title,title,1,2,1,1,0,title : title,0.010362694300518135,1.0,1.0,Dynamic Meta Embeddings for Improved Sentence Representations,7, , ,title
natural_language_inference,65,"Given a pair ( q , a ) consisting of a question q and a candidate answer a , both networks score the pair by first computing fixed - length independent continuous vector representations r q and r a , and then computing the cosine similarity between these two vectors .",system description,Neural Networks for Answer Selection,0,45,4,4,0,system description : Neural Networks for Answer Selection,0.1956521739130435,0.1,0.3333333333333333,Given a pair q a consisting of a question q and a candidate answer a both networks score the pair by first computing fixed length independent continuous vector representations r q and r a and then computing the cosine similarity between these two vectors ,45,In this section we briefly review two NN architectures that have previously been applied to the answer selection task : QA - CNN and QA - biLSTM .,In we present a joint illustration of these two neural networks .,method
temporal_information_extraction,1,"By adding the VC and TD datasets into the training set , we retrained our local baseline and achieved comparable performance to : Temporal awareness scores given gold events but with no gold pairs , which show that the proposed S+I methods outperformed state - of - the - art systems in various settings .",baseline,TE3 Task C -Relation Only,0,215,19,11,0,baseline : TE3 Task C -Relation Only,0.8365758754863813,0.34545454545454546,0.6111111111111112,By adding the VC and TD datasets into the training set we retrained our local baseline and achieved comparable performance to Temporal awareness scores given gold events but with no gold pairs which show that the proposed S I methods outperformed state of the art systems in various settings ,50,"We can see that UT - Time is about 3 % better than AP - 1 in the absolute value of F 1 , which is expected since UTTime included more advanced features derived from syntactic parse trees .","The fourth column indicates the annotation sources used , with additional unlabeled dataset in the parentheses .",result
relation_extraction,10,Trained from scratch Character Embeddings,model,Trained from scratch Character Embeddings,0,81,27,1,0,model : Trained from scratch Character Embeddings,0.421875,0.32926829268292684,0.2,Trained from scratch Character Embeddings,5, , ,method
natural_language_inference,21,"The simple architecture of DiSAN leads to fewer parameters , less computation and easier parallelization .",abstract,abstract,0,40,38,38,0,abstract : abstract,0.13793103448275862,0.8636363636363636,0.8636363636363636,The simple architecture of DiSAN leads to fewer parameters less computation and easier parallelization ,15,"Unlike Transformer , neither stacking of attention blocks nor an encoderdecoder structure is required .","In experiments 1 , we compare DiSAN with the currently popular methods on various NLP tasks , e.g. , natural language inference , sentiment analysis , sentence classification , etc .",abstract
semantic_role_labeling,4,"To enhance the discriminative power , it is promising to apply techniques that keep label representations faraway from each other .",performance,Function F ?,0,242,49,27,0,performance : Function F ?,0.8066666666666666,1.0,1.0,To enhance the discriminative power it is promising to apply techniques that keep label representations faraway from each other ,20,"Also , the core label A2 is close to the adjunct label DIR , which are often confused by the model .", ,result
natural_language_inference,99,We then compare two methods which directly encode the passage words or use the question influence .,ablation,Ablation Results,0,222,10,10,0,ablation : Ablation Results,0.8809523809523809,0.38461538461538464,0.625,We then compare two methods which directly encode the passage words or use the question influence ,17,"We first replace our input gate mechanism into simplified feature concatenation strategy , the performance drops nearly 2.3 % on the EM score , which proves the effectiveness of our proposed dynamic input gating mechanism .",The result proves that our modification of employing question influence on the passage encoding can boost the result up to 1.3 % on the EM score .,result
sentence_compression,0,"Although deletion - based sentence compression is not as flexible as abstractive sentence compression , we chose to work on deletion - based sentence compression for the following reason .",introduction,introduction,0,29,22,22,0,introduction : introduction,0.1039426523297491,0.6111111111111112,0.6111111111111112,Although deletion based sentence compression is not as flexible as abstractive sentence compression we chose to work on deletion based sentence compression for the following reason ,27,"To this end , we extend the deletionbased LSTM model for sentence compression by .","Abstractive sentence compression allows new words to be used in a compressed sentence , i.e. , words that do not occur in the input sentence .",introduction
natural_language_inference,21,"Then , a multi-dimensional attention computes a vector representation of the entire sequence , which can be passed into a classification / regression module to compute the final prediction for a particular task .",abstract,abstract,1,38,36,36,0,abstract : abstract,0.1310344827586207,0.8181818181818182,0.8181818181818182,Then a multi dimensional attention computes a vector representation of the entire sequence which can be passed into a classification regression module to compute the final prediction for a particular task ,32,"In DiSAN , the input sequence is processed by directional ( forward and backward ) self - attentions to model context dependency and produce context - aware representations for all tokens .","Unlike Transformer , neither stacking of attention blocks nor an encoderdecoder structure is required .",abstract
text_summarization,4,"The context vector ct is computed using the additive attention mechanism , which matches the current decoder state st and each encoder state hi to get an importance score .",model,Base model,0,89,19,11,0,model : Base model,0.3449612403100775,0.2065217391304348,0.7857142857142857,The context vector ct is computed using the additive attention mechanism which matches the current decoder state st and each encoder state hi to get an importance score ,29,"At each time step t , the previous token y t?1 , the previous hidden state s t?1 , and the previous context vector c t ?1 are passed to a GRU to calculate the new hidden state st , as shown in the equation below .",The scores are then passed to a softmax and are used to pool the encoder states using weighted sum .,method
question_generation,1,An interesting line of work in this respect is the work of .,introduction,introduction,0,17,8,8,0,introduction : introduction,0.04336734693877551,0.26666666666666666,0.26666666666666666,An interesting line of work in this respect is the work of ,13,"Further , show that unanswered questions can be used for improving VQA , Image captioning and Object Classification .",Here the au-thors have proposed the challenging task of generating natural questions for an image .,introduction
semantic_parsing,2,"Compared with previous neural models that utilize syntax or grammatical information ( SEQ2 TREE , ASN ; the second block in ) , our method performs competitively despite the use of relatively simple decoders .",analysis,Results and Analysis,1,260,9,9,0,analysis : Results and Analysis,0.8934707903780069,0.2647058823529412,0.2647058823529412,Compared with previous neural models that utilize syntax or grammatical information SEQ2 TREE ASN the second block in our method performs competitively despite the use of relatively simple decoders ,30,The results also show that removing the sketch encoder harms performance since the decoder loses access to additional contextual information .,"As an upper bound , we report model accuracy when gold meaning sketches are given to the fine meaning decoder ( + oracle sketch ) .",result
natural_language_inference,66,"Although this is a very crude approximation , it reduces the number of parameters we need to update , and as it turns out , we can still achieve better performance than .",implementation,Implementation Details,0,132,11,11,0,implementation : Implementation Details,0.4731182795698925,1.0,1.0,Although this is a very crude approximation it reduces the number of parameters we need to update and as it turns out we can still achieve better performance than ,30,Then we do not update any word embedding when learning our model ., ,experiment
question_answering,5,Now we can view each aspect of the question as a column vector ( i.e. a hidden state at each word position in the answer-question concatenation ) in the enhanced question representation H aq .,method,EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0,109,58,29,0,method : EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0.394927536231884,0.6304347826086957,0.4603174603174603,Now we can view each aspect of the question as a column vector i e a hidden state at each word position in the answer question concatenation in the enhanced question representation H aq ,35,"As most of the answer candidates do not appear in the question , this is for better matching with the passage and finding more answer - related information from the passage .",Then the task becomes to measure how well each column vector can be matched by the union passage ; and we achieve this by computing the attention vector Parikh et al. for each hidden state of sequences a and q as follows :,method
sentiment_analysis,24,"In AE , we concatenate the word embedding , the initial shared representation h s ( 0 ) i , and the task - specific representation h ae i as the final representation of the ith token .",method,Aspect-Level Tasks,0,100,45,27,0,method : Aspect-Level Tasks,0.3215434083601286,0.3358208955223881,0.9,In AE we concatenate the word embedding the initial shared representation h s 0 i and the task specific representation h ae i as the final representation of the ith token ,32,The self - attention layer outputs h as i = n j=1 A ij h as j .,"In AS , we concatenate h s ( 0 ) i and h as i as the final representation .",method
sentiment_analysis,0,"Furthermore , the occurrence of the incorrect "" sad - to - happy "" cases in the TRE model is reduced from 16 . 20 % to 9.15 % .",analysis,Error analysis,1,170,12,12,0,analysis : Error analysis,0.9550561797752808,1.0,1.0,Furthermore the occurrence of the incorrect sad to happy cases in the TRE model is reduced from 16 20 to 9 15 ,23,The values arranged along the diagonal axis show that all of the accuracies of the correctly predicted class have increased ., ,result
natural_language_inference,100,We compare the results with previous deep learning models with additional features .,result,Learning with Combining Additional Features,0,321,60,5,0,result : Learning with Combining Additional Features,0.8770491803278688,0.7317073170731707,0.2272727272727273,We compare the results with previous deep learning models with additional features ,13,We use the implementation of Lambda MART in jforests,shows the results on TRAIN and TRAIN - ALL when combining additional features .,result
relation-classification,1,"In the similar way , the backward LSTM layer will encode wt based on the contextual information from w n tow t , which is marked as ? ? ht .",method,The End-to-end Model,0,115,50,19,0,method : The End-to-end Model,0.467479674796748,0.7142857142857143,0.4871794871794872,In the similar way the backward LSTM layer will encode wt based on the contextual information from w n tow t which is marked as ht ,27,"For each word wt , the forward LSTM layer will encode wt by considering the contextual information from word w 1 tow t , which is marked as ? ? ht .","Finally , we concatenate ? ? ht and ? ? ht to represent word t 's encoding information , denoted as",method
natural_language_inference,23,Therefore we propose to constrain the matrix UT V to be symmetric .,architecture,FULLY-AWARE FUSION NETWORK,0,133,61,9,0,architecture : FULLY-AWARE FUSION NETWORK,0.25925925925925924,0.4295774647887324,0.45,Therefore we propose to constrain the matrix UT V to be symmetric ,13,"However , we suspect that two large matrices interacting directly will make the neural model harder to train .",symmetric matrix can always be decomposed into,method
sentiment_analysis,38,"But some of the classification tasks they are evaluated on , such as sentiment analysis of reviews of consumer goods , do not have much overlap with the text of novels .",introduction,introduction,0,43,32,32,0,introduction : introduction,0.2559523809523809,0.7441860465116279,0.7441860465116279,But some of the classification tasks they are evaluated on such as sentiment analysis of reviews of consumer goods do not have much overlap with the text of novels ,30,Skip - thought vectors were trained on a corpus of books .,"We propose this distributional issue , combined with the limited capacity of current models , results in representational underfitting .",introduction
text-to-speech_synthesis,2,"We train the synthesis network on 1.2K speakers and show that training the encoder on a much larger set of 18K speakers improves adaptation quality , and further enables synthesis of completely novel speakers by sampling from the embedding prior .",introduction,introduction,0,23,15,15,0,introduction : introduction,0.09387755102040816,0.3947368421052632,0.3947368421052632,We train the synthesis network on 1 2K speakers and show that training the encoder on a much larger set of 18K speakers improves adaptation quality and further enables synthesis of completely novel speakers by sampling from the embedding prior ,41,We demonstrate that the speaker encoder and synthesis networks can be trained on unbalanced and disjoint sets of speakers and still generalize well .,"There has been significant interest in end - to - end training of TTS models , which are trained directly from text - audio pairs , without depending on hand crafted intermediate representations .",introduction
machine-translation,6,https://github.com/salesforce/awd-lstm-lm,experiment,Settings,0,198,42,24,0,experiment : Settings,0.6804123711340206,0.8076923076923077,0.7058823529411765,https github com salesforce awd lstm lm,7,http://mattmahoney.net/dc/textdata.html,https://github.com/zihangdai/mos,experiment
natural_language_inference,94,"This emulates the gathering of useful external information to complete paths within the context , e.g. , house ? child , daughter ? child .",method,method,0,145,77,77,0,method : method,0.3785900783289817,0.6062992125984252,0.6062992125984252,This emulates the gathering of useful external information to complete paths within the context e g house child daughter child ,21,"We then allow an unconstrained hop into c 3 's neighbors in ConceptNet , getting to c 4 ? n bh ( c 3 ) via r 3 ( nbh ( v ) is the set of nodes that can be reached from v in one hop ) .","This emulates the gathering of useful external information to complete paths within the context , e.g. , house ? child , daughter ? child .",method
natural_language_inference,30,"That is , we want the triple that labels a given question to be scored higher than other triples in K by a margin of 0.1 .",architecture,architecture,0,132,13,13,0,architecture : architecture,0.5116279069767442,0.20967741935483872,0.38235294117647056,That is we want the triple that labels a given question to be scored higher than other triples in K by a margin of 0 1 ,27,where 0.1 is the margin .,"We also enforce a constraint on the norms of the columns of V and W , i.e. ? i , | |v i | | 2 ? 1 and ? j , ||w j || 2 ? 1 .",method
machine-translation,0,We trained the CSLM model on 7 - grams from the target corpus .,model,Neural Language Model,0,140,4,4,0,model : Neural Language Model,0.6392694063926939,0.2857142857142857,0.2857142857142857,We trained the CSLM model on 7 grams from the target corpus ,13,"Especially , the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder - Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT sys - tem add up or are redundant .","Each input word was projected into the embedding space R 512 , and they were concatenated to form a 3072 dimensional vector .",method
negation_scope_resolution,0,"In 2009 , used IGTREE , which is a memory - based learning algorithm , as implemented in TiMBL , to detect cues .",approach,Machine Learning Approaches,0,90,13,13,0,approach : Machine Learning Approaches,0.39130434782608703,0.14285714285714285,0.3939393939393939,In 2009 used IGTREE which is a memory based learning algorithm as implemented in TiMBL to detect cues ,19,This was a novel approach to negation detection at the time and was performed on the BioScope Corpus .,"For scope resolution , they used a metalearner that used the predictions by 3 classifiers which predicted whether a given token was the beginning of a scope , end of the scope of neither .",method
text_summarization,14,"At test time , we use beam search with beam size 10 to generate the summary .",dataset,Dataset,0,147,14,14,0,dataset : Dataset,0.6447368421052632,0.9333333333333332,0.9333333333333332,At test time we use beam search with beam size 10 to generate the summary ,16,"To speedup the training for our RAML model , we continue the RAML training based on the pre-trained model with ML training with the current decayed learning rate .","We report ROUGE F1 score including ROUGE - 1 , ROUGE - 2 and ROUGE - L for Gigaword corpus and ROUGE recall score for DUC 2004 corpus .",experiment
natural_language_inference,14,"Interannotator agreement was computed over a set of 50 additional questions with a pairwise average Cohen - Kappa score of 0.89 , which is interpreted as almost perfect agreement by some guidelines .",evaluation,evaluation,0,132,6,6,0,evaluation : evaluation,0.7719298245614035,1.0,1.0,Interannotator agreement was computed over a set of 50 additional questions with a pairwise average Cohen Kappa score of 0 89 which is interpreted as almost perfect agreement by some guidelines ,32,"Human annotators answered 95.3 % of questions correctly , presenting a 7 - fold reduction in error compared to the fine - turned BERT model .", ,result
sentiment_analysis,45,"The only difference from the SentiHood is that the target - aspect pairs {t , a} become only aspects a .",dataset,Datasets,0,97,8,8,0,dataset : Datasets,0.6736111111111112,0.8888888888888888,0.8888888888888888,The only difference from the SentiHood is that the target aspect pairs t a become only aspects a ,19,We also evaluate our method on SemEval - 2014 Task 4 dataset 4 for aspectbased sentiment analysis .,This setting allows us to jointly evaluate subtask 3 ( Aspect Category Detection ) and subtask 4 ( Aspect Category Polarity ) .,experiment
natural_language_inference,23,"To obtain the precise answer , common knowledge is required to know that employing a full - time staff will not avoid political interference .",model,FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0,487,94,71,0,model : FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0.949317738791423,0.7833333333333333,0.8352941176470589,To obtain the precise answer common knowledge is required to know that employing a full time staff will not avoid political interference ,23,Only the second one is suggested to avoid political problems .,ID : 57111713a58dae1900cd6c02-high-conf-turk2,method
text_summarization,14,"Note that the entailment score is 0.57 for seq2seq model with selective encoding , and we believe that the selective mechanism can filter out secondary information in the input , which will reduce the possibility to generate irrelevant information .",analysis,Does our summarization model learn entailment knowledge?,1,194,8,6,0,analysis : Does our summarization model learn entailment knowledge?,0.8508771929824561,0.2285714285714285,0.2857142857142857,Note that the entailment score is 0 57 for seq2seq model with selective encoding and we believe that the selective mechanism can filter out secondary information in the input which will reduce the possibility to generate irrelevant information ,39,"When we adopt entailmentbased strategies , the entailment score rises to 0.63 for seq2seq model .",Entailment - aware selective model achieves a high entailment reward of 0.71 .,result
natural_language_inference,49,Interaction Layer : The interaction layer models the interaction between premise encoded representation P enc and hypothesis encoded representation H enc as follows :,model,DENSELY INTERACTIVE INFERENCE NETWORK,0,99,53,29,0,model : DENSELY INTERACTIVE INFERENCE NETWORK,0.38976377952755903,0.7794117647058824,0.6590909090909091,Interaction Layer The interaction layer models the interaction between premise encoded representation P enc and hypothesis encoded representation H enc as follows ,23,The penalization aims to ensure the parallel structure learns the similar functionality but is aware of the subtle semantic difference between premise and hypothesis .,"where P i is the i - th row vector of P , and H j is the j - th row vector of H .",method
named-entity-recognition,8,"To train sentence representations , prior work has used objectives to rank candidate next sentences , left - to - right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives .",system description,Unsupervised Feature-based Approaches,0,45,6,6,0,system description : Unsupervised Feature-based Approaches,0.11627906976744184,0.15384615384615385,0.4615384615384616,To train sentence representations prior work has used objectives to rank candidate next sentences left to right generation of next sentence words given a representation of the previous sentence or denoising autoencoder derived objectives ,35,"These approaches have been generalized to coarser granularities , such as sentence embeddings or paragraph embeddings .",ELMo and its predecessor generalize traditional word embedding research along a different dimension .,method
question_answering,3,"Our model is lightweight , fast and efficient , achieving state - of - the - art or highly competitive performance on three datasets .",introduction,introduction,0,47,37,37,0,introduction : introduction,0.16785714285714284,1.0,1.0,Our model is lightweight fast and efficient achieving state of the art or highly competitive performance on three datasets ,20,"We conduct extensive experiments on three large - scale and challenging RC datasets - RACE , Search QA and Narrative QA .", ,introduction
natural_language_inference,78,This layer learns an alignment of sub - phrases between P and H. Let F ( . ) be a standard projection layer with ReLU activation function .,model,Inter-Attention Alignment Layer,0,99,24,2,0,model : Inter-Attention Alignment Layer,0.35869565217391297,0.27586206896551724,0.14285714285714285,This layer learns an alignment of sub phrases between P and H Let F be a standard projection layer with ReLU activation function ,24, ,The alignment matrix of two sequences is defined as follows :,method
sentiment_analysis,15,shows the interface annotators saw .,system description,Stanford Sentiment Treebank,0,78,42,16,0,system description : Stanford Sentiment Treebank,0.2888888888888889,0.7924528301886793,0.5925925925925926,shows the interface annotators saw ,6,"We then used Amazon Mechanical Turk to label the resulting 215 , 154 phrases .",The slider has 25 different values and is initially set to neutral .,method
sentiment_analysis,12,"Based on the above analysis , we propose a novel incremental approach to automatically mine influential context words from training instances , which can be then exploited as attention supervision information for neural ASC models .",approach,Details of Our Approach,0,81,16,2,0,approach : Details of Our Approach,0.3616071428571429,0.2222222222222222,0.0425531914893617,Based on the above analysis we propose a novel incremental approach to automatically mine influential context words from training instances which can be then exploited as attention supervision information for neural ASC models ,34, ,"As shown in Algorithm 1 , we first use the initial training corpus D to conduct model training , and then obtain the initial model parameters ? ( 0 ) ( Line 1 ) .",method
sentiment_analysis,1,") We conduct extensive experiment in both subjectdependent and subject - independent classification settings on two public EEG datasets , namely SEED and SEED - IV .",introduction,introduction,0,66,53,53,0,introduction : introduction,0.16666666666666666,0.9298245614035088,0.9298245614035088, We conduct extensive experiment in both subjectdependent and subject independent classification settings on two public EEG datasets namely SEED and SEED IV ,24,") We propose two regularizers : a node - wise domain adversarial training ( NodeDAT ) and an emotionaware distribution learning ( EmotionDL ) , which aim to improve the robustness of our model against cross - subject variations and noisy labels , respectively .",Experimental results demonstrate the effectiveness of our proposed model and regularizers .,introduction
topic_models,0,"The encoder models the posterior distribution of latent variables given the input , i.e. , p ? ( z | x ) , and the decoder models distribution of input data given the latent variable , i.e. , p ? ( x | z ) .",model,model,0,244,3,3,0,model : model,0.5922330097087378,0.375,0.375,The encoder models the posterior distribution of latent variables given the input i e p z x and the decoder models distribution of input data given the latent variable i e p x z ,35,Neural variational document model ( NVDM ) is an adaptation of variational auto - encoders for document modelling .,"In NVDM , the authors used bag - of - words as input , while their encoder and decoders are two - layer feed - forward neural networks .",method
semantic_parsing,2,Natural Language to Source Code,training,Natural Language to Source Code,0,153,44,1,0,training : Natural Language to Source Code,0.5257731958762887,0.3728813559322034,0.05555555555555555,Natural Language to Source Code,5, , ,experiment
natural_language_inference,80,Evaluation results show that such preprocessing allows our single model to achieve the same accuracy as the state - of - the - art ensemble model and improves our ensemble model to outperform the state - of - the - art ensemble model by a remarkable margin of 0.7 % .,introduction,introduction,0,37,28,28,0,introduction : introduction,0.12758620689655173,0.9655172413793104,0.9655172413793104,Evaluation results show that such preprocessing allows our single model to achieve the same accuracy as the state of the art ensemble model and improves our ensemble model to outperform the state of the art ensemble model by a remarkable margin of 0 7 ,45,"Furthermore , we demonstrate the importance of a simple preprocessing step performed on the SNLI dataset .","Finally , we perform an extensive analysis to clarify the strengths and weaknesses of our models .",introduction
text_generation,2,"The MANAGER and WORKER modules both start from an all - zero hidden state , denoted ash M 0 and h W 0 respectively .",methodology,Hierarchical Structure of G,0,98,28,6,0,methodology : Hierarchical Structure of G,0.28,0.459016393442623,0.15384615384615385,The MANAGER and WORKER modules both start from an all zero hidden state denoted ash M 0 and h W 0 respectively ,23,Next we will first describe the detailed generator model in LeakGAN and then show how the MANAGER and WORKER are trained with the guiding signals from D ? .,"At each step , the MANAGER receives the leaked feature vector ft from the discriminator D ? , which is further combined with current hidden state of the MANAGER to produce the goal vector gt a ? where M ( ; ? m ) denotes the MANAGER module implemented by an LSTM with parameters ? m and h",method
natural_language_inference,42,"Indeed , when running applications such as search tools or user interfaces , the inference time is critical to tackle real - world problems .",evaluation,FABIR vs BiDAF,0,250,25,13,0,evaluation : FABIR vs BiDAF,0.8650519031141869,0.4716981132075472,0.8125,Indeed when running applications such as search tools or user interfaces the inference time is critical to tackle real world problems ,22,"FABIR 's faster inference is a substantial advantage in large - scale applications , such as information extraction in large corpora .","Concerning the number of training variables , FABIR has almost 50 % fewer parameters than BiDAF , which incurs two major advantages .",result
text_summarization,2,"An encoder condenses the entire source text to a continuous vector ; it also learns a vector representation for each unit of the source text ( e.g. , words as units ) .",approach,The Basic Framework,0,61,10,3,0,approach : The Basic Framework,0.21942446043165467,0.07633587786259542,0.10714285714285714,An encoder condenses the entire source text to a continuous vector it also learns a vector representation for each unit of the source text e g words as units ,30,We build an encoder - decoder architecture for this work .,"In this work we use a two - layer stacked bi-directional Long Short - Term Memory networks as the encoder , where the input to the second layer is the concatenation of hidden states from the forward and backward passes of the first layer .",method
natural_language_inference,79,"Don already seen paragraphs ; and 2 ) "" the inference stage "" to get paragraph vectors D for new paragraphs ( never seen before ) by adding more columns in D and gradient descending on D while holding W , U , b fixed .",model,model,0,113,33,33,0,model : model,0.4216417910447761,0.6,0.9705882352941176,Don already seen paragraphs and 2 the inference stage to get paragraph vectors D for new paragraphs never seen before by adding more columns in D and gradient descending on D while holding W U b fixed ,38,") training to get word vectors W , softmax weights U , band paragraph vectors","We use D to make a prediction about some particular labels using a standard classifier , e.g. , logistic regression .",method
natural_language_inference,56,"Although the term "" relational reasoning "" is similar to terms in other branches of science , like relational logic or first order logic , no direct parallel is intended .",introduction,introduction,0,38,25,25,0,introduction : introduction,0.1130952380952381,0.8928571428571429,0.8928571428571429,Although the term relational reasoning is similar to terms in other branches of science like relational logic or first order logic no direct parallel is intended ,27,"Following , we use the term "" relational reasoning "" liberally for an object - and interaction - centric approach to problem solving .","This paper considers many - step relational reasoning , a challenging task for deep learning architectures .",introduction
named-entity-recognition,7,The ( partially ) processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions .,introduction,introduction,1,24,14,14,0,introduction : introduction,0.1518987341772152,0.7777777777777778,0.7777777777777778,The partially processed nested mentions in the stack are encoded with recursive neural networks where composition functions are used to capture dependencies between nested mentions ,26,"Following , we employ Stack - LSTM to represent the system 's state , which consists of the states of input , stack and action history , in a continuous space incrementally .","Based on the observation that letter - level patterns such as capitalization and prefix can be beneficial in detecting mentions , we incorporate a characterlevel LSTM to capture such morphological information .",introduction
machine-translation,8,This gradient can be used to train the alignment model as well as the whole translation model jointly .,system description,DECODER: GENERAL DESCRIPTION,0,76,46,20,0,system description : DECODER: GENERAL DESCRIPTION,0.229607250755287,0.6388888888888888,0.6666666666666666,This gradient can be used to train the alignment model as well as the whole translation model jointly ,19,"Instead , the alignment model directly computes a soft alignment , which allows the gradient of the cost function to be backpropagated through .","We can understand the approach of taking a weighted sum of all the annotations as computing an expected annotation , where the expectation is over possible alignments .",method
text-classification,8,"Intuitively , z takes the information of every sequence element into account via the addition operation .",system description,Simple Word-Embedding Model,0,71,24,6,0,system description : Simple Word-Embedding Model,0.2639405204460967,0.3636363636363637,0.125,Intuitively z takes the information of every sequence element into account via the addition operation ,16,"The model in can be seen as an average pooling operation , which takes the mean over each of the K dimensions for all word embeddings , resulting in a representation z with the same dimension as the embedding itself , termed here SWEM - aver .","Motivated by the observation that , in general , only a small number of key words contribute to final predictions , we propose another SWEM variant , that extracts the most salient features from every word - embedding dimension , by taking the maximum value along each dimension of the word vectors .",method
machine-translation,7,"Gruslys et al . ( 2016 ) describe a technique for drastically reducing the number of stored activations in an unrolled RNN , at the cost of recomputing forward activations .",performance,THE SHRINKING BATCH PROBLEM,0,134,33,31,1,performance : THE SHRINKING BATCH PROBLEM,0.35924932975871315,0.2844827586206897,0.96875,Gruslys et al 2016 describe a technique for drastically reducing the number of stored activations in an unrolled RNN at the cost of recomputing forward activations ,27,"For example , the weight matrices of a LSTM or other RNN could be replaced by a MoE. Sadly , such models break the convolutional trick from the last paragraph , since the input to the MoE atone timestep depends on the output of the MoE at the previous timestep .",This would allow for a large increase in batch size .,result
natural_language_inference,50,This is also supported by ( b ) which shows the majority of the word norms are clustered with ?w ? ? 3 . This would be reasonable considering that the leaf nodes of both question and answer hierarchies would reside in the middle .,analysis,analysis,0,306,4,4,0,analysis : analysis,0.9652996845425869,0.5,0.8,This is also supported by b which shows the majority of the word norms are clustered with w 3 This would be reasonable considering that the leaf nodes of both question and answer hierarchies would reside in the middle ,40,"Overall , depicts our key intuitions regarding the inner workings of Hyper QA which explains both RQ2 and RQ3 .",This is also supported by ( b ) which shows the majority of the word norms are clustered with ?w ? ? 3 . This would be reasonable considering that the leaf nodes of both question and answer hierarchies would reside in the middle .,result
relation-classification,6,"The type2 has a lot of entities related to machines and engineering like engine , woofer , and motor .",model,Self Attention,0,80,34,23,0,model : Self Attention,0.4419889502762431,0.4415584415584416,0.8846153846153846,The type2 has a lot of entities related to machines and engineering like engine woofer and motor ,18,"In the type 1 , the words are related to human 's jobs and foods .","Finally , in type3 , there are many words with bad meanings related associated with dis asters and :",method
natural_language_inference,48,"The alternating attention mechanism runs only for a fixed number of steps ( T = 8 in our tests ) , which is orders of magnitude smaller than a typical document or query in our datasets ( see ) .",training,Training Details,0,127,11,11,0,training : Training Details,0.5746606334841629,0.8461538461538461,0.8461538461538461,The alternating attention mechanism runs only for a fixed number of steps T 8 in our tests which is orders of magnitude smaller than a typical document or query in our datasets see ,34,"Computational Complexity Similar to previous state - of - the - art models which use a bidirectional encoder , the major bottleneck of our method is computing the document and query encodings .",The repeated attentions each require a softmax over ? 1000 locations which is typically fast on recent GPU architectures .,experiment
natural_language_inference,40,"By further replacing GRU units in existing reading comprehension models with MAGE - GRUs we achieve stateof - the - art performance on three well studied benchmarks the b Abi QA tasks , the LAMBADA dataset , and the CNN dataset .",introduction,introduction,0,48,38,38,0,introduction : introduction,0.17328519855595667,0.9743589743589745,0.9743589743589745,By further replacing GRU units in existing reading comprehension models with MAGE GRUs we achieve stateof the art performance on three well studied benchmarks the b Abi QA tasks the LAMBADA dataset and the CNN dataset ,37,"MAGE - GRU leads to a consistent improvement over the vanilla GRU , as well as a baseline where the coreference information is added as input features to the model .",An analysis of the learned representations by the model also show its effectiveness in encoding fine - grained information about the entities in a document .,introduction
relation_extraction,6,The length n naturally varies from sentence to sentence and an RNN provides away to accommodate inputs of various sizes .,system description,Model variants,0,75,30,12,0,system description : Model variants,0.5769230769230769,0.8823529411764706,0.75,The length n naturally varies from sentence to sentence and an RNN provides away to accommodate inputs of various sizes ,21,We apply a recurrent neural network ( RNN ) on the token embeddings .,It maps a sequence of n vectors to a fixedsize output vector o s ? R o .,method
sentiment_analysis,36,"Based on Eq. 10 and Eq. 11 , the words close to the target will be highlighted and those faraway will be downgraded .",model,Convolutional Feature Extractor,0,126,79,9,0,model : Convolutional Feature Extractor,0.504,0.8404255319148937,0.375,Based on Eq 10 and Eq 11 the words close to the target will be highlighted and those faraway will be downgraded ,23,"Then , we use v to help CNN locate the correct opinion w.r.t. the given target :",is also applied on the intermediate output to introduce the position information into each CPT layer .,method
natural_language_inference,33,APPENDIX 7 MODEL TRAINING,evaluation,APPENDIX 7 MODEL TRAINING,0,176,52,1,0,evaluation : APPENDIX 7 MODEL TRAINING,0.6743295019157088,0.3795620437956204,0.07692307692307693,APPENDIX 7 MODEL TRAINING,4, , ,result
question-answering,0,Fine - tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 : carefully tuning the similarity makes a clear difference .,result,Results,1,207,11,11,0,result : Results,0.8023255813953488,0.2682926829268293,0.5238095238095238,Fine tuning the embedding model is very beneficial to optimize the top of the list and grants a bump of 5 points of F1 carefully tuning the similarity makes a clear difference ,33,"We actually conducted experiments with several variants of our model , which tried to take the word ordering into account ( e.g. with convolutions ) , and they all failed to outperform our best performance without word order , once again perhaps because the supervision is not clean enough to allow for such elaborated language modeling .","All versions of our system greatly outperform paralex : the fine - tuned model improves the F1 - score by almost 20 points and , according to , is better in precision for all levels of recall .",result
natural_language_inference,18,"The variational lower bound to be optimised : ( 32 ) ? = sq ( | q | ) ||s a ( | a | ) ||s y ( 33 ) ? ? = tanh ( W 6 ? + b 6 ) ( 34 ) ? ? = tanh ( W 7 ? ? + b 7 ) ( 35 ) ? = W 8 ? ? + b 8 ( 36 ) log ? ? = W 9 ? ? + b 9 ( 37 ) h ? N ( ? ( q , a , y ) , diag ( ? 2 ? ( q , a , y ) ) )",t-SNE Visualis ation of Document Representations,t-SNE Visualis ation of Document Representations,0,254,3,3,0,t-SNE Visualis ation of Document Representations : t-SNE Visualis ation of Document Representations,0.9304029304029304,0.13636363636363635,0.42857142857142855,The variational lower bound to be optimised 32 sq q s a a s y 33 tanh W 6 b 6 34 tanh W 7 b 7 35 W 8 b 8 36 log W 9 b 9 37 h N q a y diag 2 q a y ,50,2 ) Generative Model p ? ( X|h ) :,"The variational lower bound to be optimised : ( 32 ) ? = sq ( | q | ) ||s a ( | a | ) ||s y ( 33 ) ? ? = tanh ( W 6 ? + b 6 ) ( 34 ) ? ? = tanh ( W 7 ? ? + b 7 ) ( 35 ) ? = W 8 ? ? + b 8 ( 36 ) log ? ? = W 9 ? ? + b 9 ( 37 ) h ? N ( ? ( q , a , y ) , diag ( ? 2 ? ( q , a , y ) ) )",others
natural_language_inference,41,- length spans correspond to the insertion of [ MASK ] tokens .,architecture,Pre-training BART,0,56,18,13,0,architecture : Pre-training BART,0.2178988326848249,0.20224719101123595,0.65, length spans correspond to the insertion of MASK tokens ,11,Each span is replaced with a single [ MASK ] token .,"Text infilling is inspired by Span - BERT , but SpanBERT samples span lengths from a different ( clamped geometric ) distribution , and replaces each span with a sequence of [ MASK ] tokens of exactly the same length .",method
text-to-speech_synthesis,1,"We conduct ablation studies to verify the effectiveness of several components in FastSpeech , including 1D Convolution and sequence - level knowledge distillation .",ablation,Ablation Study,0,183,2,2,0,ablation : Ablation Study,0.8356164383561644,0.6666666666666666,0.6666666666666666,We conduct ablation studies to verify the effectiveness of several components in FastSpeech including 1D Convolution and sequence level knowledge distillation ,22, ,We conduct CMOS evaluation for these ablation studies .,result
sentiment_analysis,5,) The subject - independent experiment :,experiment,The EEG emotion recognition experiments,1,188,53,23,0,experiment : The EEG emotion recognition experiments,0.7094339622641509,0.6091954022988506,0.6571428571428571, The subject independent experiment ,6,This indicates our differential features are indeed more discriminative .,"In this experiment , we adopt the leave - one - subject - out ( LOSO ) cross-validation strategy to evaluate the proposed BiHDM model .",experiment
natural_language_inference,89,where ? and ? are two hyper - parameters that control the weight of two auxiliary losses .,approach,Independent Span Loss,0,102,36,20,0,approach : Independent Span Loss,0.3923076923076923,0.4090909090909091,0.9523809523809524,where and are two hyper parameters that control the weight of two auxiliary losses ,15,"Finally , we combine the above losses as follows :",where ? and ? are two hyper - parameters that control the weight of two auxiliary losses .,method
natural_language_inference,44,Note that the span is not given as the groundtruth .,training,SQuAD-Adversarial,0,267,52,14,0,training : SQuAD-Adversarial,0.9335664335664337,0.7428571428571429,0.4375,Note that the span is not given as the groundtruth ,11,of Venice and which other city ? The groundtruth answer text is in red text .,"In the first example classified into ' N / A ' , the question is not answerable even given whole documents , because there is no word ' corlourful ' or ' enchantment ' in the given documents .",experiment
text-to-speech_synthesis,1,Autoregressive Transformer TTS model,model,Autoregressive Transformer TTS model,0,124,9,1,0,model : Autoregressive Transformer TTS model,0.5662100456621004,0.6923076923076923,0.2,Autoregressive Transformer TTS model,4, , ,method
natural_language_inference,39,"For the answer selection task ( Wiki - QA and TrecQA ) , we used the official trec eval scorer to compute the metrics Mean Average Precision ( MAP ) and Mean Reciprocal Rank ( MRR ) and 0.7993 0.7538 0.3692 0.8070 0.7489 0.3550 0.8268 0.7721 0.3224",experiment,Experimental Setup,0,159,29,29,0,experiment : Experimental Setup,0.7718446601941747,0.725,0.725,For the answer selection task Wiki QA and TrecQA we used the official trec eval scorer to compute the metrics Mean Average Precision MAP and Mean Reciprocal Rank MRR and 0 7993 0 7538 0 3692 0 8070 0 7489 0 3550 0 8268 0 7721 0 3224,48,We used the SICK development set for tuning and then applied exactly the same hyperparameters to all ten test sets .,selected the best development model based on MRR for final testing .,experiment
natural_language_inference,94,"where W 4 , W 5 , and W 6 are trainable parameters .",method,method,0,104,36,36,0,method : method,0.2715404699738904,0.28346456692913385,0.28346456692913385,where W 4 W 5 and W 6 are trainable parameters ,12,We obtain the self attention representation c :,The output of the self - attention layer is generated by another layer of bidirectional LSTM .,method
natural_language_inference,39,"Given the input x at ? sent a at time step t where a ? { 1 , 2 } , it s Bi - LSTMs hidden state h bi at is the concatenation of the forward state hf or at and the end for 13 : end for 14 : return simCube backward state h back at .",system description,Pairwise Word Interaction Modeling,0,72,9,9,0,system description : Pairwise Word Interaction Modeling,0.3495145631067961,0.13432835820895522,0.42857142857142855,Given the input x at sent a at time step t where a 1 2 it s Bi LSTMs hidden state h bi at is the concatenation of the forward state hf or at and the end for 13 end for 14 return simCube backward state h back at ,50,Algorithm 1 provides details of the modeling process .,"Given the input x at ? sent a at time step t where a ? { 1 , 2 } , it s Bi - LSTMs hidden state h bi at is the concatenation of the forward state hf or at and the end for 13 : end for 14 : return simCube backward state h back at .",method
part-of-speech_tagging,5,The feature that distinguishes our model most from previous work is that we apply a bidirectional recurrent layer ( LSTM ) on all characters of a sentence to induce fully context sensitive initial word encodings .,model,Sentence-based Character Model,0,54,4,2,0,model : Sentence-based Character Model,0.2673267326732673,0.09523809523809523,0.09523809523809523,The feature that distinguishes our model most from previous work is that we apply a bidirectional recurrent layer LSTM on all characters of a sentence to induce fully context sensitive initial word encodings ,34, ,"That is , we do not restrict the context of this layer to the words themselves ( as in ) .",method
natural_language_inference,41,"The researchers suggested the algae , like warming temperatures , might render the corals ' chemical defenses less effective , and the fish were protecting the coral by removing the algae .",system description,BART Summary,0,229,5,5,0,system description : BART Summary,0.8910505836575876,0.15151515151515152,0.15151515151515152,The researchers suggested the algae like warming temperatures might render the corals chemical defenses less effective and the fish were protecting the coral by removing the algae ,28,"The researchers found when fish were plentiful , they would eat algae and seaweed off the corals , which appeared to leave them more resistant to the bacterium Vibrio coralliilyticus , a bacterium associated with bleaching .","Fisheries off the coast of Fiji are protecting coral reefs from the effects of global warming , according to a study in the journal Science .",method
question_answering,5,shows the effects of K on the performance of coveragebased re-ranker .,analysis,analysis,0,225,33,33,0,analysis : analysis,0.8152173913043478,0.66,0.66,shows the effects of K on the performance of coveragebased re ranker ,13,"Therefore , there is a trade - off between the coverage of rank lists and the difficulty of re-ranking ; and selecting an appropriate K becomes important .","We train and test the coverage - based re-ranker on the top - K predictions from the baseline , where K ? { 3 , 5 , 10 }.",result
natural_language_inference,36,"To help model better understand natural language , we are motivated to discover an effective way to distill semantics inside the input sentence explicitly , such as semantic role labeling , instead of completely relying on uncontrollable model parameter learning or manual pruning .",introduction,introduction,0,16,7,7,0,introduction : introduction,0.0761904761904762,0.2692307692307692,0.2692307692307692,To help model better understand natural language we are motivated to discover an effective way to distill semantics inside the input sentence explicitly such as semantic role labeling instead of completely relying on uncontrollable model parameter learning or manual pruning ,41,"Typically , an MRC model pays great attention to non-significant words and ignores important ones .","Semantic role labeling ( SRL ) is a shallow semantic parsing task aiming to discover who did what to whom , when and why , providing explicit contextual semantics , which naturally matches the task target of text comprehension .",introduction
machine-translation,5,"For filtering , we required probabilistic dictionaries , which were obtained from the parallel corpora ( different dictionaries for the constrained and unconstrained scenarios ) using fast align .",system,Data Filtering,0,53,30,5,0,system : Data Filtering,0.3706293706293706,0.40540540540540543,0.3333333333333333,For filtering we required probabilistic dictionaries which were obtained from the parallel corpora different dictionaries for the constrained and unconstrained scenarios using fast align ,25,"Contrary to Tilde 's submissions for WMT 2017 , isolated sentence pair filtering for the WMT 2018 submissions was supplemented with a maximum content overlap filter ( i.e. only one target sentence for each source sentence was preserved and vice versa based on the content overlap filter 's score for each sentence pair ) .",The dictionaries were filtered using the transliteration - based probabilistic dictionary filtering method by .,method
natural_language_inference,23,"This multi-level attention mechanism captures different levels of information independently , while taking all levels of information into account .",architecture,Low-level fusion:?,0,185,113,20,0,architecture : Low-level fusion:?,0.3606237816764133,0.7957746478873241,0.6451612903225806,This multi level attention mechanism captures different levels of information independently while taking all levels of information into account ,20,High - level fusion : ? 3 .,new BiLSTM is applied to obtain the representation for C fully fused with information in the question Q:,method
named-entity-recognition,8,From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies .,ablation,ablation,0,385,24,24,0,ablation : ablation,0.9948320413436692,0.9230769230769232,0.9230769230769232,From the table it can be seen that fine tuning is surprisingly robust to different masking strategies ,18,"For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 .","However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER .",result
text_summarization,4,the available words from the original text .,introduction,introduction,0,18,7,7,0,introduction : introduction,0.06976744186046513,0.25,0.25,the available words from the original text ,8,Names are arranged alphabetically .,"Due to the limitations of extractive summarization on incoherent texts and unnatural methodology , the research trend has shifted towards abstractive summarization .",introduction
natural_language_inference,58,We use AdaDelta optimizer for parameter optimization with an initial learning rate of 0.5 and a batch size Results :,system description,Small Graph Large Graph,1,248,15,15,0,system description : Small Graph Large Graph,0.7402985074626866,0.15151515151515152,0.625,We use AdaDelta optimizer for parameter optimization with an initial learning rate of 0 5 and a batch size Results ,21,The maximum reasoning step T max is set to 10 in SQuAD experiments .,"In the , we report the performance of all models in the SQuAD leaderboard .",method
natural_language_inference,34,"Since not every piece of text is relevant to the question , we train a sub-network to select relevant paragraphs .",system description,Paragraph Selection,0,111,8,3,0,system description : Paragraph Selection,0.37627118644067803,0.07920792079207921,0.375,Since not every piece of text is relevant to the question we train a sub network to select relevant paragraphs ,21,"For each question , we assume that Np paragraphs are given ( e.g. Np = 10 in Hotpot QA ) .",The sub-network is based on a pre-trained BERT model followed by a sentence classification layer with sigmoid prediction .,method
text-classification,7,We explore three strategies to boost the accuracy of routing process by alleviating the disturbance of some noisy capsules :,model,Dynamic Routing,0,87,49,4,0,model : Dynamic Routing,0.3580246913580247,0.5212765957446809,0.5714285714285714,We explore three strategies to boost the accuracy of routing process by alleviating the disturbance of some noisy capsules ,20,"For each potential parent , the capsule network can increase or decrease the connection strength by dynamic routing , which is more effective than the primitive routing strategies such as max - pooling in CNN that essentially detects whether a feature is present in any position of the text , but loses spatial information about the feature .","Inspired by , an additional "" orphan "" category is added to the network , which can capture the "" background "" information of the text such as stop words and the words that are unrelated to specific categories , helping the capsule network model the child - parent relationship more efficiently .",method
sentiment_analysis,8,Term Frequency - Inverse Document Frequency ( TFIDF ) :,methodology,Implementation Details,0,97,51,2,0,methodology : Implementation Details,0.41452991452991456,0.5730337078651685,0.08,Term Frequency Inverse Document Frequency TFIDF ,7, ,TFIDF is a numerical statistic that shows the correlation between a word and a document in a collection or corpus .,method
natural_language_inference,91,"Because TreeRNNs use a different model structure for each sentence , as in , efficient batching is impossible in standard implementations .",introduction,introduction,0,19,11,11,0,introduction : introduction,0.0815450643776824,0.5238095238095238,0.5238095238095238,Because TreeRNNs use a different model structure for each sentence as in efficient batching is impossible in standard implementations ,20,"Batched computation - performing synchronized computation across many examples at once - yields order - of - magnitude improvements in model run time , and is crucial in enabling neural networks to be trained efficiently on large datasets .","Partly to address efficiency problems , standard TreeRNN models commonly only operate on sentences that have already been processed by a syntactic parser , which slows and complicates the use of these models at test time for most applications .",introduction
natural_language_inference,31,Many previous deep neural networks contain a single intersequence alignment layer .,introduction,introduction,0,15,8,8,0,introduction : introduction,0.05434782608695652,0.27586206896551724,0.27586206896551724,Many previous deep neural networks contain a single intersequence alignment layer ,12,Semantic alignment and comparison of two text sequences are the keys in neural text matching .,"To make full use of this only alignment process , the model has to take rich external syntactic features or hand - designed align - ment features as additional inputs of the alignment layer , adopt a complicated alignment mechanism , or build avast amount of post-processing layers to analyze the alignment result .",introduction
natural_language_inference,81,Archaeology can be a destructive science for the finite resources of the archaeological record are lost to excavation .,APPENDIX,Answer town,0,324,116,36,0,APPENDIX : Answer town,0.7695961995249406,0.5446009389671361,0.2706766917293233,Archaeology can be a destructive science for the finite resources of the archaeological record are lost to excavation ,19,Other threats to the archaeological record include natural phenomena and scavenging .,Therefore archaeologists limit the amount of excavation that they do at each site and keep meticulous records of what is found .,others
part-of-speech_tagging,1,"We use pre-trained word embeddings for initialization , GloVe 100 - dimension word embeddings for English , and fastText 300 dimension word embeddings for Spanish , Dutch , and German .",training,training,1,182,8,8,0,training : training,0.728,0.4705882352941176,0.4705882352941176,We use pre trained word embeddings for initialization GloVe 100 dimension word embeddings for English and fastText 300 dimension word embeddings for Spanish Dutch and German ,27,"The number of convolutional layers are 5 and 9 for IntNet - 5 and IntNet - 9 , respectively , and we have adopted the same weight initialization as that of ResNet .",The state size of the bi-directional LSTMs is set to 256 .,experiment
text_summarization,10,"good rewritten summary is one that contains all the salient information from the document , is logically followed ( entailed ) by it , and avoids redundant information .",introduction,introduction,0,15,7,7,0,introduction : introduction,0.05703422053231939,0.35,0.35,good rewritten summary is one that contains all the salient information from the document is logically followed entailed by it and avoids redundant information ,25,"Despite these strong recent advancements , there is still a lot of scope for improving the summary quality generated by these models .","The redundancy aspect was addressed by coverage models , but we still need to teach these models about how to better detect salient information from the input document , as well as about better logicallydirected natural language inference skills .",introduction
natural_language_inference,78,"Firstly , we introduce our factorization operation , which lives at the core of our neural model .",model,Alignment Factorization Layer,0,114,39,3,0,model : Alignment Factorization Layer,0.4130434782608696,0.4482758620689655,1.0,Firstly we introduce our factorization operation which lives at the core of our neural model ,16,This layer aims to learn a scalar valued feature for each comparison between aligned sub-phrases ., ,method
sentiment_analysis,35,The statistics of the YelpAspect dataset are summarized in .,dataset,dataset,0,172,5,5,0,dataset : dataset,0.6935483870967742,0.4545454545454545,0.4545454545454545,The statistics of the YelpAspect dataset are summarized in ,10,"Specifically , YelpAspect contains three domains : Restaurant ( R1 ) , Beautyspa ( B ) , and Hotel ( H ) .",Yelp reviews are collected in US cities over six years .,experiment
sentiment_analysis,40,"TD - LSTM performs less competitive than our method on all the datasets , particularly on the tweet dataset , because in this dataset sentiment words are usually far from person names , for which case the multiple - attention mechanism is designed to work .",result,Main Results,1,172,9,9,0,result : Main Results,0.7713004484304933,0.17307692307692307,0.5625,TD LSTM performs less competitive than our method on all the datasets particularly on the tweet dataset because in this dataset sentiment words are usually far from person names for which case the multiple attention mechanism is designed to work ,41,"However , dependency parsing is not guaranteed to work well on irregular texts such as tweets , which may still result in long path between the opinion word and its target , so that the opinion features would also be lost while being propagated .","TD - LSTM - A also performs worse than our method , because it s two attentions , i.e. one for the text before the target and the other for the after , can not tackle some cases where more than one features being attended are at the same side of the target .",result
negation_scope_resolution,0,We also use a 5th label for the padded tokens and set the class weights for that token category to 0 to avoid training on it .,methodology,Methodology,0,177,9,9,0,methodology : Methodology,0.7695652173913043,0.4736842105263158,0.4736842105263158,We also use a 5th label for the padded tokens and set the class weights for that token category to 0 to avoid training on it ,27,"Hence , when we test inter-dataset performance , we consider cues thatare affixes as normal cues , and predictions of affixes as predictions of normal cues. ( i.e. 0 and 1 are considered as the same label for the purpose of evaluation ) .","For scope resolution , we use a binary labelling scheme , 0 as not a token and 1 as a token .",method
natural_language_inference,72,It was also advised that we admit the patient to a high dependency unit and manage him according to the usual protocol for a tricyclic overdose if complications arose .,Training details and hyper-parameter optimization,Bridging inference passage,0,302,42,15,0,Training details and hyper-parameter optimization : Bridging inference passage,0.9679487179487181,0.8076923076923077,0.6,It was also advised that we admit the patient to a high dependency unit and manage him according to the usual protocol for a tricyclic overdose if complications arose ,30,"Although there was no precedent on how to treat a significant rectal overdose of amitriptyline , it was advised that the patient be administered a phosphate enema and if failed to adequately remove the tablets then the patient should be given whole bowel irrigation with 2 litre of Klean - Prep via a nasogastric tube .",It seems reasonable to attempt careful removal of the drug from the rectum and if that fails to consider and whole bowel irrigation .,experiment
relation_extraction,11,Such features from dependency parse have been exploited in the past by .,system description,Relation Alias Side Information,0,155,88,6,0,system description : Relation Alias Side Information,0.625,0.6875,0.2857142857142857,Such features from dependency parse have been exploited in the past by ,13,"Further , we extend P by including tokens atone hop distance in dependency path from target entities .",The degree of match between the extracted phrases in P and aliases of a relation can give important clues about the relevance of that relation for the sentence .,method
named-entity-recognition,1,is therefore a square matrix of size k + 2 .,model,model,0,60,11,11,0,model : model,0.2898550724637681,0.5789473684210527,0.5789473684210527,is therefore a square matrix of size k 2 ,10,"where A is a matrix of transition scores such that A i , j represents the score of a transition from the tag i to tag j. y 0 and y n are the start and end tags of a sentence , that we add to the set of possible tags .",softmax over all possible tag sequences yields a probability for the sequence y:,method
natural_language_inference,47,shows a set of randomly chosen validated examples from the development set with their labels .,system description,Data validation,0,105,70,18,0,system description : Data validation,0.4883720930232558,0.8860759493670886,0.8571428571428571,shows a set of randomly chosen validated examples from the development set with their labels ,16,"Regardless , the over all rate of agreement is extremely high , suggesting that the corpus is sufficiently high quality to pose a challenging but realistic machine learning task .","Qualitatively , we find the data that we collected draws fairly extensively on commonsense knowledge , and that hypothesis and premise sentences often differ structurally in significant ways , suggesting that there is room for improvement beyond superficial word alignment models .",method
machine-translation,6,The results on three word similarity tasks are listed in .,result,Word Similarity,0,212,4,2,0,result : Word Similarity,0.7285223367697594,0.8,0.6666666666666666,The results on three word similarity tasks are listed in ,11, ,"Paras "" denotes the number of model parameters .",result
natural_language_inference,26,It also outperforms all the published works and achieves comparable performance with a few unpublished models from the leaderboard .,dataset,Tasks and Datasets,0,159,28,28,0,dataset : Tasks and Datasets,0.75,0.8235294117647058,0.8235294117647058,It also outperforms all the published works and achieves comparable performance with a few unpublished models from the leaderboard ,20,SemBERT boosts the strong BERT baseline essentially on both EM and F1 .,shows SemBERT also achieves a new state - of the - art on SNLI benchmark and even outperforms all the ensemble models 10 by a large margin .,experiment
sentiment_analysis,1,"Graph analysis for human brain has been studied extensively in the neuroscience literature , .",introduction,introduction,0,50,37,37,0,introduction : introduction,0.12626262626262627,0.6491228070175439,0.6491228070175439,Graph analysis for human brain has been studied extensively in the neuroscience literature ,14,"In this paper , we propose a regularized graph neural network ( RGNN ) aiming to address all three aforementioned challenges .","However , making an accurate connectome is still an open question and subject to different scales .",introduction
sentiment_analysis,13,"As a result , the joint loss of post - training is defined as L = L DK + L MRC .",system description,Post-training,0,158,82,23,0,system description : Post-training,0.5683453237410072,0.82,0.5609756097560976,As a result the joint loss of post training is defined as L L DK L MRC ,18,"We let the loss on SQuAD be L MRC , which is in a similar setting as the loss L RRC for RRC .",One major issue of post - training on such a loss is the prohibitive cost of GPU memory usage .,method
machine-translation,7,"We constructed a similar training set consisting of shuffled unique sentences from Google 's internal news corpus , totalling roughly 100 billion words .",performance,100 BILLION WORD GOOGLE NEWS CORPUS,0,184,83,2,0,performance : 100 BILLION WORD GOOGLE NEWS CORPUS,0.4932975871313673,0.7155172413793104,0.2,We constructed a similar training set consisting of shuffled unique sentences from Google s internal news corpus totalling roughly 100 billion words ,23, ,"Similarly to the previous section , we tested a series of models with similar computational costs of about 8 million ops / timestep .",result
temporal_information_extraction,1,The non-uniform distribution of all the relation types makes it difficult to separate lowfrequency ones from the others ( see in ) .,system description,Temporal Relation Types,0,60,6,6,0,system description : Temporal Relation Types,0.2334630350194553,0.2857142857142857,0.6,The non uniform distribution of all the relation types makes it difficult to separate lowfrequency ones from the others see in ,22,"However , current systems usually use a reduced set of relation types , mainly due to the following reasons .","For example , relations such as immediately before or immediately after barely exist in a corpus compared to before and after .",method
sentiment_analysis,23,"On the other hand , words like the , will , even are known as stop words .",model,Visualization,0,280,44,15,0,model : Visualization,0.9589041095890408,0.88,0.7142857142857143,On the other hand words like the will even are known as stop words ,15,"Hence , large fractions ( 0.24 and 0.19 ) of their features , after convolution , are gathered by pooling .","They are mostly noninformative for sentiment ; hence , no ( or minimal ) features are gathered .",method
sentiment_analysis,11,also showcases the superiority of CMN and its variants over bc - LSTM .,result,Increasing attention,0,317,31,10,0,result : Increasing attention,0.9215116279069768,0.5849056603773585,0.625,also showcases the superiority of CMN and its variants over bc LSTM ,13,"Overall , multimodal systems outperform the unimodal variants justifying the design of CMN as a multimodal system .",The proposed model achieves better performance over the state of the art in all the unimodal and multimodal segments .,result
text-classification,6,is the universal sentence encoder ( USE ) using Transformer .,model,Combined Transfer Models,0,92,5,5,0,model : Combined Transfer Models,0.6216216216216216,0.4166666666666667,0.4166666666666667,is the universal sentence encoder USE using Transformer ,9,Model performance on transfer tasks .,Dis the universal encoder DAN model .,method
question_generation,1,"We present different experiments with Tag Net in which we explore the performance of various tags ( Noun , Verb , and Question tags ) and different ways of combining them to get the context vectors .",Supplementary Material,Supplementary Material,0,234,5,5,0,Supplementary Material : Supplementary Material,0.5969387755102041,0.2380952380952381,0.625,We present different experiments with Tag Net in which we explore the performance of various tags Noun Verb and Question tags and different ways of combining them to get the context vectors ,33,"We report BLEU1 , BLEU2 , BLEU3 , BLEU4 , METEOR , ROUGE and CIDER metric scores for VQG - COCO dataset .",Algorithm 1 Multimodal Differential Network 1 : procedure MDN ( x,others
machine-translation,7,We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.02949061662198392,0.2195121951219512,0.2195121951219512,We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers ,22,"We apply the MoE to the tasks of language modeling and machine translation , where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora .","On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",abstract
part-of-speech_tagging,6,"Dependency parsing has become a key research topic in NLP in the last decade , boosted by the success of the shared tasks on multilingual dependency parsing .",introduction,introduction,0,9,2,2,0,introduction : introduction,0.060402684563758385,0.14285714285714285,0.14285714285714285,Dependency parsing has become a key research topic in NLP in the last decade boosted by the success of the shared tasks on multilingual dependency parsing ,27, ,identify two types of data - driven methodologies for dependency parsing : graph - based approaches and transition - based approaches .,introduction
semantic_role_labeling,0,Long - distance dependencies shows the performance breakdown by binned distance between arguments to the given predicates .,analysis,Effectiveness of beam pruning,0,86,10,2,0,analysis : Effectiveness of beam pruning,0.8190476190476189,0.43478260869565216,0.13333333333333333,Long distance dependencies shows the performance breakdown by binned distance between arguments to the given predicates ,17, ,"Our model is better at accurately predicting arguments thatare farther away from the predicates , even compared to an ensemble model that has a higher over all F 1 .",result
natural_language_inference,0,Such embeddings have been previously shown to be helpful for tasks like Named Entity Recognition and dealing with OOV tokens at test time .,model,Further Enhancements,0,101,40,7,0,model : Further Enhancements,0.5126903553299492,0.8333333333333334,0.4666666666666667,Such embeddings have been previously shown to be helpful for tasks like Named Entity Recognition and dealing with OOV tokens at test time ,24,We also utilize a character composition model C ( w ) which generates an orthographic embedding of the token .,"The embedding C ( w ) is generated by taking the final outputs z f nc and z b nc of a Bi - GRU applied to embeddings from a lookup table of characters in the token , and applying a linear transformation :",method
sentiment_analysis,18,Hot would be close to words like warm or cold and dog would be close to animals like cat .,introduction,introduction,0,36,22,22,0,introduction : introduction,0.1506276150627615,0.5789473684210527,0.5789473684210527,Hot would be close to words like warm or cold and dog would be close to animals like cat ,20,"For example , we can not obtain a good representation for "" hot dog "" by averaging the word vectors of "" hot "" and "" dog "" .",The average would not be close to other food like burgers or spaghetti .,introduction
relation_extraction,11,"In this module , we use additional supervision from KBs and utilize Open IE methods forgetting relevant side information .",system description,RESIDE Overview,0,109,42,14,0,system description : RESIDE Overview,0.4395161290322581,0.328125,0.7,In this module we use additional supervision from KBs and utilize Open IE methods forgetting relevant side information ,19,. Side Information Acquisition :,This information is later utilized by the model as described in Section 5.2 .,method
natural_language_inference,18,"The models are trained using Adam , with hyperparameters selected by optimising the MAP score on the development set .",experiment,Dataset & Setup for Answer Sentence Selection,0,179,53,25,0,experiment : Dataset & Setup for Answer Sentence Selection,0.6556776556776557,0.6625,0.8620689655172413,The models are trained using Adam with hyperparameters selected by optimising the MAP score on the development set ,19,"Considering the trade - off between computational cost and variance , we chose 20 samples for prediction in all the experiments .",that the evaluation scripts used by previous work are noisy - 4 out of 72 questions in the test set are treated answered incorrectly .,experiment
question-answering,3,"introduced the attention mechanism into the CNN model , and captured the best performance ( the fifth row of ) .",model,Comparing with State-of-the-art Models,0,221,51,22,0,model : Comparing with State-of-the-art Models,0.8700787401574803,0.7285714285714285,0.5365853658536586,introduced the attention mechanism into the CNN model and captured the best performance the fifth row of ,18,The corresponding performance is given at the fourth row of .,The semantic matching phase in our model is similar to the attention mechanism .,method
natural_language_inference,44,"shows error cases on SQuAD , which MINIMAL fails to answer correctly .",training,training,0,242,27,27,0,training : training,0.8461538461538461,0.3857142857142857,0.7714285714285715,shows error cases on SQuAD which MINIMAL fails to answer correctly ,12,"The other two diagrams in shows the error cases of each model , broken down by the sentence where the model 's prediction is from .","In the first International Airport ? In 1994 , Wet Wet Wet had their biggest hit , a cover version of the troggs ' single "" Love is All Around "" , which",experiment
named-entity-recognition,4,Recent state - of - the - art neural language models compute a context - independent token representation x LM k ( via token embeddings or a CNN over characters ) then pass it through L layers of forward LSTMs .,model,Bidirectional language models,0,50,3,3,0,model : Bidirectional language models,0.18382352941176472,0.05172413793103448,0.2727272727272727,Recent state of the art neural language models compute a context independent token representation x LM k via token embeddings or a CNN over characters then pass it through L layers of forward LSTMs ,35,"Given a sequence of N tokens , ( t 1 , t 2 , ... , t N ) , a forward language model computes the probability of the sequence by modeling the probability of to - ken t k given the history ( t 1 , ... , t k?1 ) :","At each position k , each LSTM layer outputs a context - dependent representation ? ? h LM k , j where j = 1 , . . . , L. The top layer LSTM output , ? ? h LM k , L , is used to predict the next token t k +1 with a Softmax layer .",method
named-entity-recognition,7,Please see the Appendix for the full list of restrictions .,model,Action Constraints,0,80,37,7,0,model : Action Constraints,0.5063291139240507,0.5441176470588235,0.6363636363636364,Please see the Appendix for the full list of restrictions ,11,"For example , reduce - action can only be conducted when there are at least two elements in the stack .","Formally , we use V(S , i , A ) to denote the valid actions given the parser state .",method
natural_language_inference,77,It follows that H is the sum of the outputs from the forward - and backward - LSTM at the beginning of training .,system description,Encoding,0,94,62,11,0,system description : Encoding,0.3443223443223443,0.8493150684931506,0.7857142857142857,It follows that H is the sum of the outputs from the forward and backward LSTM at the beginning of training ,22,"We initialize the projection matrix B with [ I n ; In ] , where In denotes the n-dimensional identity matrix .","As mentioned before , we utilize the same encoder parameters for both question and context , except the projection matrix B which is not shared .",method
natural_language_inference,54,Here again we listed the ones thatare correctly identified by SEDT but not BiDAF .,analysis,Evangelical Lutheran Church in America,0,197,34,8,0,analysis : Evangelical Lutheran Church in America,0.8602620087336245,0.85,0.5714285714285714,Here again we listed the ones thatare correctly identified by SEDT but not BiDAF ,15,The advantages of using the dependency tree in our model can be illustrated using the questions in .,"As we can see that the answer provided by BiDAF for first question broke the parenthesis incorrectly , this problem that can be easily solved by utilizing dependency information .",result
semantic_parsing,2,"The first element between a pair of brackets is an operator or predicate name , and any remaining elements are its arguments .",training,Natural Language to Logical Form,0,135,26,17,0,training : Natural Language to Logical Form,0.4639175257731959,0.22033898305084745,0.5,The first element between a pair of brackets is an operator or predicate name and any remaining elements are its arguments ,22,Placeholder for terminal return t,Algorithm 1 shows the pseudocode used to extract sketches from ?- calculus - based meaning representations .,experiment
sentiment_analysis,39,"For the sentences that do not comply with our schema , we define the two following special labels .",system description,Out of scope,0,92,37,2,0,system description : Out of scope,0.3755102040816327,0.5211267605633803,0.16666666666666666,For the sentences that do not comply with our schema we define the two following special labels ,18, ,Sentences marked with one of the these labels are removed from the dataset .,method
natural_language_inference,5,QA is an answer selection QA dataset constructed from real queries of Bing and Wikipedia .,dataset,dataset,0,109,3,3,0,dataset : dataset,0.7414965986394558,0.75,0.75,QA is an answer selection QA dataset constructed from real queries of Bing and Wikipedia ,16,"To test the performance of the model , we utilize the TREC - QA , WikiQA and QNLI datasets ] .","Following the literature , we use only questions that contain at least one correct answer among the list of answer candidates .",experiment
text_summarization,13,Experiments were performed on a version of the CNN / Dailymail dataset from Hermann et al ..,experiment,experiment,0,171,3,3,1,experiment : experiment,0.6404494382022472,0.375,0.375,Experiments were performed on a version of the CNN Dailymail dataset from Hermann et al ,16,"This , however , incurs a cost in computational complexity .","Each data point is a news document accompanied by up to 4 "" highlights "" , and we take the first of these as our target summary .",experiment
sentiment_analysis,14,"We train Emo2 Vec by multi-task learning six different emotion - related tasks , including emotion / sentiment analysis , sarcasm classification , stress detection , abusive language classification , insult detection , and personality recognition .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.03289473684210526,0.6,0.6,We train Emo2 Vec by multi task learning six different emotion related tasks including emotion sentiment analysis sarcasm classification stress detection abusive language classification insult detection and personality recognition ,30,"In this paper , we propose Emo2 Vec which encodes emotional semantics into vectors .","Our evaluation of Emo2 Vec shows that it outperforms existing affect - related representations , such as Sentiment - Specific Word Embedding and DeepMoji embeddings with much smaller training corpora .",abstract
sentiment_analysis,35,"Moreover , we build a Coarse2 Fine ( C2F ) attention upon the C2A module to specifically model the source aspect before feeding to the PaS module .",system description,An Overview of the MGAN model,0,76,17,7,0,system description : An Overview of the MGAN model,0.3064516129032258,0.17346938775510204,0.6363636363636364,Moreover we build a Coarse2 Fine C2F attention upon the C2A module to specifically model the source aspect before feeding to the PaS module ,25,"In MGAN , two basic hop units are used similarly as common attention - based RNN models , where the Context2Aspect ( C2A ) attention aims to measure the importance of each aspect word and generate the aspect representation with the aid of each context word , and the Position - aware Sentiment ( PaS ) attention utilizes the obtained aspect representation and the position information of the aspect to capture relevant sentiment features in the context for encoding the aspect - specific representation .",The C2F module uses the source aspect representation to attend corresponding aspect terms in the context and then the attended context features is conversely predicted the category of the source aspect ( pseudo - label ) .,method
natural_language_inference,11,They show that there is indeed a slightly larger benefit when employing background knowledge from ConcepNet ( A ) in the more impoverished settings with largest improvements when using around 10 k examples and reduced dimensionality to 10 .,implementation,Reducing Training Data & Dimensionality of Pre-trained Word Embeddings,0,254,57,7,0,implementation : Reducing Training Data & Dimensionality of Pre-trained Word Embeddings,0.9202898550724636,0.7215189873417721,0.7777777777777778,They show that there is indeed a slightly larger benefit when employing background knowledge from ConcepNet A in the more impoverished settings with largest improvements when using around 10 k examples and reduced dimensionality to 10 ,37,11 Our joint data and dimensionality reduction results are presented in .,"However , we observe that the biggest over all impact over the baseline ESIM model stems from our contextual refinement strategy ( i.e. , reading only the premise p and hypothesis q) which is especially pronounced for the 1 k and 3 k experiments .",experiment
relation-classification,4,Neural sequence model .,model,Baseline Models,1,132,11,11,0,model : Baseline Models,0.5038167938931297,0.7857142857142857,0.7857142857142857,Neural sequence model ,4,"Earlier , our group compared and with sequence models , and we report these results ; for ( 3 ) we report results with our own implementation .","Our group presented a competitive sequence model that employs a position - aware attention mechanism over LSTM outputs ( PA - LSTM ) , and showed that it outperforms several CNN and dependency - based models by a substantial margin .",method
natural_language_inference,100,"Within a NMM architecture , this problem has already been handled with QA matching matrix .",result,Learning with Combining Additional Features,0,333,72,17,0,result : Learning with Combining Additional Features,0.9098360655737704,0.8780487804878049,0.7727272727272727,Within a NMM architecture this problem has already been handled with QA matching matrix ,15,"This exact match information includes match between numbers and proper nouns , which are highlighted in previous research work as especially important for factoid questions answering , where most of the questions are of type what , when , who thatare looking for answers containing numbers or proper nouns .",Thus incorporating word overlap features will not help much for improving the performance of a NMM .,result
sentiment_analysis,11,"For pooling , we set mp to be 3 whose output is fed to a fully connected layer with 100 neurons .",approach,Visual Feature Extraction,0,135,40,12,0,approach : Visual Feature Extraction,0.39244186046511625,0.8163265306122449,0.8571428571428571,For pooling we set mp to be 3 whose output is fed to a fully connected layer with 100 neurons ,21,"In our model , we use 128 feature maps for 3 D filters of size 5 .",All the values are decided using hyperparameter tuning ( see Section 5 ) .,method
text_summarization,10,"SotA models , which instead produce extraneous , unrelated words not present ( in any paraphrased form ) in the source document .",model,Question Generation,0,91,46,19,0,model : Question Generation,0.3460076045627377,0.4946236559139785,1.0,SotA models which instead produce extraneous unrelated words not present in any paraphrased form in the source document ,19,"We share the ' blue ' color representations across all the three tasks , i.e. , second layer of encoder , attention parameters , and first layer of decoder .", ,method
question_similarity,0,Ali went away .,model,Input,0,58,10,6,0,model : Input,0.42028985507246375,0.3125,1.0,Ali went away ,4,"For example , the word "" "" will have different embedding vectors related to the following two sentences as they have different Translation :", ,method
paraphrase_generation,0,The problem of obtaining a semantic embedding for a sentence that ensures that the related sentences are closer and unrelated sentences are farther lies at the core of understanding languages .,introduction,introduction,0,16,2,2,0,introduction : introduction,0.06751054852320675,0.06896551724137931,0.06896551724137931,The problem of obtaining a semantic embedding for a sentence that ensures that the related sentences are closer and unrelated sentences are farther lies at the core of understanding languages ,31, ,This would be relevant for a wide variety of machine reading comprehension and related tasks such as sentiment analysis .,introduction
named-entity-recognition,8,They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning .,SQuAD v1.1,SQuAD v1.1,0,353,9,9,0,SQuAD v1.1 : SQuAD v1.1,0.9121447028423773,0.5294117647058824,0.5294117647058824,They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning ,23,Benchmark is a collection of sentence pairs drawn from news headlines and other sources .,"Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources , with human annotations for whether the sentences in the pair are semantically equivalent .",others
natural_language_inference,61,is paper is organized as follows .,introduction,introduction,0,55,48,48,0,introduction : introduction,0.3525641025641026,0.9411764705882352,0.9411764705882352,is paper is organized as follows ,7,"We believe that the architecture of Bi - a LSTM has potentially to be used in other NLP tasks such as text classi cation , machine translation and soon .",We introduce the general frameworks of ESIM and a ESIM in Section 2 .,introduction
sentiment_analysis,40,"If the target is a phrase , v ? takes the average of word embeddings .",model,Recurrent Attention on Memory,0,122,57,15,0,model : Recurrent Attention on Memory,0.5470852017937221,0.9193548387096774,0.8823529411764706,If the target is a phrase v takes the average of word embeddings ,14,"where [ , v ? ] indicates when the attention result relies on particular aspects such as those of products , we also add the target vector v ? because different product aspects have different preference on opinion words ; when the target is a person , there is no need to do so .","We utilize the previous episode for the current attention , since it can guide the model to attend different useful information . ) also adopts multiple attentions , but they do n't combine the results of different attentions .",method
natural_language_inference,0,"We show in our experiments that the proposed GA reader , despite its relative simplicity , consis-tently improves over a variety of strong baselines on three benchmark datasets .",introduction,introduction,0,25,14,14,0,introduction : introduction,0.12690355329949238,0.875,0.875,We show in our experiments that the proposed GA reader despite its relative simplicity consis tently improves over a variety of strong baselines on three benchmark datasets ,28,"Such a fine - grained attention enables our model to learn conditional token representations w.r.t. the given question , leading to accurate answer selections .","Our key contribution , the GA module , provides a significant improvement for large datasets .",introduction
machine-translation,3,We obtain the new BLEU score of 39.2 with a single Deep - Att model .,method,Post processing,0,268,17,14,0,method : Post processing,0.8562300319488818,0.85,0.8235294117647058,We obtain the new BLEU score of 39 2 with a single Deep Att model ,16,"We find this method provides an additional 1.5 BLEU points , which is consistent with the conclusion in .","For the ensemble models of Deep - Att , the BLEU score rises to 40.4 .",method
natural_language_inference,96,Ambiguous Both endings seem equally likely .,analysis,Qualitative examples,0,281,22,3,0,analysis : Qualitative examples,0.7205128205128205,0.5641025641025641,0.15,Ambiguous Both endings seem equally likely ,7,"The bad ending is semantically or grammatically malformed , e.g. ' the man is getting out of the horse .","12.0 % that ESIM + ELMo answered incorrectly , for each extracting both the gold ending and the model 's preferred ending .",result
natural_language_inference,85,"We show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference composition and by incorporating it into our framework , we achieve additional improvement , increasing the performance to a new state of the art with an 88.6 % accuracy .",introduction,introduction,1,30,20,20,0,introduction : introduction,0.1276595744680851,1.0,1.0,We show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference composition and by incorporating it into our framework we achieve additional improvement increasing the performance to a new state of the art with an 88 6 accuracy ,46,"In this paper , we are interested in exploring this within the neural network frameworks , with the presence of relatively large training data .", ,introduction
sentiment_analysis,0,"The data in each fold are split into training , development , and testing datasets ( 8:0.5:1.5 , respectively ) .",evaluation,Performance evaluation,0,140,3,3,0,evaluation : Performance evaluation,0.7865168539325843,0.14285714285714285,0.14285714285714285,The data in each fold are split into training development and testing datasets 8 0 5 1 5 respectively ,20,"As the dataset is not explicitly split beforehand into training , development , and testing sets , we perform 5 - fold cross validation to determine the over all performance of the model .","After training the model , we measure the weighted average precision ( WAP ) over the 5 - fold dataset .",result
sentiment_analysis,45,"The polarity in ' appearance ' is positive , and the polarity regarding ' price ' is negative .",introduction,introduction,0,13,7,7,0,introduction : introduction,0.09027777777777778,0.25,0.25,The polarity in appearance is positive and the polarity regarding price is negative ,14,"However , the users ' comments may contain different aspects , such as : "" This book is a hardcover version , but the price is a bit high . """,Aspect - based sentiment analysis ( ABSA ) aims to identify fine - grained polarity towards a specific aspect .,introduction
natural_language_inference,72,"The CUI information is still an integral part of the answer field in our dataset , so it can be used by other researchers if preferred .",evaluation,Evaluation,0,191,10,10,0,evaluation : Evaluation,0.6121794871794872,0.4166666666666667,0.4166666666666667,The CUI information is still an integral part of the answer field in our dataset so it can be used by other researchers if preferred ,26,"In the current setup , we are able to keep both the original word phrase as well as the extended answers .",with different metrics described below .,result
natural_language_inference,85,"where ? is the sigmoid function , is the elementwise multiplication of two vectors , and all W ? R dl , U ? R dd are weight matrices to be learned .",model,Input Encoding,0,75,30,22,0,model : Input Encoding,0.3191489361702128,0.28846153846153844,0.7857142857142857,where is the sigmoid function is the elementwise multiplication of two vectors and all W R dl U R dd are weight matrices to be learned ,27,"The memory cell ct considers each child 's cell vector , c L t?1 and c R t?1 , which are gated by the left forget gate f Lt and right forget gate f R t , respectively .","where ? is the sigmoid function , is the elementwise multiplication of two vectors , and all W ? R dl , U ? R dd are weight matrices to be learned .",method
text-classification,1,"Although these models are larger than those in , training / testing is still faster than the LSTM models due to simplicity of the region embeddings .",system description,Experiments (supervised),0,155,109,32,0,system description : Experiments (supervised),0.60546875,0.545,0.5333333333333333,Although these models are larger than those in training testing is still faster than the LSTM models due to simplicity of the region embeddings ,25,We show in the table above that one - hot CNNs with two layers ( of 1000 feature maps each ) with two different region sizes 4 rival oh - 2 LST Mp .,"By comparison , the strength of LSTM to embed larger regions appears not to be a big contributor here .",method
sentiment_analysis,24,"During model training , we only consider AS predictions on these aspect term - related tokens for computing the AS loss and ignore the sentiments predicted on other tokens 6 .",method,Learning,0,137,82,9,0,method : Learning,0.4405144694533762,0.6119402985074627,0.14754098360655735,During model training we only consider AS predictions on these aspect term related tokens for computing the AS loss and ignore the sentiments predicted on other tokens 6 ,29,We label each token that is part of any aspect term with the sentiment of the corresponding aspect term .,"When trained on document - level instances , we 6 Let l ( y as i , j ,? where N ds and N dd denote the number of training instances for DS and DD respectively , and y ds i and y dd i denote the one - hot encoding of the gold label .",method
relation_extraction,8,"We employ dropout ( Hinton et al. , 2012 ) on the penultimate layer for regularization .",methodology,Softmax Output,0,162,78,5,1,methodology : Softmax Output,0.6022304832713755,0.7572815533980582,0.4545454545454545,We employ dropout Hinton et al 2012 on the penultimate layer for regularization ,14,"1 ? Rn 1 3n is the transformation matrix , and o ? Rn 1 is the final output of the network , where n 1 is equal to the number of possible relation types for the relation extraction system .",Dropout prevents the co-adaptation of hidden units by randomly dropping out a proportion p of the hidden units during forward computing .,method
relation_extraction,12,The shortest dependency path between these entities is highlighted in bold ( edges and tokens ) .,introduction,introduction,0,37,22,22,0,introduction : introduction,0.11246200607902736,0.55,0.55,The shortest dependency path between these entities is highlighted in bold edges and tokens ,15,An example dependency tree for two sentences expressing a relation ( sensitivity ) among three entities .,The root node of the LCA subtree of entities is present .,introduction
sentiment_analysis,31,"Using this labeling scheme , in the restaurant data , 1034 out of 1117 test points have the same sentencelevel and aspect - level labels .",result,Case Study & Discussion,0,147,25,7,0,result : Case Study & Discussion,0.9245283018867924,0.8620689655172413,0.6363636363636364,Using this labeling scheme in the restaurant data 1034 out of 1117 test points have the same sentencelevel and aspect level labels ,23,"For a sentence containing multiple aspects , we assume the majority of the aspect - level sentiment label is the sentence - level sentiment label .","Thus , a sentencelevel classifier with accuracy 75 % also classifies 70 % aspect - labels correctly .",result
natural_language_inference,38,"ing set into their part - of - speech ( POS ) tags and namedentity recognition ( NER ) tags , which can be showed in 3 .",model,Encoding of Context and Query,0,57,16,11,0,model : Encoding of Context and Query,0.3015873015873016,0.2424242424242425,0.5238095238095238,ing set into their part of speech POS tags and namedentity recognition NER tags which can be showed in 3 ,21,"The first row ( green ) is the original sentence from the passage , the second row ( red ) is the name- entity recognition ( NER ) tags , and the last row ( blue ) is the part - of - speech ( POS ) tags .","Then we employ skip - sram model , which is one of the core algorithms in the popular off - the - shelf embedding word2vec , to the transformed "" passage "" just like it works in word2vec for the normal passage .",method
sentence_classification,0,"We manually define regular expression patterns mappings to normalized section titles : "" introduction "" , "" related work "" , "" method "" , "" experiments "" , "" conclusion "" .",system description,Data for scaffold tasks,0,150,24,5,0,system description : Data for scaffold tasks,0.5617977528089888,0.9230769230769232,0.7142857142857143,We manually define regular expression patterns mappings to normalized section titles introduction related work method experiments conclusion ,18,"For the second scaffold ( citation section title ) , respective to each test dataset , we sample citations from the ACL - ARC corpus and Semantic Scholar corpus 9 and extract the citation context as well as their corresponding sections .",Section titles which did not map to any of the aforementioned titles were excluded from the dataset .,method
relation-classification,5,The RC component further uses these latent vectors r i for relation classification .,model,Our proposed model,0,50,16,16,0,model : Our proposed model,0.4424778761061947,0.6956521739130435,0.6956521739130435,The RC component further uses these latent vectors r i for relation classification ,14,"As for NER , the RC component also uses a BiLSTM ( BiLSTM RC ) to learn another set of latent feature vectors , but from the sequence x 1:n :",We propose a novel use of the deep biaffine attention mechanism for relation classification .,method
named-entity-recognition,4,SRL F 1 increased a marginal 0.1 % to 82.2 for the ?= 1 case compared to using the last layer only .,analysis,Alternate layer weighting schemes,0,141,21,15,0,analysis : Alternate layer weighting schemes,0.5183823529411765,0.3088235294117647,0.2419354838709677,SRL F 1 increased a marginal 0 1 to 82 2 for the 1 case compared to using the last layer only ,23,"For SNLI , averaging all layers with ?= 1 improves development accuracy from 88.2 to 88.7 % over using just the last layer .",Where to include ELMo ? All of the task architectures in this paper include word embeddings only as input to the lowest layer biRNN .,result
natural_language_inference,88,Perplexity results for KN5 and RNN are taken from .,model,Language Modeling,0,167,22,22,0,model : Language Modeling,0.6761133603238867,0.6666666666666666,0.6666666666666666,Perplexity results for KN5 and RNN are taken from ,10,The results of the language modeling task are shown in .,"As can be seen , the single - layer LSTMN outperforms these : Examples of intra-attention ( language modeling ) .",method
text_summarization,2,The framework naturally combines the dependency parse tree structure with the copy mechanism of an abstractive summarization system .,introduction,introduction,0,32,23,23,0,introduction : introduction,0.11510791366906475,0.8846153846153846,0.8846153846153846,The framework naturally combines the dependency parse tree structure with the copy mechanism of an abstractive summarization system ,19,we introduce novel neural architectures that encourage salient source words / relations to be preserved in summaries .,"To the best of our knowledge , this is the first attempt at comparing various neural architectures for this purpose ; we study the effectiveness of several important components , including the vocabulary size , a coveragebased regularizer , and a beam search with reference mechanism ; through extensive experiments we demonstrate that incorporating syntactic information in neural sentence summarization is effective .",introduction
sentiment_analysis,26,"Yet , on some languages , we nevertheless note improvements over the CNN baseline .",analysis,Results and Analysis,0,190,44,44,0,analysis : Results and Analysis,0.7755102040816326,0.5641025641025641,0.676923076923077,Yet on some languages we nevertheless note improvements over the CNN baseline ,13,"Hence , it is important to feed genuine sentiment cues into the memory module .","In these cases , even if similarities between pairs of sentiment vectors initially do not carry any significance , backpropagation may have succeeded in updating the sentiment embedding matrix such that eventually the memory module becomes able to discern salient patterns in the data .",result
question_generation,0,"To generate a question with respect to a specific answer in a sentence , we propose using answer position feature to locate the target answer .",approach,Feature-Rich Encoder,0,37,12,9,0,approach : Feature-Rich Encoder,0.2032967032967033,0.32432432432432434,0.5,To generate a question with respect to a specific answer in a sentence we propose using answer position feature to locate the target answer ,25,"Lastly , the output sequence of the encoder is the concatenation of the two sequences , i.e. , Answer Position Feature","In this work , the BIO tagging scheme is used to label the position of a target answer .",method
question_similarity,0,"In this experiments set , we use the RNN cell type that gives the best results in Section 3.2 ( ON - LSTM with chunk size 8 ) and the same model structure described in Section 2.3 to explore the effect of data augmentation steps mentioned in Section 2.2 .",experiment,Effect of Data Augmentation,0,102,22,2,0,experiment : Effect of Data Augmentation,0.7391304347826086,0.5789473684210527,0.18181818181818185,In this experiments set we use the RNN cell type that gives the best results in Section 3 2 ON LSTM with chunk size 8 and the same model structure described in Section 2 3 to explore the effect of data augmentation steps mentioned in Section 2 2 ,49, ,"The data augmentation steps have an effect on two factors , the training time and the accuracy measurement ( F1 - score ) .",experiment
natural_language_inference,88,"Arrows denote which word is being focused when attention is computed , but not the direction of the relation .",model,Language Modeling,0,170,25,25,0,model : Language Modeling,0.6882591093117408,0.7575757575757576,0.7575757575757576,Arrows denote which word is being focused when attention is computed but not the direction of the relation ,19,Bold lines indicate higher attention scores .,two baselines and the LSTM by a significant margin .,method
relation_extraction,2,"In other words , we set W 1 = W 2 , b 1 = b 2 .",model,Model Architecture,0,71,13,13,0,model : Model Architecture,0.5259259259259259,0.5416666666666666,0.5416666666666666,In other words we set W 1 W 2 b 1 b 2 ,14,"We make W 1 and W 2 , b 1 and b 2 share the same parameters .","For the final hidden state vector of the first token ( i.e. ' [ CLS ] ' ) , we also add an activation operation and a fully connected layer , which is formally expressed as :",method
sentiment_analysis,16,"In this manner , targeted - context detection ( attention ) and TCS interaction are jointly modeled and work together for sentiment inference .",approach,approach,0,164,42,42,0,approach : approach,0.5157232704402516,0.5121951219512195,0.5121951219512195,In this manner targeted context detection attention and TCS interaction are jointly modeled and work together for sentiment inference ,20,Note that ? is still needed to control the importance of different contexts .,The proposed techniques introduced below also follow this core idea but with different implementations or properties .,method
natural_language_inference,54,We then feed the concatenated vectors to a stacked bi-directional LSTM with two layers to obtain the final representations of context words .,methodology,Methodology,0,52,23,23,0,methodology : Methodology,0.22707423580786024,0.8846153846153846,0.8846153846153846,We then feed the concatenated vectors to a stacked bi directional LSTM with two layers to obtain the final representations of context words ,24,"We then represent each context word as the concatenation of the embedding vector obtained from the embedding layer , the atten - The Annual Conference , ... , is the basic unit of organization within the UMC . tion vector obtained from the context - to - question attention and the context vector obtained from the question - to - context attention .",We note that our proposed structural embedding of syntactic trees can be easily applied to any attention approaches mentioned above .,method
relation_extraction,7,GRU size m 230,experiment,Experimental Settings,1,195,6,6,0,experiment : Experimental Settings,0.752895752895753,0.14634146341463414,0.6,GRU size m 230,4,We follow experienced settings for other parameters because they make little influence to our model performance .,"Word embedding dimension k 50 POS embedding dimension l 5 Batch size n 50 Entity - Task weights ( ? head , ? tail ) 0.5,0.5 Entity - Relation Task weight ? 0.3 Learning rate lr 0.001 Dropout probability p 0.5 l 2 penalty ? 0.0001",experiment
natural_language_inference,23,"EM measures how many predicted answers exactly match the correct answer , while F1 score measures the weighted average of the precision and recall at token level .",result,MAIN RESULTS,0,233,5,5,0,result : MAIN RESULTS,0.4541910331384015,0.07936507936507936,0.4545454545454545,EM measures how many predicted answers exactly match the correct answer while F1 score measures the weighted average of the precision and recall at token level ,27,Two official evaluation criteria are used : Exact Match ( EM ) and F1 score .,The evaluation results for our model and other competing approaches are shown in .,result
relation_extraction,10,The learning rate is annealed by 1 % every 100 iterations .,model,Span Pruning,1,157,13,8,0,model : Span Pruning,0.8177083333333334,0.3023255813953488,0.8,The learning rate is annealed by 1 every 100 iterations ,11,"Learning Learning is done with Adam ( Kingma and Ba , 2015 ) with default parameters .",Minibatch Size is 1 .,method
natural_language_inference,46,"We have provided baseline and benchmark results for both sets of tasks , demonstrating that while existing models give sensible results out of the box on summaries , they do not get any traction on the book - scale tasks .",Figure 1:,Review of Reading Comprehension Datasets and Models,0,295,22,17,0,Figure 1: : Review of Reading Comprehension Datasets and Models,0.9932659932659932,0.9166666666666666,0.8947368421052632,We have provided baseline and benchmark results for both sets of tasks demonstrating that while existing models give sensible results out of the box on summaries they do not get any traction on the book scale tasks ,38,"Likewise , neural models for mapping documents to answers , or determining entailment between supporting evidence and a hypothesis , typically operate on the scale of sentences rather than sets of paragraphs .","Having given a quantitative and qualitative analysis of the difficulty of the more complex tasks , we suggest research directions that may help bridge the gap between existing models and hu-man performance .",others
semantic_parsing,0,"However , compared to other large , realistic datasets such as ImageNet for object recognition and SQuAD for reading comprehension , creating such SP dataset is even more time - consuming and challenging in some aspects due to the following reasons .",introduction,introduction,0,28,17,17,0,introduction : introduction,0.10071942446043164,0.53125,0.53125,However compared to other large realistic datasets such as ImageNet for object recognition and SQuAD for reading comprehension creating such SP dataset is even more time consuming and challenging in some aspects due to the following reasons ,38,"In order to test a model 's real semantic parsing performance on unseen complex programs and its ability to generalize to new domains , an SP dataset that includes a large amount of complex programs and data bases with multiple tables is a must .","First , it is hard to find many data bases with multiple tables online .",introduction
question-answering,3,"In this subsection , we present some experiments to demonstrate the properties of our model , and find a good configuration that we use to evaluate our final model .",model,Model Properties,0,174,4,4,0,model : Model Properties,0.6850393700787402,0.057142857142857134,0.13793103448275862,In this subsection we present some experiments to demonstrate the properties of our model and find a good configuration that we use to evaluate our final model ,28,The choice of these options may affect the final performance .,All the experiments in this subsection were performed on the QASent dataset and evaluated on the development set .,method
natural_language_inference,33,We train this model on several data sources with multiple training objectives on over 100 million sentences .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.03448275862068965,0.7,0.7,We train this model on several data sources with multiple training objectives on over 100 million sentences ,18,"In this work , we present a simple , effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model .",Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods .,abstract
passage_re-ranking,1,"Despite training on a fraction of the data available , the proposed BERT - based models surpass the previous state - of - the - art models by a large margin on both of the tasks .",result,RESULTS,1,70,3,3,0,result : RESULTS,0.9589041095890408,0.5,0.5,Despite training on a fraction of the data available the proposed BERT based models surpass the previous state of the art models by a large margin on both of the tasks ,32,We show the main result in .,Training size vs performance :,result
sentence_classification,1,"? attention "" refers to the model without attention - based pooling , i.e. , in the sentence encoding layer , the final hidden state is used for the HSLN - RNN model while maxpooling is used for the HSLN - CNN model .",result,Results and Discussion,0,137,14,14,0,result : Results and Discussion,0.791907514450867,0.3333333333333333,0.3414634146341464, attention refers to the model without attention based pooling i e in the sentence encoding layer the final hidden state is used for the HSLN RNN model while maxpooling is used for the HSLN CNN model ,38,"? context "" is our model without the context enriching layer . "" ? seq. opt. "" is our model without the label sequence optimization layer . "" ? dropout reg. ' is our model using the standard dropout strategy without the expectation - linearization regularization .","and 7 detail the results of classification for each label in terms of performance scores ( precision , recall and F1 ) and confusion matrix , respectively ( for our HSLN - RNN model trained on the PubMed 20 k dataset ) .",result
natural_language_inference,84,"shows that , as expected , dictionary - enabled models significantly outperform baseline models for sentences containing rare words .",system description,ENTAILMENT PREDICTION,1,178,111,24,0,system description : ENTAILMENT PREDICTION,0.7739130434782608,1.0,1.0,shows that as expected dictionary enabled models significantly outperform baseline models for sentences containing rare words ,17,One can see that fruit words tend to be separated from tool and vehicle words ., ,method
sentiment_analysis,4,Emotional dynamics in conversations consist of two important properties : self and inter-personal dependencies ( Morris and .,introduction,introduction,0,22,13,13,0,introduction : introduction,0.06748466257668713,0.2954545454545455,0.2954545454545455,Emotional dynamics in conversations consist of two important properties self and inter personal dependencies Morris and ,17,"Here , utterances are units of speech bounded by breaths or pauses of the speaker .","Self - dependencies , also known as emotional inertia , deal with the aspect of emotional influence that speakers have on themselves during conversations .",introduction
relation-classification,9,All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library .,system description,SCIBERT,0,75,18,12,0,system description : SCIBERT,0.5102040816326531,0.3829787234042553,0.7058823529411765,All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library ,16,The BASEVOCAB models take 2 fewer days of training because they are n't trained from scratch .,All our models ( Sections 3.4 and 3.5 ) are implemented in PyTorch using AllenNLP .,method
relation-classification,0,We treat a pair as a negative relation when the detected entities are wrong or when the pair has no relation .,model,Relation Classification,0,111,63,5,0,model : Relation Classification,0.4911504424778761,0.8181818181818182,0.2631578947368421,We treat a pair as a negative relation when the detected entities are wrong or when the pair has no relation ,22,"For each relation candidate , we realize the dependency layer d p ( described above ) corresponding to the path between the word pair pin the relation candidate , and the NN receives a relation candidate vector constructed from the output of the dependency tree layer , and predicts its relation label .","We represent relation labels by type and direction , except for negative relations that have no direction .",method
relation_extraction,5,We present the top scoring edges in .,model,Understanding Model Behavior,0,204,6,6,0,model : Understanding Model Behavior,0.7756653992395437,0.2727272727272727,0.75,We present the top scoring edges in ,8,"To further understand what dependency edges contribute most to the classification of different relations , we scored each dependency edge by summing up the number of dimensions each of its connected nodes contributed to h sent .","As can be seen in the table , most of these edges are associated with indicative nouns or verbs of each relation .",method
sentiment_analysis,51,We compare our results with the standard RNN and the more sophisticated RNTN .,model,model,0,131,6,6,0,model : model,0.8733333333333333,0.5454545454545454,0.5454545454545454,We compare our results with the standard RNN and the more sophisticated RNTN ,14,Various types of recursive neural networks ( RNN ) have been applied on SST .,"Both of them were trained on SST from scratch , without pretraining .",method
topic_models,0,"We trained SMM , Bayesian SMM and NVDM on Fisher data until convergence .",result,result,0,344,35,35,0,result : result,0.8349514563106796,0.7291666666666666,0.7291666666666666,We trained SMM Bayesian SMM and NVDM on Fisher data until convergence ,13,The following experiment illustrates the idea of ESM :,"At regular checkpoints during the training , we froze the model , extracted the embeddings for both training and test data .",result
sentiment_analysis,43,"MemNet continuously learns the attended vector on the context word embedding memory , and updates the query vector at each hop .",performance,Overall Performance Comparison,0,201,10,10,0,performance : Overall Performance Comparison,0.8375,0.5,0.5,MemNet continuously learns the attended vector on the context word embedding memory and updates the query vector at each hop ,21,Our method consistently performs better than IAN since we utilize the finegrained attention vectors to relieve the information loss in IAN .,"BILSTM - ATT - G models left context and right context using attention - based LSTMs , which achieves better performance than MemNet .",result
sentiment_analysis,28,"IAN learns the representations of the target and context with two LSTMs and attentions interactively , which generates the representations for targets and contexts with respect to each other .",model,model,1,146,13,13,0,model : model,0.8111111111111111,0.52,0.52,IAN learns the representations of the target and context with two LSTMs and attentions interactively which generates the representations for targets and contexts with respect to each other ,29,"Wang et al. , 2016 ) strengthens the effect of target embeddings , which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification .",RAM strengthens Mem - Net by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sentence representation .,method
passage_re-ranking,0,"Focusing on question answering , we train a sequence - to - sequence model , that given a document , generates possible questions that the document might answer .",introduction,introduction,1,25,15,15,0,introduction : introduction,0.20491803278688525,0.625,0.625,Focusing on question answering we train a sequence to sequence model that given a document generates possible questions that the document might answer ,24,"In this paper , we explore an alternative approach based on enriching the document representation ( prior to indexing ) .",An overview of the proposed method is shown in .,introduction
relation-classification,3,"For the DREC dataset , we use two evaluation methods .",experiment,Experimental setup,1,117,38,38,0,experiment : Experimental setup,0.8540145985401459,0.8085106382978723,0.8085106382978723,For the DREC dataset we use two evaluation methods ,10,These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model .,"In the boundaries evaluation , the baseline has an improvement of ? 3 % on both tasks compared to , whose quadratic scoring layer complicates NER .",experiment
natural_language_inference,45,"Hashtags and HTML tags have been removed from the body of the tweet , and user names and URLs are replaced with special tokens .",experiment,EVALUATING WORD-CHARACTER GATING ON TWITTER,0,141,9,6,0,experiment : EVALUATING WORD-CHARACTER GATING ON TWITTER,0.7085427135678392,0.45,0.35294117647058826,Hashtags and HTML tags have been removed from the body of the tweet and user names and URLs are replaced with special tokens ,24,The Twitter dataset consists of English tweets with at least one hashtag from Twitter .,"The dataset contains 2 million tweets for training , 10 K for validation and 50 K for testing , with a total of 2,039 distinct hashtags .",experiment
sentiment_analysis,9,"where is the number of sentiment categories , and represents the polarity predicted by aspect polarity classifier .",system description,Aspect Polarity Classifier,0,155,5,5,0,system description : Aspect Polarity Classifier,0.5595667870036101,0.625,1.0,where is the number of sentiment categories and represents the polarity predicted by aspect polarity classifier ,17,then a Softmax operation is applied to predict the sentiment polarity ., ,method
natural_language_inference,95,6 ] A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies .,introduction,introduction,0,43,35,35,0,introduction : introduction,0.18376068376068366,0.603448275862069,0.603448275862069,6 A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies ,28,You can obtain a pure culture by picking out a small portion of the mixed culture . . .,pure culture is a culture consisting of only one strain .,introduction
sentiment_analysis,29,"Because of its capacity of learning representations from data without feature engineering , neural networks are becoming popular in this task .",introduction,introduction,0,23,15,15,0,introduction : introduction,0.13218390804597702,0.2830188679245283,0.2830188679245283,Because of its capacity of learning representations from data without feature engineering neural networks are becoming popular in this task ,21,The other type is based on neural networks using end - to - end training without any prior knowledge .,"Because of advantages of neural networks , we approach this aspect level sentiment classification problem based on long short - term memory ( LSTM ) neural networks .",introduction
natural_language_inference,31,"The sequence encoder , implemented by convolutional networks , models local and phrase - level semantics , which helps to build correct alignment for each position .",analysis,Analysis,0,223,40,40,0,analysis : Analysis,0.8079710144927537,0.6557377049180327,0.6557377049180327,The sequence encoder implemented by convolutional networks models local and phrase level semantics which helps to build correct alignment for each position ,23,These features connect with the first stage of text matching .,"For example , consider the pair "" A red car is next to a green house "" and "" A red car is parked near a house "" .",result
relation_extraction,8,"Since the parameters for all the model are determined by grid search , we can observe that CNNs can not achieve competitive results compared to PCNNs when increasing the size of the hidden layer of convolutional neural networks .",evaluation,Manual Evaluation,0,257,21,21,0,evaluation : Manual Evaluation,0.9553903345724908,0.75,0.75,Since the parameters for all the model are determined by grid search we can observe that CNNs can not achieve competitive results compared to PCNNs when increasing the size of the hidden layer of convolutional neural networks ,38,"Moreover , compared with CNNs + MIL , PCNNs achieve slightly higher precision when the recall is greater than 0.08 .",It means that we can not capture more useful information by simply increasing the network parameter .,result
natural_language_inference,57,"We also evaluate against EOP classifier , a 2 - class baseline introduced by , and against a version of our model where our order-violation penalty is replaced with the symmetric cosine distance , order - embeddings ( symmetric ) .",result,RESULTS,0,113,11,11,0,result : RESULTS,0.6569767441860465,0.24444444444444444,0.7857142857142857,We also evaluate against EOP classifier a 2 class baseline introduced by and against a version of our model where our order violation penalty is replaced with the symmetric cosine distance order embeddings symmetric ,35,We did not backpropagate through the skip - thought encoder .,The results for all models are shown in .,result
natural_language_inference,23,"With this fully - aware attention , we put forward a multi -level attention mechanism to understand the information in the question , and exploit it layer by layer on the context side .",introduction,introduction,1,41,25,25,0,introduction : introduction,0.07992202729044834,0.7352941176470589,0.7352941176470589,With this fully aware attention we put forward a multi level attention mechanism to understand the information in the question and exploit it layer by layer on the context side ,31,This leads to an attention that thoroughly captures the complete information between the question and the context .,"All of these innovations are integrated into a new end - to - end structure called FusionNet in , with details described in Section 3 .",introduction
relation-classification,5,The first six rows in compare our results with previous state - of - the - art published results on the same test set .,result,Main results,0,83,3,3,0,result : Main results,0.7345132743362832,0.10344827586206896,0.10344827586206896,The first six rows in compare our results with previous state of the art published results on the same test set ,22,End - to - end results :,"In particular , our model obtains 2 + % absolute higher NER and RC scores ( Setup 1 ) than the BiLSTM - CRF - based multihead selection model .",result
natural_language_inference,93,The hidden size used to compute attention scores is also 75 .,implementation,Implementation Details,1,113,11,11,0,implementation : Implementation Details,0.5458937198067633,0.6470588235294118,0.7857142857142857,The hidden size used to compute attention scores is also 75 ,12,The hidden vector length is set to 75 for all layers .,We also apply dropout between layers with a dropout rate of 0.2 .,experiment
sentiment_analysis,47,"Accordingly , the LCR - Rot model is composed of three Bi - LSTMs , i.e. , left - , center - , and right - Bi - LSTM , respectively modeling left context , target phrase and right context in the sentence .",model,Left-Center-Right Separated LSTMs,0,80,10,8,0,model : Left-Center-Right Separated LSTMs,0.372093023255814,0.2222222222222222,0.6666666666666666,Accordingly the LCR Rot model is composed of three Bi LSTMs i e left center and right Bi LSTM respectively modeling left context target phrase and right context in the sentence ,32,"Taking Example 1 for instance , with respect to the target "" the life of battery "" , left context is "" i am pleased with "" , target phrase is "" the life of battery "" , right context is "" , but the windows 8 operating system is so bad . "" ; with respect to the target "" windows 8 operating system "" , left context is "" i am pleased with the life of battery , but the "" , target phrase is "" windows 8 operating system "" , right context is "" is so bad . "" .","Specifically , each word is represented as word embedding .",method
text-classification,7,"Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling against the dollar and mark , dealers said .",ablation,Interest Rates,0,195,45,3,0,ablation : Interest Rates,0.8024691358024691,0.8035714285714286,0.25,Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling against the dollar and mark dealers said ,20,Money / Foreign Exchange Interest rates on the London money market were slightly firmer on news U.K .,"They said this had come as a surprise and expected the targets , 2.90 marks and 1.60 dlrs , to be promptly tested in the foreign exchange markets .",result
natural_language_inference,53,"Many modern NLP systems rely on word embeddings , previously trained in an unsupervised manner on large corpora , as base features .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.019230769230769232,0.2857142857142857,0.2857142857142857,Many modern NLP systems rely on word embeddings previously trained in an unsupervised manner on large corpora as base features ,21, ,"Efforts to obtain embeddings for larger chunks of text , such as sentences , have however not been so successful .",abstract
natural_language_inference,60,"From early distributional semantic models to deep learningbased word embeddings , word - level meaning representations have found applications in a wide variety of core NLP tasks , to the extent that they are now ubiquitous in the field .",introduction,introduction,0,10,3,3,0,introduction : introduction,0.05181347150259067,0.13043478260869565,0.13043478260869565,From early distributional semantic models to deep learningbased word embeddings word level meaning representations have found applications in a wide variety of core NLP tasks to the extent that they are now ubiquitous in the field ,37,It is no exaggeration to say that word embeddings have revolutionized NLP .,sprawling literature has emerged about what types of embeddings are most useful for which tasks .,introduction
natural_language_inference,51,Ablation study 's learning losses with mean and error bar are plotted in .,Details on Few-shot Learning Task,Details on bAbI Task,0,265,11,9,0,Details on Few-shot Learning Task : Details on bAbI Task,1.0,1.0,1.0,Ablation study s learning losses with mean and error bar are plotted in ,14,"For all tasks , ? t is fixed to 0.1 , reducing with decay rate of 0.9 .", ,others
sentiment_analysis,42,"Among these four location - based models , we prefer Model 2 as it is intuitive and has less computation cost without loss of accuracy .",method,Effects of Location Attention,0,204,20,15,0,method : Effects of Location Attention,0.8095238095238095,0.9090909090909092,0.8823529411764706,Among these four location based models we prefer Model 2 as it is intuitive and has less computation cost without loss of accuracy ,24,All these models perform comparably when the number of hops is larger than five .,We also find that Model 4 is very sensitive to the choice of neural gate .,method
topic_models,0,"According to our generative process , we assume that every document x is a sample from Multinomial distribution ( 3 ) , and the log -likelihood is given as follows :",model,model,0,69,14,14,0,model : model,0.1674757281553398,0.2692307692307692,0.3043478260869565,According to our generative process we assume that every document x is a sample from Multinomial distribution 3 and the log likelihood is given as follows ,27,"In numerator of ( 4 ) , p ( w ) represents prior distribution of document embeddings ( 1 ) and p ( x |w ) represents the likelihood of observed data .",where ti represents a row in matrix T .,method
natural_language_inference,23,Additional ablation study on input vectors can be found in Appendix C. Detailed experimental settings can be found in Appendix E.,experiment,EXPERIMENTS,0,219,5,5,0,experiment : EXPERIMENTS,0.4269005847953216,1.0,1.0,Additional ablation study on input vectors can be found in Appendix C Detailed experimental settings can be found in Appendix E ,22,"Finally , we conduct experiments to validate the effectiveness of our proposed components .", ,experiment
topic_models,0,"Let M denote a matrix of class means , with representing a column .",training,training,0,168,61,61,0,training : training,0.4077669902912621,0.7349397590361446,0.7349397590361446,Let M denote a matrix of class means with representing a column ,13,"GLC assumes that every class is Gaussian distributed with a specific mean , and a shared precision matrix D.",GLC is described by the following model :,experiment
text_summarization,8,"Throughout this paper , we consider a set of pairs of texts ( X , Y ) where x ? X corresponds to source tokens x 1 , . . . , x n and y ? Y to a summary y 1 , . . . , y m with m n .",system description,Background: Neural Summarization,0,72,2,2,0,system description : Background: Neural Summarization,0.2517482517482518,0.02197802197802198,0.2,Throughout this paper we consider a set of pairs of texts X Y where x X corresponds to source tokens x 1 x n and y Y to a summary y 1 y m with m n ,38, ,"Throughout this paper , we consider a set of pairs of texts ( X , Y ) where x ? X corresponds to source tokens x 1 , . . . , x n and y ? Y to a summary y 1 , . . . , y m with m n .",method
natural_language_inference,91,"When a reduce operation is performed , the vector representations of two tree nodes are popped off of the stack and fed into a composition function , which is a neural network function that produces a representation for a new tree node that is the parent of the two popped nodes .",system description,Composition and representation,0,59,6,6,0,system description : Composition and representation,0.2532188841201717,0.14634146341463414,0.4,When a reduce operation is performed the vector representations of two tree nodes are popped off of the stack and fed into a composition function which is a neural network function that produces a representation for a new tree node that is the parent of the two popped nodes ,50,"Correspondingly , its reduce operation combines two vector representations from the stack into another vector using a neural network function .",This new node is pushed onto the stack .,method
relation_extraction,1,"In this line of research on dependency - based SRL , previous papers seldom report the accuracy of predicate dis ambiguation separately ( results are often mixed with argument identification and classification ) , causing difficulty in determining the source of gains .",experiment,Dependency-Based SRL Results,0,70,8,4,0,experiment : Dependency-Based SRL Results,0.7865168539325843,0.5714285714285714,0.4444444444444444,In this line of research on dependency based SRL previous papers seldom report the accuracy of predicate dis ambiguation separately results are often mixed with argument identification and classification causing difficulty in determining the source of gains ,38,The predicate sense dis ambiguation subtask applies only to the CoNLL 2009 benchmark .,"Here , we report predicate dis ambiguation accuracy in .",experiment
natural_language_inference,87,"With the guidance of syntax , the keywords name , legislation and 1850 in the question are highlighted , and ( the ) Missouri , and Compromise in the passage are also paid great attention , which is exactly the right answer .",Effect of Answering Long Questions,Visualization,0,157,10,3,0,Effect of Answering Long Questions : Visualization,0.8920454545454546,0.9090909090909092,0.75,With the guidance of syntax the keywords name legislation and 1850 in the question are highlighted and the Missouri and Compromise in the passage are also paid great attention which is exactly the right answer ,36,"To have an insight that how syntax - guided attention works , we draw attention distributions of the vanilla attention of the last layer of BERT and our proposed syntaxguided self - attention 9 , as shown in .","The visualization verifies that benefiting from syntax - guided attention layer , our model is effective at selecting the vital parts , guiding the downstream layer to collect more relevant pieces to make predictions .",others
sentiment_analysis,36,"Finally , the concatenation of r ? i and hi is fed into a fully - connected layer to obtain the i - th targetspecific word representationh i ( l ) :",model,Context-Preserving Mechanism,0,88,41,3,0,model : Context-Preserving Mechanism,0.35200000000000004,0.4361702127659575,0.09375,Finally the concatenation of r i and hi is fed into a fully connected layer to obtain the i th targetspecific word representationh i l ,26,measures the relatedness between the j - th target word representation h ? j and the i - th word - level representation h ( l ) i :,"Finally , the concatenation of r ? i and hi is fed into a fully - connected layer to obtain the i - th targetspecific word representationh i ( l ) :",method
question_answering,5,"This method is based on the hypothesis that the more passages that entail a particular answer , the stronger the evidence for that answer and the higher it should be ranked .",method,Measuring Strength by Count,0,72,21,2,0,method : Measuring Strength by Count,0.2608695652173913,0.2282608695652173,0.2,This method is based on the hypothesis that the more passages that entail a particular answer the stronger the evidence for that answer and the higher it should be ranked ,31, ,To implement this we count the number of occurrences of each answer in the top - K answer spans generated by the baseline QA model and return the answer with the highest count .,method
part-of-speech_tagging,6,"For the i th word w i in the input sentence s , we create an input vector e i which is a concatenation ( ) of the corresponding word embedding and character - based embedding vectors :",model,BiLSTM-based latent feature representations:,0,32,11,6,0,model : BiLSTM-based latent feature representations:,0.21476510067114093,0.3235294117647059,0.2857142857142857,For the i th word w i in the input sentence s we create an input vector e i which is a concatenation of the corresponding word embedding and character based embedding vectors ,34,"of the word type w , computed as :","Then , we feed the sequence of input vectors e 1:n with an additional index i corresponding to a context position into another BiLSTM ( BiLSTM ctx ) , resulting in shared feature vectors vi representing the i th words w i in the sentence s: vi = BiLSTM ctx ( e 1:n , i ) POS tagging :",method
sentiment_analysis,38,has a full list of results on the IMDB dataset .,analysis,Review Sentiment Analysis,0,120,22,22,0,analysis : Review Sentiment Analysis,0.7142857142857143,0.4150943396226415,1.0,has a full list of results on the IMDB dataset ,11,This is an improvement of only 0.58 % over the sentiment unit suggesting that almost all information the model retains that is relevant to sentiment analysis is represented in the very compact form of a single scalar ., ,result
machine-translation,1,"Given source and target sequences sand t with respective lengths | s | and | t| , one first chooses a sufficiently tight upper bound | t| on the target length | t | as a linear function of the source length | s | : |",model,Desiderata,0,85,35,24,0,model : Desiderata,0.4228855721393035,0.5737704918032787,0.48,Given source and target sequences sand t with respective lengths s and t one first chooses a sufficiently tight upper bound t on the target length t as a linear function of the source length s ,37,"We circumvent this issue via a mechanism which we call dynamic unfolding , which works as follows .","The tight upper bound | t| is chosen in such away that , on the one hand , it is greater than the actual length | t | in almost all cases and , on the other hand , it does not increase excessively the amount of computation that is required .",method
sentiment_analysis,35,"For IAN , we report the results in the original paper and use the source codes of other methods for experiments .",baseline,baseline,0,201,6,6,0,baseline : baseline,0.8104838709677419,0.35294117647058826,0.35294117647058826,For IAN we report the results in the original paper and use the source codes of other methods for experiments ,21,It is our proposed base model ( BiLSTM + C2A + Pas ) trained on D t for the target task .,"To investigate the effectiveness of the CFA , we also compare the following transfer methods :",result
sentiment_analysis,11,"Self - influence relates to the concept of emotional inertia , i.e. , the degree to which a person 's feelings carryover from one moment to another .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.061046511627906974,0.2340425531914893,0.2340425531914893,Self influence relates to the concept of emotional inertia i e the degree to which a person s feelings carryover from one moment to another ,26,Emotional dynamics in a conversation is known to be driven by two prime factors : self and interspeaker emotional influence .,Inter- speaker emotional influence is another trait where the other person acts as an influencer in the speaker 's emotional state .,introduction
natural_language_inference,83,We present a hidden variable approach to weigh these aspects in a story 's context .,introduction,introduction,0,54,44,44,0,introduction : introduction,0.17647058823529413,0.9777777777777776,0.9777777777777776,We present a hidden variable approach to weigh these aspects in a story s context ,16,We design linguistic features that incorporate world knowledge and narrative awareness .,We empirically demonstrate that our approach significantly outperforms state - of the - art methods .,introduction
natural_language_inference,49,"The transition layer has a convolution layer with 1 1 kernel for scaling down purpose , followed by a max pooling layer with stride",model,DENSELY INTERACTIVE INFERENCE NETWORK,0,112,66,42,0,model : DENSELY INTERACTIVE INFERENCE NETWORK,0.4409448818897638,0.9705882352941176,0.9545454545454546,The transition layer has a convolution layer with 1 1 kernel for scaling down purpose followed by a max pooling layer with stride,23,The Dense Net block contains n layers of 3 3 convolution layer with growth rate of g .,. The transition scale down ratio in transition layer is ?.,method
semantic_parsing,0,show that template - based approaches can get even higher results .,system description,Related Work and Existing Datasets,0,63,20,20,0,system description : Related Work and Existing Datasets,0.2266187050359713,0.19047619047619047,0.5882352941176471,show that template based approaches can get even higher results ,11,"However , this accuracy is artificially inflated because the model merely needs to decide which template to use during testing .","To avoid getting this inflated result , Finegan - Dollak et al. propose a new , program - based splitting evaluation , where the exact same queries do not appear in both training and testing .",method
sentiment_analysis,47,No - Attention is based on No - Target - Attention .,performance,The Effect of Rotatory Attention,0,190,38,4,0,performance : The Effect of Rotatory Attention,0.8837209302325582,0.6551724137931034,0.16666666666666666,No Attention is based on No Target Attention ,9,"To verify the effectiveness of rotatory attention , we further design the following models based on LCR - Rot :","We continue to remove the target2 context attention mechanism in No - Target - Attention and use the average of hidden states to represent left and right contexts ; 2 . Attention - Reverse is based on LCR - Rot , where we reverse the order of attention .",result
sentiment_analysis,47,The Effect of Rotatory Attention,performance,The Effect of Rotatory Attention,0,187,35,1,0,performance : The Effect of Rotatory Attention,0.8697674418604651,0.603448275862069,0.04166666666666666,The Effect of Rotatory Attention,5, , ,result
sentiment_analysis,38,also raises concern about current evaluation tasks in their recent work which provides a thorough survey of architectures and objectives for learning unsupervised sentence representations - including the above mentioned skip - thoughts .,introduction,introduction,0,47,36,36,0,introduction : introduction,0.2797619047619048,0.8372093023255814,0.8372093023255814,also raises concern about current evaluation tasks in their recent work which provides a thorough survey of architectures and objectives for learning unsupervised sentence representations including the above mentioned skip thoughts ,32,The experimental and evaluation protocols maybe underestimating the quality of unsupervised representation learning for sentences and documents due to certain seemingly insignificant design decisions .,"In this work , we test whether this is the case .",introduction
sentiment_analysis,16,"Different from the targeted - context detection problem which is captured by attention ( discussed in Section 1 ) , here the targetcontext sentiment ( TCS ) interaction measures the sentiment - oriented interaction effect between targets and contexts , which we refer to as TCS interaction ( or sentiment interaction ) for short in the rest of this paper .",approach,approach,0,148,26,26,0,approach : approach,0.4654088050314466,0.3170731707317073,0.3170731707317073,Different from the targeted context detection problem which is captured by attention discussed in Section 1 here the targetcontext sentiment TCS interaction measures the sentiment oriented interaction effect between targets and contexts which we refer to as TCS interaction or sentiment interaction for short in the rest of this paper ,51,The third approach is to formulate explicit target - context sentiment interaction terms .,"Such sentiment interaction is captured by a new set of vectors , and we thus also call such vectors TCS vectors .",method
sentiment_analysis,5,"Meanwhile , both the results of BiHDM and BiDANN indicate the importance of considering the difference between the left and right cerebral hemispheric data for EEG emotion recognition .",experiment,The EEG emotion recognition experiments,0,180,45,15,0,experiment : The EEG emotion recognition experiments,0.6792452830188679,0.5172413793103449,0.42857142857142855,Meanwhile both the results of BiHDM and BiDANN indicate the importance of considering the difference between the left and right cerebral hemispheric data for EEG emotion recognition ,28,"In contrast , the latter ( i.e. , the proposed BiHDM ) focuses on constructing model to learn the discrepancy relation between two hemispheres and these differential components are beneficial for emotion recognition .","To test if the proposed BiHDM is statistically significantly better than the baseline method , paired t- test statistical analysis is conducted at the significant level of 0.05 .",experiment
named-entity-recognition,9,Learning word representations from a large amount of unannotated text is a long - established method .,method,BERT: bidirectional encoder representations from transformers,0,51,5,2,0,method : BERT: bidirectional encoder representations from transformers,0.2562814070351759,0.1,0.18181818181818185,Learning word representations from a large amount of unannotated text is a long established method ,16, ,"While previous models ( e.g. Word2 Vec , GloVe ) focused on learning context independent word representations , recent works have focused on learning context dependent word representations .",method
natural_language_inference,6,Our training data consists of the combination of the following publicly available parallel corpora :,training,Training data,0,190,2,2,0,training : Training data,0.7661290322580645,0.03333333333333333,0.0625,Our training data consists of the combination of the following publicly available parallel corpora ,15, ,Europarl : 21 European languages .,experiment
sentiment_analysis,27,We evaluate the proposed approach on the SemEval 2014 datasets .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.03260869565217391,0.7777777777777778,0.7777777777777778,We evaluate the proposed approach on the SemEval 2014 datasets ,11,"Our model firstly introduces bidirectional attention mechanism with position encoding to model aspect - specific representations between each aspect and its context words , then employs GCN over the attention mechanism to capture the sentiment dependencies between different aspects in one sentence .",Experiments show that our model outperforms the state - of - the - art methods .,abstract
natural_language_inference,89,This is achieved by introducing an independent no - answer loss as :,approach,Independent Span Loss,0,97,31,15,0,approach : Independent Span Loss,0.3730769230769231,0.3522727272727273,0.7142857142857143,This is achieved by introducing an independent no answer loss as ,12,"Therefore , we consider exclusively encouraging the prediction on no -answer detection .",where ? is the sigmoid activation function .,method
machine-translation,4,The decoder is also composed of four identical layers .,model,Model Architecture,0,65,8,8,0,model : Model Architecture,0.2719665271966527,0.2,0.5714285714285714,The decoder is also composed of four identical layers ,10,Each layer consists of a multi-head self - attention and a simple position - wise fully connected feed - forward network .,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sublayer , which performs multi-head attention over the output of the encoder stack .",method
natural_language_inference,88,"Although it is feasible to apply both write and read operations to the memories with attention , we concentrate on the latter .",system description,Long Short-Term Memory-Network,0,95,23,13,0,system description : Long Short-Term Memory-Network,0.38461538461538464,0.3484848484848485,0.40625,Although it is feasible to apply both write and read operations to the memories with attention we concentrate on the latter ,22,This design enables the LSTM to reason about relations between tokens with a neural attention layer and then perform non-Markov state updates .,We conceptualize the read operation as attentively linking the current token to previous memories and selecting useful content when processing it .,method
natural_language_inference,10,"ST Gumbel - Softmax estimator is useful when a model needs to utilize discrete values directly , for example in the case that a model alters its computation path based on samples drawn from a categorical distribution .",model,Gumbel-Softmax,0,95,39,27,0,model : Gumbel-Softmax,0.4008438818565401,0.5,1.0,ST Gumbel Softmax estimator is useful when a model needs to utilize discrete values directly for example in the case that a model alters its computation path based on samples drawn from a categorical distribution ,36,See for the visualization of the forward and backward pass ., ,method
natural_language_inference,0,"The numbers reported for GA Reader are for single best models , though we compare to both ensembles and single models from prior work .",performance,Performance Comparison,0,132,3,3,0,performance : Performance Comparison,0.6700507614213198,0.13636363636363635,0.13636363636363635,The numbers reported for GA Reader are for single best models though we compare to both ensembles and single models from prior work ,24,"Tables 1 and 3 show a comparison of the performance of GA Reader with previously published results on WDW and CNN , Daily Mail , CBT datasets respectively .","GA Reader -- refers to an earlier version of the model , unpublished but described in a preprint , with the following differences - ( 1 ) it does not utilize token - specific attentions within the GA module , as described in equation , ( 2 ) it does not use a character composition model , ( 3 ) it is initialized with word embeddings pretrained on the corpus itself rather than Glo Ve .",result
part-of-speech_tagging,1,Table 2 presents our proposed model in comparison with state - of - the - art results .,method,method,1,214,23,23,0,method : method,0.856,0.5476190476190477,0.5476190476190477,Table 2 presents our proposed model in comparison with state of the art results ,15,State - of - the - art Results,LSTM - CRF is our baseline which uses fine - tuned pre-trained word embeddings .,method
natural_language_inference,45,Our finegrained gating approach achieves new state - of - the - art performance on both settings and outperforms the current state - of - the - art results by up to 1.76 % without using ensembles .,performance,CLOZE-STYLE QUESTIONS,0,166,14,11,0,performance : CLOZE-STYLE QUESTIONS,0.8341708542713567,0.4516129032258064,0.6470588235294118,Our finegrained gating approach achieves new state of the art performance on both settings and outperforms the current state of the art results by up to 1 76 without using ensembles ,32,"We report the results on common noun ( CN ) questions and named entity ( NE ) questions , which are two widely used question categories in CBT .","Our method outperforms the baseline GA reader by up to 2.4 % , which indicates the effectiveness of the fine - grained gating mechanism .",result
sentiment_analysis,37,Sentiment analysis has immense implications in modern businesses through user-feedback mining .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.015810276679841896,0.3333333333333333,0.3333333333333333,Sentiment analysis has immense implications in modern businesses through user feedback mining ,13, ,Large product - based enterprises like Samsung and Apple make crucial business decisions based on the large quantity of user reviews and suggestions available in different e-commerce websites and social media platforms like Amazon and Facebook .,abstract
natural_language_inference,3,The massive scale of the datasets allows us to train a very deep neural networks .,model,model,0,41,18,18,0,model : model,0.1971153846153846,0.1592920353982301,0.9473684210526316,The massive scale of the datasets allows us to train a very deep neural networks ,16,"Compared to the previous works on text matching , we perform extensive empirical studies on two very large datasets .",Experiment results demonstrate that our proposed architecture is more effective than state - of - the - art methods .,method
natural_language_inference,88,"As can be seen , the single - layer LSTMN outperforms these : Examples of intra-attention ( language modeling ) .",model,Language Modeling,0,168,23,23,0,model : Language Modeling,0.680161943319838,0.696969696969697,0.696969696969697,As can be seen the single layer LSTMN outperforms these Examples of intra attention language modeling ,17,Perplexity results for KN5 and RNN are taken from .,Bold lines indicate higher attention scores .,method
natural_language_inference,76,"Hence , using the same LSTM to encode the hypothesis ( in one direction ) and the premise ( in the other direction ) might lead to noise in the training signal .",experiment,EXPERIMENTS,0,119,34,34,0,experiment : EXPERIMENTS,0.8150684931506851,0.9714285714285714,0.9714285714285714,Hence using the same LSTM to encode the hypothesis in one direction and the premise in the other direction might lead to noise in the training signal ,28,We suspect that this is due to entailment being an asymmetric relation .,This could be addressed by training different LSTMs at the cost of doubling the number of model parameters .,experiment
natural_language_inference,80,"The goal of NLI is to identify the logical relationship ( entailment , neutral , or contradiction ) between a premise and a corresponding hypothesis .",introduction,introduction,1,12,3,3,0,introduction : introduction,0.04137931034482759,0.10344827586206896,0.10344827586206896,The goal of NLI is to identify the logical relationship entailment neutral or contradiction between a premise and a corresponding hypothesis ,22,Natural Language Inference ( NLI ; a.k.a. is an important and challenging task for natural language understanding .,shows few example relationships from the Stanford Natural Language Inference ( SNLI ) dataset .,introduction
sentiment_analysis,3,IEMOCAP : Emotional dialogues .,dataset,Datasets and Evaluations,0,193,4,4,0,dataset : Datasets and Evaluations,0.660958904109589,0.5714285714285714,0.5714285714285714,IEMOCAP Emotional dialogues ,4,The statistics are reported in .,"The emotion labels include neutral , happiness , sadness , anger , frustrated , and excited .",experiment
text-classification,1,"Altogether , elimination of the word embedding layer was found to be useful ; thus , we base our work on one - hot LSTM .",system description,Elimination of the word embedding layer,0,90,44,10,0,system description : Elimination of the word embedding layer,0.3515625,0.22,1.0,Altogether elimination of the word embedding layer was found to be useful thus we base our work on one hot LSTM ,22,"We will show later , however , that this type of approach falls behind our approach of learning region embeddings through training one - hot LSTM on unlabeled data .", ,method
sentiment_analysis,15,RNTN : Recursive Neural Tensor Network,model,RNTN:Recursive Neural Tensor Network,0,140,51,1,0,model : RNTN:Recursive Neural Tensor Network,0.5185185185185185,0.4766355140186916,0.0625,RNTN Recursive Neural Tensor Network,5, , ,method
sentiment_analysis,41,TC - LSTM : TC - LSTM extended TD - LSTM by incorporating a target into the representation of a sentence .,model,Models,0,185,6,6,0,model : Models,0.8295964125560538,0.5,0.5,TC LSTM TC LSTM extended TD LSTM by incorporating a target into the representation of a sentence ,18,"Since there is no attention mechanism in TD - LSTM , it can not "" know "" which words are important for a given aspect .",It is worth noting that TC - LSTM performs worse than LSTM and TD - LSTM in .,method
text_summarization,11,Abstractive summarization requires the core information at each encoding time step .,system description,Convolutional Gated Unit,0,47,18,9,0,system description : Convolutional Gated Unit,0.3263888888888889,0.4390243902439024,0.28125,Abstractive summarization requires the core information at each encoding time step ,12,"where C refers to the cell state in the LSTM , and g ( ) refers to a non-linear function .","To reach this goal , we implement a gated unit on top of the encoder outputs at each time step , which is a CNN that convolves all the encoder outputs .",method
text_summarization,5,These results largely reduce the informativeness and readability of the generated summaries .,introduction,introduction,0,19,10,10,0,introduction : introduction,0.07539682539682539,0.2777777777777778,0.2777777777777778,These results largely reduce the informativeness and readability of the generated summaries ,13,"For example , 3 % of summaries contain less than 3 words , while there are 4 summaries repeating a word for even 99 times .","In addition , we find seq2seq models usually focus on copying source words in order , without any actual "" summarization "" .",introduction
natural_language_inference,67,"We also set the maximum passage length to be 300 tokens , and pruned all the tokens after the 300 - th token in the training set to save memory and speedup the training process .",implementation,implementation,1,144,10,10,0,implementation : implementation,0.7058823529411765,0.4545454545454545,0.4545454545454545,We also set the maximum passage length to be 300 tokens and pruned all the tokens after the 300 th token in the training set to save memory and speedup the training process ,34,We trained in mini-batch style ( mini - batch size is 180 ) and applied zero - padding to the passage and question inputs in each batch .,This step reduced the training set size by about 1.6 % .,experiment
named-entity-recognition,6,"The authors use an architecture similar to ours , but use a binary gazetteer feature set , while we use our LS representation .",training,Training and Implementation,0,151,14,14,0,training : Training and Implementation,0.7122641509433962,0.42424242424242425,0.4516129032258064,The authors use an architecture similar to ours but use a binary gazetteer feature set while we use our LS representation ,22,shows the development set performance of our final models on each dataset compared to the work of .,"Since our systems involve random initialization , we report the mean as well as the standard deviation over five runs .",experiment
named-entity-recognition,7,We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.0379746835443038,0.5,0.5,We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest ,23,This paper introduces a scalable transition - based method to model the nested structure of mentions .,Our shiftreduce based system then learns to construct the forest structure in a bottom - up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length .,abstract
natural_language_inference,26,"Recently , end - toend SRL system neural models have been introduced .",model,Explicit Contextual Semantics,0,56,22,10,0,model : Explicit Contextual Semantics,0.2641509433962264,0.3666666666666665,0.5263157894736842,Recently end toend SRL system neural models have been introduced ,11,"To parse the predicate - argument structure , we have an NLP task , semantic role labeling ( SRL ) .",These studies tackle argument identification and argument classification in one shot .,method
natural_language_inference,96,"As our task requires world knowledge , we tried a rule - based system on top of the : Performance of all models in accuracy ( % ) .",model,Other models,0,224,33,6,0,model : Other models,0.5743589743589743,0.8461538461538461,0.5,As our task requires world knowledge we tried a rule based system on top of the Performance of all models in accuracy ,23,"For this baseline , we always choose the shortest ending .","All models substantially underperform humans , although performance increases as more context is provided ( left to right ) .",method
text_generation,0,"where Y n 1:t = ( y 1 , . . . , y t ) and Y n t+1:T is sampled based on the roll - out policy G ? and the current state .",system description,SeqGAN via Policy Gradient,0,93,31,16,0,system description : SeqGAN via Policy Gradient,0.28703703703703703,0.2897196261682243,0.25,where Y n 1 t y 1 y t and Y n t 1 T is sampled based on the roll out policy G and the current state ,29,We represent an N - time Monte Carlo search as,"where Y n 1:t = ( y 1 , . . . , y t ) and Y n t+1:T is sampled based on the roll - out policy G ? and the current state .",method
text_summarization,6,LenEmb uses a mechanism to control the summary length by considering the length embedding vector as the input .,method,Comparative Methods,1,204,17,17,0,method : Comparative Methods,0.7786259541984732,0.85,0.85,LenEmb uses a mechanism to control the summary length by considering the length embedding vector as the input ,19,"For the attention based sequence decoding process , RAS - Elman selects Elman RNN as decoder , and RAS - LSTM selects Long Short - Term Memory architecture .",ASC+ FSC 1 ) uses a generative model with attention mechanism to conduct the sentence compression problem .,method
sentiment_analysis,33,"The training of CNN - RNF - LSTM models takes 10 - 20 mins on the Stanford Sentiment Treebank , which is 3 - 8x faster than the training of LSTM models , on an NVIDIA Tesla P100 GPU accelerator .",result,Results,0,89,11,11,0,result : Results,0.7120000000000001,1.0,1.0,The training of CNN RNF LSTM models takes 10 20 mins on the Stanford Sentiment Treebank which is 3 8x faster than the training of LSTM models on an NVIDIA Tesla P100 GPU accelerator ,35,"CNN - RNF models are much faster to train than their corresponding RNN counterparts , despite they have the same numbers of parameters , as RNFs are applied to word sequences of shorter lengths and the computation is parallelizable .", ,result
relation_extraction,4,"For example , the SemEval dataset focuses on semantic relations ( e.g. , Cause - Effect , Component - Whole ) between two nominals .",introduction,introduction,0,74,64,64,0,introduction : introduction,0.35748792270531393,0.7804878048780488,0.7804878048780488,For example the SemEval dataset focuses on semantic relations e g Cause Effect Component Whole between two nominals ,19,"This is mainly because : ( 1 ) these datasets are relatively small for effectively training high - capacity models ( see ) , and ( 2 ) they capture very different types of relations .",One can further argue that it is easy to obtain a large amount of training data using distant supervision .,introduction
sentence_classification,2,"For example , the green class , or the numeric question type , is circled in the Arabic space as it is clearly separated from other classes , while such separation can not be observed in English .",introduction,introduction,0,25,15,15,0,introduction : introduction,0.0992063492063492,0.35714285714285715,0.35714285714285715,For example the green class or the numeric question type is circled in the Arabic space as it is clearly separated from other classes while such separation can not be observed in English ,34,yellow circle signifies a clear separation of a class .,"Meanwhile , location type questions ( in orange ) are better classified in English .",introduction
named-entity-recognition,2,Let D be the domain size of each y i .,model,model,0,48,3,3,0,model : model,0.22535211267605634,0.05084745762711865,0.13043478260869565,Let D be the domain size of each y i ,11,"Let x = [ x 1 , . . . , x T ] be our input text and y = [ y 1 , . . . , y T ] be per-token output tags .","We predict the most likely y , given a conditional model P ( y|x ) .",method
question_answering,2,"With FVTA attention , the model takes into account of the sequential dependency in image or text sequence , respectively , and cross-modal visual - text correlations .",approach,Problem Formulation,0,78,12,11,0,approach : Problem Formulation,0.2815884476534296,0.9230769230769232,0.9166666666666666,With FVTA attention the model takes into account of the sequential dependency in image or text sequence respectively and cross modal visual text correlations ,25,"Given the visualtext sequence input X img , X txt , we obtain a good joint representation by attention model .","Meanwhile , the computed attention weights over input sequences can be utilized to derive meaningful justifications .",method
sentiment_analysis,10,Emotion GRU F1 -+ 55.56 + - 57.38 + + 59.89 : Ablated DialogueRNN for IEMOCAP dataset .,ablation,Party State,0,248,5,2,0,ablation : Party State,0.9649805447470816,0.5555555555555556,0.3333333333333333,Emotion GRU F1 55 56 57 38 59 89 Ablated DialogueRNN for IEMOCAP dataset ,15, ,"As expected , following , party state stands very important , as without its presence the performance falls by 4.33 % .",result
text-classification,0,One of the most obvious facts one could observe from table 4 and figure 3 a is that the bag - of - means model performs worse in every case .,method,Figure 3: Relative errors with comparison models,0,216,114,29,0,method : Figure 3: Relative errors with comparison models,0.9515418502202644,0.9579831932773109,0.8529411764705882,One of the most obvious facts one could observe from table 4 and figure 3 a is that the bag of means model performs worse in every case ,29,Bag - of - means is a misuse of word2vec .,"Comparing with traditional models , this suggests such a simple use of a distributed word representation may not give us an advantage to text classification .",method
topic_models,0,"In Bayesian SMM , the topic - word distributions ( T ) are not Discrete , hence it can model the correlations between words and ( latent ) topics .",model,model,0,220,30,30,0,model : model,0.5339805825242718,0.8108108108108109,0.8108108108108109,In Bayesian SMM the topic word distributions T are not Discrete hence it can model the correlations between words and latent topics ,23,"The topic distributions over vocabulary ? , however , still remained Discrete .","The variational inference in CTM is similar to that of LDA including the mean - field approximation , because of the discrete latent variable z ) .",method
relation-classification,3,"During training , for both EC ( softmax ) and NER tasks ( CRF ) , we minimize the cross - entropy loss L NER .",model,Joint learning as head selection,0,51,18,17,0,model : Joint learning as head selection,0.3722627737226277,0.39130434782608703,0.53125,During training for both EC softmax and NER tasks CRF we minimize the cross entropy loss L NER ,19,"For decoding , in the CRF setting , we use the Viterbi algorithm .","The entity tags are later fed into the relation extraction layer as label embeddings ( see ) , assuming that knowledge of the entity types is beneficial in predicting the relations between the involved entities .",method
sentence_classification,2,"In this paper , we propose the usage of translations as compelling and effective domain - free contexts , or contexts that are always available no matter what the task domain is .",introduction,introduction,1,20,10,10,0,introduction : introduction,0.07936507936507936,0.2380952380952381,0.2380952380952381,In this paper we propose the usage of translations as compelling and effective domain free contexts or contexts that are always available no matter what the task domain is ,30,"Moreover , topics inferred using topic models may produce less useful topics when the data set is domain - specific such as movie review sentiment classification .",We observe two opportunities when using translations .,introduction
part-of-speech_tagging,2,"One appealing property of such systems is their generality , as excellent performance can be achieved with a unified architecture and without task - specific feature engineering .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02808988764044944,0.3333333333333333,0.3333333333333333,One appealing property of such systems is their generality as excellent performance can be achieved with a unified architecture and without task specific feature engineering ,26,Recent papers have shown that neural networks obtain state - of - the - art performance on several different sequence tagging tasks .,"However , it is unclear if such systems can be used for tasks without large amounts of training data .",abstract
natural_language_inference,7,"If the question is only integrated through the inclusion of a passage - independent representation , performance drops drastically .",model,MODEL VARIATIONS,0,132,7,7,0,model : MODEL VARIATIONS,0.7415730337078652,0.15217391304347827,0.7777777777777778,If the question is only integrated through the inclusion of a passage independent representation performance drops drastically ,18,"The passage - aligned question representation is crucial , since lexically similar regions of the passage provide strong signal for relevant answer spans .","The passage - independent question representation over the BiLSTM is less important , but it still accounts for over 3 % exact match and F 1 .",method
text_generation,2,Note that one could design specific structure for different tasks to refine the CNN performance .,training,Training Settings,0,165,7,7,0,training : Training Settings,0.4714285714285714,0.3684210526315789,0.3684210526315789,Note that one could design specific structure for different tasks to refine the CNN performance ,16,"For the discriminator , we choose the CNN architecture as the feature extractor and the binary classifier .","For the synthetic data experiment , the CNN kernel size ranges from 1 to T .",experiment
natural_language_inference,70,The ten questions related to an original question are retrieved using a search engine .,system description,Ranking Features,0,79,50,2,0,system description : Ranking Features,0.4514285714285714,0.5376344086021505,0.6666666666666666,The ten questions related to an original question are retrieved using a search engine ,15, ,We use their absolute and relative ranks 4 as features for subtasks B and C ( for the latter the question rank is given to all the comments within the related question thread ) .,method
sentence_compression,1,bit indicating whether the parent word has 2 https://code.google.com/p/word2vec/,baseline,Baseline,0,109,72,72,0,baseline : Baseline,0.5215311004784688,0.8372093023255814,0.8372093023255814,bit indicating whether the parent word has 2 https code google com p word2vec ,15,The label predicted for the last word ( 3 dimensions ) .,already been seen and kept in the compression ( 1 dimension ) .,result
text_summarization,10,"Moreover , we found 93 more cases where our 2 - way - QG MTL model detects 2 or more additional salient keywords than the pointer baseline model ( as opposed to vice versa ) , showing that sentence - level question generation task is helping the document - level summarization task in finding more salient terms .",ablation,Quantitative Improvements in Entailment,0,214,24,12,0,ablation : Quantitative Improvements in Entailment,0.8136882129277566,0.8275862068965517,0.7058823529411765,Moreover we found 93 more cases where our 2 way QG MTL model detects 2 or more additional salient keywords than the pointer baseline model as opposed to vice versa showing that sentence level question generation task is helping the document level summarization task in finding more salient terms ,50,"The results are shown in Table 10 , where the 2 - way - QG MTL model ( with question generation ) versus baseline improvement is stat. significant ( p < 0.01 ) .","Qualitative Examples on Entailment and Saliency Improvements presents an example of output summaries generated by , our baseline , and our 3 - way multitask model .",result
natural_language_inference,3,Experiment results demonstrate that our proposed architecture is more effective than state - of - the - art methods .,model,model,0,42,19,19,0,model : model,0.20192307692307693,0.16814159292035402,1.0,Experiment results demonstrate that our proposed architecture is more effective than state of the art methods ,17,The massive scale of the datasets allows us to train a very deep neural networks ., ,method
query_wellformedness,0,Query is grammatical .,dataset,Dataset Construction,0,34,8,8,0,dataset : Dataset Construction,0.3541666666666667,0.32,0.32,Query is grammatical ,4,We define a query to be a well - formed natural language question if it satisfies the following :,. Query is an explicit question .,experiment
sentiment_analysis,8,Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.017094017094017096,0.2222222222222222,0.2222222222222222,Identifying emotion from speech is a nontrivial task pertaining to the ambiguous definition of emotion itself ,17, ,"In this work , we adopt a featureengineering based approach to tackle the task of speech emotion recognition .",abstract
topic_models,0,) ULMFiT : The third baseline system is the universal language model fine - tuned for classification ( ULMFiT ) .,baseline,baseline,1,299,16,16,0,baseline : baseline,0.7257281553398058,0.6153846153846154,0.6153846153846154, ULMFiT The third baseline system is the universal language model fine tuned for classification ULMFiT ,17,"The experimental analysis in Section VII - C shows that Bayesian SMM is more robust to over-fitting when compared to SMM and NVDM , and does not require an early stopping mechanism .",The pre-trained 5 model consists of 3 BiLSTM layers .,result
natural_language_inference,53,"We use the same approach as with SICK - E , except that our classifier has only 2 classes .",evaluation,Paraphrase detection The Microsoft Research,0,125,33,5,0,evaluation : Paraphrase detection The Microsoft Research,0.6009615384615384,0.7173913043478259,0.2777777777777778,We use the same approach as with SICK E except that our classifier has only 2 classes ,18,Sentence pairs have been human - annotated according to whether they capture a paraphrase / semantic equivalence relationship .,The caption - image retrieval task evaluates joint image and language feature models .,result
sentiment_analysis,30,"In contrast to memory networks , where each input sentence / word occupies a memory slot and is then accessed via attention independently , recent advances in machine reading suggest that processing inputs sequentially is beneficial to over all performance .",introduction,introduction,0,20,13,13,0,introduction : introduction,0.15625,0.65,0.65,In contrast to memory networks where each input sentence word occupies a memory slot and is then accessed via attention independently recent advances in machine reading suggest that processing inputs sequentially is beneficial to over all performance ,38,"Their models are largely based on memory networks , originally developed for reasoning - focused machine reading comprehension tasks .","However , successful machine reading models may not be directly applicable to TABSA due to the key difference in the granularity of inputs between the two tasks : on the Children 's Book Test corpus ( CBT ) , for example , competitive models take as input a window of text , centred around candidate entities , with crucial information contained within that window .",introduction
natural_language_inference,56,"Mirroring the results from the "" Sort - of - CLEVR "" dataset the MLP perfectly solves the non-relational questions , but struggle with even single jump questions and seem to lower bound the performance of the relational networks .",experiment,Pretty-CLEVR,1,140,61,32,0,experiment : Pretty-CLEVR,0.4166666666666667,0.5700934579439252,0.8888888888888888,Mirroring the results from the Sort of CLEVR dataset the MLP perfectly solves the non relational questions but struggle with even single jump questions and seem to lower bound the performance of the relational networks ,36,For details see the supplementary material .,"The relational network solves the non-relational questions as well as the ones requiring a single jump , but the accuracy sharply drops off with more jumps .",experiment
sentiment_analysis,6,The convolution kernels are thus applied to these concatenated word vectors instead of individual words .,method,text-CNN: Textual Features Extraction,0,77,19,6,0,method : text-CNN: Textual Features Extraction,0.2664359861591695,0.2878787878787879,0.5,The convolution kernels are thus applied to these concatenated word vectors instead of individual words ,16,These vectors are the publicly available 300 - dimensional word2vec vectors trained on 100 billion words from Google News .,Each utterance is wrapped to a window of 50 words which serves as the input to the CNN .,method
sentiment_analysis,42,"To compute sentence representation , researchers develop denoising autoencoder , convolutional neural network , sequence based recurrent neural models and tree - structured neural networks .",system description,Compositionality in Vector Space,0,239,5,5,0,system description : Compositionality in Vector Space,0.9484126984126984,0.4545454545454545,0.8333333333333334,To compute sentence representation researchers develop denoising autoencoder convolutional neural network sequence based recurrent neural models and tree structured neural networks ,22,Yessenalina and Cardie ( 2011 ) use matrix multiplication as compositional function to compute vectors for longer phrases .,Several recent studies calculate continuous representation for documents with neural networks .,method
machine-translation,1,"In more general terms , the size of a representation should be proportional to the amount of information it represents or predicts .",model,Desiderata,0,69,19,8,0,model : Desiderata,0.34328358208955223,0.3114754098360656,0.16,In more general terms the size of a representation should be proportional to the amount of information it represents or predicts ,22,This is to avoid burdening the model with an additional memorization step before translation .,"Third , the path traversed by forward and backward signals in the network ( between input and ouput tokens ) should be short .",method
natural_language_inference,42,We pre-processed the texts with the NLTK Tokenizer .,experiment,experiment,1,219,4,4,0,experiment : experiment,0.7577854671280276,0.4,0.4,We pre processed the texts with the NLTK Tokenizer ,10,We developed our model in Tensorflow and made it available at https://worksheets.codalab.org/worksheets/ 0xee647ea284674396831ecb5aae9ca297 / for replicability .,"As suggested in , we have chosen the Adam optimizer with the same hyperparameters , except for the learning rate , which was divided by two in our implementation .",experiment
sentiment_analysis,50,"As a result , the macro - F1 scores on D3 and D4 are affected more .",model,Model Comparison,0,107,9,9,0,model : Model Comparison,0.6604938271604939,0.6,0.6,As a result the macro F1 scores on D3 and D4 are affected more ,15,"Thus , the precision and recall on neutral class will be largely affected by even a small prediction difference ( e.g. , with 5 more neutral examples correctly identified , recall is increased by more than 10 % on both datasets ) .",indicates that a large percentage of the performance gain comes from PRET .,method
sentiment_analysis,17,"We define the LSTM unit at each time step t to be a collection of vectors in Rd : an input gate it , a forget gate ft , an output gate o t , a memory cell ct and a hidden state ht .",system description,Overview,0,52,13,12,0,system description : Overview,0.2311111111111111,0.16666666666666666,0.5714285714285714,We define the LSTM unit at each time step t to be a collection of vectors in Rd an input gate it a forget gate ft an output gate o t a memory cell ct and a hidden state ht ,41,"While numerous LSTM variants have been described , here we describe the version used by .","The entries of the gating vectors it , ft and o tare in [ 0 , 1 ] .",method
passage_re-ranking,0,https://github.com/dfcf93/MSMARCO /,experiment,experiment,0,74,14,14,0,experiment : experiment,0.6065573770491803,0.6363636363636364,0.6363636363636364,https github com dfcf93 MSMARCO ,6,We then index and rank the expanded documents exactly as in the BM25 method above .,tree / master / Ranking 3 http://anserini.io/,experiment
natural_language_inference,59,"We tuned the following hyperparameters by grid search on the development set ( settings for our best model are in parenthesis ) : embedding dimension ( 300 ) , shape of all feedforward networks ( two layers with 400 and 200 width ) , character n -gram sizes ( 5 ) , context size ( 1 ) , learning rate ( 0.1 for both pretraining and tuning ) , batch size ( 256 for pretraining and 64 for tuning ) , dropout ratio ( 0.1 for tuning ) and prediction threshold ( positive paraphrase for a score ? 0.3 ) .",hyperparameters,hyperparameters,1,99,2,2,0,hyperparameters : hyperparameters,0.7333333333333333,0.4,0.4,We tuned the following hyperparameters by grid search on the development set settings for our best model are in parenthesis embedding dimension 300 shape of all feedforward networks two layers with 400 and 200 width character n gram sizes 5 context size 1 learning rate 0 1 for both pretraining and tuning batch size 256 for pretraining and 64 for tuning dropout ratio 0 1 for tuning and prediction threshold positive paraphrase for a score 0 3 ,78, ,"We examined whether self - attention helps or not for all model variants , and found that it does for our best model .",experiment
relation-classification,1,"In addition to "" O "" , the other tags consist of three parts : the word position in the entity , the relation type , and the relation role .",method,Method,0,73,8,8,0,method : Method,0.2967479674796748,0.11428571428571427,0.5333333333333333,In addition to O the other tags consist of three parts the word position in the entity the relation type and the relation role ,25,"Tag "" O "" represents the "" Other "" tag , which means that the corresponding word is independent of the extracted results .","We use the "" BIES "" ( Begin , Inside , End , Single ) signs to represent the position information of a word in the entity .",method
natural_language_inference,39,We use three similarity functions for richer measurement .,system description,Pairwise Word Interaction Modeling,0,70,7,7,0,system description : Pairwise Word Interaction Modeling,0.3398058252427185,0.1044776119402985,0.3333333333333333,We use three similarity functions for richer measurement ,9,"Cosine distance ( cos ) measures the distance of two vectors by the angle between them , while L 2 Euclidean distance ( L 2 Euclid ) and dot-product distance ( DotProduct ) measure magnitude differences .",Algorithm 1 provides details of the modeling process .,method
question_generation,1,We experimented with ITML based metric learning for image features .,method,Finding Exemplars,0,106,26,4,0,method : Finding Exemplars,0.2704081632653061,0.2857142857142857,0.5714285714285714,We experimented with ITML based metric learning for image features ,11,"This is obtained through a coarse quantization of nearest neighbors of the training examples into 50 clusters , and selecting the nearest as supporting and farthest as the contrasting exemplars .","Surprisingly , the KNN - based approach outperforms the latter one .",method
natural_language_inference,52,We used embeddings of size 50 as we did not see a performance improvement with higher dimensionality .,baseline,Corpora,0,160,15,9,0,baseline : Corpora,0.6130268199233716,1.0,1.0,We used embeddings of size 50 as we did not see a performance improvement with higher dimensionality ,18,"The training was done using the word2 vec algorithm as implemented by , such that the context for each word in a sentence is composed of all the other words in the same sentence .", ,result
semantic_role_labeling,2,"In , we compare the SRL accuracy with syntactic constraints specified by gold parse or automatic parses .",analysis,BIO Violations,0,193,76,38,0,analysis : BIO Violations,0.8616071428571429,0.8837209302325582,0.7916666666666666,In we compare the SRL accuracy with syntactic constraints specified by gold parse or automatic parses ,17,"Specifically , if the decoded sequence contains k arguments that do not match any unlabeled syntactic constituent , it will receive a penalty of kC , where C is a single parameter dictating how much the model should trust the provided syntax .","When using gold syntax , the predictions improve up to 2 F1 as the penalty increases .",result
sentiment_analysis,15,We use f = tanh in all experiments .,experiment,Experiments,1,208,12,12,0,experiment : Experiments,0.7703703703703704,0.17391304347826084,0.6666666666666666,We use f tanh in all experiments ,8,Initial experiments showed that the recursive models worked significantly worse ( over 5 % drop in accuracy ) when no nonlinearity was used .,"We compare to commonly used methods that use bag of words features with Naive Bayes and SVMs , as well as Naive Bayes with bag of bigram features .",experiment
natural_language_inference,10,Note that we denote the representation of i - th node at t-th layer as rt i = ht i ; ct i .,model,Gumbel Tree-LSTM,0,104,48,9,0,model : Gumbel Tree-LSTM,0.4388185654008439,0.6153846153846154,0.2307692307692308,Note that we denote the representation of i th node at t th layer as rt i ht i ct i ,22,"In Eq. 10 , W leaf ? R 2D h Dx and b leaf ? R 2D h .","Assume that t- th layer consists of Mt node representations : ( r t 1 , , rt Mt ) .",method
text_summarization,3,"The training objective is then to minimize L RL = ( r ( ? ) ? r ( y s ) ) m t =1 log P ( y st |y s 1 , , y s t?1 , x ) ( 10 ) It is noteworthy that the samples y s are selected from a wide range of vocabularies extended by all the concept candidates .",system description,Evaluation based Reinforcement Learning (RL),0,123,68,7,0,system description : Evaluation based Reinforcement Learning (RL),0.5082644628099173,0.5190839694656488,0.5833333333333334,The training objective is then to minimize L RL r r y s m t 1 log P y st y s 1 y s t 1 x 10 It is noteworthy that the samples y s are selected from a wide range of vocabularies extended by all the concept candidates ,52,"In the RL training stage , two separate output candidates at each time step are produced : y sis sampled from the probability distribution P ( y st |y s 1 , , y s t?1 , x ) , and ? is the baseline output .",This strategy ensures that the model learns to generate sequences with higher rewards by better exploring a set of close concepts .,method
sentiment_analysis,10,The rest of the paper is organized as follows :,introduction,introduction,0,32,25,25,0,introduction : introduction,0.1245136186770428,0.9615384615384616,0.9615384615384616,The rest of the paper is organized as follows ,10,We believe that Dialogue RNN outperforms state - of - the - art contextual emotion classifiers such as ) because of better context representation .,"Section 2 discusses related work ; Section 3 provides detailed description of our model ; Sections 4 and 5 present the experimental results ; finally , Section 6 concludes the paper .",introduction
sentiment_analysis,14,We train Emo2 Vec using an end - to - end multi-task learning framework with one larger dataset and several small task - specific datasets .,methodology,Methodology,0,29,2,2,0,methodology : Methodology,0.19078947368421054,0.13333333333333333,0.13333333333333333,We train Emo2 Vec using an end to end multi task learning framework with one larger dataset and several small task specific datasets ,24, ,"The model is divided into two parts : a shared embedding layer ( i.e. Emo2 Vec ) , and task - specific classifiers .",method
natural_language_inference,19,presented a sentence - encoding based model reflecting directional information in a sentence .,introduction,introduction,0,27,12,12,0,introduction : introduction,0.10588235294117647,0.5714285714285714,0.5714285714285714,presented a sentence encoding based model reflecting directional information in a sentence ,13,One of the approaches to solving the NLI task is to use sentence - encoding based models .,"However , the distance between words was not considered at all in their model , and the directional information simply involved words before and after the reference word .",introduction
natural_language_inference,15,We evaluate our proposed architecture on highly competitive benchmark datasets related to sentence matching .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.04424778761061947,0.8888888888888888,0.8888888888888888,We evaluate our proposed architecture on highly competitive benchmark datasets related to sentence matching ,15,"To alleviate the problem of an ever - increasing size of feature vectors due to dense concatenation operations , we also propose to use an autoencoder after dense concatenation .","Experimental results show that our architecture , which retains recurrent and attentive features , achieves state - of - the - art performances for most of the tasks .",abstract
sentiment_analysis,37,The primary distinction between our model and the literature is the consideration of the neighboring aspects in a sentence with the target aspect .,model,Model,0,62,2,2,0,model : Model,0.2450592885375494,0.027027027027027032,0.5,The primary distinction between our model and the literature is the consideration of the neighboring aspects in a sentence with the target aspect ,24, ,"We assume that our inter-aspect relation modeling ( IARM ) architecture 1 models the relation between the target aspect and surrounding aspects , while filtering out irrelevant information .",method
natural_language_inference,51,The KV employs key - value attention yet excludes the regularization loss presented in Eq. ( 6 ) .,result,Ablation study on Associative Recall,0,161,28,5,0,result : Ablation study on Associative Recall,0.6075471698113207,0.4,0.38461538461538464,The KV employs key value attention yet excludes the regularization loss presented in Eq 6 ,16,The meta - network P I in DA generates the attention weight w pt directly .,The training curves over 5 runs are plotted in .,result
sentiment_analysis,4,These features are extracted for each utterance in the conversation and their concatenated vectors serve as the utterance representations .,methodology,Multimodal Feature Extraction,0,113,8,3,0,methodology : Multimodal Feature Extraction,0.34662576687116564,0.08163265306122447,0.5,These features are extracted for each utterance in the conversation and their concatenated vectors serve as the utterance representations ,20,"ICON adopts a multimodal framework and performs feature extraction from three modalities , i.e. , language ( transcripts ) , audio and visual .",The motivation of this setup derives from previous works that demonstrate the effectiveness of multimodal features in creating rich feature representations .,method
sentiment_analysis,35,"All weights in networks are randomly initialized from a uniform distribution U ( ? 0.01 , 0.01 ) .",implementation,implementation,1,189,7,7,0,implementation : implementation,0.7620967741935484,0.5384615384615384,0.5384615384615384,All weights in networks are randomly initialized from a uniform distribution U 0 01 0 01 ,17,Gradients with the 2 norm larger than 40 are normalized to be 40 .,"The batch sizes are 64 and 32 for source and target domains , respectively .",experiment
natural_language_inference,36,"Bowman et al. released Stanford Natural language Inference ( SNLI ) dataset , which is a high - quality and largescale benchmark , thus inspiring various significant work .",system description,Text Comprehension,0,67,31,31,1,system description : Text Comprehension,0.31904761904761897,0.484375,0.8611111111111112,Bowman et al released Stanford Natural language Inference SNLI dataset which is a high quality and largescale benchmark thus inspiring various significant work ,24,"In this task , a model is presented with a pair of sentences and asked to judge the relationship between their meanings , including entailment , neutral and contradiction .","Most of existing NLI models apply attention mechanism to jointly interpret and align the premise and hypothesis , while transfer learning from external knowledge is popular recently .",method
natural_language_inference,35,"As a result , P q and P pare the copy distributions over the extended vocabulary :",model,Dual Attention Layer,0,145,106,73,0,model : Dual Attention Layer,0.5513307984790875,0.7571428571428571,0.7934782608695652,As a result P q and P pare the copy distributions over the extended vocabulary ,16,"For the question , our model uses another identical layer and obtains ? qt ? R J and c qt ? Rd .",where k ( l ) means the passage index corresponding to the l - th word in the concatenated passages .,method
sentiment_analysis,27,Ma et al. design a model which learns the representations of the aspect and context interactively with two attention mechanisms .,introduction,introduction,0,28,17,17,1,introduction : introduction,0.10144927536231883,0.4047619047619048,0.4047619047619048,Ma et al design a model which learns the representations of the aspect and context interactively with two attention mechanisms ,21,mechanism with a gated recurrent unit network to capture the relevance between each context word and the aspect .,"Song et al. propose an attentional encoder network , which employ multi-head attention for the modeling between context and aspect .",introduction
machine-translation,7,This technique allows us to increase the number of experts ( and hence the number of parameters ) by proportionally increasing the number of devices in the training cluster .,performance,THE SHRINKING BATCH PROBLEM,0,121,20,18,0,performance : THE SHRINKING BATCH PROBLEM,0.32439678284182305,0.1724137931034483,0.5625,This technique allows us to increase the number of experts and hence the number of parameters by proportionally increasing the number of devices in the training cluster ,28,Each secondary MoE resides on one device .,"The total batch size increases , keeping the batch size per expert constant .",result
natural_language_inference,43,"Moreover , for the subset of questions for which the right answer can be found in one of the web snippets , we outperform the semantic parser ( 51.9 F 1 vs. 48.5 F 1 ) .",introduction,introduction,0,22,14,14,0,introduction : introduction,0.14193548387096774,0.6086956521739131,0.6086956521739131,Moreover for the subset of questions for which the right answer can be found in one of the web snippets we outperform the semantic parser 51 9 F 1 vs 48 5 F 1 ,35,"We find that a simple QA model , despite having no access to the target KB , performs reasonably well on this dataset ( ? 35 F 1 compared to the state - of - the - art of 41 F 1 ) .","We analyze results for different types of compositionality and find that superlatives and relation composition constructions are challenging for a webbased QA system , while conjunctions and events with multiple arguments are easier .",introduction
text_summarization,11,An example of the summary of the conventional attention - based seq2seq model on the Gigaword dataset .,introduction,introduction,0,17,10,10,0,introduction : introduction,0.11805555555555555,0.4545454545454545,0.4545454545454545,An example of the summary of the conventional attention based seq2seq model on the Gigaword dataset ,17,Gold : fatah officially elects abbas as candidate for presidential election :,"The text highlighted indicates repetition , "" # "" refers to masked number .",introduction
natural_language_inference,45,EVALUATING WORD - CHARACTER GATING ON TWITTER,experiment,EVALUATING WORD-CHARACTER GATING ON TWITTER,0,136,4,1,0,experiment : EVALUATING WORD-CHARACTER GATING ON TWITTER,0.6834170854271356,0.2,0.05882352941176471,EVALUATING WORD CHARACTER GATING ON TWITTER,6, , ,experiment
semantic_role_labeling,4,"Word representations Typical word representations , such as SENNA and GloVe , have been used and contributed to the performance improvement .",system description,system description,0,267,24,24,0,system description : system description,0.89,0.8571428571428571,0.8571428571428571,Word representations Typical word representations such as SENNA and GloVe have been used and contributed to the performance improvement ,20,improved the self - attention SRL model by incorporating syntactic information .,"Recently , integrated contextualized word representation , ELMo , into the model of and improved the performance by 3.2 F1 score .",method
machine-translation,8,"where e ij = a (s i?1 , h j ) is an alignment model which scores how well the inputs around position j and the output at position i match .",system description,DECODER: GENERAL DESCRIPTION,0,71,41,15,0,system description : DECODER: GENERAL DESCRIPTION,0.21450151057401806,0.5694444444444444,0.5,where e ij a s i 1 h j is an alignment model which scores how well the inputs around position j and the output at position i match ,30,The weight ? ij of each annotation h j is computed by,"The score is based on the RNN hidden state s i ?1 ( just before emitting y i , Eq. ( 4 ) ) and the j - th annotation h j of the input sentence .",method
named-entity-recognition,8,"In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .",ablation,ablation,0,381,20,20,0,ablation : ablation,0.9844961240310076,0.7692307692307693,0.7692307692307693,In the table MASK means that we replace the target token with the MASK symbol for MLM SAME means that we keep the target token as is RND means that we replace the target token with another random token ,40,The results are presented in .,"The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training ( BERT uses 80 % , 10 % , 10 % ) .",result
sentiment_analysis,22,"As described in Section 1 , we also introduce position embeddings , which have been widely used in CNN based models , as apart of the inputs to the model .",model,model,0,69,10,10,0,model : model,0.30131004366812225,0.3225806451612903,0.3225806451612903,As described in Section 1 we also introduce position embeddings which have been widely used in CNN based models as apart of the inputs to the model ,28,"Let ? ? be a word embedding lookup table generated by an unsupervised method such as Glo Ve or CBOW , where is the dimension of the word embeddings and is the size of word vocabulary .","Similar as the word embedding layer , the position embedding layer is a ? ? , where is the dimension of the position embeddings and is the number of possible relevant positions between each word and the target .",method
named-entity-recognition,2,"Following previous work , we use the same OntoNotes data split used for co-reference resolution in the CoNLL - 2012 shared task .",evaluation,Data and Evaluation,0,156,3,3,0,evaluation : Data and Evaluation,0.7323943661971831,0.42857142857142855,0.42857142857142855,Following previous work we use the same OntoNotes data split used for co reference resolution in the CoNLL 2012 shared task ,22,We evaluate using labeled data from the CoNLL - 2003 shared task ( Tjong and OntoNotes 5.0 .,"For both datasets , we convert the IOB boundary encoding to BILOU as previous work found this encoding to result in improved performance .",result
natural_language_inference,94,"raising her head and speaking as if she did not see the people around her ) they re all gone now , and there is n't anything more the sea can do tome ... .",training,training,0,289,24,24,0,training : training,0.7545691906005222,0.2033898305084746,0.2033898305084746,raising her head and speaking as if she did not see the people around her they re all gone now and there is n t anything more the sea can do tome ,33,this speech of maurya sis famous in irish drama :,"ll have no call now to be up crying and praying when the wind breaks from the south , and you can hear the surf is in the east , and the surf is in the west , making a great stir with the two noises , and they hitting one on the other .",experiment
sentiment_analysis,1,EEG - Based Emotion Recognition Using Regularized Graph Neural Networks,title,title,1,2,1,1,0,title : title,0.005050505050505051,1.0,1.0,EEG Based Emotion Recognition Using Regularized Graph Neural Networks,9, , ,title
text-classification,8,"The compositional function , X ? z , aims to combine word embeddings into a fixed - length sentence / document representation z .",system description,Models & training,0,52,5,5,0,system description : Models & training,0.1933085501858736,0.07575757575757576,0.8333333333333334,The compositional function X z aims to combine word embeddings into a fixed length sentence document representation z ,19,"Let {v 1 , v 2 , .... , v L } denote the respective word embeddings for each token , where v l ? R K .","These representations are then used to make predictions about sequence X. Below , we describe different types of functions considered in this work .",method
natural_language_inference,34,"To address this , we develop a Graph2 Doc module to keep information flowing from entity back to tokens in the context .",system description,Reasoning with the Fusion Block,0,181,78,48,0,system description : Reasoning with the Fusion Block,0.6135593220338983,0.7722772277227723,0.8727272727272727,To address this we develop a Graph2 Doc module to keep information flowing from entity back to tokens in the context ,22,"However , the unrestricted answer still can not be backtraced .",Therefore the text span pertaining to the answers can be localized in the context .,method
text_summarization,5,"Rerank largely outperforms First , which verifies the effectiveness of the Rerank module .",evaluation,Informativeness Evaluation,1,177,16,16,0,evaluation : Informativeness Evaluation,0.7023809523809523,0.25396825396825395,0.8,Rerank largely outperforms First which verifies the effectiveness of the Rerank module ,13,"As shown in , the performance of Random is terrible , indicating it is impossible to use one summary template to fit various actual summaries .","However , according to Max and Rerank , we find the Rerank performance of Re 3 Sum is far from perfect .",result
text-classification,0,There are a large number categories but most of them contain only few articles .,method,Large-scale Datasets and Results,0,152,50,10,0,method : Large-scale Datasets and Results,0.6696035242290749,0.4201680672268908,0.2222222222222222,There are a large number categories but most of them contain only few articles ,15,This gives us a large corpus of news articles labeled with their categories .,"We choose 5 categories - "" sports "" , "" finance "" , "" entertainment "" , "" automobile "" and "" technology "" .",method
natural_language_inference,38,"In this layer , we follow to use the boundary model of pointer networks to locate the answer in the passage .",model,Output layer,0,101,60,2,0,model : Output layer,0.5343915343915344,0.9090909090909092,0.25,In this layer we follow to use the boundary model of pointer networks to locate the answer in the passage ,21, ,"Moreover , we follow to initialize the hidden state of the pointer network by a query - aware representation :",method
text_summarization,10,"Also see Sec. 7 and supplementary for qualitative examples of how our multi-task model with the entailment auxiliary task is able to generate more logically - entailed summaries than the baseline and We also tried to generate all the questions at once from the full document , but we obtained low accuracy because of this task 's challenging nature and over all less training data . :",model,Question Generation,0,88,43,16,0,model : Question Generation,0.3346007604562737,0.4623655913978494,0.8421052631578947,Also see Sec 7 and supplementary for qualitative examples of how our multi task model with the entailment auxiliary task is able to generate more logically entailed summaries than the baseline and We also tried to generate all the questions at once from the full document but we obtained low accuracy because of this task s challenging nature and over all less training data ,65,See Sec. 7 and for a quantitative evaluation showing that the multi-task model is better entailed by the source document and has fewer extraneous facts .,"Overview of our multi-task model with parallel training of three tasks : abstractive summary generation ( SG ) , question generation ( QG ) , and entailment generation ( EG ) .",method
sentiment_analysis,25,Incompatibilities of data are fixed during merging .,dataset,Datasets and Experiment Preparation,0,137,13,13,0,dataset : Datasets and Experiment Preparation,0.6171171171171171,0.4642857142857143,0.4642857142857143,Incompatibilities of data are fixed during merging ,8,"By merging restaurant reviews of three years 2014 - 2016 , we obtain a larger dataset called "" Restaurant - Large "" .",We replace conflict labels with neutral labels in the 2014 dataset .,experiment
sentiment_analysis,39,"In the case of the LSTM , we evaluate the loss on both training set and dev set after each iteration .",experiment,experiment,0,191,5,5,0,experiment : experiment,0.7795918367346939,0.5,0.5,In the case of the LSTM we evaluate the loss on both training set and dev set after each iteration ,21,We choose the best model with respect to the dev set .,We save the best model which has the lowest loss on the dev set over all the iterations .,experiment
question-answering,5,"We found that setting embedding regularization to 0.0001 , T = 8 , d = 384 , h = 128 , s = 512 worked robustly across the datasets .",training,Training Details,0,124,8,8,0,training : Training Details,0.5610859728506787,0.6153846153846154,0.6153846153846154,We found that setting embedding regularization to 0 0001 T 8 d 384 h 128 s 512 worked robustly across the datasets ,23,the inputs to both the query and the document attention mechanisms .,"Our model is implemented in Theano , using the Keras library .",experiment
natural_language_inference,66,Our model builds on top of a recently proposed neural attention model for NLI but is based on a significantly different idea .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.025089605734767026,0.4166666666666667,0.4166666666666667,Our model builds on top of a recently proposed neural attention model for NLI but is based on a significantly different idea ,23,"In this paper , we propose a special long short - term memory ( LSTM ) architecture for NLI .","Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification , our solution uses a match - LSTM to perform wordby - word matching of the hypothesis with the premise .",abstract
sentiment_analysis,23,"For a constituency tree , it is not completely obvious how to pool features to more than 3 slots and comply with the aforementioned criteria at the same time .",analysis,Question Classification,0,152,11,10,0,analysis : Question Classification,0.5205479452054794,0.5,0.4761904761904762,For a constituency tree it is not completely obvious how to pool features to more than 3 slots and comply with the aforementioned criteria at the same time ,29,nodes are pooled to slots LOWER LEFT or LOWER RIGHT according to their relative position with respect to the root node .,"Therefore , we regard 3 - slot pooling for c - TBCNN is a "" hard mechanism "" temporarily .",result
sentiment_analysis,16,"In addition , we report the accuracy ( same as F1-Micro ) , as it is used in previous studies .",evaluation,Evaluation Measure,0,244,5,5,0,evaluation : Evaluation Measure,0.7672955974842768,0.8333333333333334,0.8333333333333334,In addition we report the accuracy same as F1 Micro as it is used in previous studies ,18,"As our problem requires finegrained sentiment interaction , the class - based F1 provides more indicative information .","However , we suggest using F1 - score because accuracy biases towards the majority class .",result
named-entity-recognition,8,"The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre-training ( BERT uses 80 % , 10 % , 10 % ) .",ablation,ablation,0,382,21,21,0,ablation : ablation,0.9870801033591732,0.8076923076923077,0.8076923076923077,The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre training BERT uses 80 10 10 ,27,"In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token .",The right part of the paper represents the Dev set results .,result
relation_extraction,8,"Thus , it might be necessary to utilize all local features and perform this prediction globally .",methodology,Convolution,0,122,38,3,0,methodology : Convolution,0.4535315985130112,0.36893203883495146,0.16666666666666666,Thus it might be necessary to utilize all local features and perform this prediction globally ,16,"In relation extraction , an input sentence that is marked as containing the target entities corresponds only to a relation type ; it does not predict labels for each word .","When using a neural network , the convolution approach is a natural means of merging all these features .",method
natural_language_inference,84,"Instead , we present results on a trio of well - established tasks , namely reading comprehension , recognizing textual entailment , and a variant on language modelling .",introduction,introduction,0,32,23,23,0,introduction : introduction,0.1391304347826087,0.8214285714285714,0.8214285714285714,Instead we present results on a trio of well established tasks namely reading comprehension recognizing textual entailment and a variant on language modelling ,24,"However , there are no ( or significantly fewer ) established neural network - based baselines on these tasks , which makes it harder to validate baseline results .","For each task , we compare baseline models with embeddings trained directly only on the task objective to those same models with our on the fly embedding method .",introduction
natural_language_inference,13,"common denominator in many of these models is the compositional encoder , i.e. , usually a bidirectional recurrent - based ( LSTM or GRU ) encoder that sequentially parses the text sequence word - by - word .",introduction,introduction,0,17,5,5,0,introduction : introduction,0.07423580786026203,0.14705882352941174,0.14705882352941174,common denominator in many of these models is the compositional encoder i e usually a bidirectional recurrent based LSTM or GRU encoder that sequentially parses the text sequence word by word ,32,"This has been an extremely productive are a of research in the recent years , giving rise to many highly advanced neural network architectures .","This helps to model compositionality of words , capturing rich and complex linguistic and syntactic structure in language .",introduction
machine-translation,3,We call these connections fastforward connections .,system description,Deep Topology,0,78,36,4,0,system description : Deep Topology,0.2492012779552716,0.3,0.8,We call these connections fastforward connections ,7,The shortest paths through the proposed connections do not include any nonlinear transformations and do not rely on any recurrent computation .,"Within the deep topology , we also introduce an interleaved bi-directional architecture to stack the LSTM layers .",method
topic_models,0,"The authors in ( SCDV ) have shown superior classification results as compared to paragraph vector , LDA , NTSG , and other systems .",result,result,0,374,17,17,0,result : result,0.9077669902912622,0.6296296296296297,0.6296296296296297,The authors in SCDV have shown superior classification results as compared to paragraph vector LDA NTSG and other systems ,20,"Sparse composite document vector ( SCDV ) exploits pre-trained word embeddings to obtain sparse document embeddings , whereas neural tensor skip - gram model ( NTSG ) extends the idea of a skipgram model for obtaining document embeddings .",The next rows in present our baselines and proposed systems .,result
natural_language_inference,19,"First , the two input sentences , premise and hypothesis , are encoded as vectors , u and v respectively , through identical sentence encoders .",architecture,Overall Architecture,0,79,4,4,0,architecture : Overall Architecture,0.30980392156862746,0.04938271604938271,0.5,First the two input sentences premise and hypothesis are encoded as vectors u and v respectively through identical sentence encoders ,21,We follow the conventional architecture for training NLI data .,"For the encoded vectors u and v , the representation of relation between the two vectors is generated by the concatenation of u , v , | u ? v| , and u * v.",method
text_summarization,5,We use mini-batch Stochastic Gradient Descent ( SGD ) to tune model parameters .,system description,Learning,0,103,44,10,0,system description : Learning,0.4087301587301587,0.9361702127659576,0.7692307692307693,We use mini batch Stochastic Gradient Descent SGD to tune model parameters ,13,"To make full use of supervisions from both sides , we combine the above two costs as the final loss function :",The batch size is 64 .,method
sentiment_analysis,2,"Therefore , some researchers have designed attention networks to address the aspect - level sentiment analysis and have obtained comparable results , such as AE - LSTM , ATAE - LSTM and IAN .",introduction,introduction,0,31,15,15,0,introduction : introduction,0.1365638766519824,0.4545454545454545,0.4545454545454545,Therefore some researchers have designed attention networks to address the aspect level sentiment analysis and have obtained comparable results such as AE LSTM ATAE LSTM and IAN ,28,"Attention , which is widely applied to Computer Vision ( CV ) and NLP fields , is an effective mechanism and has been demonstrated in image recognition , machine translation and reading comprehension .","However , these existing work ignores or does not explicitly model the position information of the aspect term in a sentence , which has been studied for improving performance in information retrieval ( IR ) .",introduction
text_summarization,8,This shows that the benefit of abstractive models has been less in their ability to produce better paraphrasing but more in the ability to create fluent summaries from a mostly extractive process .,analysis,Analysis and Discussion,0,253,26,26,0,analysis : Analysis and Discussion,0.8846153846153846,0.7647058823529411,0.7647058823529411,This shows that the benefit of abstractive models has been less in their ability to produce better paraphrasing but more in the ability to create fluent summaries from a mostly extractive process ,33,"However , since generated summaries are typically not longer than 40 - 50 words , the difference between an abstractive system with and without bottom - up attention is less than one novel word per summary .","also shows the part - of - speech - tags of the novel generated words , and we can observe an interesting effect .",result
relation_extraction,3,"However , the gap between BERT SP method with and without entity - aware attention is small .",method,Method,0,131,8,8,0,method : Method,0.9562043795620438,0.8888888888888888,0.8888888888888888,However the gap between BERT SP method with and without entity aware attention is small ,16,"Adding structured prediction layer to BERT ( i.e. , BERT SP ) also leads to a similar amount of improvement .","This is likely because of the bias of data distribution : the assumption that only two target entities exist , makes the two techniques have similar effects .",method
named-entity-recognition,3,"Following we added 50 % dropout to the character embeddings , the input to each LSTM layer ( but not recurrent connections ) and to the output of the final LSTM layer .",experiment,experiment,1,89,20,20,0,experiment : experiment,0.4810810810810811,0.4651162790697674,0.4651162790697674,Following we added 50 dropout to the character embeddings the input to each LSTM layer but not recurrent connections and to the output of the final LSTM layer ,29,The sequence layer uses two bidirectional LSTMs with 200 hidden units .,Pre-trained language models .,experiment
sentiment_analysis,37,"To amplify the sentimentally relevant words to aspect a i , we employ an attention layer to obtain the aspect - aware sentence representation ( it is effectively a refined aspect representation ) r a i :",model,Aspect-Aware Sentence Representation,0,120,60,12,0,model : Aspect-Aware Sentence Representation,0.4743083003952569,0.8108108108108109,1.0,To amplify the sentimentally relevant words to aspect a i we employ an attention layer to obtain the aspect aware sentence representation it is effectively a refined aspect representation r a i ,33,"We obtain Ra i = GRU s ( S a i ) , where Ra i ? R LDs and the GRU s has the following parameters :", ,method
text-classification,8,Makes me wonder why everyone likes food fight so much .,model,model,0,181,51,51,0,model : model,0.6728624535315985,0.7083333333333334,0.7083333333333334,Makes me wonder why everyone likes food fight so much ,11,"Food is just okay , not great .","The store is small , but it carries specialties thatare difficult to find in Pittsburgh .",method
sentiment_analysis,24,"After initialization by f ?s , this sequence of latent vectors is later updated by combining information propagated from different task components through message passing .",method,Proposed Method,0,62,7,7,0,method : Proposed Method,0.19935691318327967,0.05223880597014925,0.3888888888888889,After initialization by f s this sequence of latent vectors is later updated by combining information propagated from different task components through message passing ,25,"The output off ?s is a sequence of latent vectors {h s 1 , h s 2 , ... , h s n } shared among all the tasks .","We denote h s ( t ) i as the value of the shared latent vector corresponding to xi after t rounds of message passing , with h s ( 0 ) i denoting the value after initialization .",method
natural_language_inference,10,Node representations which are not selected are copied to the corresponding positions at layer t + 1 .,model,Gumbel Tree-LSTM,0,107,51,12,0,model : Gumbel Tree-LSTM,0.4514767932489451,0.6538461538461539,0.3076923076923077,Node representations which are not selected are copied to the corresponding positions at layer t 1 ,17,"If two adjacent nodes , say rt i and rt i + 1 , are selected to be merged , then Eqs. 1 - 3 are applied by assuming [ h l ; cl ] = rt i and [ h r ; c r ] = rt i + 1 to obtain the parent representation [ h p ; c p ] = r t+1 i .","In other words , the ( t + 1 ) - th layer is composed of",method
sentiment_analysis,5,"shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .",experiment,The EEG emotion recognition experiments,1,200,65,35,0,experiment : The EEG emotion recognition experiments,0.7547169811320755,0.7471264367816092,1.0,shows the t test statistical analysis results from which we can see BiHDM is significantly better than the baseline method ,21,"On the other hand , we also perform the paired t- test between BiHDM and the baseline method at the significant level of 0.05 to see whether BiHDM has an improvement of recognition rate .", ,experiment
sentiment_analysis,12,"In this regard , pre-vious representative models are mostly discriminative classifiers based on manual feature engineering , such as Support Vector Machine .",introduction,introduction,0,14,3,3,0,introduction : introduction,0.0625,0.09090909090909093,0.09090909090909093,In this regard pre vious representative models are mostly discriminative classifiers based on manual feature engineering such as Support Vector Machine ,22,"Aspect - level sentiment classification ( ASC ) , as an indispensable task in sentiment analysis , aims at inferring the sentiment polarity of an input sentence in a certain aspect .","Recently , with the rapid development of deep learning , dominant ASC models have evolved into neural network ( NN ) based models , which are able to automatically learn the aspect - related semantic representation of an input sentence and thus exhibit better performance .",introduction
relation-classification,4,"Tree - LSTM , which is a recursive model that generalizes the LSTM to arbitrary tree structures .",model,Baseline Models,1,128,7,7,0,model : Baseline Models,0.4885496183206106,0.5,0.5,Tree LSTM which is a recursive model that generalizes the LSTM to arbitrary tree structures ,16,"2 ) Shortest Dependency Path LSTM ( SDP - LSTM ) , which applies a neural sequence model on the shortest path between the subject and object entities in the dependency tree .","We investigate the child - sum variant of Tree - LSTM , and apply it to the dependency tree ( or part of it ) .",method
natural_language_inference,96,"Second , our proposed adversarial filtering methodology allows for cost-effective construction of a large - scale dataset while substantially reducing known annotation artifacts .",introduction,introduction,0,43,33,33,0,introduction : introduction,0.11025641025641024,0.9428571428571428,0.9428571428571428,Second our proposed adversarial filtering methodology allows for cost effective construction of a large scale dataset while substantially reducing known annotation artifacts ,23,"First , our dataset poses a new challenge of grounded commonsense inference that is easy for humans ( 88 % ) while hard for current state - of the - art NLI models ( < 60 % ) .","The generality of adversarial filtering allows it to be applied to build future datasets , ensuring that they serve as reliable benchmarks .",introduction
prosody_prediction,0,For BiLSTM we use pre-trained 300D Glo Ve 840B word embeddings .,experiment,Experimental Setup,1,92,12,10,0,experiment : Experimental Setup,0.4791666666666667,0.5,0.4545454545454545,For BiLSTM we use pre trained 300D Glo Ve 840B word embeddings ,13,We use a batch size of 32 and fine - tune the model for 2 epochs .,The initial word embeddings are fine - tuned during training .,experiment
sentiment_analysis,17,"Note that when the tree is simply a chain , both Eqs. 2 - 8 and Eqs. 9 - 14 reduce to the standard LSTM transitions , Eqs .",system description,-ary Tree-LSTMs,0,102,63,6,0,system description : -ary Tree-LSTMs,0.4533333333333333,0.8076923076923077,0.2857142857142857,Note that when the tree is simply a chain both Eqs 2 8 and Eqs 9 14 reduce to the standard LSTM transitions Eqs ,25,"wherein Eq. 10 , k = 1 , 2 , . . . , N .",. The introduction of separate parameter matrices for each child k allows the N - ary Tree - LSTM model to learn more fine - grained conditioning on the states of a unit 's children than the Child - Sum Tree - LSTM .,method
natural_language_inference,5,Pretrained Language Model ( LM ) :,approach,Pretrained Language Model (LM):,0,76,4,1,0,approach : Pretrained Language Model (LM):,0.5170068027210885,0.1176470588235294,0.03225806451612903,Pretrained Language Model LM ,5, , ,method
text-to-speech_synthesis,1,Neural network based TTS has outperformed conventional concatenative and statistical parametric approaches in terms of speech quality .,introduction,introduction,1,18,5,5,0,introduction : introduction,0.0821917808219178,0.1851851851851852,0.1851851851851852,Neural network based TTS has outperformed conventional concatenative and statistical parametric approaches in terms of speech quality ,18,"Those models usually first generate mel-spectrogram autoregressively from text input and then synthesize speech from the mel-spectrogram using vocoder such as Griffin - Lim , WaveNet , Parallel WaveNet , or WaveGlow .","In current neural network based TTS systems , mel- spectrogram is generated autoregressively .",introduction
sarcasm_detection,0,"The corpus has 1.3 million sarcastic statements - 10 times more than any previous dataset - and many times more instances of non-sarcastic statements , allowing for learning in both balanced and unbalanced label regimes .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.028409090909090905,0.6,0.6,The corpus has 1 3 million sarcastic statements 10 times more than any previous dataset and many times more instances of non sarcastic statements allowing for learning in both balanced and unbalanced label regimes ,35,"We introduce the Self - Annotated Reddit Corpus ( SARC ) , a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection .","Each statement is furthermore self - annotated - sarcasm is labeled by the author , not an independent annotator - and provided with user , topic , and conversation context .",abstract
natural_language_inference,28,"Instead of a reset gate , however , the model learns a memory target variable ? ? RN h .",architecture,THE RUM ARCHITECTURE,0,104,6,6,0,architecture : THE RUM ARCHITECTURE,0.3837638376383764,0.14634146341463414,0.14634146341463414,Instead of a reset gate however the model learns a memory target variable RN h ,16,RUM consists of an update gate u ? RN h that has the same function as in GRU .,"Instead of a reset gate , however , the model learns a memory target variable ? ? RN h .",method
sentiment_analysis,20,"We obtain the final annotation for a given word x i , by concatenating the annotations from both directions :",model,MSA Model (message-level),0,91,15,13,0,model : MSA Model (message-level),0.48924731182795705,0.21739130434782608,0.5416666666666666,We obtain the final annotation for a given word x i by concatenating the annotations from both directions ,19,bidirectional LSTM consists of a forward LSTM ? ? f that reads the sentence from x 1 to x T and a backward LSTM ? ? f that reads the sentence from x T to x 1 .,where denotes the concatenation operation and L the size of each LSTM .,method
text_summarization,14,"This is our proposed model with entailment - aware encoder , which applies a multi-task learning ( MTL ) framework to seq2seq model .",method,Comparative Methods,1,157,9,9,0,method : Comparative Methods,0.6885964912280702,0.45,0.45,This is our proposed model with entailment aware encoder which applies a multi task learning MTL framework to seq2seq model ,21,This is a standard seq2seq model with attention mechanism .,Seq2seq + MTL ( Share decoder ) .,method
natural_language_inference,68,"Alternatively , if we want to ensure consecutivity , that is , if we want to ensure that we indeed select a subsequence from the passage as an answer , we can use the Ptr-Net to predict only the start and the end of an answer .",method,OUR METHOD,0,83,33,12,0,method : OUR METHOD,0.3333333333333333,0.2894736842105263,0.5454545454545454,Alternatively if we want to ensure consecutivity that is if we want to ensure that we indeed select a subsequence from the passage as an answer we can use the Ptr Net to predict only the start and the end of an answer ,44,"Specifically , we represent the answer as a sequence of integers a = ( a 1 , a 2 , . . . ) , where each a i is an integer between 1 and P , indicating a certain position in the passage .","In this case , the Ptr - Net only needs to select two tokens from the input passage , and all the tokens between these two tokens in the passage are treated as the answer .",method
natural_language_inference,55,"Our results also verify our hypothesis of Section 3.1 , that a richer representation for answers ( using the local subgraph ) can store more pertinent information .",experiment,Experiments,1,138,8,8,0,experiment : Experiments,0.9387755102040816,0.5333333333333333,0.5333333333333333,Our results also verify our hypothesis of Section 3 1 that a richer representation for answers using the local subgraph can store more pertinent information ,26,"However , using all 2 - hops connections as a candidate set is also detrimental , because the larger number of candidates confuses ( and slows a lot ) our ranking based inference .","Finally , we demonstrate that we greatly improve upon the model of , which actually corresponds to a setting with the Path representation and C 1 as candidate set .",experiment
natural_language_inference,74,Experiments show that our method achieves the state - of - the - art performance on several large - scale datasets .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.04464285714285714,0.8,0.8,Experiments show that our method achieves the state of the art performance on several large scale datasets ,18,"Moreover , we use reinforcement learning to optimize a new objective function with a reward defined by the property of the NLI datasets to make full use of the labels information .",Here sentences mean either the whole sentences or the main clauses of a compound sentence .,abstract
machine-translation,8,"For each source sentence , we also show the goldstandard translation .",model,Reference,0,328,115,10,0,model : Reference,0.9909365558912386,0.9745762711864406,0.7692307692307693,For each source sentence we also show the goldstandard translation ,11,The translations generated by RNNenc - 50 and RNNsearch - 50 from long source sentences ( 30 words or more ) selected from the test set .,The translations by Google Translate were made on 27 August 2014 .,method
named-entity-recognition,4,Importance of ? in Eqn .,system description,system description,0,202,14,14,0,system description : system description,0.7426470588235294,0.42424242424242425,0.42424242424242425,Importance of in Eqn ,5,"However , for sentiment classification development set accuracy is approximately the same regardless whether a fine tuned biLM was used .","The ? parameter in Eqn. ( 1 ) was of practical importance to aid optimization , due to the different distributions between the biLM internal representations and the task specific representations .",method
relation_extraction,5,"layers gives us a deep GCN network , where we set h Moreover , the propagation of information between tokens occurs in parallel , and the runtime does not depend on the depth of the dependency tree .",model,Graph Convolutional Networks over Dependency Trees,0,55,20,18,0,model : Graph Convolutional Networks over Dependency Trees,0.20912547528517111,0.29850746268656714,0.7826086956521741,layers gives us a deep GCN network where we set h Moreover the propagation of information between tokens occurs in parallel and the runtime does not depend on the depth of the dependency tree ,35,Stacking this operation over,Note that the GCN model presented above uses the same parameters for all edges in the dependency graph .,method
text-classification,6,The sentence encoding models are made publicly available on TF Hub .,introduction,introduction,0,22,10,10,0,introduction : introduction,0.14864864864864866,0.625,0.625,The sentence encoding models are made publicly available on TF Hub ,12,We find that our sentence embeddings can be used to obtain surprisingly good task performance with remarkably little task specific training data .,Engineering characteristics of models used for transfer learning are an important consideration .,introduction
text-classification,1,"shows how much accuracy and / or training speed can be improved by elimination of the word embedding layer , pooling , chopping , removing the input / output gates , and adding the backward LSTM .",system description,More simplifications,0,123,77,33,0,system description : More simplifications,0.48046875,0.385,1.0,shows how much accuracy and or training speed can be improved by elimination of the word embedding layer pooling chopping removing the input output gates and adding the backward LSTM ,31,"The resulting model is a one - hot bidirectional LSTM with pooling , and we abbreviate it to oh - 2 LSTMp .", ,method
relation-classification,9,"For sequence labeling ( i.e. NER and PICO ) , we feed the final BERT vector for each token into a linear classification layer with softmax output .",system description,Finetuning BERT,0,84,27,4,0,system description : Finetuning BERT,0.5714285714285714,0.5744680851063829,0.3636363636363637,For sequence labeling i e NER and PICO we feed the final BERT vector for each token into a linear classification layer with softmax output ,26,"For text classification ( i.e. CLS and REL ) , we feed the final BERT vector for the [ CLS ] token into a linear classification layer .","We differ slightly in using an additional conditional random field , which made evaluation easier by guaranteeing well - formed entities .",method
text_summarization,2,"We describe it in Eq. ( 18 ) , where where S ( w ) denotes the score of word w.",approach,Learning Objective and Beam Search,0,178,127,27,0,approach : Learning Objective and Beam Search,0.6402877697841727,0.9694656488549618,0.8709677419354839,We describe it in Eq 18 where where S w denotes the score of word w ,17,describe a beam search with reference mechanism that rewards system summaries that have a high degree of bigram overlap with the source texts .,"( y <t , x ) measures the number of bigrams shared by the system summary ( up to time step t- 1 ) and the source text ; {y <t , w} add s a word w to the end of the system summary .",method
natural_language_inference,25,"Analysis of current RC models has shown that models tend to react to simple word - matching between the question and document , as well as benefit from explicitly providing matching information in model inputs .",introduction,introduction,0,12,6,6,0,introduction : introduction,0.1153846153846154,0.5,0.5,Analysis of current RC models has shown that models tend to react to simple word matching between the question and document as well as benefit from explicitly providing matching information in model inputs ,34,"The vast majority of RC systems encode contextualized representations of words in both the document and question as hidden states of bidirectional RNNs , and focus model design and capacity around question - document interaction , carry - ing out calculations where information from both is available .","In this work , we hypothesize that the stillrelatively - small size of RC datasets drives this behavior , which leads to models that make limited use of context when representing word tokens .",introduction
text-classification,7,The Architectures of Capsule Network,model,The Architectures of Capsule Network,1,124,86,1,0,model : The Architectures of Capsule Network,0.5102880658436214,0.9148936170212766,0.1111111111111111,The Architectures of Capsule Network,5, , ,method
natural_language_inference,39,We outperform the recent multiperspective convolutional neural networks of and demonstrate state - of - the - art accuracy on all five tasks .,introduction,introduction,0,23,16,16,0,introduction : introduction,0.1116504854368932,0.9411764705882352,0.9411764705882352,We outperform the recent multiperspective convolutional neural networks of and demonstrate state of the art accuracy on all five tasks ,21,We conducted thorough evaluations on ten test sets from three SemEval STS competitions and two answer selection tasks .,"In addition , we conducted ablation studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement .",introduction
sentiment_analysis,16,"The vectors from D affect the final sentiment score through w I d i , d t , where w I is a sentimentspecific vector and d i , d t ? R denotes the dot product of the two TCS vectors d i and d t .",approach,approach,0,157,35,35,0,approach : approach,0.4937106918238994,0.4268292682926829,0.4268292682926829,The vectors from D affect the final sentiment score through w I d i d t where w I is a sentimentspecific vector and d i d t R denotes the dot product of the two TCS vectors d i and d t ,44,"Unlike input and output embeddings A and C , Dis designed to capture the sentiment interac- tion .","The vectors from D affect the final sentiment score through w I d i , d t , where w I is a sentimentspecific vector and d i , d t ? R denotes the dot product of the two TCS vectors d i and d t .",method
text-to-speech_synthesis,0,We use Adam optimizer for all models and follow the learning rate schedule in .,training,Training and Evaluation,1,121,3,3,0,training : Training and Evaluation,0.7469135802469136,0.2727272727272727,0.2727272727272727,We use Adam optimizer for all models and follow the learning rate schedule in ,15,We implement experiments with the fairseq - py 4 library in Py-Torch .,"The dropout is 0.3 for Bi - LSTM and CNN models , while the residual dropout , attention dropout and ReLU dropout for Transformer models is 0.2 , 0.4 , 0.4 respectively .",experiment
relation_extraction,8,"Because the position dimension has little effect on the result , we heuristically choose d p = 5 .",experiment,Parameter Settings,1,215,16,5,0,experiment : Parameter Settings,0.7992565055762082,0.8,0.5555555555555556,Because the position dimension has little effect on the result we heuristically choose d p 5 ,17,"We use a grid search to determine the optimal parameters and manually specify subsets of the parameter spaces : w ? { 1 , 2 , 3 , , 7 } and n ? { 50 , 60 , , 300}. shows all parameters used in the experiments .",The batch size is fixed to 50 .,experiment
question-answering,7,We analyzed the memory access order and the compositionality of memory slot and the input word in the NSE model trained on the SNLI data .,analysis,Memory Access and Compositionality,0,247,48,3,0,analysis : Memory Access and Compositionality,0.8981818181818182,0.75,0.15789473684210525,We analyzed the memory access order and the compositionality of memory slot and the input word in the NSE model trained on the SNLI data ,26,NSE is capabable of performing multiscale composition by retrieving associative slots for a particular input at a time step .,shows the word association graphs for the two sentence picked from SNLI test set .,result
natural_language_inference,15,The over all architecture of the proposed DRCN is shown in .,method,Methods,0,62,4,4,0,method : Methods,0.2743362831858407,0.08,1.0,The over all architecture of the proposed DRCN is shown in ,12,"We denote two input sentences as P = {p 1 , p 2 , , p I } and Q = {q 1 , q 2 , , q J } where pi / q j is the i th /j th word of the sentence P / Q and I/ J is the word length of P / Q .", ,method
text-classification,9,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .",experiment,Experimental Setting,1,154,16,15,0,experiment : Experimental Setting,0.6111111111111112,0.5714285714285714,0.5555555555555556,For the final sentence vector we concatenate the feature maps to get a 300 dimension vector ,17,"For our CNN , we use rectified linear units and three filters with different window sizes h = 3 , 4 , 5 with 100 feature maps each , following .",We use dropout on all nonlinear connections with a dropout rate of 0.5 .,experiment
relation_extraction,9,"In our work , we apply the idea of modeling attention to a rather different kind of scenario involving heterogeneous objects , namely a sentence and two entities .",model,Figure 2: Input and Primary Attention,0,97,50,2,0,model : Figure 2: Input and Primary Attention,0.4641148325358852,0.5681818181818182,0.08695652173913042,In our work we apply the idea of modeling attention to a rather different kind of scenario involving heterogeneous objects namely a sentence and two entities ,27, ,"With this , we seek to give our model the capability to determine which parts of the sentence are most influential with respect to the two entities of interest .",method
text_generation,1,It can be seen that the proposed Rank GAN performs more favourably compared to the state - of - the - art methods in terms of BLEU - 2 score .,experiment,Results on Chinese poems composition,1,211,47,9,0,experiment : Results on Chinese poems composition,0.7700729927007299,0.4563106796116505,0.6428571428571429,It can be seen that the proposed Rank GAN performs more favourably compared to the state of the art methods in terms of BLEU 2 score ,27,summarizes the BLEU - 2 score of different methods .,This indicates that the proposed objective is able to learn effective language generator with real - world data .,experiment
topic_models,0,"In such case , the proper ML training procedure should maximize the expected classconditional likelihood , with the expectation over w d calculated for each training example with respect to its posterior distri-",training,training,0,173,66,66,0,training : training,0.4199029126213592,0.7951807228915663,0.7951807228915663,In such case the proper ML training procedure should maximize the expected classconditional likelihood with the expectation over w d calculated for each training example with respect to its posterior distri ,32,"In our case , however , the training examples come in the form of posterior distributions , ) as extracted using our Bayesian SMM .","However , it is more convenient to introduce an equivalent model , where the observations are the means ? d of the posteriors q ( w d ) and the uncertainty encoded in ? ? 1 dis introduced into the model through the latent variable yd as , The resulting model is called GLCU .",experiment
relation-classification,2,We adopt this setting to produce comparable results with previous studies ) .,result,Results,0,237,17,16,0,result : Results,0.8033898305084746,0.4358974358974359,0.42105263157894735,We adopt this setting to produce comparable results with previous studies ,12,"In the relaxed setting , we perform an EC task instead of NER assuming that the boundaries of the entities are given .","Similar to , we present results of single models and no ensembles .",result
question_answering,3,"Given the transformed tokensw 1 , w 2 w /r j , we then expand / unfold them into the original sequence length .",system description,Unfold Operation,0,70,23,2,0,system description : Unfold Operation,0.25,0.41818181818181815,0.3333333333333333,Given the transformed tokensw 1 w 2 w r j we then expand unfold them into the original sequence length ,21, ,"Note that for each r j , the parameters W a , b a are not shared between blocks .",method
natural_language_inference,38,The character - level embeddings are generated by using Convolutional Neural Networks ( CNN ) which is applied to the characters of each word .,model,Encoding of Context and Query,0,51,10,5,0,model : Encoding of Context and Query,0.2698412698412698,0.15151515151515152,0.2380952380952381,The character level embeddings are generated by using Convolutional Neural Networks CNN which is applied to the characters of each word ,22,We use pre-trained word vectors Glo Ve to obtain the fixed word embedding of each word .,This layer maps each token to a high dimensional vector space and is proved to be helpful in handling out - ofvocab ( OOV ) words .,method
named-entity-recognition,0,"No surprisingly , RRSS Nie is incredibly time - consuming as N grows the order of curves looks higher than quadratic .",experiment,Speed Comparisons,0,218,18,9,0,experiment : Speed Comparisons,0.8044280442804428,0.5454545454545454,0.375,No surprisingly RRSS Nie is incredibly time consuming as N grows the order of curves looks higher than quadratic ,20,"As we shall see , both selection time of TED and RRSS Nie increases dramatically as N increases .","Actually , the theoretical complexity of RRSS Nie is highly up to ON 4 as analyzed in Remark",experiment
text-classification,4,CNNs are typically applied to tasks where feature extrac-tion and a corresponding supervised task are approached jointly .,introduction,introduction,0,13,3,3,0,introduction : introduction,0.05882352941176471,0.125,0.125,CNNs are typically applied to tasks where feature extrac tion and a corresponding supervised task are approached jointly ,19,"In the last few years , convolutional neural networks ( CNNs ) have demonstrated remarkable progress in various natural language processing applications , including sentence / document classification , text sequence matching , generic text representations , language modeling , machine translation and abstractive sentence summarization .","As an encoder network for text , CNNs typically convolve a set of filters , of window size n , with an inputsentence embedding matrix obtained via word2vec or Glove .",introduction
natural_language_inference,46,"However , summaries contain more complex relationships and timelines than news articles or short paragraphs from the web and thus provide a task different in nature .",method,method,0,139,39,39,0,method : method,0.46801346801346794,0.78,0.78,However summaries contain more complex relationships and timelines than news articles or short paragraphs from the web and thus provide a task different in nature ,26,The task of answering questions based on summaries is similar in scope to previous datasets .,We hope that Narrative QA will motivate the design of architectures capable of modeling such relationships .,method
sentiment_analysis,11,"With the increasing infusion of interactive systems in our lives , the need for empathetic machines with emotional understanding is paramount .",introduction,introduction,0,13,3,3,0,introduction : introduction,0.0377906976744186,0.06382978723404255,0.06382978723404255,With the increasing infusion of interactive systems in our lives the need for empathetic machines with emotional understanding is paramount ,21,Development of machines with emotional intelligence has been a long - standing goal of AI .,Previous research in affective computing has looked at dialogues as an essential basis to learn emotional dynamics .,introduction
relation-classification,5,We also obtain 7 + % higher EC and RC scores ( Setup 2 ) than Adel and Schtze ( 2017 ) .,result,Main results,0,85,5,5,1,result : Main results,0.7522123893805309,0.1724137931034483,0.1724137931034483,We also obtain 7 higher EC and RC scores Setup 2 than Adel and Schtze 2017 ,17,"In particular , our model obtains 2 + % absolute higher NER and RC scores ( Setup 1 ) than the BiLSTM - CRF - based multihead selection model .","Note that Gupta et al. ( 2016 ) use the same test set as we do , however they report final results on a 80/0 / 20 training / development / test split rather than our 64/16 /20 , i.e. Gupta et al. ( 2016 ) use a larger training set , but producing about 1.5 % lower EC score and similar RC score against ours .",result
natural_language_inference,100,Thus MAP is the average performance on all correct answers .,evaluation,Evaluation and Metrics,0,260,12,12,0,evaluation : Evaluation and Metrics,0.7103825136612022,0.9230769230769232,0.9230769230769232,Thus MAP is the average performance on all correct answers ,11,is the average precision for each query q ? Q .,We use the official trec_eval 2 scripts for computing these metrics .,result
named-entity-recognition,8,We fine - tune the model for 3 epochs with a learning rate of 2 e - 5 and a batch size of 16 .,experiment,GLUE,1,221,69,67,0,experiment : GLUE,0.5710594315245479,0.9718309859154928,0.9710144927536232,We fine tune the model for 3 epochs with a learning rate of 2 e 5 and a batch size of 16 ,23,The only task - specific parameters introduced is a vector whose dot product with the [ CLS ] token representation C denotes a score for each choice which is normalized with a softmax layer .,Results are presented in .,experiment
natural_language_inference,66,An input gate controls whether the input at the current position should be used in deriving the final hidden state of the current position .,result,Values of Gate Vectors,0,230,69,6,0,result : Values of Gate Vectors,0.8243727598566308,0.7419354838709677,0.2,An input gate controls whether the input at the current position should be used in deriving the final hidden state of the current position ,25,"Again , a darker color indicates a higher value .","From the three plots of the input gates , we can observe that generally for stop words such as prepositions and articles the input gates have lower values , suggesting that the matching of these words is less important .",result
natural_language_inference,41,"We use the same pre-training data as , consisting of 160 Gb of news , books , stories , and web text .",experiment,Experimental Setup,0,161,9,9,0,experiment : Experimental Setup,0.6264591439688716,0.28125,0.6923076923076923,We use the same pre training data as consisting of 160 Gb of news books stories and web text ,20,"To help the model better fit the data , we dis abled dropout for the final 10 % of training steps .",compares the performance of BART with several recent approaches on the well - studied SQuAD and GLUE tasks .,experiment
relation_extraction,3,"However , the final prediction layers used in the original model is not applicable to MRE tasks .",approach,Structured Prediction with BERT for MRE,0,35,8,3,0,approach : Structured Prediction with BERT for MRE,0.2554744525547445,0.2285714285714285,0.2727272727272727,However the final prediction layers used in the original model is not applicable to MRE tasks ,17,The BERT model has been successfully applied to various NLP tasks .,The MRE task essentially requires to perform edge predictions over a graph with entities as nodes .,method
relation_extraction,5,over examples in the TACRED dev set .,ablation,Complementary Strengths of GCNs and PA-LSTMs,0,192,14,8,0,ablation : Complementary Strengths of GCNs and PA-LSTMs,0.7300380228136882,0.7,0.5714285714285714,over examples in the TACRED dev set ,8,We also include edges for more relations in the supplementary material .,"Specifically , for each model , we trained it for 5 independent runs with different seeds , and for each example we evaluated the model 's accuracy over these 5 runs .",result
text_generation,2,"2 ) What you have to stop , if we do that , as late , law enforcement and where schools use a list of aid , it can rise .",system description,Turing Test and Generated Samples,0,240,26,18,0,system description : Turing Test and Generated Samples,0.6857142857142857,1.0,1.0,2 What you have to stop if we do that as late law enforcement and where schools use a list of aid it can rise ,26,2 ) This is the first time that the Fed has been able to launch a probe into the country ' s nuclear program .,"To verify that LeakGAN successfully exploits of the leaked message , we visualize the feature vector f T extracted from the real data by discriminator .",method
natural_language_inference,62,In what borough is the garment business prominent ? Brooklyn :,system description,Passage Question,0,32,6,6,0,system description : Passage Question,0.14285714285714285,0.3157894736842105,0.3333333333333333,In what borough is the garment business prominent Brooklyn ,10,"Manufacturing accounts for a significant but declining share of employment , although the city 's garment industry is showing a resurgence in Brooklyn .",In what borough is the garment business prominent ? Brooklyn :,method
natural_language_inference,64,"TANDA improves all the models : BERT - Base , RoBERTa- Base , BERT - Large and RoBERTa - Large , outperforming the previous state of the art with all of them .",result,TREC-QA,1,177,15,5,0,result : TREC-QA,0.7051792828685259,0.26785714285714285,0.4545454545454545,TANDA improves all the models BERT Base RoBERTa Base BERT Large and RoBERTa Large outperforming the previous state of the art with all of them ,26,"Both BERT - Base and Large fine purely tuned on the TREC - QA corpus can surpass the previous state of the art , probably because the size of TREC - QA training corpus is larger than that of Wiki QA .","Finally , the model obtained with FT on just ASNQ produces the expected results : it performs much lower than any TANDA model and also lower than FT on just TREC - QA since the target domain of TREC questions is significantly different from that of ASNQ .",result
natural_language_inference,22,where G c ? R 8 d C .,model,Sequence Encoding Layers,0,84,39,22,0,model : Sequence Encoding Layers,0.3818181818181817,0.5909090909090909,0.9565217391304348,where G c R 8 d C ,8,The most relevant word vector representation is an attention - weighted sum defined b ? We then obtain the final query - aware context representation by,where G c ? R 8 d C .,method
prosody_prediction,0,For the experiments we used the larger train - 360 training set .,experiment,Experimental Setup,0,102,22,20,0,experiment : Experimental Setup,0.53125,0.9166666666666666,0.9090909090909092,For the experiments we used the larger train 360 training set ,12,All systems except the Minitagger and CRF are our implementations using PyTorch and are made available on GitHub : https://github.com/Helsinki - NLP / prosody .,We report both 2 - way and 3 - way classification results .,experiment
natural_language_inference,93,"First , the question and passage are processed by a bi-directional recurrent network separately .",system description,Question and Passage Encoder,0,54,16,5,0,system description : Question and Passage Encoder,0.2608695652173913,0.25396825396825395,0.35714285714285715,First the question and passage are processed by a bi directional recurrent network separately ,15,Networks gives an overview of the gated selfmatching networks .,"We then match the question and passage with gated attention - based recurrent networks , obtaining question - aware representation for the passage .",method
question_answering,1,"Following , we obtain the characterlevel embedding of each word using Convolutional Neural Networks ( CNN ) .",model,Output,0,117,85,7,0,model : Output,0.3690851735015773,0.8415841584158416,0.3043478260869565,Following we obtain the characterlevel embedding of each word using Convolutional Neural Networks CNN ,15,"Let {x 1 , . . . x T } and {q 1 , . . . q J } represent the words in the input context paragraph and query , respectively .","Characters are embedded into vectors , which can be considered as 1D inputs to the CNN , and whose size is the input channel size of the CNN .",method
sentiment_analysis,6,It is to be noted that both sc - LSTM and bc - LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets .,model,Performance of Different Models,1,242,9,9,0,model : Performance of Different Models,0.8373702422145328,0.6923076923076923,0.6923076923076923,It is to be noted that both sc LSTM and bc LSTM perform quite well on the multimodal emotion recognition and sentiment analysis datasets ,25,Comparison of Different Network Variants,"Since bc - LSTM has access to both the preceding and following information of the utterance sequence , it performs consistently better on all the datasets over sc - LSTM .",method
temporal_information_extraction,0,"For example , "" 7 PM tonight "" ( 2015-12-12T19:00 ) IS INCLUDED in "" today "" ( 2015 - 12-12 ) .",system,Temporal Rule-Based Sieve,0,45,18,5,1,system : Temporal Rule-Based Sieve,0.2380952380952381,0.2769230769230769,1.0,For example 7 PM tonight 2015 12 12T19 00 IS INCLUDED in today 2015 12 12 ,17,"For timex - timex relations , we take into account temporal expressions of types DATE and TIME , and determine the relation types based on their normalized values .", ,method
sentiment_analysis,31,We adopt one widely used dataset from SemEval 2014 Task 4 .,dataset,Experiments Setting Dataset,0,105,2,2,0,dataset : Experiments Setting Dataset,0.660377358490566,0.2857142857142857,0.2857142857142857,We adopt one widely used dataset from SemEval 2014 Task 4 ,12, ,It contains two domain - specific datasets for laptops and restaurants .,experiment
text-classification,3,"Despite its importance , text preprocessing has not received much attention in the deep learning literature .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.04065040650406504,0.375,0.375,Despite its importance text preprocessing has not received much attention in the deep learning literature ,16,"Text preprocessing is often the first step in the pipeline of a Natural Language Processing ( NLP ) system , with potential impact in its final performance .","In this paper we investigate the impact of simple text preprocessing decisions ( particularly tokenizing , lemmatizing , lowercasing and multiword grouping ) on the performance of a standard neural text classifier .",abstract
sentiment_analysis,41,"We evaluate our approach on a benchmark dataset , which contains restaurants and laptops data .",introduction,introduction,0,28,17,17,0,introduction : introduction,0.12556053811659193,0.68,0.68,We evaluate our approach on a benchmark dataset which contains restaurants and laptops data ,15,"In order to capture important information in response to a given aspect , we design an attentionbased LSTM .",The main contributions of our work can be summarized as follows :,introduction
text_generation,5,Related work has used simpler decoders that model text as a bag of words .,introduction,introduction,0,30,19,19,0,introduction : introduction,0.1016949152542373,0.5757575757575758,0.5757575757575758,Related work has used simpler decoders that model text as a bag of words ,15,"In particular , they observe that the LSTM decoder in VAE does not make effective use of the latent representation during training and , as a result , VAE collapses into a simple language model .","Their results indicate better use of latent representations , but their decoders can not effectively model longer - range dependencies in text and thus underperform in terms of final perplexity .",introduction
question-answering,7,The read module now emits the additional key vector z n t for the shared memory and the composition function f M LP c combines more than one slots .,training,Shared and Multiple Memory Accesses,0,119,46,10,0,training : Shared and Multiple Memory Accesses,0.4327272727272728,0.9387755102040816,0.7692307692307693,The read module now emits the additional key vector z n t for the shared memory and the composition function f M LP c combines more than one slots ,30,and this is almost the same as standard NSE .,"In MMA - NSE , the different memory slots are retrieved from the shared memories depending on their encoded semantic representations .",experiment
text_generation,2,Here we propose a training scheme called interleaved training to alleviate such a problem .,training,Training Techniques,0,147,16,16,0,training : Training Techniques,0.42,0.6666666666666666,0.6666666666666666,Here we propose a training scheme called interleaved training to alleviate such a problem ,15,"In traditional generative adversarial models , mode collapse is a common problem .","As its name is , we adopt an interleaving of supervised training ( i.e. MLE ) and adversarial training ( i.e. GAN ) instead of full GAN after the pre-training .",experiment
sentiment_analysis,35,"Aspect categories and sentiment labels are identified by the "" industrial - strength "" Opinion Parser ( OP ) system .",dataset,dataset,0,174,7,7,0,dataset : dataset,0.7016129032258065,0.6363636363636364,0.6363636363636364,Aspect categories and sentiment labels are identified by the industrial strength Opinion Parser OP system ,16,Yelp reviews are collected in US cities over six years .,"To be consistent with the target domain datasets , YelpAspect is preprocessed in the sentence level by OP , while the dataset in ( Bauman , is in the document level .",experiment
part-of-speech_tagging,0,"Additionally , we see that our AT model achieves notably large improvements over the baseline in resource - poor languages ( the bottom of , with average improvement 0.35 % , as compared to that for resource - rich languages , 0.20 % .",result,Results,0,146,9,9,0,result : Results,0.5934959349593496,0.6428571428571429,0.6428571428571429,Additionally we see that our AT model achieves notably large improvements over the baseline in resource poor languages the bottom of with average improvement 0 35 as compared to that for resource rich languages 0 20 ,37,training of word rarity maybe of particular help in processing morphologically complex words .,"To further visualize the regularization effects , we present the learning curves for three representative languages , English ( WSJ ) , French ( UD - fr ) and Romanian ( UD - ro , low - resource ) , based on the development loss ( see ) .",result
machine-translation,2,"Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state h t?1 and the input for position t.",introduction,introduction,0,24,5,5,0,introduction : introduction,0.10714285714285714,0.4166666666666667,0.4166666666666667,Aligning the positions to steps in computation time they generate a sequence of hidden states ht as a function of the previous hidden state h t 1 and the input for position t ,34,Recurrent models typically factor computation along the symbol positions of the input and output sequences .,"This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .",introduction
named-entity-recognition,9,Pre-processed GAD and EU - ADR datasets are available with our provided codes .,dataset,Datasets,0,107,10,10,0,dataset : Datasets,0.5376884422110553,0.5,0.5,Pre processed GAD and EU ADR datasets are available with our provided codes ,14,The RE datasets contain gene - disease relations and protein - chemical relations ) .,"For the CHEMPROT dataset , we used the same pre-processing procedure described in .",experiment
sentiment_analysis,18,The comparison results are shown in .,model,Model Comparisons,0,172,10,10,0,model : Model Comparisons,0.7196652719665272,0.4,0.4,The comparison results are shown in ,7,We re-implement RAM following the instructions in its paper as the code is not available .,Both accuracy and macro - F1 are used for evaluation as the label distributions are unbalanced .,method
machine-translation,6,": until Converge 8 : Output : ? model , ? emb , ? D .",experiment,5:,0,170,14,6,0,experiment : 5:,0.584192439862543,0.2692307692307692,0.6, until Converge 8 Output model emb D ,9,Update ? D by gradient ascent according to Eqn. ( 4 ) with vocabulary V .,": until Converge 8 : Output : ? model , ? emb , ? D .",experiment
natural_language_inference,27,"F1 measures the portion of overlap tokens between the predicted answer and groundtruth , while exact match score is 1 if the prediction is exactly the same as groundtruth or 0 otherwise .",result,RESULTS,0,215,4,4,0,result : RESULTS,0.636094674556213,0.125,0.3636363636363637,F1 measures the portion of overlap tokens between the predicted answer and groundtruth while exact match score is 1 if the prediction is exactly the same as groundtruth or 0 otherwise ,32,The F1 and Exact Match ( EM ) are two evaluation metrics of accuracy for the model performance .,We show the results in comparison with other methods in .,result
natural_language_inference,57,"In contrast , we propose to exploit the partial order structure of the visual - semantic hierarchy by learning a mapping which is not distance - preserving but order - preserving between the visualsemantic hierarchy and a partial order over the embedding space .",introduction,introduction,1,26,19,19,0,introduction : introduction,0.1511627906976744,0.7037037037037037,0.7037037037037037,In contrast we propose to exploit the partial order structure of the visual semantic hierarchy by learning a mapping which is not distance preserving but order preserving between the visualsemantic hierarchy and a partial order over the embedding space ,40,"Notably , no existing approach directly imposes the transitivity and antisymmetry of the partial order , leaving the model to induce these properties from data .",We call embeddings learned in this way order- embeddings .,introduction
natural_language_inference,21,"This pre-process can be written as x = W ( e ) v , where word embedding weight matrix W ( e ) ? R de N and x ? R den .",system description,Sentence Encoding,0,52,5,5,0,system description : Sentence Encoding,0.1793103448275862,0.03937007874015748,0.5,This pre process can be written as x W e v where word embedding weight matrix W e R de N and x R den ,26,"pre-trained token embedding ( e.g. , word2vec or GloVe ) is applied to v and transforms all discrete tokens to a sequence of low - dimensional dense vector representations x = [ x 1 , x 2 , . . . , x n ] with x i ? R de .","This pre-process can be written as x = W ( e ) v , where word embedding weight matrix W ( e ) ? R de N and x ? R den .",method
sentence_classification,2,"Finally , we incorporate the self usability function ? i ( v k ) to reflect the self usability of a sentence .",model,Vector Fixing Module,0,118,33,14,0,model : Vector Fixing Module,0.4682539682539682,0.6226415094339622,0.4117647058823529,Finally we incorporate the self usability function i v k to reflect the self usability of a sentence ,19,"We extend the module such that ( 1 ) each sentence vector v k has its own projection matrix X k ? R dd , and ( 2 ) each projection matrix X k can be used as projection matrix of both the source ( e.g. when sentence k is the current source ) and the context vectors .","Finally , we incorporate the self usability function ? i ( v k ) to reflect the self usability of a sentence .",method
part-of-speech_tagging,1,"The improvement is more pronounced for non-English datasets , for example , IntNet improves the F - 1 score over the stateof - the - art results by more than 2 % for Dutch and Spanish .",method,method,1,221,30,30,0,method : method,0.884,0.7142857142857143,0.7142857142857143,The improvement is more pronounced for non English datasets for example IntNet improves the F 1 score over the stateof the art results by more than 2 for Dutch and Spanish ,32,These experiments show that our char - IntNet generally improves results across different models and datasets .,"It also shows that the results of LSTM - CRF are significantly improved after adding character - to - word models , which confirms that word shape information is very important for sequence labeling .",method
relation-classification,0,We present a novel end - to - end model to extract relations between entities on both word sequence and dependency tree structures .,introduction,introduction,1,27,17,17,0,introduction : introduction,0.11946902654867254,0.7083333333333334,0.7083333333333334,We present a novel end to end model to extract relations between entities on both word sequence and dependency tree structures ,22,"However , previous RNNbased models focus on only one of these linguistic structures .",Our model allows joint modeling of entities and relations in a single model by using both bidirectional sequential ( left - to - right and right - to - left ) and bidirectional tree - structured ( bottom - up and top - down ) LSTM - RNNs .,introduction
negation_scope_resolution,0,"This paper is organized as follows : In Section 2 , we extensively review available literature on the subject .",introduction,introduction,0,38,27,27,0,introduction : introduction,0.16521739130434782,0.9,0.9,This paper is organized as follows In Section 2 we extensively review available literature on the subject ,18,"Since the BioScope dataset is primarily from the biomedical domain , while the Sherlock dataset is taken from stories by Sir Author Conan Doyle ( literary work ) , and the SFU Review Corpus is a collection of product reviews ( free text by human users ) , the 3 datasets belong to different domains .","Section 3 contains the details of the methodology used for Neg - BERT , while 4 includes experimental details for our experimentation .",introduction
natural_language_inference,85,"The approach proposed by considers tree - based CNN to capture sentence - level semantics , while the model of introduces a stack - augmented parser - interpreter neural network ( SPINN ) which combines parsing and interpretation within a single tree - sequence hybrid model .",result,Results,0,181,7,7,0,result : Results,0.7702127659574468,0.3043478260869565,0.3043478260869565,The approach proposed by considers tree based CNN to capture sentence level semantics while the model of introduces a stack augmented parser interpreter neural network SPINN which combines parsing and interpretation within a single tree sequence hybrid model ,39,"The model in uses unsupervised "" skip - thoughts "" pre-training in GRU encoders .","The work by uses BiLSTM to generate sentence representations , and then replaces average pooling with intra-attention .",result
question_answering,1,We also evaluate our model on the task of cloze - style reading comprehension using the CNN and Daily Mail datasets .,experiment,CLOZE TEST EXPERIMENTS,0,231,68,2,0,experiment : CLOZE TEST EXPERIMENTS,0.7287066246056783,0.7555555555555555,0.08333333333333333,We also evaluate our model on the task of cloze style reading comprehension using the CNN and Daily Mail datasets ,21, ,"In a cloze test , the reader is asked to fill in words that have been removed from a passage , for measuring one 's ability to comprehend text .",experiment
paraphrase_generation,0,The error rates of other methods are reported in Figure 3 : The mean rank of all the models on the basis of BLEU score are plotted on the x - axis .,model,Model,0,174,6,6,0,model : Model,0.7341772151898734,0.6,0.6666666666666666,The error rates of other methods are reported in Figure 3 The mean rank of all the models on the basis of BLEU score are plotted on the x axis ,31,35.6 : Performance of our method compared to other approaches on the Stanford Sentiment Treebank Dataset .,Here EDD - LG - S refers to our EDD - LG shared model and others are the different variations of our model described in section 4.1.3 and the models on the right are the different variations proposed in .,method
natural_language_inference,49,"BIMPM models different perspective of matching between sentence pair on both direction , then aggregates matching vector with LSTM .",experiment,EXPERIMENT ON QUORA QUESTION PAIR DATASET,1,182,68,4,0,experiment : EXPERIMENT ON QUORA QUESTION PAIR DATASET,0.7165354330708661,0.9714285714285714,0.6666666666666666,BIMPM models different perspective of matching between sentence pair on both direction then aggregates matching vector with LSTM ,19,"Other than our baselines , we compare with and .",DECATT word and DECATT char uses automatically collected in - domain paraphrase data to noisy pretrain n-gram word embedding and ngram subword embedding correspondingly on decomposable attention model proposed by .,experiment
sentiment_analysis,16,"d ) Except NP , we do not apply non-linear projection to the sentiment score layer .",approach,approach,0,193,71,71,0,approach : approach,0.6069182389937107,0.8658536585365854,0.8658536585365854,d Except NP we do not apply non linear projection to the sentiment score layer ,16,"While A and C handle targeted - context detection ( attention ) , D captures the TCS interaction .","Although adding non-linear transformation to it may further improve model performance , the individual sentiment effect from each context will become untraceable , i.e. , losing some interpretability .",method
natural_language_inference,23,"From the results , we can see that our models not only perform well on the original SQuAD dataset , but also outperform all previous models by more than 5 % in EM score on the adversarial datasets .",result,MAIN RESULTS,1,238,10,10,0,result : MAIN RESULTS,0.4639376218323586,0.15873015873015872,0.9090909090909092,From the results we can see that our models not only perform well on the original SQuAD dataset but also outperform all previous models by more than 5 in EM score on the adversarial datasets ,36,"However , all models are trained only on the original SQuAD , so the model never sees the 40.4 / 51.0 Match-LSTM 64.7 / 73.7 BiDAF 68.0 / 77.3 SEDT 68. 70.8 / 78.7 DrQA 70.7 / 79.4 70.6 / 79.4 R. Mnemonic Reader and , respectively .",This shows that FusionNet is better at language understanding of both the context and question .,result
sentiment_analysis,50,Combined ( PRET + MULT ) :,approach,Transfer Approaches,0,73,17,17,0,approach : Transfer Approaches,0.4506172839506173,0.8947368421052632,0.8947368421052632,Combined PRET MULT ,4,"where U is the loss function of document - level classification . ? ? ( 0 , 1 ) is a hyperparameter that controls the weight of U .","In this setting , we first perform PRET on document - level examples .",method
sentiment_analysis,10,"2 ) , we calculate attention scores ? over the previous global states representative of the previous utterances .",model,Our Model,0,93,33,33,0,model : Our Model,0.3618677042801556,0.559322033898305,0.5892857142857143,2 we calculate attention scores over the previous global states representative of the previous utterances ,16,"Hence , we capture context ct relevant to the utterance u t as follows :","2 ) , we calculate attention scores ? over the previous global states representative of the previous utterances .",method
natural_language_inference,41,"To test how well BART performs in this regime , and to create a useful model for downstream tasks , we trained BART using the same scale as the RoBERTa model .",result,Large-scale Pre-training Experiments,0,152,25,3,0,result : Large-scale Pre-training Experiments,0.5914396887159533,1.0,1.0,To test how well BART performs in this regime and to create a useful model for downstream tasks we trained BART using the same scale as the RoBERTa model ,30,Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes and corpora ., ,result
machine-translation,3,We evaluate our method mainly on the widely used WMT ' 14 English - to - French translation task .,experiment,Experiments,0,185,2,2,0,experiment : Experiments,0.5910543130990416,0.5,0.5,We evaluate our method mainly on the widely used WMT 14 English to French translation task ,17, ,"In order to validate our model on more difficult language pairs , we also provide results on the WMT ' 14 English - to - German translation task .",experiment
sentiment_analysis,10,Our proposed DialogueRNN system employs three gated recurrent units ( GRU ) to model these aspects .,introduction,introduction,1,18,11,11,0,introduction : introduction,0.07003891050583658,0.4230769230769231,0.4230769230769231,Our proposed DialogueRNN system employs three gated recurrent units GRU to model these aspects ,15,"Hence , to extract the context , it is crucial to consider the preceding turns of both the speaker and the listener at a given moment .","The incoming utterance is fed into two GRUs called global GRU and party GRU to update the context and party Copyright 2019 , Association for the Advancement of Artificial Intelligence ( www.aaai.org ) .",introduction
named-entity-recognition,8,It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences .,model,model,0,140,62,62,0,model : model,0.3617571059431525,0.8378378378378378,1.0,It is critical to use a document level corpus rather than a shuffled sentence level corpus such as the Billion Word Benchmark in order to extract long contiguous sequences ,30,"For Wikipedia we extract only the text passages and ignore lists , tables , and headers .", ,method
text-classification,7,Dealers noted the chancellor said he would achieve his goals on sterling by a combination of intervention in currency markets and interest rates .,ablation,Interest Rates,0,198,48,6,0,ablation : Interest Rates,0.8148148148148148,0.8571428571428571,0.5,Dealers noted the chancellor said he would achieve his goals on sterling by a combination of intervention in currency markets and interest rates ,24,Sterling opened 0.3 points lower in trade weighted terms at 71.3 .,Operators feel the foreign exchanges are likely to test sterling on the downside and that this seems to make a fall in U.K .,result
text-classification,4,This sentence and the input to the filter - generation module maybe identical ( as in ) or different ( as in .,model,Extension to text sequence matching,0,87,40,12,0,model : Extension to text sequence matching,0.3936651583710407,0.4444444444444444,0.21052631578947367,This sentence and the input to the filter generation module maybe identical as in or different as in ,19,The adaptive convolution module takes as inputs the generated filters f and an input sentence .,"With the sample - specific filters , the input sentence is adaptively encoded , again , via a basic CNN architecture as in Section 3.1 , i.e. , one convolutional and one pooling layer .",method
natural_language_inference,33,"We also consider training skip - thoughts by predicting only the next sentence given the current , motivated by results in where it is demonstrated that predicting the next sentence alone leads to comparable performance .",system description,TRAINING OBJECTIVES & EVALUATION,0,87,30,11,0,system description : TRAINING OBJECTIVES & EVALUATION,0.3333333333333333,0.4477611940298508,0.7857142857142857,We also consider training skip thoughts by predicting only the next sentence given the current motivated by results in where it is demonstrated that predicting the next sentence alone leads to comparable performance ,34,The encoder for the current sentence and the decoders for the previous ( STP ) and next sentence ( STN ) are typically parameterized as separate RNNs .,and demonstrated that NMT can be formulated as a sequence - to - sequence learning problem where the input is a sentence in the source language and the output is its corresponding translation in the target language .,method
sentiment_analysis,46,"Different from , the designed Char - CNN is a fully convolutional network without max - pooling layer to capture better semantic information in character chunk .",model,Coupled Word Embedding,0,31,8,4,0,model : Coupled Word Embedding,0.2540983606557377,0.14545454545454545,0.3636363636363637,Different from the designed Char CNN is a fully convolutional network without max pooling layer to capture better semantic information in character chunk ,24,We first design a character - level convolution neural network ( Char - CNN ) to obtain character - level embedding .,"Specifically , we first input onehot - encoding character sequences to a 1 1 convolution layer to enhance the semantic nonlinear representation ability of our model , and the output is then fed into a multi-gram ( i.e. different window sizes ) convolution layer to capture different local character chunk information .",method
sentence_classification,2,"1 ) MCFA is attached after encoding the sentence , which makes it widely adaptable to other models .",introduction,introduction,1,49,39,39,0,introduction : introduction,0.19444444444444445,0.9285714285714286,0.9285714285714286,1 MCFA is attached after encoding the sentence which makes it widely adaptable to other models ,17,Listed below are the three main strengths of the MCFA attachment .,2 ) MCFA is extensible and improves the accuracy as the number of translated sentences increases .,introduction
natural_language_inference,100,"For example , models based on CNNs treat all the question terms as equally important when matching to answer terms .",introduction,introduction,0,46,36,36,0,introduction : introduction,0.12568306010928962,0.8571428571428571,0.8571428571428571,For example models based on CNNs treat all the question terms as equally important when matching to answer terms ,20,Most existing text matching models do not explicitly model question focus .,Models based on LSTMs usually model question terms closer to the end to be more important .,introduction
question_answering,5,We report all testing results with K = 50 .,analysis,analysis,0,239,47,47,0,analysis : analysis,0.8659420289855072,0.94,0.94,We report all testing results with K 50 ,9,"When K is smaller , such incorrect answers appear less because statistically they have lower prediction scores .",Examples shows an example from Quasar - T where the re-ranker successfully corrected the wrong answer predicted by the baseline .,result
natural_language_inference,87,"Benefiting from the highly accurate parser , neural network models could enjoy even higher accuracy gains by leveraging syntactic information rather than ignoring it .",system description,Syntactic Structures,0,40,7,3,0,system description : Syntactic Structures,0.2272727272727273,0.08860759493670886,0.2307692307692308,Benefiting from the highly accurate parser neural network models could enjoy even higher accuracy gains by leveraging syntactic information rather than ignoring it ,24,"Recently , dependency syntactic parsing have been further developed with neural network and attained new state - of the - art results .","Syntactic dependency parse tree provides a form that is capable of indicating the existence and type of linguistic dependency relation among words , which has been shown generally beneficial in various natural language understanding tasks ( Bowman et al. 2016 ) .",method
sentence_compression,2,For this measure the value 0 indicates that the word was read at most once by this reader .,system description,Our contributions,0,42,23,23,0,system description : Our contributions,0.4158415841584158,0.4693877551020408,0.92,For this measure the value 0 indicates that the word was read at most once by this reader ,19,"Since the reader by definition has already had a chance to recognize the word , regressions are associated with semantic confusion and contradiction , incongruence and syntactic complexity , as famously experienced in garden path sentences .",See for an example of first pass duration and regression duration annotations for one reader and sentence .,method
natural_language_inference,45,"More specifically , for each token pi in the document , The same function f is also applied to the tokens in the query .",system description,READING COMPREHENSION SETTING,0,81,13,9,0,system description : READING COMPREHENSION SETTING,0.40703517587939697,0.203125,0.5,More specifically for each token pi in the document The same function f is also applied to the tokens in the query ,23,"For each token in the document and the query , we compute a vector representation using a function f .","More specifically , for each token pi in the document , The same function f is also applied to the tokens in the query .",method
text_generation,1,We further evaluate our method on the large - scale dataset for the purpose of testing the stability of our model .,experiment,Results on COCO image captions,0,241,77,2,0,experiment : Results on COCO image captions,0.8795620437956204,0.7475728155339806,0.1,We further evaluate our method on the large scale dataset for the purpose of testing the stability of our model ,21, ,We test our method on the image captions provided by the COCO dataset .,experiment
natural_language_inference,69,"For 26 % , more than one candidate is plausibly supported by the documents , including the correct answer .",analysis,Qualitative Analysis,0,158,5,5,0,analysis : Qualitative Analysis,0.4579710144927536,0.0925925925925926,0.5,For 26 more than one candidate is plausibly supported by the documents including the correct answer ,17,"For 45 % , the true answer either uniquely follows from multiple texts directly or is suggested as likely .","This is often due to hypernymy , where the appropriate level of granularity for the answer is difficult to predict - e.g. ( west suffolk , administrative entity , ?)",result
named-entity-recognition,8,"Similar to ELMo , their model is feature - based and not deeply bidirectional .",system description,Unsupervised Feature-based Approaches,0,51,12,12,0,system description : Unsupervised Feature-based Approaches,0.13178294573643412,0.3076923076923077,0.9230769230769232,Similar to ELMo their model is feature based and not deeply bidirectional ,13,proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs .,shows that the cloze task can be used to improve the robustness of text generation models .,method
natural_language_inference,97,"The top N function contributes very little to the over all performance , suggesting that most multi questions have their evidence distributed across contiguous sentences .",analysis,analysis,1,263,7,7,0,analysis : analysis,0.9006849315068494,0.25,0.25,The top N function contributes very little to the over all performance suggesting that most multi questions have their evidence distributed across contiguous sentences ,25,MCTest - 500 accuracy ( % ) Single means for synthesizing distributed evidence .,"Ablating the sentential component made the most significant difference , reducing performance by more than 5 % .",result
named-entity-recognition,1,present semi-supervised bootstrapping algorithms for named entity recognition by co-training character - level ( word - internal ) and token - level ( context ) features .,architecture,Network architectures,0,197,9,9,0,architecture : Network architectures,0.9516908212560388,0.6428571428571429,0.6428571428571429,present semi supervised bootstrapping algorithms for named entity recognition by co training character level word internal and token level context features ,22,NER models like ours have also been proposed in the past .,use Bayesian nonparametrics to construct a data base of named entities in an almost unsupervised setting .,method
natural_language_inference,0,In machine learning it is common to see the effect of feature engineering diminish with increasing data size .,performance,Performance Comparison,0,139,10,10,0,performance : Performance Comparison,0.7055837563451777,0.4545454545454545,0.4545454545454545,In machine learning it is common to see the effect of feature engineering diminish with increasing data size ,19,"We note that anonymization of the latter datasets means that there is already some feature engineering ( it add s hints about whether a token is an entity ) , and these are much larger than the other four .","Similarly , fixing the word embeddings provides an improvement for the WDW and CBT , but not for CNN and Daily Mail .",result
text_summarization,12,Our SEASS model with beam search outperforms all baseline models by a large margin .,result,Results,1,189,9,9,0,result : Results,0.8289473684210527,0.39130434782608703,0.6,Our SEASS model with beam search outperforms all baseline models by a large margin ,15,"We acquire the test set from so we can make fair comparisons to the baselines . In , we report the ROUGE F1 score of our model and the baseline methods .","Even for greedy search , our model still performs better than other methods which used beam search .",result
question_answering,1,"One of the key factors to the advancement has been the use of neural attention mechanism , which enables the system to focus on a targeted are a within a context paragraph ( for MC ) or within an image ( for Visual QA ) , that is most relevant to answer the question .",introduction,introduction,0,12,4,4,0,introduction : introduction,0.03785488958990536,0.16666666666666666,0.16666666666666666,One of the key factors to the advancement has been the use of neural attention mechanism which enables the system to focus on a targeted are a within a context paragraph for MC or within an image for Visual QA that is most relevant to answer the question ,49,Systems trained end - to - end now achieve promising results on a variety of tasks in the text and image domains .,Attention mechanisms in previous works typically have one or more of the following characteristics .,introduction
text_summarization,8,"j | x , y 1:j?1 ) q i , which leads to a fully differentiable model .",system description,End-to-End Alternatives,0,144,74,18,0,system description : End-to-End Alternatives,0.5034965034965035,0.8131868131868132,0.9473684210526316,j x y 1 j 1 q i which leads to a fully differentiable model ,16,"Here we jointly optimize both objectives , but use predicted selection probabilities to softly mask the copy attention p( i j | x , y 1:j?1 ) = p ( a",This model is used with the same soft mask at test time .,method
sentiment_analysis,32,The IAN model is composed of two parts which model the target and context interactively .,system description,Interactive Attention Networks,0,52,2,2,0,system description : Interactive Attention Networks,0.2260869565217392,0.043478260869565216,0.043478260869565216,The IAN model is composed of two parts which model the target and context interactively ,16, ,"With word embeddings as input , we employ LSTM networks to obtain hidden states of words on the word level for a target and its context respectively .",method
paraphrase_generation,1,"The VAE does so by learning a latent representation or "" code "" z ? R K for an input x ? RD such that the original input x can be well reconstructed from the latent code z .",methodology,Variational Autoencoder (VAE),0,50,9,4,0,methodology : Variational Autoencoder (VAE),0.2262443438914027,0.3333333333333333,0.18181818181818185,The VAE does so by learning a latent representation or code z R K for an input x RD such that the original input x can be well reconstructed from the latent code z ,35,"macro-view of our model : the paraphrase generation model is also conditioned on the original sentence that allows learning rich , nonlinear representations for highdimensional inputs .","The VAE does so by learning a latent representation or "" code "" z ? R K for an input x ? RD such that the original input x can be well reconstructed from the latent code z .",method
sentiment_analysis,0,"This result seems plausible because the model can benefit from the differences among the distributions of words in happy and neutral expressions , which gives more emotional information to the model than that of the audio signal data .",analysis,Error analysis,0,166,8,8,0,analysis : Error analysis,0.9325842696629212,0.6666666666666666,0.6666666666666666,This result seems plausible because the model can benefit from the differences among the distributions of words in happy and neutral expressions which gives more emotional information to the model than that of the audio signal data ,38,"Interestingly , the TRE model ( ) shows greater prediction gains in predicting the happy class when compared to the ARE model ( 35.15 % to 75. 73 % ) .","On the other hand , it is striking that the TRE model incorrectly predicts instances of the sad class as the happy class 16 . 20 % of the time , even though these emotional states are opposites of one another .",result
natural_language_inference,31,"In the first block , the alignment results are almost word - or phrase - level .",analysis,Analysis,0,237,54,54,0,analysis : Analysis,0.8586956521739131,0.8852459016393442,0.8852459016393442,In the first block the alignment results are almost word or phrase level ,14,"The premise is "" A green bike is parked next to a door "" , and the hypothesis is "" The bike is chained to the door "" .","parked next to "" is associated mostly with "" bike "" and "" door "" since there is a weaker direct connection between "" parked "" and "" chained "" .",result
sentiment_analysis,16,"Despite the fact that it also uses the non-linear projection , this approach incorporates the interplay between a context word and the given target into its ( output ) context representation .",approach,approach,0,135,13,13,0,approach : approach,0.4245283018867925,0.15853658536585366,0.15853658536585366,Despite the fact that it also uses the non linear projection this approach incorporates the interplay between a context word and the given target into its output context representation ,30,Contextual Non-linear Projection ( CNP ) :,We thus name it Contextual Non-linear Projection ( CNP ) .,method
natural_language_inference,68,We can see that both of our two models have clearly outper - :,result,RESULTS,0,188,3,3,0,result : RESULTS,0.7550200803212851,0.075,0.17647058823529413,We can see that both of our two models have clearly outper ,13,The results of our models as well as the results of the baselines given by and are shown in .,Visualization of the attention weights ? for three questions associated with the same passage .,result
semantic_parsing,2,The smoothing parameter was set to 0.1 .,experiment,Experimental Setup,1,240,13,10,0,experiment : Experimental Setup,0.8247422680412371,0.6190476190476191,0.5555555555555556,The smoothing parameter was set to 0 1 ,9,Label smoothing was employed for GEO and ATIS .,"For WIKISQL , the hidden size of ? ( ) ZC07 86.1 84.6 UBL 87.9 71.4 FUBL 88.6 82.8 GUSP ++ - 83.5 KCAZ13 89.0 - DCS + L 87.9 - TISP 88.9 84.2 SEQ2SEQ 84.6 84.2 SEQ2TREE 87.1 84.6 ASN 85.7 85.3 ASN + SUPATT and ? ( ) in Equation was set to 64 .",experiment
natural_language_inference,92,"Let f denote a task - specific , deterministic function which maps a solution to the textual form of the answer ( e.g. by simply returning the string associated with a particular selected mention or solving an equation to get the final number , see ) .",method,Setup,0,59,6,4,0,method : Setup,0.2048611111111111,0.061224489795918366,0.2,Let f denote a task specific deterministic function which maps a solution to the textual form of the answer e g by simply returning the string associated with a particular selected mention or solving an equation to get the final number see ,43,"We define a solution as a particular derivation that a model is supposed to produce for the answer prediction ( e.g. a span in the document or an equation to compute the answer , see ) .",Our goal is to learn a model ( with parameters ?) which takes an input x and outputs a solution z such that f ( z ) = y.,method
text_summarization,9,The output of convolution is given by :,architecture,Attentive Encoder,0,70,33,9,0,architecture : Attentive Encoder,0.457516339869281,0.7674418604651163,0.4736842105263158,The output of convolution is given by ,8,"Let there bed such matrices ( k ? { 1 , . . . , d} ) .",where bk j is the j - th column of the matrix Bk .,method
part-of-speech_tagging,5,Separate optimization leads to better accuracy for 34 out of 40 treebanks for the morphological features task and for 30 out of 39 treebanks for xpos tagging .,ablation,Impact of the Training Schema,1,168,10,4,0,ablation : Impact of the Training Schema,0.8316831683168316,0.2702702702702703,0.5714285714285714,Separate optimization leads to better accuracy for 34 out of 40 treebanks for the morphological features task and for 30 out of 39 treebanks for xpos tagging ,28,shows that separately optimized models are significantly more accurate on average than jointly optimized models .,"Separate optimization outperformed joint optimization by up to 2.1 percent absolute , while joint never out - performed separate by more than 0.5 % absolute .",result
natural_language_inference,77,Another interesting finding is the fact that additional character based embeddings have a notable effect on the over all performance which was already observed by ; .,model,model,0,170,30,30,0,model : model,0.6227106227106227,0.5769230769230769,0.9090909090909092,Another interesting finding is the fact that additional character based embeddings have a notable effect on the over all performance which was already observed by ,26,"For example , if a person is mentioned in the question the context encoder only needs to remember that the "" question - person "" was mentioned but not the concrete name of the person .",We see further improvements when employing representation fusion to allow for more interaction .,method
sentiment_analysis,37,"Here , ? i is the measure of relatedness between target aspect and aspect i i.e. , the attention score .",model,Inter-Aspect Dependency Modeling,0,87,27,15,0,model : Inter-Aspect Dependency Modeling,0.3438735177865613,0.3648648648648649,0.5,Here i is the measure of relatedness between target aspect and aspect i i e the attention score ,19,We compute the match between the query q and the memory slots in Q with inner product :,"Here , ? i is the measure of relatedness between target aspect and aspect i i.e. , the attention score .",method
text_generation,4,The sentence encoding approach draws inspiration from the skip - gram model in producing vector representations using previous and next sentences .,system description,Skip-Thought Vectors,0,41,14,4,0,system description : Skip-Thought Vectors,0.3761467889908257,0.35,0.8,The sentence encoding approach draws inspiration from the skip gram model in producing vector representations using previous and next sentences ,21,The encoder maps sentences sharing semantic and syntactic properties to similar vector representations and the decoder reconstructs the surrounding sentences of an encoded passage .,"The Skip - Thought model uses an RNN encoder with GRU activations and an RNN decoder with conditional GRU , the combination being identical to the RNN encoder - decoder of ( Cho et al. , 2014 ) used in neural machine translation .",method
natural_language_inference,39,"Our model outperforms previous neural network models , most of which are based on sentence modeling .",experiment,Experimental Setup,0,163,33,33,0,experiment : Experimental Setup,0.7912621359223301,0.825,0.825,Our model outperforms previous neural network models most of which are based on sentence modeling ,16,"Due to sentence length variations , for the SICK and MSRVID data we padded the sentences to 32 words ; for the STS2014 , WikiQA , and TrecQA data , we padded the sentences to 48 words ..","The ConvNet work and TreeLSTM work achieve comparable accuracy ; for example , their difference in Pearson 's r is only 0.1 % .",experiment
named-entity-recognition,3,"The token embeddings , wk , are obtained as a lookup E ( , ? w ) , initialized using pre-trained word embeddings , and fine tuned during training .",model,Baseline sequence tagging model,0,32,6,6,0,model : Baseline sequence tagging model,0.17297297297297298,0.13953488372093026,0.13953488372093026,The token embeddings wk are obtained as a lookup E w initialized using pre trained word embeddings and fine tuned during training ,23,"It is parameterized by C ( , ? c ) with parameters ? c .","To learn a context sensitive representation , we employ multiple layers of bidirectional RNNs .",method
natural_language_inference,100,"However in our model , the weight associated with anode depends on its value .",model,Value-shared Weighting,0,158,29,18,0,model : Value-shared Weighting,0.43169398907103823,0.5178571428571429,0.6,However in our model the weight associated with anode depends on its value ,14,"We can see that for position - shared weights , the weight associated with anode only depends on its position or relative location as specified by the filters in CNN .","The value of anode denotes the strength of the matching signal between term pairs of questions and answers from the QA matching matrix , as explained in Section 3.1 .",method
natural_language_inference,74,We apply Tensorflow r 1.3 as our neural network framework .,implementation,Implementation Details,1,156,5,5,0,implementation : Implementation Details,0.6964285714285714,0.38461538461538464,0.38461538461538464,We apply Tensorflow r 1 3 as our neural network framework ,12,The dataset we use to train the embeddings of POS tags and NER tags are the training set given by SNLI .,"We set the hidden size as 300 for all the LSTM layers and apply dropout between layers with an initial ratio of 0.9 , the decay rate as 0.97 for every 5000 step .",experiment
natural_language_inference,87,We can see that the performance of BERT is very strong .,result,Main Results,0,141,5,5,0,result : Main Results,0.8011363636363636,0.5,0.5555555555555556,We can see that the performance of BERT is very strong ,12,Various state of the art models from the official leaderboard are also listed for reference .,"However , our model is more powerful , boosting the BERT baseline essentially .",result
machine-translation,7,We add residual connections around all LSTM and MoE layers to encourage gradient flow .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,329,107,10,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.8820375335120644,0.7086092715231788,0.3125,We add residual connections around all LSTM and MoE layers to encourage gradient flow ,15,"Our LSTM layers have 2048 hidden units , with a 512 - dimensional output projection .","Similar to GNMT , to effectively deal with rare words , we used subword units ( also known as "" wordpieces "" )",others
sentiment_analysis,28,"For a training sample x with the original ground - truth label distribution q ( k | x ) , we replace q ( k| x ) with",training,training,0,112,6,6,0,training : training,0.6222222222222222,0.42857142857142855,0.42857142857142855,For a training sample x with the original ground truth label distribution q k x we replace q k x with,21,"LSR can reduce overfitting by preventing a network from assigning the full probability to each training example during training , replaces the 0 and 1 targets for a classifier with smoothed values like 0.1 or 0.9 .","where u ( k ) is the prior distribution over labels , and is the smoothing parameter .",experiment
named-entity-recognition,5,"+ 1 and g t?1 , together with their corresponding cell values :",baseline,Sentence-State LSTM,0,97,26,9,0,baseline : Sentence-State LSTM,0.4641148325358852,0.35135135135135137,0.21428571428571427, 1 and g t 1 together with their corresponding cell values ,13,"As shown in , the value of each ht i is computed based on the values of","where ? ti is the concatenation of hidden vectors of a context window , and l ti , rt i , ft i , st i and it i are gates that control information flow from ? ti and xi to ct i .",result
natural_language_inference,92,How many sports are not olympic sports but are featured in the asian games ? ( A : 10 ) P : The first 30 sports were announced by the singapore national olympic council on 10 december 2013 on the sidelines of the 27th sea games in myanmar .,model,Examples,0,271,53,9,0,model : Examples,0.9409722222222222,0.7571428571428571,0.3461538461538461,How many sports are not olympic sports but are featured in the asian games A 10 P The first 30 sports were announced by the singapore national olympic council on 10 december 2013 on the sidelines of the 27th sea games in myanmar ,44,Question & Passage Q :,How many sports are not olympic sports but are featured in the asian games ? ( A : 10 ) P : The first 30 sports were announced by the singapore national olympic council on 10 december 2013 on the sidelines of the 27th sea games in myanmar .,method
sentiment_analysis,27,"After feeding word embedding to Bi - LSTM , the forward hidden state ? ? ht ? Rd hid 1 and the backward hidden state ? ? ht ? Rd hid 1 are obtained , where d hid is the number of hidden units .",methodology,Bidirectional Long Short-Term Memory (Bi-LSTM),0,99,22,3,0,methodology : Bidirectional Long Short-Term Memory (Bi-LSTM),0.35869565217391297,0.25882352941176473,0.375,After feeding word embedding to Bi LSTM the forward hidden state ht Rd hid 1 and the backward hidden state ht Rd hid 1 are obtained where d hid is the number of hidden units ,36,We employ Bi - LSTM on top of the embedding layer to capture the contextual information for each word .,"After feeding word embedding to Bi - LSTM , the forward hidden state ? ? ht ? Rd hid 1 and the backward hidden state ? ? ht ? Rd hid 1 are obtained , where d hid is the number of hidden units .",method
natural_language_inference,93,The input of the answer recurrent network is the attention - pooling vector based on current predicted probability at :,system description,Output Layer,0,97,59,6,0,system description : Output Layer,0.4685990338164252,0.9365079365079364,0.6,The input of the answer recurrent network is the attention pooling vector based on current predicted probability at ,19,Here ha t ? 1 represents the last hidden state of the answer recurrent network ( pointer network ) .,"When predicting the start position , ha t?1 represents the initial hidden state of the answer recurrent network .",method
machine-translation,4,"In the proposed model , the independent weights of the two encoders are expected to learn and encode the hidden features about the internal characteristics of each language , such as the terminology , style , and sentence structure .",model,Networks,0,87,30,16,0,model : Networks,0.3640167364016737,0.75,0.6153846153846154,In the proposed model the independent weights of the two encoders are expected to learn and encode the hidden features about the internal characteristics of each language such as the terminology style and sentence structure ,36,"Compared to which use the fully shared encoder , we only share partial weights for the encoders and decoders .",The shared weights are utilized to map the hidden features extracted by the independent weights to the shared - latent space .,method
sentiment_analysis,34,The work has developed since then and it spanned different topics and fields such as social media .,introduction,introduction,0,18,10,10,0,introduction : introduction,0.12162162162162166,0.5,0.5,The work has developed since then and it spanned different topics and fields such as social media ,18,"Work on SA started in early 2000s , particularly with the work of , where they studied the sentiment of movies ' reviews .",SA gained a lot of interest from researchers who recognised its importance and benefits .,introduction
semantic_parsing,0,"Second , given a data base , annotators have to understand the complex data base schema to create a set of questions such that their corresponding SQL queries cover all SQL patterns .",introduction,introduction,0,30,19,19,0,introduction : introduction,0.1079136690647482,0.59375,0.59375,Second given a data base annotators have to understand the complex data base schema to create a set of questions such that their corresponding SQL queries cover all SQL patterns ,31,"First , it is hard to find many data bases with multiple tables online .","Moreover , it is even more challenging to write different complex SQL queries .",introduction
part-of-speech_tagging,0,Poor tagging accuracy on rare / unseen words is one of the bottlenecks in current POS taggers .,analysis,Word-level Analysis,0,160,9,2,0,analysis : Word-level Analysis,0.6504065040650406,0.3461538461538461,0.1111111111111111,Poor tagging accuracy on rare unseen words is one of the bottlenecks in current POS taggers ,17, ,"Aiming to reveal the effects of AT on rare / unseen words , we analyze tagging performance at the word level , considering vocabulary statistics .",result
natural_language_inference,95,"Therefore , special consideration is required for such multi -passage MRC problem .",introduction,introduction,0,22,14,14,0,introduction : introduction,0.09401709401709404,0.2413793103448276,0.2413793103448276,Therefore special consideration is required for such multi passage MRC problem ,12,"As is shown by , these confusing answer candidates could be quite difficult for MRC models to distinguish .","In this paper , we propose to leverage the answer candidates from different passages to verify the final correct answer and rule out the noisy incorrect answers .",introduction
sarcasm_detection,1,This characteristic aligns well with the main goal of this paper .,dataset,Dataset,0,241,20,20,0,dataset : Dataset,0.7215568862275449,0.8333333333333334,0.8333333333333334,This characteristic aligns well with the main goal of this paper ,12,"First , this corpus is the first of its kind that was purposely developed to investigate the necessity of contextual information in sarcasm classification .","Second , the large size of the corpus allows for statistically - relevant analyses .",experiment
question_answering,2,"To obtain a final context representation , we first summarize the focal context representation separately for visual sequence and text sequence , emphasizing the most important information using the intra-sequence attention .",architecture,Cross Sequence Interaction,0,174,95,25,0,architecture : Cross Sequence Interaction,0.628158844765343,0.8715596330275229,0.6410256410256411,To obtain a final context representation we first summarize the focal context representation separately for visual sequence and text sequence emphasizing the most important information using the intra sequence attention ,31,Algorithm 1 summarizes the steps to compute the proposed FVTA attention .,"Then , we obtain the final representation by summing the sequence vector representation based on the intersequence importance .",method
relation-classification,1,"Besides , the other words irrelevant to the final result are labeled as "" O "" .",method,The Tagging Scheme,0,86,21,6,0,method : The Tagging Scheme,0.34959349593495936,0.3,1.0,Besides the other words irrelevant to the final result are labeled as O ,14,"The other entity "" Trump "" , which is corresponding to "" United States "" , is labeled as "" S - CP - 2 "" .", ,method
prosody_prediction,0,The good results 6 from this experiment provide further support for the quality of the new dataset .,result,Results,1,131,27,27,0,result : Results,0.6822916666666666,0.8181818181818182,0.9,The good results 6 from this experiment provide further support for the quality of the new dataset ,18,The results of this experiment are shown in .,Notice also that the difference between BERT and BiLSTM is much bigger with this test set ( + 3.9 % compared to + 1.1 % ) .,result
text_summarization,7,The basic idea of our method is to jointly estimate the upper-bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .,introduction,introduction,1,17,11,11,0,introduction : introduction,0.11258278145695363,0.7333333333333333,0.7333333333333333,The basic idea of our method is to jointly estimate the upper bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step ,42,"From this background , this paper tackles this issue and proposes a method to overcome it in ABS tasks .",We refer to our additional component as a wordfrequency estimation ( WFE ) sub-model .,introduction
relation_extraction,6,Our architecture uses an LSTM - based encoder to jointly learn representations for all relations in a single sentence .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.038461538461538464,0.375,0.375,Our architecture uses an LSTM based encoder to jointly learn representations for all relations in a single sentence ,19,We demonstrate that for sentence - level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation .,We combine the context representations with an attention mechanism to make the final prediction .,abstract
machine-translation,4,"We then apply the public implementation 7 of the method proposed by to map these 6 LDC2002L27 , LDC2002T01 , LDC2002E18 , LDC2003E07 , LDC2004T08 , LDC2004E12 , LDC2005T10 7 https://github.com/artetxem/vecmap",data set,Data Sets and Preprocessing,0,165,20,20,0,data set : Data Sets and Preprocessing,0.6903765690376569,0.9523809523809524,0.9523809523809524,We then apply the public implementation 7 of the method proposed by to map these 6 LDC2002L27 LDC2002T01 LDC2002E18 LDC2003E07 LDC2004T08 LDC2004E12 LDC2005T10 7 https github com artetxem vecmap,29,"Since the proposed system relies on the pretrained cross - lingual embeddings , we utilize the monolingual corpora described above to train the embeddings for each language independently by using word2 vec .",embeddings to a shared - latent space 8 .,experiment
text-classification,4,"The output feature maps of the convolutional layer , i.e. , p ? R K(T ?h+1 ) are then passed to the pooling layer , which takes the maximum value in every row of p , forming a K-dimensional vector , z.",model,Basic CNN for text representations,0,60,13,12,0,model : Basic CNN for text representations,0.27149321266968324,0.14444444444444446,0.8,The output feature maps of the convolutional layer i e p R K T h 1 are then passed to the pooling layer which takes the maximum value in every row of p forming a K dimensional vector z ,40,"Parameter b ? R K is the bias term and f ( ) is a non-linear function , implemented as a rectified linear unit ( ReLU ) in our experiments .",This operation attempts to keep the most salient feature detected by every filter and discard the information from less fundamental text fragments .,method
sentiment_analysis,27,The evidence of the usefulness of the sentiment dependencies is that we can easily guess the true sentiment of food even if we mask the word horrible .,methodology,Methodology,0,83,6,6,0,methodology : Methodology,0.3007246376811594,0.07058823529411765,0.5454545454545454,The evidence of the usefulness of the sentiment dependencies is that we can easily guess the true sentiment of food even if we mask the word horrible ,28,"The dependencies can be inferred by some knowledge in the sentence , e.g. , conjunction .",It is required to construct a sentiment classifier that predicts the sentiment polarities of the multiple aspect teams .,method
natural_language_inference,79,This type of models is commonly known as neural language models .,system description,Learning Vector Representation of Words,0,72,26,23,0,system description : Learning Vector Representation of Words,0.26865671641791045,0.7647058823529411,0.7666666666666667,This type of models is commonly known as neural language models ,12,The neural network based word vectors are usually trained using stochastic gradient descent where the gradient is obtained via backpropagation .,particular implementation of neural network based algorithm for training the word vectors is available at code.google.com/p/word2vec/.,method
semantic_parsing,2,"The dropout rate was selected from { 0.3 , 0.5 } .",experiment,Experimental Setup,1,238,11,8,0,experiment : Experimental Setup,0.8178694158075601,0.5238095238095238,0.4444444444444444,The dropout rate was selected from 0 3 0 5 ,11,"Dimensions of hidden vectors and word embeddings were selected from { 250 , 300 } and { 150 , 200 , 250 , 300 } , respectively .",Label smoothing was employed for GEO and ATIS .,experiment
natural_language_inference,35,"It first computes a similarity matrix U pk ? R LJ between the question and the k - th passage , as done in , where",model,Dual Attention Layer,0,75,36,3,0,model : Dual Attention Layer,0.2851711026615969,0.2571428571428571,0.03260869565217391,It first computes a similarity matrix U pk R LJ between the question and the k th passage as done in where,22,This layer uses a dual attention mechanism to fuse information from the question to the passages as well as from the passages to the question .,"It first computes a similarity matrix U pk ? R LJ between the question and the k - th passage , as done in , where",method
relation_extraction,0,"In order to compare our system with the previous systems , we report micro F1 -scores , Precision and Recall on both entities and relations similar to and .",evaluation,Evaluation Metrics,0,164,2,2,0,evaluation : Evaluation Metrics,0.640625,0.4,0.4,In order to compare our system with the previous systems we report micro F1 scores Precision and Recall on both entities and relations similar to and ,27, ,An entity is considered correct if we can identify its head and the entity type correctly .,result
relation-classification,1,"Because , the relation role of "" Trump "" is "" 2 "" and "" United States "" is "" 1 "" , the final result is { United States , Country - President , Trump } .",method,From Tag Sequence To Extracted Results,0,91,26,5,0,method : From Tag Sequence To Extracted Results,0.3699186991869919,0.3714285714285713,0.5,Because the relation role of Trump is 2 and United States is 1 the final result is United States Country President Trump ,23,"Accordingly , "" Trump "" and "" United States "" can be combined into a triplet whose relation type is "" Country - President "" .","The same applies to { Apple Inc , Company - Founder , Steven Paul Jobs } .",method
sentiment_analysis,15,"The conjunction is interpreted as an argument for the second conjunct , with the first functioning concessively .",experiment,Model Analysis: Contrastive Conjunction,0,232,36,3,0,experiment : Model Analysis: Contrastive Conjunction,0.8592592592592593,0.5217391304347826,0.375,The conjunction is interpreted as an argument for the second conjunct with the first functioning concessively ,17,"In this section , we use a subset of the test set which includes only sentences with an ' X but Y ' structure : A phrase X being followed by but which is followed by a phrase Y .",contains an example .,experiment
natural_language_inference,64,"To show that our results can generalize well , we tested our models using four different AS2 datasets created with questions sampled from the customers ' interaction with Alexa Virtual Assistant .",result,Experiments on data from Alexa,0,218,56,2,0,result : Experiments on data from Alexa,0.8685258964143426,1.0,1.0,To show that our results can generalize well we tested our models using four different AS2 datasets created with questions sampled from the customers interaction with Alexa Virtual Assistant ,30, , ,result
natural_language_inference,79,Our algorithm represents each document by a dense vector which is trained to predict words in the document .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.033582089552238806,0.7,0.7,Our algorithm represents each document by a dense vector which is trained to predict words in the document ,19,"In this paper , we propose Paragraph Vector , an unsupervised algorithm that learns fixed - length feature representations from variable - length pieces of texts , such as sentences , paragraphs , and documents .",Its construction gives our algorithm the potential to overcome the weaknesses of bag - ofwords models .,abstract
text_generation,2,frequently occurring example is the formal subject .,model,Model Explanation,0,257,17,17,0,model : Model Explanation,0.7342857142857143,0.85,0.85,frequently occurring example is the formal subject ,8,"If a peak occurs in the curve , there must be some token that triggers along suffix .","iii ) Although hard to observe , we do find connections of the 7th dimension and the substructure of a sentence .",method
machine-translation,5,The table is then used to count the number of occurrences of different translations .,system,System Combination,0,122,25,8,0,system : System Combination,0.8531468531468531,0.8620689655172413,0.6666666666666666,The table is then used to count the number of occurrences of different translations ,15,The alignments are used to generate a table of all possible word translations relative to each position in the primary hypothesis .,The word translations with the highest count at each position constitute the resulting combined hypothesis .,method
named-entity-recognition,0,"Additionally , via another equivalent derivation , we give an accelerated solver for Nie 's method , theoretically reducing the computational complexity from ON 4 to ON 2 L + N L 3 as listed in , empirically obtaining a 500 + times speedup compared with the authorial solver .",system description,Classifiers,0,56,34,34,0,system description : Classifiers,0.2066420664206642,0.19101123595505606,0.8717948717948718,Additionally via another equivalent derivation we give an accelerated solver for Nie s method theoretically reducing the computational complexity from ON 4 to ON 2 L N L 3 as listed in empirically obtaining a 500 times speedup compared with the authorial solver ,44,The selection quality is highly encouraging as shown in .,We use boldface uppercase letters to denote matrices and boldface lowercase letters to represent vectors .,method
machine-translation,7,Equally major contributors Work done as a member of the Google Brain Residency program ( g.co/ brainresidency ),abstract,abstract,0,13,11,11,0,abstract : abstract,0.03485254691689008,0.2682926829268293,0.2682926829268293,Equally major contributors Work done as a member of the Google Brain Residency program g co brainresidency ,18,"On large language modeling and machine translation benchmarks , these models achieve significantly better results than state - of - the - art at lower computational cost .",INTRODUCTION AND RELATED WORK 1 .,abstract
natural_language_inference,28,"We believe that the unit of memory R t gives advantage to RUM over other gated mechanisms , such as LSTM and GRU .",architecture,THE RUM ARCHITECTURE,0,139,41,41,0,architecture : THE RUM ARCHITECTURE,0.5129151291512916,1.0,1.0,We believe that the unit of memory R t gives advantage to RUM over other gated mechanisms such as LSTM and GRU ,23,"By feeding inputs to RUM , ? adapts to encode rotations , which align the hidden states in desired locations in RN h , without changing the norm of h .", ,method
temporal_information_extraction,0,"These pairs are then grouped together into four different groups : timex - timex ( T - T ) , event - DCT ( E - D ) , event - timex ( E - T ) and event - event ( E - E ) .",system,Temporal Relation Identification,0,35,8,4,0,system : Temporal Relation Identification,0.1851851851851852,0.12307692307692307,1.0,These pairs are then grouped together into four different groups timex timex T T event DCT E D event timex E T and event event E E ,28,"All pairs of temporal entities satisfying one of the following rules , inspired by the TempEval - 3 task description , are considered as having temporal links ( TLINK s ) : ( i ) two main events of consecutive sentences , ( ii ) two events in the same sentence , ( iii ) an event and a timex in the same sentence , ( iv ) an event and a document creation time and ( v ) pairs of all possible timexes ( including document creation time ) linked with each other .", ,method
natural_language_inference,60,We examine the atten -,analysis,Multi-domain Embeddings,0,157,21,5,0,analysis : Multi-domain Embeddings,0.8134715025906736,1.0,1.0,We examine the atten ,5,"We train embeddings on three kinds of data : Wikipedia , the Toronto Books Corpus and the English OpenSubtitles 4 .", ,result
text-to-speech_synthesis,0,Sequitur G2P is a well established G2P conversion tool using joint sequence modelling and is widely used as a baseline for comparison .,result,Achieving State-Of-The-Art Accuracy,0,133,4,3,0,result : Achieving State-Of-The-Art Accuracy,0.8209876543209876,0.2857142857142857,0.2307692307692308,Sequitur G2P is a well established G2P conversion tool using joint sequence modelling and is widely used as a baseline for comparison ,23,"We first compare our method with previous works on CMUDict 0.7 b dataset , as shown in .",used the ensemble of Bi - LSTM and joint n-gram model .,result
natural_language_inference,79,We turn our attention to an information retrieval task which requires fixed - length representations of paragraphs .,model,Information Retrieval with Paragraph Vectors,0,207,22,2,0,model : Information Retrieval with Paragraph Vectors,0.7723880597014925,0.3492063492063492,0.07142857142857142,We turn our attention to an information retrieval task which requires fixed length representations of paragraphs ,17, ,"Here , we have a dataset of paragraphs in the first 10 results returned by a search engine given each of 1,000,000 most popular queries .",method
natural_language_inference,31,"These components , which the name RE2 stands for , are previous aligned features ( Residual vectors ) , original point - wise features ( Embedding vectors ) , and contextual features ( Encoded vectors ) .",introduction,introduction,1,25,18,18,0,introduction : introduction,0.09057971014492754,0.6206896551724138,0.6206896551724138,These components which the name RE2 stands for are previous aligned features Residual vectors original point wise features Embedding vectors and contextual features Encoded vectors ,26,"Meanwhile , we highlight three key components for an efficient text matching model .",The re-maining components can be as simple as possible to keep the model fast while still yielding strong performance .,introduction
sentiment_analysis,10,"We update these states based on the current ( at time t) role of a participant in the conversation , which is either speaker or listener , and the incoming utterance u t .",model,Our Model,0,87,27,27,0,model : Our Model,0.3385214007782101,0.4576271186440678,0.4821428571428572,We update these states based on the current at time t role of a participant in the conversation which is either speaker or listener and the incoming utterance u t ,31,"These states are representative of the speakers ' state in the conversation , relevant to emotion classification .",These state vectors are initialized with null vectors for all the participants .,method
relation_extraction,13,Test results for FewRel few - shot relation classification task .,system description,Matching the Blanks Training,0,171,53,12,0,system description : Matching the Blanks Training,0.8028169014084507,0.8833333333333333,0.631578947368421,Test results for FewRel few shot relation classification task ,10,"Instead of summing over all pairs of relation statements that do not contain the same pair of entities , we sample a set of negatives thatare either randomly sampled uniformly from the set of all relation statement pairs , or are sampled from the set of relation statements that share just a single 5 - way 5 - way 10 - way 10 - way 1 - shot 5 - shot :",Net is the best published system from .,method
question_answering,0,Our system can be used with a pre-trained model to answer factual questions with Wikidata or trained a new on any data set that has question - answer pairs .,data set,data set,0,46,2,2,0,data set : data set,0.15593220338983052,0.4,0.4,Our system can be used with a pre trained model to answer factual questions with Wikidata or trained a new on any data set that has question answer pairs ,30, ,"The complete code , the scripts that produce the evaluation data and the installation instructions can be found here :",experiment
paraphrase_generation,1,"Comparing the results across different variants of supervised model , VAE - SVG - eq performs the best .",result,Results,1,201,35,35,0,result : Results,0.9095022624434388,0.7142857142857143,0.7142857142857143,Comparing the results across different variants of supervised model VAE SVG eq performs the best ,16,"We also report the results on different training sizes , and as expected , as we increase the training data size , results improve .","This is primarily due to the fact that in VAE - SVG - eq , the parameters of the input question encoder are shared by the encoding side and the decoding side .",result
natural_language_inference,42,"To have a comprehensive comparison , we took a state - of - the - art model developed in Tensorflow that had it s code openly available at https://github.com/allenai/bi-att-flow.",evaluation,FABIR vs BiDAF,0,240,15,3,0,evaluation : FABIR vs BiDAF,0.8304498269896193,0.2830188679245283,0.1875,To have a comprehensive comparison we took a state of the art model developed in Tensorflow that had it s code openly available at https github com allenai bi att flow ,32,This section compares our model to traditional RNNbased question - answering models .,"That way , we could run our experiments with both models in the same piece of hardware to have a fair comparison between them .",result
natural_language_inference,27,"For example , the current ( as of Dec 19 , 2017 ) best documented model , , achieves 84.4 F1 score which is on par with our method . :",model,MODEL OVERVIEW,0,61,17,9,1,model : MODEL OVERVIEW,0.1804733727810651,0.12781954887218044,0.15789473684210525,For example the current as of Dec 19 2017 best documented model achieves 84 4 F1 score which is on par with our method ,25,"After our first submission of the draft , there are other unpublished results either on the leaderboard or arxiv .",An overview of the QANet architecture ( left ) which has several Encoder Blocks .,method
semantic_role_labeling,4,is a parameter matrix and {? m },architecture,architecture,0,140,52,52,0,architecture : architecture,0.4666666666666667,0.8965517241379308,0.8965517241379308,is a parameter matrix and m ,7,"Firstly , we combine span representations h","m=1 are trainable , softmax - normalized parameters .",method
sentiment_analysis,37,"In this section , we discuss the dataset used and different experimental settings devised for the evaluation of our model .",experiment,Experiments,0,154,2,2,0,experiment : Experiments,0.6086956521739131,1.0,1.0,In this section we discuss the dataset used and different experimental settings devised for the evaluation of our model ,20, , ,experiment
natural_language_inference,30,The data set WikiAnswers + ReVerb contains no labeled examples but some are needed for evaluating models .,evaluation,Test Set,0,186,4,2,0,evaluation : Test Set,0.7209302325581395,0.2857142857142857,0.2857142857142857,The data set WikiAnswers ReVerb contains no labeled examples but some are needed for evaluating models ,17, ,"We used the test set which has been created by in the following way : ( 1 ) they identified 37 questions from a heldout portion of WikiAnswers which were likely to have at least one answer in ReVerb , ( 2 ) they added all valid paraphrases of these questions to obtain a set of 691 questions , ( 3 ) they ran various versions of their paralex system on them to gather candidate triples ( for a total of 48 k ) , which they finally hand - labeled .",result
sentiment_analysis,51,"If we consider all five labels , we get SST - 5 .",dataset,dataset,0,72,11,11,0,dataset : dataset,0.48,0.8461538461538461,0.8461538461538461,If we consider all five labels we get SST 5 ,11,"If we only consider positivity and negativity , we get the binary SST - 2 dataset .","For this research , we evaluate the performance of various models on all nodes as well as on just the root nodes , and on both SST - 2 and SST - 5 .",experiment
named-entity-recognition,0,"Third , compared with TED , both RRSS and ARSS achieve an appreciable advantage .",system description,Prediction Accuracy,1,249,9,9,0,system description : Prediction Accuracy,0.918819188191882,0.4736842105263158,0.4736842105263158,Third compared with TED both RRSS and ARSS achieve an appreciable advantage ,13,"Second , in general , our method performs the best ; for a few cases , our method achieves comparable results with the best performances .",The above analyses are better illustrated in the last row of .,method
natural_language_inference,86,This is the core layer within our MPCM model .,system description,Context Representation Layer,0,75,41,10,0,system description : Context Representation Layer,0.43859649122807015,0.6507936507936508,0.3125,This is the core layer within our MPCM model ,10,Multi - Perspective Context Matching Layer .,The goal of this layer is to compare each contextual embedding of the passage with the question with multi-perspectives .,method
question-answering,6,Forget bias of 2.5 is used for update gates ( no bias for reset gates ) .,model,MODEL DETAILS,0,209,29,29,0,model : MODEL DETAILS,0.6333333333333333,0.3670886075949367,0.8529411764705882,Forget bias of 2 5 is used for update gates no bias for reset gates ,16,"Weights in the QRN unit are initialized using techniques by , and are tied across the layers .",L2 weight decay of 0.001 ( 0.0005 for QA 10 k ) is used for all weights .,method
sentence_compression,3,shows the graphical illustration of our model .,methodology,Task and Framework,0,34,9,8,0,methodology : Task and Framework,0.27419354838709675,0.2647058823529412,0.4705882352941176,shows the graphical illustration of our model ,8,"Three vector representations are concatenated , [ e ( w i ) ; p ( w i ) ; d ( w i ) ] as the input to the next part , policy network .","The policy network is a bi-directional RNN that uses the input [ e ( w i ) ; p ( w i ) ; d ( w i ) ] and yields the hidden states in the forward direction , .. , hf n ) , and hidden states in the backward direction , ( h b 1 , h b 2 , ... , h b n ) .",method
sentiment_analysis,30,"Ultimately , given a sentence s , we are interested in both detecting the mention of an aspect a for target t ( a label other than none ) , and also identifying the specific sentiment y w.r.t. the target - aspect pair .",experiment,Experimental Setup,0,87,8,7,0,experiment : Experimental Setup,0.6796875,0.25806451612903225,0.2333333333333333,Ultimately given a sentence s we are interested in both detecting the mention of an aspect a for target t a label other than none and also identifying the specific sentiment y w r t the target aspect pair ,40,given target tin s .,detailed description of the task is presented in Section 2 .,experiment
natural_language_inference,15,"However , this network is a structure with increasing input features as layers get deeper , and has a large number of parameters especially in the fully - connected layer .",method,Bottleneck component,0,95,37,3,0,method : Bottleneck component,0.42035398230088494,0.74,0.5,However this network is a structure with increasing input features as layers get deeper and has a large number of parameters especially in the fully connected layer ,28,Our network uses all layers ' outputs as a community of semantic knowledge .,"To address this issue , we employ an autoencoder as a bottleneck component .",method
text_summarization,14,"Adam ( Kingma and Ba , 2014 ) optimizer is applied with the learning rate of 0.001 , momentum parameters ? 1 = 0.9 and ? 1 = 0.999 , and = 10 ?8 . For RAML , ? = 0.85 . The mini - batch size is set to 64 .",dataset,Dataset,0,141,8,8,1,dataset : Dataset,0.618421052631579,0.5333333333333333,0.5333333333333333,Adam Kingma and Ba 2014 optimizer is applied with the learning rate of 0 001 momentum parameters 1 0 9 and 1 0 999 and 10 8 For RAML 0 85 The mini batch size is set to 64 ,40,"We use the full source and target vocabularies collected from the training data , which have 119 , 505 and 68 , 885 words , respectively .","We test the model performance ( ROUGE - 2 F1 score ) on validation set for every 2,000 batches .",experiment
relation_extraction,1,"In terms of F 1 , our system obtains the best known score among individual models , but our score is still below that of the interpolation model of because of lower recall .",experiment,Experiments,0,61,10,10,0,experiment : Experiments,0.6853932584269663,1.0,1.0,In terms of F 1 our system obtains the best known score among individual models but our score is still below that of the interpolation model of because of lower recall ,32,leverage the pretrained language model GPT and achieves better recall than our system ., ,experiment
relation_extraction,2,"The dataset contains 10,717 sentences , with each containing two nominals e 1 and e 2 , and the corresponding relation type in the sentence .",dataset,Dataset and Evaluation Metric,0,88,5,5,0,dataset : Dataset and Evaluation Metric,0.6518518518518519,0.5555555555555556,0.5555555555555556,The dataset contains 10 717 sentences with each containing two nominals e 1 and e 2 and the corresponding relation type in the sentence ,25,"The nine relation types are Cause - Effect , Component - Whole , Content - Container , Entity - Destination , Entity - Origin , Instrument - Agency , Member - Collection , Message - Topic and Product - Producer .","The relation is directional , which means that Component - Whole ( e1 , e 2 ) is different from Component - Whole ( e 2 , e 1 ) .",experiment
natural_language_inference,83,The first row describes the story of a man hurting himself .,ablation,Ablation Study,0,205,11,11,0,ablation : Ablation Study,0.6699346405228758,0.5238095238095238,0.5238095238095238,The first row describes the story of a man hurting himself ,12,"Event - sequence , and dark grey and black blocks represent Sentiment - trajectory and Topical consistency respectively .",human reader can guess from commonsense knowledge that people usually recover ( correct ending ) after being hurt and do not repeat their mistake ( incorrect ending ) .,result
natural_language_inference,46,"We collected 1,567 stories , evenly split between books and movie scripts .",method,method,0,126,26,26,0,method : method,0.42424242424242425,0.52,0.52,We collected 1 567 stories evenly split between books and movie scripts ,13,All but 2.3 % of the questions were judged as answerable .,"We partitioned the dataset into non-overlapping training , validation , and test portions , along stories / summaries .",method
natural_language_inference,91,"Finally , it supports a novel tree - sequence hybrid architecture for handling local linear context in sentence interpretation .",introduction,introduction,1,26,18,18,0,introduction : introduction,0.11158798283261803,0.8571428571428571,0.8571428571428571,Finally it supports a novel tree sequence hybrid architecture for handling local linear context in sentence interpretation ,18,"Secondly , it supports batched computation for both parsed and unparsed sentences , yielding dramatic speedups over standard TreeRNNs .",This model is a basically plausible model of human sentence processing and yields substantial accuracy gains over pure sequence - or tree - based models .,introduction
relation-classification,2,"propose a method that relies on RNNs but uses a lot of hand - crafted features and additional NLP tools to extract features such as POS - tags , etc. replicate the context around the entities with Convolutional Neural Networks ( CNNs ) .",introduction,introduction,0,22,12,12,0,introduction : introduction,0.07457627118644068,0.375,0.375,propose a method that relies on RNNs but uses a lot of hand crafted features and additional NLP tools to extract features such as POS tags etc replicate the context around the entities with Convolutional Neural Networks CNNs ,39,"Similarly , in the work of for entity and relation extraction from biomedical text , a model which also uses tree - LSTMs is applied to extract dependency information .","Note that the aforementioned works examine pairs of entities for relation extraction , rather than modeling the whole sentence directly .",introduction
text_generation,1,large airplane is taking off from a runway .,experiment,Human-written,0,221,57,5,0,experiment : Human-written,0.8065693430656934,0.5533980582524272,0.8333333333333334,large airplane is taking off from a runway ,9,bottle of wine near stacks of dishes and food .,Little girl wearing blue clothing carrying purple bag sitting outside cafe .,experiment
sentiment_analysis,32,We can see that IAN achieves the best performance among all baselines .,model,Model Comparisons,1,153,24,24,0,model : Model Comparisons,0.6652173913043479,0.7741935483870968,0.7741935483870968,We can see that IAN achieves the best performance among all baselines ,13,Our IAN model takes a further step towards emphasizing the importance of targets through learning target and context representation interactively .,"Compared with ATAE - LSTM model , IAN improves the performance about 1.4 % and 3.2 % on the Restaurant and Laptop categories respectively .",method
sentiment_analysis,6,We propose several architectural variants of it later in the paper .,method,Long Short-Term Memory,0,123,65,18,0,method : Long Short-Term Memory,0.42560553633218,0.9848484848484848,1.0,We propose several architectural variants of it later in the paper ,12,We term our architecture ' contextual LSTM ' ., ,method
natural_language_inference,53,"Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling , trained on the Stanford Natural Language Inference ( SNLI ) dataset , yields state - of - the - art sentence embeddings compared to all existing alternative unsupervised approaches like SkipThought or FastSent , while be-ing much faster to train .",introduction,introduction,0,23,14,14,0,introduction : introduction,0.11057692307692307,0.9333333333333332,0.9333333333333332,Our experiments show that an encoder based on a bi directional LSTM architecture with max pooling trained on the Stanford Natural Language Inference SNLI dataset yields state of the art sentence embeddings compared to all existing alternative unsupervised approaches like SkipThought or FastSent while be ing much faster to train ,51,"Hence , we investigate the impact of the sentence encoding architecture on representational transferability , and compare convolutional , recurrent and even simpler word composition schemes .",We establish this finding on a broad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information .,introduction
natural_language_inference,91,"With a large but practical batch size of 512 , the largest on which we tested the TreeRNN , our model is about 25 faster than the standard CPU implementation , and about 4 slower than the RNN baseline .",implementation,Inference speed,0,149,55,14,0,implementation : Inference speed,0.6394849785407726,0.9821428571428572,0.9333333333333332,With a large but practical batch size of 512 the largest on which we tested the TreeRNN our model is about 25 faster than the standard CPU implementation and about 4 slower than the RNN baseline ,37,We observe a substantial difference in runtime between the CPU and thin - stack implementations that increases with batch size .,"Though this experiment only covers SPINN - PI - NT , the results should be similar for the full SPINN model : most of the computation involved in running SPINN is involved in populating the buffer , applying the composition function , and manipulating the buffer and the stack , with the low - dimensional tracking and parsing components adding only a small additional load .",experiment
topic_models,0,shows objective ( ELBO ) plotted for two different initializations of variational distribution .,result,result,0,314,5,5,0,result : result,0.7621359223300971,0.10416666666666667,0.10416666666666667,shows objective ELBO plotted for two different initializations of variational distribution ,12,"Hence we initialized the variational distribution to N ( 0 , diag ( 0.1 ) ) to speedup the convergence .","Here , the model was trained on 20 Newsgroups corpus , with the embedding dimension K = 100 , regularization weight ? = 1.0 and prior set to standard Normal .",result
natural_language_inference,23,Many neural network models have been proposed for this challenge and they generally frame this problem as a machine reading comprehension ( MRC ) task .,introduction,introduction,1,29,13,13,0,introduction : introduction,0.056530214424951264,0.38235294117647056,0.38235294117647056,Many neural network models have been proposed for this challenge and they generally frame this problem as a machine reading comprehension MRC task ,24,This is considered a challenging task in artificial intelligence and has already attracted numerous research efforts from the neural network and natural language processing communities .,"The key innovation in recent models lies in how to ingest information in the question and characterize it in the context , in order to provide an accurate answer to the question .",introduction
natural_language_inference,96,"Our work has focused on producing a dataset with minimal annotation artifacts , which in turn helps to avoid some gender and racial biases that stem from elicitation .",dataset,dataset,0,321,7,7,0,dataset : dataset,0.823076923076923,0.4666666666666667,0.4666666666666667,Our work has focused on producing a dataset with minimal annotation artifacts which in turn helps to avoid some gender and racial biases that stem from elicitation ,28,Reducing gender / racial bias Prior work has sought to reduce demographic biases in word embeddings as well as in image recognition models .,"However , it is not perfect in this regard , particularly due to biases in movies .",experiment
relation_extraction,8,The size of g is fixed and is no longer related to the sentence length .,methodology,Piecewise Max Pooling,0,157,73,20,0,methodology : Piecewise Max Pooling,0.5836431226765799,0.7087378640776699,1.0,The size of g is fixed and is no longer related to the sentence length ,16,where g ? R 3 n ., ,method
text_summarization,10,"In our soft - sharing approach , we encourage shared parameters to be close in representation space by penalizing their l 2 distances .",model,Soft vs. Hard Parameter Sharing,0,113,68,7,0,model : Soft vs. Hard Parameter Sharing,0.4296577946768061,0.7311827956989247,0.5,In our soft sharing approach we encourage shared parameters to be close in representation space by penalizing their l 2 distances ,22,"As a result , gradient information from multiple tasks will directly pass through shared parameters , hence forcing a common space representation for all the related tasks .","Unlike hard sharing , this approach gives more flexibility for the tasks by only loosely coupling the shared space representations .",method
machine-translation,0,This covers approximately 93 % of the dataset .,baseline,Data and Baseline System,0,133,14,14,0,baseline : Data and Baseline System,0.6073059360730594,0.8235294117647058,0.8235294117647058,This covers approximately 93 of the dataset ,8,"For training the neural networks , including the proposed RNN Encoder - Decoder , we limited the source and target vocabulary to the most frequent 15,000 words for both English and French .",All the out - of - vocabulary words were mapped to a special token ( [ UNK ] ) .,result
smile_recognition,0,"Training time per epoch are 82 seconds and 41 seconds for the mouth and face input models , respectively .",experiment,experiment,0,82,22,22,0,experiment : experiment,0.9111111111111112,0.88,0.88,Training time per epoch are 82 seconds and 41 seconds for the mouth and face input models respectively ,19,"In the entire face , that sort of distortions maybe less of a problem because other parts of the face such as the cheeks contribute to smile recognition , too .",Experiments have shown that the training time mostly depends on the number of convolutions .,experiment
natural_language_inference,54,"The fixed word embedding has a dimension of 100 , which is provided by the GloVe data set .",experiment,Experiment Setting,1,130,18,8,0,experiment : Experiment Setting,0.5676855895196506,0.6923076923076923,0.5,The fixed word embedding has a dimension of 100 which is provided by the GloVe data set ,18,The max length of SQuAD is 16 which means there are a maximum 16 words in a sentence .,The settings for syntactic embedding are slightly different for each model .,experiment
smile_recognition,0,"Unsupervised pre-training methods , such as autoencoders 8 allow to initialize the weights well in order for backpropagation to quickly optimize them .",system description,Deep neural networks,0,26,6,6,0,system description : Deep neural networks,0.2888888888888889,0.18181818181818185,0.18181818181818185,Unsupervised pre training methods such as autoencoders 8 allow to initialize the weights well in order for backpropagation to quickly optimize them ,23,"In order to overcome these issues , a variety of new concepts have been proposed in the literature , of which only a few can be named in this chapter .",The Rectified Linear Unit ( ReLU ) 7 and dropout 10 are new regularization methods .,method
sentiment_analysis,46,"Each sample is marked as very negative , negative , neutral , positive , or very positive .",dataset,Datasets and Sentiment Resources,0,85,6,6,0,dataset : Datasets and Sentiment Resources,0.6967213114754098,0.75,0.75,Each sample is marked as very negative negative neutral positive or very positive ,14,"SST consists of 8,545 training samples , 1,101 validation samples , 2210 test samples .","Sentiment lexicon combines the sentiment words from both and , resulting in 10,899 sentiment words in total .",experiment
text_summarization,10,"Also , the supplementary shows some output examples from our entailment generation model .",result,Entailment Generation,0,186,11,9,0,result : Entailment Generation,0.7072243346007605,0.7333333333333333,0.6923076923076923,Also the supplementary shows some output examples from our entailment generation model ,13,"Our pointer mechanism gives a performance boost , since the entailment generation task involves copying from the given premise sentence , whereas the 2 - layer model seems comparable to the 1 - layer model .","Again , we use same architecture as described in Sec. 3.1 along with pointer mechanism for the task of question generation .",result
natural_language_inference,69,first observation is that there is a significant bias in the answer distribution of WIKIREADING .,dataset,Mitigating Dataset Biases,0,122,5,5,0,dataset : Mitigating Dataset Biases,0.3536231884057971,0.625,0.625,first observation is that there is a significant bias in the answer distribution of WIKIREADING ,16,"As only carried out limited analysis of their WIKIREADING dataset , we present an analysis of the downstream effects we observe on WIKIHOP .","For example , in the majority of the samples the property country has the United States of America as the answer .",experiment
natural_language_inference,46,We consider basic IR baselines which retrieve an answer by selecting a span of tokens from the context document based on a similarity measure between the candidate span and a query .,baseline,baseline,0,156,2,2,0,baseline : baseline,0.5252525252525253,0.060606060606060615,0.060606060606060615,We consider basic IR baselines which retrieve an answer by selecting a span of tokens from the context document based on a similarity measure between the candidate span and a query ,32, ,We compare two queries : the question and ( as an oracle ) the gold standard answer .,result
natural_language_inference,40,"Continuing with our motivating example , shows an example where the first sequence is a context passage and the second sequence is a question posed over the passage .",method,Multiple Sequences,0,133,56,3,0,method : Multiple Sequences,0.48014440433213,0.8615384615384616,0.25,Continuing with our motivating example shows an example where the first sequence is a context passage and the second sequence is a question posed over the passage ,28,"In certain applications , we have multiple sequences whose elements interact with each other via known relationships .","The sequences are further augmented with coreference and hypernymy relations , resulting in an undirected cyclic graph .",method
sentiment_analysis,31,convolution filter w ? R hk with width h is applied to the word matrix to get high - level representative features .,system description,Convolutional Neural Networks,0,57,13,7,0,system description : Convolutional Neural Networks,0.3584905660377358,0.23214285714285715,0.3684210526315789,convolution filter w R hk with width h is applied to the word matrix to get high level representative features ,21,"sentence of length n can be represented as a matrix s = [ v 1 , v 2 , ... , v n ] ? R nk .",convolution filter w ? R hk with width h is applied to the word matrix to get high - level representative features .,method
natural_language_inference,75,For some question there can be multiple correct answers .,model,Question-Answer Pairs,0,153,104,4,0,model : Question-Answer Pairs,0.7536945812807881,0.6753246753246753,0.3636363636363637,For some question there can be multiple correct answers ,10,"They range in scope from specific - such as actor to movie : "" What movies did Harrison Ford star in ? "" and movie to actors : "" Who starred in Blade Runner ? "" - to more general , such as tag to movie : "" Which films can be described by dystopian ? "" ; see for the full list .","Using SimpleQuestions , an existing open - domain question answering dataset based on Freebase , we identified the subset of questions posed by human annotators that covered our question types .",method
relation-classification,9,"In addition , SCIBERT outperforms the SOTA on Sci - Cite .",result,Multiple Domains,1,124,20,3,0,result : Multiple Domains,0.8435374149659864,0.9523809523809524,0.75,In addition SCIBERT outperforms the SOTA on Sci Cite ,10,We observe that SCIBERT outperforms BERT - Base on the multidomain tasks ( + 0.49 F1 with finetuning and + 0.93 F1 without ) .,No prior published SOTA results exist for the Paper Field dataset .,result
natural_language_inference,47,"First , the examples were grounded in specific scenarios , and the premise and hypothesis sentences in each example were constrained to describe that scenario from the same perspective , which helps greatly in controlling event and entity coreference .",system description,new corpus for NLI,0,55,20,20,0,system description : new corpus for NLI,0.2558139534883721,0.2531645569620253,0.9090909090909092,First the examples were grounded in specific scenarios and the premise and hypothesis sentences in each example were constrained to describe that scenario from the same perspective which helps greatly in controlling event and entity coreference ,37,"To do this , we employed a crowdsourcing framework with the following crucial innovations .","Second , the prompt gave participants the freedom to produce entirely novel sentences within the task setting , which led to richer examples than we see with the more proscribed string - editing techniques of earlier approaches , without sacrificing consistency .",method
natural_language_inference,75,"As a result , even if KBs can be satisfactory for closed - domain problems , they are unlikely to scale up to answer general questions on any topic .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.07881773399014777,0.3,0.3,As a result even if KBs can be satisfactory for closed domain problems they are unlikely to scale up to answer general questions on any topic ,27,"Since information extraction ( IE ) , intended to fill in missing information in KBs , is neither accurate nor reliable enough , collections of raw textual resources and documents such as Wikipedia will always contain more information .","Starting from this observation , in this work we study the problem of answering by directly reading documents .",introduction
machine-translation,2,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs multi-head attention over the output of the encoder stack .",model,Encoder and Decoder Stacks,1,58,17,11,0,model : Encoder and Decoder Stacks,0.25892857142857145,0.1559633027522936,0.7857142857142857,In addition to the two sub layers in each encoder layer the decoder inserts a third sub layer which performs multi head attention over the output of the encoder stack ,31,The decoder is also composed of a stack of N = 6 identical layers .,"Similar to the encoder , we employ residual connections around each of the sub-layers , followed by layer normalization .",method
sentiment_analysis,22,"In the following , we describe our encoding layer in detail .",model,model,0,75,16,16,0,model : model,0.3275109170305677,0.5161290322580645,0.5161290322580645,In the following we describe our encoding layer in detail ,11,"In this paper , we apply a Bi - GRU to learn a more abstract representation of the sentence .","In the encoding phase , we first transform each token in the sentence into a real - valued vector using the concatenation of the following vectors :",method
machine-translation,7,Our metrics of expert utilization change to the following :,APPENDICES A LOAD-BALANCING LOSS,HIERACHICAL MIXTURE OF EXPERTS,0,256,34,6,0,APPENDICES A LOAD-BALANCING LOSS : HIERACHICAL MIXTURE OF EXPERTS,0.6863270777479893,0.2251655629139073,0.6,Our metrics of expert utilization change to the following ,10,The output of the MoE is given by :,Load primary and Load i deonte the Load functions for the primary gating network and i th secondary gating network respectively .,others
natural_language_inference,44,"Among the answerable examples , 92 % are answerable with a single sentence , 6 % with two sentences , and 2 % with three or more sentences .",system description,Human studies,0,38,25,6,0,system description : Human studies,0.13286713286713284,0.5952380952380952,0.6666666666666666,Among the answerable examples 92 are answerable with a single sentence 6 with two sentences and 2 with three or more sentences ,23,"For instance , in the last example in , the question requires the background knowledge that Charles Dickens is an English Victorian author .",We perform a similar analysis on the TriviaQA ( Wikipedia ) development ( verified ) set .,method
relation_extraction,4,3 ) alone contributes about 1 % F 1 score .,analysis,analysis,1,168,5,5,0,analysis : analysis,0.8115942028985508,0.2380952380952381,0.2380952380952381,3 alone contributes about 1 F 1 score ,9,"The entire attention mechanism contributes about 1.5 % F 1 , where the position - aware term in Eq.",Impact of negative examples .,result
sentiment_analysis,35,"Second , considering that a sentence may contain multiple aspects with different sentiments , thus capturing incorrect sentiment features towards the aspect can mislead feature alignment .",introduction,introduction,0,44,30,30,0,introduction : introduction,0.1774193548387097,0.8571428571428571,0.8571428571428571,Second considering that a sentence may contain multiple aspects with different sentiments thus capturing incorrect sentiment features towards the aspect can mislead feature alignment ,25,"The C2F module makes up these missing information for the source task , which effectively reduces the aspect granularity gap between tasks and facilitates the subsequent feature alignment .","To prevent false alignment , we adopt the Contrastive Feature Alignment ( CFA ) ( Motiian et al. 2017 ) to semantically align aspect - specific representations .",introduction
question_answering,2,"For multiple - choices questions , the task is to se - lect one answer from a few candidate choices given the context and the question .",architecture,Focal Visual-Text Attention,0,100,21,8,0,architecture : Focal Visual-Text Attention,0.36101083032490977,0.1926605504587156,0.4,For multiple choices questions the task is to se lect one answer from a few candidate choices given the context and the question ,24,"Output Layer After summarizing the input using the FVTA attention , we use a feed - forward layer to obtain the answer candidate .","Let k denote the number of candidate answers , we utilize the bi-directional LSTM to encode each of the answer choice and use the last hidden state as the representation for answers E ? R k2d .",method
sentiment_analysis,26,"Then , each vocabulary word index j is assigned a new ndimensional word vector x j = ( w 1 , j , , w n , j ) that incorporates the linear coefficients for that word across the different linear models .",approach,Sentiment Embedding Computation,0,36,15,14,0,approach : Sentiment Embedding Computation,0.1469387755102041,0.17857142857142858,0.3783783783783784,Then each vocabulary word index j is assigned a new ndimensional word vector x j w 1 j w n j that incorporates the linear coefficients for that word across the different linear models ,35,"Specifically , we train n linear models f i ( x ) = w ix + bi for tasks i = 1 , . . . , n.",minor challenge is that navely using bag - ofword features can lead to counter - intuitive weights .,method
sentiment_analysis,34,"In this paper , we present Mazajak 2 , an Online Arabic sentiment analysis system that utilises deep learning and massive Arabic word embeddings .",introduction,introduction,1,27,19,19,0,introduction : introduction,0.18243243243243248,0.95,0.95,In this paper we present Mazajak 2 an Online Arabic sentiment analysis system that utilises deep learning and massive Arabic word embeddings ,23,"However , it uses a basic dictionary - based approach that works with Arabic MSA and terribly fails with dialects which is the main language used in social media .",The system is available as an online API that can be used by other researchers .,introduction
named-entity-recognition,4,"At the input layer , the task model favors the first biLSTM layer .",analysis,Alternate layer weighting schemes,0,183,63,57,0,analysis : Alternate layer weighting schemes,0.6727941176470589,0.9264705882352942,0.9193548387096774,At the input layer the task model favors the first biLSTM layer ,13,visualizes the softmax - normalized learned layer weights .,"For coreference and SQuAD , the this is strongly favored , but the distribution is less peaked for the other tasks .",result
sentiment_analysis,1,"Lan et al. compared several domain adaptation techniques such as maximum independence domain adaptation ( MIDA ) , transfer component analysis ( TCA ) , subspace alignment ( SA ) , etc .",introduction,introduction,0,43,30,30,1,introduction : introduction,0.1085858585858586,0.5263157894736842,0.5263157894736842,Lan et al compared several domain adaptation techniques such as maximum independence domain adaptation MIDA transfer component analysis TCA subspace alignment SA etc ,24,"In recent years , several studies , investigated the transferability of EEG - based emotion recognition models across subjects .","Lan et al. compared several domain adaptation techniques such as maximum independence domain adaptation ( MIDA ) , transfer component analysis ( TCA ) , subspace alignment ( SA ) , etc .",introduction
named-entity-recognition,4,"For some tasks ( e.g. , SNLI , SQuAD ) , we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with [ h k ; ELMo task k ] .",model,Using biLMs for supervised NLP tasks,0,80,33,10,0,model : Using biLMs for supervised NLP tasks,0.29411764705882354,0.5689655172413793,0.6666666666666666,For some tasks e g SNLI SQuAD we observe further improvements by also including ELMo at the output of the task RNN by introducing another set of output specific linear weights and replacing h k with h k ELMo task k ,42,"To add ELMo to the supervised model , we first freeze the weights of the biLM and then concatenate the ELMo vector ELMo task k with x k and pass the ELMo enhanced representation [ x k ; ELMo task k ] into the task RNN .","As the remainder of the supervised model remains unchanged , these additions can happen within the context of more complex neural models .",method
sentiment_analysis,43,"Given the word embeddings of a context sentence sand a corresponding aspect a j , we will employ the BiLSTM separately and get the sentence contextual output H ? R 2 d * N and aspect contextual output Q ? R 2 d * M .",approach,Contextual Layer,0,75,20,8,0,approach : Contextual Layer,0.3125,0.2298850574712644,0.5714285714285714,Given the word embeddings of a context sentence sand a corresponding aspect a j we will employ the BiLSTM separately and get the sentence contextual output H R 2 d N and aspect contextual output Q R 2 d M ,41,The backward LSTM does the similar process and we can get the concatenated output,"Given the word embeddings of a context sentence sand a corresponding aspect a j , we will employ the BiLSTM separately and get the sentence contextual output H ? R 2 d * N and aspect contextual output Q ? R 2 d * M .",method
natural_language_inference,45,We assume c and Ew have the same length d e in this work .,system description,WORD-CHARACTER FINE-GRAINED GATING,0,97,29,7,0,system description : WORD-CHARACTER FINE-GRAINED GATING,0.4874371859296482,0.453125,0.3043478260869565,We assume c and Ew have the same length d e in this work ,15,We perform a matrix - vector multiplication Ew to obtain a word - level representation .,"Previous methods defined f using the word - level representation Ew , the character - level representation c , or the concatenation [ Ew ; c ] .",method
natural_language_inference,75,There are a variety of ways to employ key - value memories that can have important effects on over all performance .,model,Key-Value Memories,0,80,31,2,0,model : Key-Value Memories,0.3940886699507389,0.2012987012987013,0.04878048780487805,There are a variety of ways to employ key value memories that can have important effects on over all performance ,21, ,"The ability to encode prior knowledge in this way is an important component of KV - MemNNs , and we are free to define ? X , ? Y , ? K and ? V for the query , answer , keys and values respectively .",method
natural_language_inference,47,"With only 100 training examples , the cross-bigram classifier is already performing better .",analysis,Analysis and discussion,0,175,7,7,0,analysis : Analysis and discussion,0.813953488372093,0.175,0.2692307692307692,With only 100 training examples the cross bigram classifier is already performing better ,14,The minibatch size of 64 that we used to tune the LSTM sets a lower bound on data for that model .,"Empirically , we find that the top weighted features for the classifier trained on 100 examples tend to be high precision entailments ; e.g. , playing ? outside ( most scenes are outdoors ) , a banana ? person eating .",result
natural_language_inference,90,"gives a breakdown of accuracy on the development set showing that most of our gains stem from neutral , while most losses come from contradiction pairs .",result,Results,0,132,5,5,0,result : Results,0.88,0.2631578947368421,0.2631578947368421,gives a breakdown of accuracy on the development set showing that most of our gains stem from neutral while most losses come from contradiction pairs ,26,Adding intra-sentence attention gives a considerable improvement of 0.5 percentage points over the existing state of the art .,shows some wins and losses .,result
sentiment_analysis,15,Most of the compositionality algorithms and related datasets capture two word compositions .,system description,Compositionality in Vector Spaces.,0,46,10,2,0,system description : Compositionality in Vector Spaces.,0.17037037037037034,0.18867924528301888,0.25,Most of the compositionality algorithms and related datasets capture two word compositions ,13, ,"Mitchell and Lapata ( 2010 ) use e.g. two - word phrases and analyze similarities computed by vector addition , multiplication and others .",method
sentiment_analysis,35,"e , d h , d u are set to be 200 , 150 and 100 , respectively .",implementation,implementation,0,185,3,3,0,implementation : implementation,0.7459677419354839,0.2307692307692308,0.2307692307692308,e d h d u are set to be 200 150 and 100 respectively ,15,The word embeddings are initialized with 200 - dimension GloVE vectors and fine - tuned during the training .,The fc layer size is 300 .,experiment
natural_language_inference,73,"Particularly , ReSAN outperforms the feature engineering method by a large margin , e.g. , Meaning Factory and ECNU .",experiment,Semantic Relatedness,0,211,31,6,0,experiment : Semantic Relatedness,0.8053435114503816,0.6888888888888889,0.6666666666666666,Particularly ReSAN outperforms the feature engineering method by a large margin e g Meaning Factory and ECNU ,18,|. The representation is fed into a classifica - show that the ReSAN achieves state - of - the - art or competitive performance for all three metrics .,"ReSAN also significantly outperforms the recursive models , which is widely used in semantic relatedness task , especially ones that demand external parsing results , e.g. , DT / SDT - RNN and Tree - LSTMs .",experiment
natural_language_inference,81,"The interior consists of a plateau characterised by sand and lava fields , mountains and glaciers , while many glacial rivers flow to the sea through the lowlands .",APPENDIX,Answer town,0,341,133,53,0,APPENDIX : Answer town,0.8099762470308789,0.6244131455399061,0.3984962406015037,The interior consists of a plateau characterised by sand and lava fields mountains and glaciers while many glacial rivers flow to the sea through the lowlands ,27,Iceland is volcanically and geologically active .,"Iceland is warmed by the Gulf Stream and has a temperate climate , despite a high latitude just outside the Arctic Circle .",others
natural_language_inference,56,Recurrent node updates .,system description,Recurrent Relational Networks,0,63,22,22,0,system description : Recurrent Relational Networks,0.1875,0.5789473684210527,0.5789473684210527,Recurrent node updates ,4,"In our experiments , since the messages in ( 1 ) are linear , this is similar to how log-probabilities are summed in belief propagation .",Finally we update the node hidden state via,method
sentiment_analysis,46,"SST consists of 8,545 training samples , 1,101 validation samples , 2210 test samples .",dataset,Datasets and Sentiment Resources,0,84,5,5,0,dataset : Datasets and Sentiment Resources,0.6885245901639344,0.625,0.625,SST consists of 8 545 training samples 1 101 validation samples 2210 test samples ,15,We adopt the same data split as in .,"Each sample is marked as very negative , negative , neutral , positive , or very positive .",experiment
sentiment_analysis,16,"As these two sentiment words can modify both aspects , we can construct four snippets "" high price "" , "" low price "" , "" high resolution "" and "" low resolution "" .",system description,Problem of the above Model for Target-Sensitive Sentiment,0,95,43,17,0,system description : Problem of the above Model for Target-Sensitive Sentiment,0.29874213836477986,0.6142857142857143,0.8947368421052632,As these two sentiment words can modify both aspects we can construct four snippets high price low price high resolution and low resolution ,24,"We also have two possible context words "" high "" and "" low "" ( denoted ash and l ) .","Their sentiments are negative , positive , positive , and negative respectively .",method
sentiment_analysis,9,"In addition , for the first time , BERT - SPC has increased the F 1 score of ATE subtask on three datasets up to 99 % .",analysis,Overall Performance Analysis,1,247,13,13,0,analysis : Overall Performance Analysis,0.8916967509025271,0.3714285714285713,0.8666666666666667,In addition for the first time BERT SPC has increased the F 1 score of ATE subtask on three datasets up to 99 ,24,"Compared with the BERT - BASE model , BERT - SPC significantly improves the accuracy and F 1 score of aspect polarity classification .","ATEPC - Fusion is a supplementary scheme of LCF mechanism , and it adopts a moderate approach to generate local context features .",result
text-classification,7,More comprehensive comparison results are demonstrated in . 4 in Supplementary Material .,ablation,Ablation Study,0,155,5,5,0,ablation : Ablation Study,0.6378600823045267,0.08928571428571429,1.0,More comprehensive comparison results are demonstrated in 4 in Supplementary Material ,12,"Generally , all three proposed dynamic routing strategies contribute to the effectiveness of Capsule - B by alleviating the disturbance of some noise capsules which may contain "" background "" information such as stop words and the words that are unrelated to specific categories .", ,result
query_wellformedness,0,Every query was labeled by five different crowdworkers with a binary label indicating whether a query is well - formed or not .,dataset,Dataset Construction,0,38,12,12,0,dataset : Dataset Construction,0.3958333333333333,0.48,0.48,Every query was labeled by five different crowdworkers with a binary label indicating whether a query is well formed or not ,22,shows some examples that were shown to the annotators to illustrate each of the above conditions .,We average the ratings of the five annotators to get the probability of a query being well - formed .. 1 shows some queries with obtained human annotation .,experiment
text_summarization,1,"In order to factorize the multimodal distribution into the two stages ( select and generate ) , we introduce a latent variable called focus .",analysis,Select and Generate,0,87,8,2,0,analysis : Select and Generate,0.3625,0.20512820512820512,0.060606060606060615,In order to factorize the multimodal distribution into the two stages select and generate we introduce a latent variable called focus ,22, ,"The intuition is that in the select stage we sample several meaningful focus , each of which indicates which part of the source sequence should be considered important .",result
semantic_role_labeling,3,Semantic Role Labeling ( SRL ) is believed to be a crucial step towards natural language understanding and has been widely studied .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.015151515151515154,0.25,0.25,Semantic Role Labeling SRL is believed to be a crucial step towards natural language understanding and has been widely studied ,21, ,"Recent years , end - to - end SRL with recurrent neural networks ( RNN ) has gained increasing attention .",abstract
natural_language_inference,34,Then we obtain the updated entity embeddings,system description,Reasoning with the Fusion Block,0,172,69,39,0,system description : Reasoning with the Fusion Block,0.5830508474576271,0.6831683168316832,0.7090909090909091,Then we obtain the updated entity embeddings,7,Bi is the set of neighbors of entity i .,"reasoning chain contains multiple steps , and the newly visited entities by one step will be the start entities of the next step .",method
natural_language_inference,89,"The model is a multi-layer Transformer decoder , which is first trained with a language modeling objective on a large unlabeled text corpus and then finetuned on the specific target task .",approach,Answer Verifier,0,111,45,8,0,approach : Answer Verifier,0.4269230769230769,0.5113636363636364,0.1568627450980392,The model is a multi layer Transformer decoder which is first trained with a language modeling objective on a large unlabeled text corpus and then finetuned on the specific target task ,32,Then we adapt the recently proposed Generative Pre-trained Transformer ( OpenAI GPT ) to perform the task .,"Specifically , given an answer sentence S , a question Q and an extracted answer A , we concatenate the two sentences with the answer while adding a delimiter token in between to get [ S ; Q ; $ ; A ] .",method
text_summarization,4,Text d Linked entity : https://en.wikipedia.org/wiki/United_States,result,Results,0,244,50,50,0,result : Results,0.9457364341085271,0.8620689655172413,0.8620689655172413,Text d Linked entity https en wikipedia org wiki United_States,10,We only report the attention scores of entities in the Gigaword example for conciseness since there are 80 linked entities in the CNN example .,"E1.1 : andy roddick got the better of dmitry tursunov in straight sets on friday , assuring the @united states a # - # lead over defending champions russia in the #### davis cup final . 0.719 E1.2 : sir alex ferguson revealed friday that david beckham 's move to the @united states had not surprised him because he knew the midfielder would not return to england if he could not comeback to manchester united .",result
natural_language_inference,17,"Here , we consider both random inference and greedy inference as two different sampling strategies : the first one encourages exploration while the latter one is for exploitation 1 .",architecture,Alignment Architecture for MRC,0,115,72,72,0,architecture : Alignment Architecture for MRC,0.4423076923076923,0.5255474452554745,0.5538461538461539,Here we consider both random inference and greedy inference as two different sampling strategies the first one encourages exploration while the latter one is for exploitation 1 ,28,We refer to this case as the convergence suppression problem .,"Therefore , we approximate the expected gradient by dynamically set the reward and baseline based on the F 1 scores of both As and .",method
natural_language_inference,9,"For vector gates , we have z t ? Rd instead of z t ? R .",TASK-WISE RESULTS,VECTOR GATE PARALLELIZATION,0,277,11,2,0,TASK-WISE RESULTS : VECTOR GATE PARALLELIZATION,0.8393939393939394,0.5238095238095238,0.16666666666666666,For vector gates we have z t Rd instead of z t R ,14, ,"For vector gates , we have z t ? Rd instead of z t ? R .",result
paraphrase_generation,0,The conditional probability of a sentence token at a particular time step is modeled using an LSTM as used in machine translation .,method,Decoder-LSTM,0,92,32,4,0,method : Decoder-LSTM,0.3881856540084388,0.5161290322580645,0.25,The conditional probability of a sentence token at a particular time step is modeled using an LSTM as used in machine translation ,23,RNN provides a nice way to condition on previous state value using a fixed length hidden vector .,"At time step t , the conditional probability is denoted by P",method
named-entity-recognition,3,We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain .,analysis,analysis,0,151,25,25,0,analysis : analysis,0.8162162162162162,1.0,1.0,We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain ,23,"For this task , Tag LM increased F 1 on the development set by 4.12 % ( from 49.93 to to 54.05 % ) for entity extraction over our baseline without LM embeddings and it was a major component in our winning submission to Science IE , Scenario 1 .", ,result
natural_language_inference,39,"Our model outperforms the work of , which already reports a Pearson 's r score of over 0.9 , STS2014 Results ) .",experiment,Experimental Setup,0,167,37,37,0,experiment : Experimental Setup,0.8106796116504854,0.925,0.925,Our model outperforms the work of which already reports a Pearson s r score of over 0 9 STS2014 Results ,21,"Note that we used the same word embeddings , sparse distribution targets , and loss function as in and , thereby representing comparable experimental conditions . ) .",Systems in the competition are ranked by the weighted mean ( the of - Model Pearson 's r Beltagy et al . 0.8300 0.8730 0.8803 0.9090,experiment
text-to-speech_synthesis,2,We train the speaker embedding network on a speaker verification task to determine if two different utterances were spoken by the same speaker .,introduction,introduction,1,20,12,12,0,introduction : introduction,0.08163265306122447,0.3157894736842105,0.3157894736842105,We train the speaker embedding network on a speaker verification task to determine if two different utterances were spoken by the same speaker ,24,"Decoupling the networks enables them to be trained on independent data , which reduces the need to obtain high quality multispeaker training data .","In contrast to the subsequent TTS model , this network is trained on untranscribed speech containing reverberation and background noise from a large number of speakers .",introduction
natural_language_inference,3,"Specifically , we introduce two coupled ways to model the interdependences of two LSTMs , coupling the local contextualized interactions of two sentences .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03365384615384615,0.7142857142857143,0.7142857142857143,Specifically we introduce two coupled ways to model the interdependences of two LSTMs coupling the local contextualized interactions of two sentences ,22,"In this paper , we propose a deep architecture to model the strong interaction of sentence pair with two coupled - LSTMs .",We then aggregate these interactions and use a dynamic pooling to select the most informative features .,abstract
natural_language_inference,25,"We additionally evaluate the incorporation of that model 's word - type representations ( referred to as RaSoR + TR + LM ( emb ) ) , which are based on characterlevel embeddings and are naturally unaffected by context around a word - token .",model,Incorporating language model representations,0,82,4,4,0,model : Incorporating language model representations,0.7884615384615384,0.4,0.4,We additionally evaluate the incorporation of that model s word type representations referred to as RaSoR TR LM emb which are based on characterlevel embeddings and are naturally unaffected by context around a word token ,36,In we list development set results for using either the LM hidden states of the first stacked LSTM layer or those of the second one .,"Overall , we observe a significant improvement with all three configurations , effectively showing the benefit of training a QA model in a semisupervised fashion with a large language model .",method
sentiment_analysis,11,An effective attention mechanism provides the capability to capture this pattern .,system description,Emotional Influence Patterns,0,234,90,10,0,system description : Emotional Influence Patterns,0.6802325581395349,0.9090909090909092,0.5263157894736842,An effective attention mechanism provides the capability to capture this pattern ,12,"In case 3 , the utterance has relevant content in the histories that do not precede immediately .",Finally case 4 presents the situation when the utterance is independent of the history .,method
natural_language_inference,43,"We train our model by maximizing the regularized conditional log-likelihood objective N i = 1 log p ? ( a ( i ) | q ( i ) , R ( i ) ) + ? ||?|| 2 2 .",model,Candidate Ranking,0,91,14,7,0,model : Candidate Ranking,0.5870967741935483,0.6086956521739131,0.4375,We train our model by maximizing the regularized conditional log likelihood objective N i 1 log p a i q i R i 2 2 ,26,Rd is a feature function .,"At test time , we return the most probable answers based on p ? ( a | q , R ) ( details in Section 4 ) .",method
natural_language_inference,65,"We use a context window of size 3 for Insurance QA , while we set this parameter to 4 for TREC - QA and Wiki QA .",system description,Neural Networks Setup,1,171,9,5,0,system description : Neural Networks Setup,0.7434782608695653,0.5625,0.4166666666666667,We use a context window of size 3 for Insurance QA while we set this parameter to 4 for TREC QA and Wiki QA ,25,The size of the word embeddings is different due to the different pre-trained versions that we used for Insurance QA and the other two datasets .,"Using the selected hyperparameters , the best results are normally achieved using between 15 and 25 training epochs .",method
machine-translation,9,The evaluation is performed using a standard kytea - based post -processing script for this dataset .,analysis,MACHINE TRANSLATION,0,221,44,12,0,analysis : MACHINE TRANSLATION,0.7700348432055749,0.5,0.375,The evaluation is performed using a standard kytea based post processing script for this dataset ,16,The vocabulary size for each language is reduced to 40K using byte - pair encoding .,"In our preliminary experiments , we found a 32 16 coding works well for a vanilla NMT model .",result
named-entity-recognition,8,"The feature - based approach , such as ELMo , uses task - specific architectures that include the pre-trained representations as additional features .",introduction,introduction,0,17,5,5,0,introduction : introduction,0.04392764857881137,0.2,0.2,The feature based approach such as ELMo uses task specific architectures that include the pre trained representations as additional features ,21,There are two existing strategies for applying pre-trained language representations to downstream tasks : feature - based and fine - tuning .,"The fine - tuning approach , such as the Generative Pre-trained Transformer ( OpenAI GPT ) , introduces minimal task - specific parameters , and is trained on the downstream tasks by simply fine - tuning all pretrained parameters .",introduction
natural_language_inference,47,We will show you the caption for a photo .,system description,Data collection,0,65,30,8,0,system description : Data collection,0.3023255813953488,0.3797468354430379,0.26666666666666666,We will show you the caption for a photo ,10,"It warned about dis allowed techniques ( e.g. , reusing the same sentence for many different prompts , which we saw in a few cases ) , provided guidance concerning sentence length and",We will not show you the photo .,method
natural_language_inference,52,"where F ( a + ) and F ( a ? ) are the model scores for a correct and incorrect answer candidate and m is the margin , and backpropagate the gradients .",model,Neural Network,0,135,53,7,0,model : Neural Network,0.5172413793103449,0.9464285714285714,0.7,where F a and F a are the model scores for a correct and incorrect answer candidate and m is the margin and backpropagate the gradients ,27,We compute the pair - wise margin ranking loss for each training pair :,"At testing time , we use the trained model to score each answer choice ( again using the maximum justification score ) and select the highest - scoring .",method
sentiment_analysis,9,"In addition , experimental result on the Laptop dataset also prove the effectiveness of domain - adaption in multi-task learning .",model,Model,0,233,12,12,1,model : Model,0.8411552346570397,0.9230769230769232,0.9230769230769232,In addition experimental result on the Laptop dataset also prove the effectiveness of domain adaption in multi task learning ,20,"The accuracy benchmark in the classical Restaurant achieving more than 90 % , which means that the LCF - ATEPC is the first ABSAoriented model obtained up to 90 % accuracy on the Restaurant dataset .","Besides , the experimental results on the laptop dataset also validate the effectiveness of domain - adapted BERT model for ABSA multi-task learning .",method
topic_models,0,"13 ) involves solving the expectation over log - sum - exp operation ( denoted by F ) , which is intractable .",model,model,0,93,38,38,0,model : model,0.22572815533980586,0.7307692307692307,0.8260869565217391,13 involves solving the expectation over log sum exp operation denoted by F which is intractable ,17,The term B from ( 11 ) is the expectation over log -likelihood of a document :,"It appears when dealing with variational inference in mixed - logit models , .",method
natural_language_inference,13,We vary this in our experiments in order to benchmark the effectiveness of our proposed MRU encoder .,model,Multiple Choice Models,0,110,18,13,0,model : Multiple Choice Models,0.480349344978166,0.36,0.4814814814814815,We vary this in our experiments in order to benchmark the effectiveness of our proposed MRU encoder ,18,Typical choices of encoders in this layer are LSTMs or GRUs .,The output of this layer is same dimensions as its inputs ( typically the hidden states of a RNN model ) .,method
natural_language_inference,60,"First , it is embedding - agnostic , meaning that one of the main ( and perhaps most important ) hyperparameters in NLP pipelines is made obsolete .",introduction,introduction,1,22,15,15,0,introduction : introduction,0.11398963730569947,0.6521739130434783,0.6521739130434783,First it is embedding agnostic meaning that one of the main and perhaps most important hyperparameters in NLP pipelines is made obsolete ,23,Why Is This a Good Idea ? Our technique brings several important benefits to NLP applications .,"Second , as we will show , it leads to improved performance on a variety of tasks .",introduction
natural_language_inference,17,We conducted extensive experiments on both the SQuAD dataset and two adversarial SQuAD datasets to evaluate the proposed model .,introduction,introduction,0,37,28,28,0,introduction : introduction,0.1423076923076923,0.9333333333333332,0.9333333333333332,We conducted extensive experiments on both the SQuAD dataset and two adversarial SQuAD datasets to evaluate the proposed model ,20,All of the above innovations are integrated into a new end - to - end neural architecture called Reinforced Mnemonic Reader in .,"On SQuAD , our single model obtains an exact match ( EM ) score of 79.5 % and F1 score of 86.6 % , while our ensemble model further boosts the result to 82.3 % and 88.5 % respectively .",introduction
natural_language_inference,0,"In this paper , we focus on combining both in a complementary manner , by designing a novel attention mechanism which gates the evolving token representations across hops .",introduction,introduction,0,22,11,11,0,introduction : introduction,0.1116751269035533,0.6875,0.6875,In this paper we focus on combining both in a complementary manner by designing a novel attention mechanism which gates the evolving token representations across hops ,27,The effectiveness of multi-hop reasoning and attentions have been explored orthogonally so far in the literature .,"More specifically , unlike existing models where the query attention is applied either token - wise or sentence - wise to allow weighted aggregation , the Gated - Attention ( GA ) module proposed in this work allows the query to directly interact with each dimension of the token embeddings at the semantic - level , and is applied layer - wise as information filters during the multi-hop representation learning process .",introduction
natural_language_inference,29,Our experiments verify that adversarial learning is capable to significantly improve the denoising and facts extracting capacity of PAAG .,introduction,introduction,0,46,34,34,0,introduction : introduction,0.12602739726027398,0.8292682926829268,0.8292682926829268,Our experiments verify that adversarial learning is capable to significantly improve the denoising and facts extracting capacity of PAAG ,20,"Furthermore , we also examine the effectiveness of each module in PAAG .","To sum up , our contributions can be summarized as follows :",introduction
sentiment_analysis,18,"In particular , it performs substantially better on groups with two or three targets .",analysis,Model Analysis,0,195,8,8,0,analysis : Model Analysis,0.8158995815899581,0.17391304347826084,0.5,In particular it performs substantially better on groups with two or three targets ,14,LSTM + SynATT performs the best on all groups .,"By analyzing a number of examples from these groups , we find that the proposed syntax - based attention is more effective in capturing the relevant opinion context for a given target when there are multiple targets in the sentence .",result
sentence_compression,0,This adds some dependency between consecutive labels .,model,Our Base Model,0,79,22,22,0,model : Our Base Model,0.2831541218637993,0.25287356321839083,0.9565217391304348,This adds some dependency between consecutive labels ,8,4 ) com-bined the predicted y i ?1 with w i to help predict y i .,We do not do this because later we will introduce an ILP layer to introduce dependencies among labels .,method
relation_extraction,2,"Of the methods , BERT - NO - SEP - NO - ENT performs worst , with its F1 8.16 absolute points worse than R - BERT .",model,CR,1,125,20,14,0,model : CR,0.925925925925926,0.8,0.7368421052631579,Of the methods BERT NO SEP NO ENT performs worst with its F1 8 16 absolute points worse than R BERT ,22,We observe that the three methods all perform worse than R - BERT .,This ablation study demonstrates that both the special separate tokens and the hidden entity vectors make important contributions to our approach .,method
machine-translation,3,Two post processing techniques are used to improve the performance further on the English - to - French task .,method,Post processing,0,256,5,2,0,method : Post processing,0.8178913738019169,0.25,0.1176470588235294,Two post processing techniques are used to improve the performance further on the English to French task ,18, ,"First , three Deep - Att models are built for ensemble results .",method
sentiment_analysis,23,"This simple heuristic is applicable to any structure , including c- TBCNN and d- TBCNN . 3 - slot pooling for c - TBCNN .",system description,Pooling Heuristics,0,137,101,13,0,system description : Pooling Heuristics,0.4691780821917808,0.961904761904762,0.7647058823529411,This simple heuristic is applicable to any structure including c TBCNN and d TBCNN 3 slot pooling for c TBCNN ,21,We take the maximum value in each dimension .,"To preserve more information over different parts of constituency trees , we propose 3 - slot pooling ) .",method
relation_extraction,9,Organic [ sesame ]e 1 [ oil ]e 2 has an . . .,architecture,Our Architectures,0,197,26,26,0,architecture : Our Architectures,0.9425837320574164,0.7647058823529411,0.7647058823529411,Organic sesame e 1 oil e 2 has an ,10,The size of a [ tree ]e 1 [ crown ] e 2 is strongly . . .,Before heading down the [ phone ] e 1 [ operator ]e 2 career . . .,method
natural_language_inference,49,"It can also utilize the preprocessing tool , e.g. named entity recognizer , part - of - speech recognizer , lexical parser and coreference identifier etc. , to incorporate more lexical and syntactical information into the feature vector .",model,INTERACTIVE INFERENCE NETWORK,0,56,10,9,0,model : INTERACTIVE INFERENCE NETWORK,0.2204724409448819,0.14705882352941174,0.39130434782608703,It can also utilize the preprocessing tool e g named entity recognizer part of speech recognizer lexical parser and coreference identifier etc to incorporate more lexical and syntactical information into the feature vector ,34,"In embedding layer , a model can map tokens to vectors with the pre-trained word representation such as GloVe , word2 Vec and fasttext .",. Encoding Layer encodes the representations by incorporating the context information or enriching the representation with desirable features for future use .,method
sentiment_analysis,20,Not all words contribute equally to the expression of the sentiment in a message .,model,MSA Model (message-level),0,95,19,17,0,model : MSA Model (message-level),0.510752688172043,0.2753623188405797,0.7083333333333334,Not all words contribute equally to the expression of the sentiment in a message ,15,We stack two layers of BiLSTMs in order to learn more abstract features .,We use an attention mechanism to find the relative contribution ( importance ) of each word .,method
text-to-speech_synthesis,2,"In Clean , the speaker encoder and synthesizer are trained on the same data , a baseline similar to the non-fine tuned speaker encoder from , except that it is trained discriminatively as in .",experiment,experiment,0,183,92,92,0,experiment : experiment,0.7469387755102042,0.8846153846153846,0.8846153846153846,In Clean the speaker encoder and synthesizer are trained on the same data a baseline similar to the non fine tuned speaker encoder from except that it is trained discriminatively as in ,33,"We first evaluate the speaker encoder trained on LibriSpeech Clean and Other sets , each of which contain a similar number of speakers .",This matched condition gives a slightly better naturalness and a similar similarity score .,experiment
natural_language_inference,20,Our model also outperforms In - fer Sent which achieves an accuracy of 85.1 % in our experiments .,evaluation,SciTail,1,140,58,4,0,evaluation : SciTail,0.5761316872427984,0.6170212765957447,0.5,Our model also outperforms In fer Sent which achieves an accuracy of 85 1 in our experiments ,18,"We obtain a score of 86.0 % after 4 epochs of training , which is + 2.7 % points absolute improvement on the previous published state of the art by .",The comparison of our results with the previous state of the art results are reported in .,result
natural_language_inference,43,Problem Setting and Dataset,dataset,Problem Setting and Dataset,0,32,1,1,0,dataset : Problem Setting and Dataset,0.2064516129032258,0.09090909090909093,0.09090909090909093,Problem Setting and Dataset,4, , ,experiment
sentiment_analysis,45,"We use the results reported in XRCE and NRC - Canada , NRC - Canada and ATAE - LSTM . "" - "" means not reported .",result,Exp-II: ABSA,0,127,10,5,0,result : Exp-II: ABSA,0.8819444444444444,1.0,1.0,We use the results reported in XRCE and NRC Canada NRC Canada and ATAE LSTM means not reported ,19,Test set results for Semeval - 2014 task 4 Subtask 3 : Aspect Category Detection ., ,result
part-of-speech_tagging,1,"Unlike some character - based approaches , we do not use any character - level prepossessing which enables our model to learn and capture regularities from prefixes to suffixes to construct character - to - word representations .",architecture,architecture,0,86,7,7,0,architecture : architecture,0.344,0.0875,0.0875,Unlike some character based approaches we do not use any character level prepossessing which enables our model to learn and capture regularities from prefixes to suffixes to construct character to word representations ,33,"This vocabulary contains all the variations of the raw text , including uppercase and lowercase letters , numbers , punctuation marks , and symbols .","The input word w is decomposed into a sequence of characters {c 1 , ... , c n } , where n is the length of w .",method
relation_extraction,12,AGGCNs for Relation Extraction,system description,AGGCNs for Relation Extraction,0,139,84,1,0,system description : AGGCNs for Relation Extraction,0.4224924012158055,0.8484848484848485,0.0625,AGGCNs for Relation Extraction,4, , ,method
question_generation,1,These are described in detail as follows :,ablation,Ablation Analysis of Model,0,253,3,3,0,ablation : Ablation Analysis of Model,0.6454081632653061,0.02112676056338028,1.0,These are described in detail as follows ,8,"While , we advocate the use of multimodal differential network ( MDN ) for generating embeddings that can be used by the decoder for generating questions , we also evaluate several variants of this architecture namely ( a ) Differential Image Network , ( b ) Tag net and ( c ) Place net .", ,result
sentiment_analysis,43,We can have the following observations .,method,Compared Methods,0,191,15,15,0,method : Compared Methods,0.7958333333333333,1.0,1.0,We can have the following observations ,7,shows the performance comparison results of MGAN with other baseline methods ., ,method
natural_language_inference,99,This helps to locate the answer boundary and promote the prediction accuracy for insurance .,introduction,introduction,0,42,32,32,0,introduction : introduction,0.16666666666666666,1.0,1.0,This helps to locate the answer boundary and promote the prediction accuracy for insurance ,15,We utilize a checking mechanism with passage self alignment on the revised pointer network ., ,introduction
natural_language_inference,30,"Our training data consists of two sources : an automatically created KB , Re - Verb , from which we generate questions and a set of pairs of questions collaboratively labeled as paraphrases from the website WikiAnswers .",training,Training Data,0,76,2,2,0,training : Training Data,0.29457364341085274,0.04444444444444445,0.18181818181818185,Our training data consists of two sources an automatically created KB Re Verb from which we generate questions and a set of pairs of questions collaboratively labeled as paraphrases from the website WikiAnswers ,34, ,The set of potential answers K is given by the KB ReVerb .,experiment
natural_language_inference,78,Table 4 reports the result of our error analysis .,analysis,Linguistic Error Analysis,0,254,4,4,0,analysis : Linguistic Error Analysis,0.9202898550724636,0.19047619047619047,0.4,Table 4 reports the result of our error analysis ,10,We compare against the model outputs of the ESIM model across 13 categories of linguistic phenenoma .,We observe that our CAFE model generally outperforms ESIM on most categories .,result
named-entity-recognition,9,"We test various pre-training strategies with different combinations and sizes of general domain corpora and biomedical corpora , and analyze the effect of each corpus on pre-training .",approach,Approach,0,39,7,7,0,approach : Approach,0.19597989949748745,0.5,0.5,We test various pre training strategies with different combinations and sizes of general domain corpora and biomedical corpora and analyze the effect of each corpus on pre training ,29,"To show the effectiveness of our approach in biomedical text mining , BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks .",We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies .,method
machine-translation,6,We hope the learned word embeddings not only minimize the task - specific training loss but also fool the discriminator .,method,Our Method,0,113,5,5,0,method : Our Method,0.3883161512027492,0.10416666666666667,0.10416666666666667,We hope the learned word embeddings not only minimize the task specific training loss but also fool the discriminator ,20,We introduce a discriminator to categorize word embeddings into two classes : popular ones or rare ones .,"By doing so , the frequency information is removed from the embedding and we call our method frequency - agnostic word embedding ( FRAGE ) .",method
part-of-speech_tagging,4,"For neural sequence labeling , however , BiLSTM - CRF does not always lead to better results compared with BiLSTM - softmax local classification .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02145922746781116,0.5,0.5,For neural sequence labeling however BiLSTM CRF does not always lead to better results compared with BiLSTM softmax local classification ,21,CRF has been used as a powerful model for statistical sequence labeling .,This can be because the simple Markov label transition model of CRF does not give much information gain over strong neural encoding .,abstract
natural_language_inference,27,"We use k = 5 , so each sentence has 25 paraphrase choices .",model,English to French NMT,0,163,119,22,0,model : English to French NMT,0.4822485207100592,0.8947368421052632,0.6111111111111112,We use k 5 so each sentence has 25 paraphrase choices ,12,"For the document paraphrasing step , we first split paragraphs into sentences and paraphrase them independently .",new document dis formed by simply replacing each sentence ind with a randomly - selected paraphrase .,method
text-classification,1,"As shown in , we concatenate the output of a forward LSTM ( left to right ) and a backward LSTM ( right to left ) , which is referred to as bidirectional LSTM in the literature .",system description,More simplifications,0,121,75,31,0,system description : More simplifications,0.47265625,0.375,0.9393939393939394,As shown in we concatenate the output of a forward LSTM left to right and a backward LSTM right to left which is referred to as bidirectional LSTM in the literature ,32,One - hot vectors memory required for training and make it practical to add one more layer of LSTM going in the opposite direction for accuracy improvement .,"The resulting model is a one - hot bidirectional LSTM with pooling , and we abbreviate it to oh - 2 LSTMp .",method
sentence_classification,0,We conduct this analysis for examples from both datasets .,analysis,Analysis,0,203,3,3,0,analysis : Analysis,0.7602996254681648,0.15,0.16666666666666666,We conduct this analysis for examples from both datasets ,10,"To gain more insight into why the scaffolds are helping the model in improved citation intent classification , we examine the attention weights assigned to inputs for our best proposed model ( ' BiLSTM - Attn w / ELMo + both scaffolds ' ) compared with the best neural baseline ( ' BiLSTM - Attn w / ELMO ' ) .",shows an example input citation along with the horizontal line and the heatmap of attention weights for this input resulting from our model versus the baseline .,result
semantic_parsing,2,"Secondly , the model can explicitly share knowledge of coarse structures for the examples that have the same sketch ( i.e. , basic meaning ) , even though their actual meaning representations are different ( e.g. , due to different details ) .",introduction,introduction,1,22,14,14,0,introduction : introduction,0.07560137457044673,0.4827586206896552,0.4827586206896552,Secondly the model can explicitly share knowledge of coarse structures for the examples that have the same sketch i e basic meaning even though their actual meaning representations are different e g due to different details ,37,"As shown in , sketches are more compact and as a result easier to generate compared to decoding the entire meaning structure in one go .","Thirdly , after generating the sketch , the decoder knows what the basic meaning of the utterance looks like , and the model can use it as global context to improve the prediction of the final details .",introduction
relation_extraction,12,"These results further show the superiority of "" soft pruning "" strategy over hard pruning strategy in utilizing full tree information .",analysis,Analysis and Discussion,0,248,15,15,0,analysis : Analysis and Discussion,0.7537993920972644,0.4838709677419355,0.4838709677419355,These results further show the superiority of soft pruning strategy over hard pruning strategy in utilizing full tree information ,20,"In addition , we notice that the performance of C - AGGCN with full trees outperforms all C - AGGCNs with pruned trees .",Performance against Sentence Length .,result
natural_language_inference,21,"Meanwhile , it has fewer parameters and exhibits much higher computation efficiency than the mod-els it outperforms , e.g. , LSTM and tree - based models .",abstract,abstract,0,44,42,42,0,abstract : abstract,0.15172413793103448,0.9545454545454546,0.9545454545454546,Meanwhile it has fewer parameters and exhibits much higher computation efficiency than the mod els it outperforms e g LSTM and tree based models ,25,"It also shows the state - of - the - art performance on the Stanford Sentiment Treebank ( SST ) , Multi - Genre natural language inference ( MultiNLI ) , SICK , Customer Review , MPQA , SUBJ and TREC question - type classification datasets .",) Lowercase denotes a vector ; 2 ) bold lowercase denotes a sequence of vectors ( stored as a matrix ) ; and 3 ) uppercase denotes a matrix or a tensor .,abstract
sentiment_analysis,39,"Many existing works in the aspect - based sentiment analysis task , 3 use a classifier , such as logistic regression or SVM , based on linguistic features such as n-grams , POS information or more hand - engineered features .",baseline,Logistic Regression,1,162,10,2,0,baseline : Logistic Regression,0.6612244897959184,0.29411764705882354,0.125,Many existing works in the aspect based sentiment analysis task 3 use a classifier such as logistic regression or SVM based on linguistic features such as n grams POS information or more hand engineered features ,36, ,We can think of these features as a sparse representation e l that enter the softmax in equation 1 .,result
sentiment_analysis,13,We adopt this baseline because of its simple implementation for reproducibility .,method,method,0,220,4,4,0,method : method,0.7913669064748201,0.12903225806451613,0.12903225806451613,We adopt this baseline because of its simple implementation for reproducibility ,12,DrQA is a baseline from the document reader 12 of DrQA .,We run the document reader with random initialization and train it directly on Review RC .,method
relation-classification,9,The vocabulary is BASE - VOCAB .,system description,Pretrained BERT Variants,0,62,5,5,0,system description : Pretrained BERT Variants,0.4217687074829932,0.10638297872340426,0.8333333333333334,The vocabulary is BASE VOCAB ,6,We use the pretrained weights for BERT - Base released with the original BERT code .,We evaluate both cased and uncased versions of this model .,method
semantic_role_labeling,3,The mathematical formulation is shown below :,system description,Self-Attention,0,78,40,22,0,system description : Self-Attention,0.2954545454545455,0.3669724770642202,0.6470588235294118,The mathematical formulation is shown below ,7,"Then the scaled dot-product attention is used to compute the relevance between queries and keys , and to output mixed representations .","Finally , all the vectors produced by parallel heads are concatenated together to form a single vector .",method
semantic_role_labeling,4,illustrates the over all architecture of our model .,architecture,architecture,0,93,5,5,0,architecture : architecture,0.31,0.08620689655172414,0.08620689655172414,illustrates the over all architecture of our model ,9,This section describes our neural networks for each function and the over all network architecture .,The first component f base uses bidirectional LSTMs ( BiLSTMs ) to calculate the base features .,method
natural_language_inference,99,The gates of the passage and the question are computed as follows :,system description,Input Embedding Layer,0,67,25,16,0,system description : Input Embedding Layer,0.2658730158730159,0.7142857142857143,0.6153846153846154,The gates of the passage and the question are computed as follows ,13,"If the model recognize a question 's type , then all the words in this question will be assigned with the same QType feature .","where W p , b p , W q , b q are the parameters and ? denotes an element - wise sigmoid function .",method
sentence_compression,3,"As shown in , deleting the object ( # 2 ) , verb ( # 3 ) , or subject ( # 4 ) results in a significant increase in "" sentence perplexity "" , implying that the syntax - based language model is highly sensitive to the lack of such syntactic components .",analysis,Evaluator Analysis,0,118,4,4,0,analysis : Evaluator Analysis,0.9516129032258064,0.6666666666666666,0.6666666666666666,As shown in deleting the object 2 verb 3 or subject 4 results in a significant increase in sentence perplexity implying that the syntax based language model is highly sensitive to the lack of such syntactic components ,38,"We converted the reward function R SLM toe ? logR SLM for a better observation ( sim - ilar to "" sentence perplexity "" , the higher the score is , the worse is the sentence ) .","Interestingly , when deleting words such as new or / and huge , the score becomes lower , suggesting that the model may prefer short sentences , with unnecessary parts such as amod being removed .",result
named-entity-recognition,1,"We provide a brief description of LSTMs and CRFs , and present a hybrid tagging architecture .",model,model,0,28,2,2,0,model : model,0.13526570048309178,0.08695652173913042,0.6666666666666666,We provide a brief description of LSTMs and CRFs and present a hybrid tagging architecture ,16, ,This architecture is similar to the ones presented by .,method
natural_language_inference,72,"We annotated 100 instances in the validation set ( with ground - truth answers provided ) , which yielded on average 2.85 skills per query .",analysis,Analysis of comprehension skills,0,134,4,4,0,analysis : Analysis of comprehension skills,0.42948717948717946,0.26666666666666666,0.26666666666666666,We annotated 100 instances in the validation set with ground truth answers provided which yielded on average 2 85 skills per query ,23,We include the skill definitions with examples from our dataset in Appendix B .,The distribution of the required skills is shown in .,result
natural_language_inference,31,"In this section , we briefly introduce datasets used in the experiments and their evaluation metrics .",dataset,Datasets,0,103,2,2,0,dataset : Datasets,0.37318840579710144,0.08333333333333333,0.08333333333333333,In this section we briefly introduce datasets used in the experiments and their evaluation metrics ,16, ,"SNLI ( Bowman et al. , 2015 ) ( Stanford Natural Language Inference ) is a benchmark dataset for natural language inference .",experiment
natural_language_inference,88,"Specifically , we use two LSTMNs to read the premise and hypothesis , and then match them by comparing their hidden state tapes .",model,Natural Language Inference,0,218,33,11,0,model : Natural Language Inference,0.8825910931174089,0.6226415094339622,0.3548387096774194,Specifically we use two LSTMNs to read the premise and hypothesis and then match them by comparing their hidden state tapes ,22,We use a similar approach to tackle this task with LSTMNs .,"We perform average pooling for the hidden state tape of each LSTMN , and concatenate the two averages to form the input to a 2 - layer neural network classifier with ReLU as the activation function .",method
question_answering,2,"To summarize , the contribution of this paper is threefold :",introduction,introduction,0,43,33,33,0,introduction : introduction,0.1552346570397112,0.8684210526315791,0.8684210526315791,To summarize the contribution of this paper is threefold ,10,"As shown in , the highlighted cubes are regions of high activations in the proposed FVTA .",We propose a novel attention kernel for VQA on visual - text data .,introduction
text_summarization,9,On the Gigaword corpus we evaluate our models in terms of perplexity on a held - out set .,result,Results,0,107,2,2,0,result : Results,0.6993464052287581,0.04651162790697674,0.04651162790697674,On the Gigaword corpus we evaluate our models in terms of perplexity on a held out set ,18, ,"We then pick the model with best perplexity on the held - out set and use it to compute the F1-score of ROUGE - 1 , ROUGE - 2 , and ROUGE - L on the test sets , all of which we report .",result
natural_language_inference,6,"We introduce an architecture to learn joint multilingual sentence representations for 93 languages , belonging to more than 30 different families and written in 28 different scripts .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.016129032258064516,0.16666666666666666,0.16666666666666666,We introduce an architecture to learn joint multilingual sentence representations for 93 languages belonging to more than 30 different families and written in 28 different scripts ,27, ,"Our system uses a single BiLSTM encoder with a shared BPE vocabulary for all languages , which is coupled with an auxiliary decoder and trained on publicly available parallel corpora .",abstract
question_answering,0,"That is , for each input question , we construct an explicit structural semantic parse ( semantic graph ) , as in .",introduction,introduction,1,20,11,11,0,introduction : introduction,0.06779661016949153,0.3142857142857143,0.3142857142857143,That is for each input question we construct an explicit structural semantic parse semantic graph as in ,18,"In this paper , we describe a semantic parsing approach to the problem of KB QA .",Semantic parses can be deterministically converted to a query to extract the answers from the KB .,introduction
natural_language_inference,26,Infulence of Accuracy of SRL,model,Infulence of Accuracy of SRL,0,197,12,1,0,model : Infulence of Accuracy of SRL,0.929245283018868,0.5714285714285714,0.1,Infulence of Accuracy of SRL,5, , ,method
text_summarization,10,"We concatenate the decoder 's ( last ) RNN layer hidden state st and context vector ct and apply a linear transformation , and then project to the vocabulary space by another linear transformation .",model,Baseline Pointer+Coverage Model,0,55,10,6,0,model : Baseline Pointer+Coverage Model,0.20912547528517111,0.10752688172043012,0.3,We concatenate the decoder s last RNN layer hidden state st and context vector ct and apply a linear transformation and then project to the vocabulary space by another linear transformation ,32,Let the decoder hidden state be st at time step t and let ct be the context vector which is defined as a weighted combination of encoder hidden states .,"Finally , the conditional vocabulary distribution at each time step t of the decoder is defined as :",method
sentiment_analysis,14,Parameters of T and CNN are randomly initialized and Adam is used for optimization .,system description,Pre-training Emo2Vec,1,89,3,3,0,system description : Pre-training Emo2Vec,0.5855263157894737,0.25,0.42857142857142855,Parameters of T and CNN are randomly initialized and Adam is used for optimization ,15,Emo2 Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone .,Best parameter settings are tuned on the validation set .,method
sentence_compression,1,"While McDonald 's original model contains deep syntactic features coming from both dependency and constituency parse trees , we use only dependency - based features .",baseline,Baseline,0,52,15,15,0,baseline : Baseline,0.2488038277511961,0.1744186046511628,0.1744186046511628,While McDonald s original model contains deep syntactic features coming from both dependency and constituency parse trees we use only dependency based features ,24,The second change concerns the feature set used .,"Additionally , and to better compare the baseline with the LSTM models , we have included as an optional feature a 256 - dimension embedding - vector representation of each input word and its syntactic parent .",result
question-answering,7,MMA - NSE first encodes the answers and then the questions by accessing its own and the answer encoding memories .,experiment,Answer Sentence Selection,0,170,48,9,0,experiment : Answer Sentence Selection,0.6181818181818182,0.6233766233766234,0.375,MMA NSE first encodes the answers and then the questions by accessing its own and the answer encoding memories ,20,We trained the MMA - NSE attention model to minimize the sigmoid cross entropy loss .,"In our preliminary experiment , we found that the multiple memory access and the attention over answer encoder outputs {h a } T t= 1 are crucial to this problem .",experiment
natural_language_inference,42,"Regarding EM and F 1 scores , FABIR and BiDAF showed similar performances .",evaluation,FABIR vs BiDAF,1,245,20,8,0,evaluation : FABIR vs BiDAF,0.8477508650519031,0.37735849056603776,0.5,Regarding EM and F 1 scores FABIR and BiDAF showed similar performances ,13,"Note that for both models , we batch the examples by paragraph length to improve computational efficiency .","Their similar scores render further comparisons even more telling , because their differences can not be explained by their over all performances , but exclusively by their architectures .",result
natural_language_inference,58,"The nal answer is either "" Yes "" or "" No "" and hence logistical regression is used as the answer module : at = ? ( W a st + b a ) ; ? a = ( W a , b a ) .",system description,Internal State Controller:,0,296,63,5,0,system description : Internal State Controller:,0.8835820895522388,0.6363636363636364,0.12195121951219512,The nal answer is either Yes or No and hence logistical regression is used as the answer module at W a st b a a W a b a ,30,The initial state of the GRU controller is s 1 .,We use the same termination module as in the CNN and Daily Mail experiments .,method
natural_language_inference,88,"Following previous work on this dataset , we used 8,544 sentences for training , 1,101 for validation , and 2,210 for testing .",analysis,Sentiment Analysis,0,182,4,4,0,analysis : Sentiment Analysis,0.7368421052631579,0.5714285714285714,0.5714285714285714,Following previous work on this dataset we used 8 544 sentences for training 1 101 for validation and 2 210 for testing ,23,"We used the Stanford Sentiment Treebank , which contains fine - grained sentiment labels ( very positive , positive , neutral , negative , very negative ) for 11,855 sentences .",The average sentence length is 19.1 .,result
text_summarization,1,We implement these methods with NQG ++ and PG .,baseline,Baselines,0,157,4,4,0,baseline : Baselines,0.6541666666666667,0.10526315789473684,0.2352941176470588,We implement these methods with NQG and PG ,9,"In particular , we compare with recent diverse search algorithms including Truncated Sampling , Diverse Beam Search , and Mixture Decoder .","For each method , we generate K = ( 3 and 5 ) hypotheses from each source sequence .",result
semantic_parsing,2,"We used standard splits for both datasets : 600 training and 280 test instances for GEO ; 4 , 480 training , 480 development , and 450 test examples for ATIS .",training,Natural Language to Logical Form,0,122,13,4,0,training : Natural Language to Logical Form,0.4192439862542955,0.11016949152542373,0.1176470588235294,We used standard splits for both datasets 600 training and 280 test instances for GEO 4 480 training 480 development and 450 test examples for ATIS ,27,Examples are shown in ( see the first and second block ) .,Meaning representations in these datasets are based on ?- calculus .,experiment
semantic_role_labeling,3,The filter width k is set to 3 in all our experiments .,system description,Nonlinear Sub-Layers,0,105,67,15,0,system description : Nonlinear Sub-Layers,0.3977272727272727,0.6146788990825688,0.6521739130434783,The filter width k is set to 3 in all our experiments ,13,"Given two filters W ? R kdd and V ? R kdd , the output activations of GLU are computed as follows :",Feed-forward Sub - Layer,method
sentiment_analysis,41,Asp illustrates the comparative results .,system description,Task Definition,0,166,7,7,0,system description : Task Definition,0.7443946188340808,0.5,0.5,Asp illustrates the comparative results ,6,"In the dataset of SemEval 2014 Task 4 , there is only restaurants data that has aspect - specific polarities .",Aspect - Term -level Classification,method
sentiment_analysis,31,Dropout rate is chosen as 0.5 .,result,Results,0,135,13,13,0,result : Results,0.8490566037735849,0.4482758620689655,0.7222222222222222,Dropout rate is chosen as 0 5 ,8,"We use filter window sizes of 1 , 2 , 3 , 4 with 100 maps each .",Early stopping based on validation accuracy is applied .,result
sentiment_analysis,0,An initial study utilized deep neural networks ( DNNs ) to extract high - level features from raw audio data and demonstrated its effectiveness in speech emotion recognition .,introduction,introduction,0,33,24,24,0,introduction : introduction,0.1853932584269663,0.75,0.75,An initial study utilized deep neural networks DNNs to extract high level features from raw audio data and demonstrated its effectiveness in speech emotion recognition ,26,"Recently , researchers have proposed various neural network - based architectures to improve the performance of speech emotion recognition .","With the advancement of deep learning methods , more complex neuralbased architectures have been proposed .",introduction
text-classification,0,The number of output units for the last layer is determined by the problem .,model,model,0,80,10,10,0,model : model,0.3524229074889868,0.3125,0.6666666666666666,The number of output units for the last layer is determined by the problem ,15,Fully - connected layers used in our experiments .,"For example , for a 10 - class classification problem it will be 10 .",method
text_summarization,3,This process is formulated as an encoder - decoder framework that consists of an encoder and an attention - equipped decoder .,system description,Encoder-Decoder Framework,0,57,2,2,0,system description : Encoder-Decoder Framework,0.23553719008264465,0.015267175572519085,0.16666666666666666,This process is formulated as an encoder decoder framework that consists of an encoder and an attention equipped decoder ,20, ,We use a two - layer bi-directional LSTM - RNN encoder and one - layer uni-directional LSTM - RNN decoder along with attention mechanism .,method
natural_language_inference,84,"To get an idea of how useful different auxiliary data sources will be for datasets of various sizes , we also apply our approach to the One Billion Words ( OBW ) language modelling task .",model,LANGUAGE MODELLING,0,181,3,3,0,model : LANGUAGE MODELLING,0.7869565217391304,0.09375,0.09375,To get an idea of how useful different auxiliary data sources will be for datasets of various sizes we also apply our approach to the One Billion Words OBW language modelling task ,33,Experiments in the previous sections used datasets of moderate size .,"In the first round of experiments , we use only 1 % of the training data ( ? 10 7 words ) and in the second we train our models on the whole training set (? 10 9 words ) .",method
question_generation,1,We concatenated all the tag words fol - lowed by fully connected layer to get feature dimension of 512 .,ablation,Tag net,0,283,33,21,0,ablation : Tag net,0.7219387755102041,0.2323943661971831,0.84,We concatenated all the tag words fol lowed by fully connected layer to get feature dimension of 512 ,19,"Finally , we applied max - pooling on this to get a vector representation of the tags as shown figure 12 .","We also explored joint networks based on concatenation of all the tags , on elementwise addition and element - wise multiplication of the tag vectors .",result
named-entity-recognition,2,"We predict the most likely y , given a conditional model P ( y|x ) .",model,model,0,49,4,4,0,model : model,0.2300469483568075,0.06779661016949153,0.17391304347826084,We predict the most likely y given a conditional model P y x ,14,Let D be the domain size of each y i .,This paper considers two factorizations of the conditional distribution .,method
named-entity-recognition,4,This can be seen as a type of domain transfer for the biLM .,model,Pre-trained bidirectional language model architecture,0,99,52,14,0,model : Pre-trained bidirectional language model architecture,0.3639705882352941,0.8965517241379308,0.7,This can be seen as a type of domain transfer for the biLM ,14,"In some cases , fine tuning the biLM on domain specific data leads to significant drops in perplexity and an increase in downstream task performance .","As a result , in most cases we used a fine - tuned biLM in the downstream task .",method
machine-translation,0,"Similarly , we perform the same procedure with those pairs whose source phrase is long but rare in the corpus .",analysis,Qualitative Analysis,0,181,31,10,0,analysis : Qualitative Analysis,0.8264840182648402,0.5535714285714286,0.4761904761904762,Similarly we perform the same procedure with those pairs whose source phrase is long but rare in the corpus ,20,"For each such source phrase , we look at the target phrases that have been scored high either by the translation probability p ( f | e ) or by the RNN Encoder - Decoder .",lists the top - 3 target phrases per source phrase favored either by the translation model or by the RNN Encoder - Decoder .,result
natural_language_inference,14,"This motivates the construction of additional datasets that pose new challenges , and serve as more reliable benchmarks for commonsense reasoning systems .",introduction,introduction,0,23,10,10,0,introduction : introduction,0.13450292397660818,0.5263157894736842,0.5263157894736842,This motivates the construction of additional datasets that pose new challenges and serve as more reliable benchmarks for commonsense reasoning systems ,22,It is instead able to exploit regularities in the SWAG dataset to score high .,"In this work , we introduce the COmmonsense Dataset Adversarially - authored by Humans ( CODAH ) for commonsense question answering in the style of SWAG multiple choice sentence completion .",introduction
natural_language_inference,9,which can be defined similarly to the update gate function :,model,EXTENSIONS,0,113,73,6,0,model : EXTENSIONS,0.3424242424242424,0.7604166666666666,0.375,which can be defined similarly to the update gate function ,11,For this we use a reset gate function ? :,"where W ( r ) ? R 1d is a weight matrix , and b ( r ) ? R is a bias term .",method
natural_language_inference,85,Reasoning and inference are central to both human and artificial intelligence .,introduction,introduction,0,12,2,2,0,introduction : introduction,0.05106382978723404,0.1,0.1,Reasoning and inference are central to both human and artificial intelligence ,12, ,"Modeling inference in human language is notoriously challenging but is a basic problem towards true natural language understanding , as pointed out by , "" a necessary ( if not sufficient ) condition for true natural language understanding is a mastery of open - domain natural language inference . """,introduction
natural_language_inference,95,"rect answers could occur more frequently in those passages and usually share some commonalities , while incorrect answers are usually different from one another .",introduction,introduction,0,47,39,39,0,introduction : introduction,0.20085470085470086,0.6724137931034483,0.6724137931034483,rect answers could occur more frequently in those passages and usually share some commonalities while incorrect answers are usually different from one another ,24,pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies .,The example in demonstrates this phenomenon .,introduction
natural_language_inference,81,Welbl et al. ( 2018 ) proposed the Qangaroo WikiHop task to facilitate the study of multi-evidence question answering .,experiment,MULTI-EVIDENCE QUESTION ANSWERING ON WIKIHOP,0,112,8,2,1,experiment : MULTI-EVIDENCE QUESTION ANSWERING ON WIKIHOP,0.2660332541567696,0.27586206896551724,0.08695652173913042,Welbl et al 2018 proposed the Qangaroo WikiHop task to facilitate the study of multi evidence question answering ,19, ,This dataset is constructed by linking entities in a document corpus ( Wikipedia ) with a knowledge base ( Wikidata ) .,experiment
machine-translation,2,"For our big models , ( described on the bottom line of table 3 ) , step time was 1.0 seconds .",training,Hardware and Schedule,0,163,13,5,0,training : Hardware and Schedule,0.7276785714285714,0.4642857142857143,0.8333333333333334,For our big models described on the bottom line of table 3 step time was 1 0 seconds ,19,"We trained the base models for a total of 100,000 steps or 12 hours .","The big models were trained for 300,000 steps ( 3.5 days ) .",experiment
sentiment_analysis,2,Licence details : http://creativecommons.org/licenses/by/4.0/,abstract,abstract,0,13,11,11,0,abstract : abstract,0.05726872246696035,0.7857142857142857,0.7857142857142857,Licence details http creativecommons org licenses by 4 0 ,10,This work is licenced under a Creative Commons Attribution 4.0 International Licence .,The aim of the target - dependent sentiment analysis is similar to aspect - level sentiment analysis .,abstract
relation_extraction,13,"We present three options for both the input encoding , and the output relation representation .",model,model,0,69,4,4,0,model : model,0.323943661971831,0.07547169811320754,0.8,We present three options for both the input encoding and the output relation representation ,15,"Since BERT has not previously been applied to the problem of relation representation , we aim to answer two primary modeling questions : ( 1 ) how do we represent entities of interest in the input to BERT , and ( 2 ) how do we extract a fixed length representation of a relation from BERT 's output .",Six combinations of these are illustrated in .,method
negation_scope_resolution,0,"This is only possible because of BERT 's pretraining , and the similarity of the sub corpora of the BioScope Corpus .",result,Results,0,218,20,20,0,result : Results,0.9478260869565216,0.9090909090909092,0.9090909090909092,This is only possible because of BERT s pretraining and the similarity of the sub corpora of the BioScope Corpus ,21,"When we trained on BioScope Abstracts and tested on the BioScope Full Papers , we surprisingly observed a stateof - the - art result of 91.24 ( a gain of 3.89 F1 points over training on BioScope Full Papers ) , which is far beyond the achievable results on training and evaluating on the Bio-Medical sub corpora .","We also notice that in general , though the cross - dataset generalizability is acceptable , it is far from what one would desire .",result
natural_language_inference,3,"h 1 i , j , c 1 i , j ) = C - LSTMs ( hi?1 , j , hi , j?1 , ci?1 , j , ci , j ? 1 , xi , yj ) , ( h 2 i , j , c 2 i , j ) = C - LSTMs ( hi?1 , j , hi , j+1 , ci?1 , j , ci , j + 1 , xi , yj ) , ( h 3 i , j , c 3 i , j ) = C - LSTMs ( hi+1 , j , hi , j+1 , ci +1 , j , ci , j + 1 , xi , yj ) , ( h 4 i , j , c 4 i , j ) = C - LSTMs ( hi+1 , j , hi , j?1 , ci +1 , j , ci , j ? 1 , xi , yj ) .",model,Four Directional Coupled-LSTMs Layers,0,108,85,4,0,model : Four Directional Coupled-LSTMs Layers,0.5192307692307693,0.7522123893805309,1.0,h 1 i j c 1 i j C LSTMs hi 1 j hi j 1 ci 1 j ci j 1 xi yj h 2 i j c 2 i j C LSTMs hi 1 j hi j 1 ci 1 j ci j 1 xi yj h 3 i j c 3 i j C LSTMs hi 1 j hi j 1 ci 1 j ci j 1 xi yj h 4 i j c 4 i j C LSTMs hi 1 j hi j 1 ci 1 j ci j 1 xi yj ,97,"Similar to bi-directional LSTM , there are four directions in coupled - LSTMs .", ,method
natural_language_inference,81,"Though physiographically apart of the continent of North America , Greenland has been politically and culturally associated with Europe ( specifically Norway and Denmark , the colonial powers , as well as the nearby island of Iceland ) for more than a millennium .",APPENDIX,Answer town,0,330,122,42,0,APPENDIX : Answer town,0.7838479809976246,0.5727699530516432,0.3157894736842105,Though physiographically apart of the continent of North America Greenland has been politically and culturally associated with Europe specifically Norway and Denmark the colonial powers as well as the nearby island of Iceland for more than a millennium ,39,"Greenland is an autonomous constituent country within the Danish Realm between the Arctic and Atlantic Oceans , east of the Canadian Arctic Archipelago .","The majority of its residents are Inuit , whose ancestors migrated began migrating from the Canadian mainland in the 13th century , gradually settling across the island .",others
sentiment_analysis,27,Aspect - level sentiment classification is a fundamental natural language processing task that gets lots of attention in recent years .,introduction,introduction,0,13,2,2,0,introduction : introduction,0.04710144927536232,0.047619047619047616,0.047619047619047616,Aspect level sentiment classification is a fundamental natural language processing task that gets lots of attention in recent years ,20, ,"It is a fine - grained task in sentiment analysis , which aims to infer the sentiment polarities of aspects in their context .",introduction
natural_language_inference,29,"Thereafter , we apply the attention - pooling operation on each review hidden state hr i , to produce the question - aware review representation c r i , shown in :",system description,Review reader,0,138,43,15,0,system description : Review reader,0.3780821917808219,0.26875,0.625,Thereafter we apply the attention pooling operation on each review hidden state hr i to produce the question aware review representation c r i shown in ,27,"i , j ? R refers to the importance score of the j - th word in the i - th review given X q .","Given an answer generation procedure , not all the reviews are useful to answer the question due to the informal style problem .",method
natural_language_inference,18,"The lack of effective and efficient inference methods hinders our ability to create highly expressive models of text , especially in the situation where the model is non-conjugate .",introduction,introduction,0,21,11,11,0,introduction : introduction,0.07692307692307693,0.3666666666666665,0.3666666666666665,The lack of effective and efficient inference methods hinders our ability to create highly expressive models of text especially in the situation where the model is non conjugate ,29,"However the computational cost of the former results in impractical training for the large and deep neural networks which are now fashionable , and the latter is conventionally confined due to the underestimation of posterior variance .","This paper introduces a neural variational framework for generative models of text , inspired by the variational autoencoder .",introduction
text-classification,3,"Interestingly , using multiword - wise embeddings on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets .",experiment,Experiment 2: Cross-preprocessing,1,109,48,13,0,experiment : Experiment 2: Cross-preprocessing,0.8861788617886179,0.9411764705882352,0.8125,Interestingly using multiword wise embeddings on the vanilla setting leads to consistently better results than using them on the same multiwordgrouped preprocessed dataset in eight of the nine datasets ,30,Apple when part of this multiword expression .,"This could provide hints on the excellent results provided by pre-trained Word2vec embeddings trained on the Google News corpus , which learns multiwords similarly to our setting .",experiment
semantic_role_labeling,3,"Since there are dependencies between semantic labels , most previous neural network models introduced a transition model for measuring the probability of jumping between the labels .",system description,Pipeline,0,138,100,13,0,system description : Pipeline,0.5227272727272727,0.9174311926605504,0.5909090909090909,Since there are dependencies between semantic labels most previous neural network models introduced a transition model for measuring the probability of jumping between the labels ,26,"Finally , we take the outputs of the topmost attention sub - layer as inputs to make the final predictions .","Different from these works , we perform SRL as a typical classification problem .",method
natural_language_inference,79,"In this task , we find that TF - IDF weighting performs better than raw counts , and therefore we only report the results of methods with TF - IDF weighting .",model,Information Retrieval with Paragraph Vectors,0,229,44,24,0,model : Information Retrieval with Paragraph Vectors,0.8544776119402985,0.6984126984126984,0.8571428571428571,In this task we find that TF IDF weighting performs better than raw counts and therefore we only report the results of methods with TF IDF weighting ,28,The results of Paragraph Vector and other baselines are reported in .,The results show that Paragraph Vector works well and gives a 32 % relative improvement in terms of error rate .,method
natural_language_inference,79,"Special characters such as , .!? are treated as a normal word .",experiment,Experimental protocols:,1,175,40,14,0,experiment : Experimental protocols:,0.6529850746268657,0.8,0.9333333333333332,Special characters such as are treated as a normal word ,11,"Special characters such as , .!? are treated as a normal word .","If the document has less than 9 words , we pre-pad with a special NULL word symbol .",experiment
natural_language_inference,44,"Nevertheless , we find that most examples are answerable with one or two sentences - among the 88 % of examples thatare answerable given the full document , 95 % can be answered with one or two sentences .",system description,Human studies,0,41,28,9,0,system description : Human studies,0.14335664335664336,0.6666666666666666,1.0,Nevertheless we find that most examples are answerable with one or two sentences among the 88 of examples thatare answerable given the full document 95 can be answered with one or two sentences ,34,"Finding the sentences to answer the question on TriviaQA is more challenging than on SQuAD , since Triv - ia QA documents are much longer than SQuAD documents ( 488 vs 5 sentences per document ) .", ,method
text_generation,1,"Their successes are mainly attributed to training the discriminator to estimate the statistical properties of the continuous real - valued data ( e.g. , pixel values ) .",introduction,introduction,0,20,10,10,0,introduction : introduction,0.072992700729927,0.3703703703703704,0.3703703703703704,Their successes are mainly attributed to training the discriminator to estimate the statistical properties of the continuous real valued data e g pixel values ,25,"Since then , GANs achieve great performance in computer vision tasks such as image synthesis .",The adversarial learning framework provides a possible way to synthesize language descriptions in high quality .,introduction
natural_language_inference,53,Our representations constitute higher - quality features for both classification and similarity tasks .,model,Task transfer,0,181,40,20,0,model : Task transfer,0.8701923076923077,0.6557377049180327,0.4878048780487805,Our representations constitute higher quality features for both classification and similarity tasks ,13,"For SST , we tried exactly the same models as for SNLI ; it is worth noting that SST is smaller than NLI .","One explanation is that the natural language inference task constrains the model to encode the semantic information of the input sentence , and that the information required to perform NLI is generally discriminative and informative .",method
natural_language_inference,64,"Another way to empirically evaluate the impact of ASNQ is to compare it with other similar datasets , e.g. , QNLI , observing the performance of the latter when used for a simple FT or in TANDA .",result,Insights on ASNQ,0,211,49,8,0,result : Insights on ASNQ,0.8406374501992032,0.875,0.6153846153846154,Another way to empirically evaluate the impact of ASNQ is to compare it with other similar datasets e g QNLI observing the performance of the latter when used for a simple FT or in TANDA ,36,Labels 2 and 3 provide the same accuracy than the three labels as the negative class .,shows that both FT and TANDA using ASNQ provide significantly better performance than QNLI on the WikiQA dataset .,result
question_answering,1,Now we use S to obtain the attentions and the attended vectors in both directions .,model,Attention Flow,0,56,24,15,0,model : Attention Flow,0.17665615141955834,0.2376237623762376,0.21739130434782608,Now we use S to obtain the attentions and the attended vectors in both directions ,16,"where ? is a trainable scalar function that encodes the similarity between it s two input vectors , H :t is t-th column vector of H , and U :j is j-th column vector of U , We choose ?( h , u ) = w ( S ) [ h ; u ; h u ] , where w ( S ) ? R 6d is a trainable weight vector , is elementwise multiplication , [ ; ] is vector concatenation across row , and implicit multiplication is matrix multiplication .",Context - to - query Attention .,method
natural_language_inference,80,This aggregation is performed in a sequential manner to avoid losing effect of latent variables that might rely on the sequence of matching vectors .,model,model,0,102,46,46,0,model : model,0.3517241379310345,0.71875,0.71875,This aggregation is performed in a sequential manner to avoid losing effect of latent variables that might rely on the sequence of matching vectors ,25,"During this phase , we use another BiLSTM to aggregate the two sequences of computed matching vectors , p and q from the attention stage ( Section 3.2 ) .","Instead of aggregating the sequences of matching vectors individually , we propose a similar dependent reading approach for the inference stage .",method
natural_language_inference,68,"The boundary model produces only the start token and the end token of the answer , and then all the tokens between these two in the original passage are considered to be the answer .",method,Answer Pointer Layer,0,139,89,37,0,method : Answer Pointer Layer,0.5582329317269076,0.7807017543859649,0.5967741935483871,The boundary model produces only the start token and the end token of the answer and then all the tokens between these two in the original passage are considered to be the answer ,34,The sequence model produces a sequence of answer tokens but these tokens may not be consecutive in the original passage .,We now explain the two models separately .,method
natural_language_inference,89,"To address this problem , we propose a novel read - then - verify system , which not only utilizes a neural reader to extract candidate answers and produce noanswer probabilities , but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.026923076923076925,0.7142857142857143,0.7142857142857143,To address this problem we propose a novel read then verify system which not only utilizes a neural reader to extract candidate answers and produce noanswer probabilities but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets ,46,"However , they fail to validate the answerability of the question by verifying the legitimacy of the predicted answer .","Moreover , we introduce two auxiliary losses to help the reader better handle answer extraction as well as noanswer detection , and investigate three different architectures for the answer verifier .",abstract
natural_language_inference,17,"For the attention redun - KL divergence - Reattention + Reattention Redundancy E 1 to E 2 0.695 0.086 0.866 0.074 E 2 to E 3 0.404 0.067 0.450 0.052 B 1 to B 2 0.976 0.092 1.207 0.121 B 2 to B 3 1.179 0.118 1.193 0.097 Deficiency E 2 to E 2 * 0.650 0.044 0.568 0.059 E 3 to E 3 * 0.536 0.047 0.482 0.035 dancy problem , we measure the distance of attention distributions in two adjacent aligning blocks , e.g. , softmax ( E 1 :j ) and softmax ( E 2 :j ) . Higher distance means less attention redundancy .",ablation,ablation,0,221,17,17,0,ablation : ablation,0.85,0.68,0.68,For the attention redun KL divergence Reattention Reattention Redundancy E 1 to E 2 0 695 0 086 0 866 0 074 E 2 to E 3 0 404 0 067 0 450 0 052 B 1 to B 2 0 976 0 092 1 207 0 121 B 2 to B 3 1 179 0 118 1 193 0 097 Deficiency E 2 to E 2 0 650 0 044 0 568 0 059 E 3 to E 3 0 536 0 047 0 482 0 035 dancy problem we measure the distance of attention distributions in two adjacent aligning blocks e g softmax E 1 j and softmax E 2 j Higher distance means less attention redundancy ,120,We further present experiments to demonstrate the effectiveness of reattention mechanism .,"For the attention deficiency problem , we take the arithmetic mean of multiple attention distributions from the ensemble model as the "" ground truth "" attention distribution softmax ( E t :j * ) , and compute the distance of individual attention softmax ( E t :j ) with it .",result
named-entity-recognition,4,"As noted in Sec. 3.4 , fine tuning the biLM on task specific data typically resulted in significant drops in perplexity .",system description,system description,0,194,6,6,0,system description : system description,0.7132352941176471,0.18181818181818185,0.18181818181818185,As noted in Sec 3 4 fine tuning the biLM on task specific data typically resulted in significant drops in perplexity ,22,All of the individual models share a common architecture in the lowest layers with a context independent token representation below several layers of stacked RNNs - LSTMs in every case except the SQuAD model that uses GRUs .,"To fine tune on a given task , the supervised labels were temporarily ignored , the biLM fine tuned for one epoch on the training split and evaluated on the development split .",method
natural_language_inference,81,"Official blind , held - out test evaluation is performed using the unmasked version .",experiment,MULTI-EVIDENCE QUESTION ANSWERING ON WIKIHOP,0,120,16,10,0,experiment : MULTI-EVIDENCE QUESTION ANSWERING ON WIKIHOP,0.2850356294536817,0.5517241379310345,0.43478260869565216,Official blind held out test evaluation is performed using the unmasked version ,13,The unmasked version of WikiHop represents candidate answers with original text while the masked version replaces them with randomly sampled placeholders in order to remove correlation between frequent answers and support documents .,We tokenize the data using Stanford CoreNLP .,experiment
machine-translation,6,"We do experiments on two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 ) .",experiment,Settings,1,184,28,10,0,experiment : Settings,0.6323024054982818,0.5384615384615384,0.29411764705882354,We do experiments on two widely used datasets Penn Treebank PTB and WikiText 2 WT2 ,16,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .,"We choose two recent works as our baselines : the AWD - LSTM model and the AWD - LSTM - MoS model , which achieves state - of - the - art performance .",experiment
question_answering,4,"However , this work demonstrates the usage and potential of the BAC as a residual connector .",system description,Densely Connected Attention Propagation (DECAPROP),0,85,42,3,0,system description : Densely Connected Attention Propagation (DECAPROP),0.3307392996108949,0.40384615384615385,0.5,However this work demonstrates the usage and potential of the BAC as a residual connector ,16,"Finally , we note that the BAC module takes inspiration from the main body of our CAFE model for entailment classification .",Densely Connected Attention Propagation ( DECAPROP ),method
natural_language_inference,21,Two types of Multi-dimensional Self - attention,system description,Directional Self-Attention,0,113,66,11,0,system description : Directional Self-Attention,0.3896551724137931,0.5196850393700787,0.4583333333333333,Two types of Multi dimensional Self attention,7,"Multi-dimensional attention , however , computes a score for each feature of each word , so it can select the features that can best describe the word 's specific meaning in any given context , and include this information in the sentence encoding output s.","When extending multi-dimension to self - attentions , we have two variants of multi-dimensional attention .",method
sentiment_analysis,23,"In the original version , RNN is built upon a binarized constituency tree .",system description,Recursive Neural Networks,0,57,21,3,0,system description : Recursive Neural Networks,0.1952054794520548,0.2,0.15,In the original version RNN is built upon a binarized constituency tree ,13,"Recursive neural networks ( RNNs ) , proposed in , utilize sentence parsing trees .","Leaf nodes correspond to words in a sentence , represented by n edimensional embeddings .",method
sentiment_analysis,31,"In aspect level sentiment classification , we are given a sentence s = [ w 1 , w 2 , ... , w i , ... , w n ] and an aspect target t = [ w i , w i + 1 , ... , w i +m ?1 ] .",system description,Problem Definition,0,49,5,2,0,system description : Problem Definition,0.3081761006289308,0.08928571428571429,0.6666666666666666,In aspect level sentiment classification we are given a sentence s w 1 w 2 w i w n and an aspect target t w i w i 1 w i m 1 ,34, ,"The goal is to classify whether the sentiment towards the aspect in the sentence is positive , negative , or neutral .",method
semantic_role_labeling,3,Semantic roles are closely related to syntax .,introduction,introduction,0,14,4,4,0,introduction : introduction,0.05303030303030303,0.14285714285714285,0.14285714285714285,Semantic roles are closely related to syntax ,8,"Semantic roles indicate the basic event properties and relations among relevant entities in the sentence and provide an intermediate level of semantic representation thus benefiting many NLP applications , such as Information Extraction , Question Answering , Machine Translation ) and Multi-document Abstractive Summarization .","Therefore , traditional SRL approaches rely heavily on the syntactic structure of a sentence , which brings intrinsic complexity and restrains these systems to be domain specific .",introduction
natural_language_inference,76,"Furthermore , the model is able to resolve one - to - many relationships ( "" kids "" to "" boy "" and "" girl "" , 3 d )",analysis,QUALITATIVE ANALYSIS,0,136,16,16,0,analysis : QUALITATIVE ANALYSIS,0.9315068493150684,0.8888888888888888,0.8888888888888888,Furthermore the model is able to resolve one to many relationships kids to boy and girl 3 d ,19,"Word - by - word attention seems to also work well when words in the premise and hypothesis are connected via deeper semantics or common - sense knowledge ( "" snow "" can be found "" outside "" and a "" mother "" is an "" adult "" , 3e and 3 g ) .","Attention can fail , for example when the two sentences and their words are entirely unrelated ( 3 f ) .",result
text-to-speech_synthesis,0,Our proposed method significantly boosts the accuracy of G2P conversion by 4.22 % WER compared with the previous works .,introduction,introduction,0,28,17,17,0,introduction : introduction,0.1728395061728395,0.7727272727272727,0.7727272727272727,Our proposed method significantly boosts the accuracy of G2P conversion by 4 22 WER compared with the previous works ,20,"We conduct experiments on CMUDict 0.7 b and our internal dataset , and also leverage additional unlabeled words crawled from the web .","Specifically , Transformer model achieves higher accuracy than RNN and CNN based models , and tokenlevel distillation outperforms sequence - level distillation .",introduction
natural_language_inference,100,The comparison of position - shared weight in CNN and value - shared weight in a NMM .,model,Value-shared Weighting,0,146,17,6,0,model : Value-shared Weighting,0.3989071038251366,0.3035714285714285,0.2,The comparison of position shared weight in CNN and value shared weight in a NMM ,16,"To solve this problem , one can use CNN with :","In CNN , the weight associated with anode only depends on its position or relative location as specified by the filters .",method
natural_language_inference,94,The improvement in Rouge - L and METEOR for all three ablation approaches have p ? 0.15 with the bootstrap test .,analysis,analysis,0,244,11,11,0,analysis : analysis,0.6370757180156658,0.7857142857142857,0.7857142857142857,The improvement in Rouge L and METEOR for all three ablation approaches have p 0 15 with the bootstrap test ,21,We also conduct human evaluation to verify that our commonsense incorporated model was indeed better than MHPGM .,"We randomly selected 100 examples from the Nar-rative QA test set , along with both models ' predicted answers , and for each datapoint , we asked 3 external human evaluators ( fluent English speakers ) to decide ( without knowing which model produced each response ) if one is strictly better than the other , or that they were similar in quality ( bothgood or both - bad ) .",result
sentiment_analysis,31,The last hidden states of these two LSTM networks are concatenated for predicting the sentiment polarity .,result,Results,0,128,6,6,0,result : Results,0.8050314465408805,0.206896551724138,0.3333333333333333,The last hidden states of these two LSTM networks are concatenated for predicting the sentiment polarity ,17,TD - LSTM uses two LSTM networks to model the preceding and following contexts surrounding the aspect term .,AT - LSTM combines the sentence hidden states from a LSTM with the aspect term embedding to generate the attention vector .,result
natural_language_inference,28,Few works have been done to improve the performance by developing stronger RNN .,experiment,QUESTION ANSWERING,0,183,44,4,0,experiment : QUESTION ANSWERING,0.6752767527675276,0.8627450980392157,0.3636363636363637,Few works have been done to improve the performance by developing stronger RNN ,14,Almost all stateof - the - art performance is achieved by the means of attention mechanisms .,"Here , we tested RUM on the b AbI Question Answering data set ) to demonstrate its ability to memorize and reason without any attention .",experiment
natural_language_inference,4,Mixed objective convergence .,experiment,EXPERIMENTS,0,145,23,23,0,experiment : EXPERIMENTS,0.7323232323232324,0.5897435897435898,0.5897435897435898,Mixed objective convergence ,4,"The mixed objective initially performs worse as it begins policy learning from scratch , but quickly outperforms the cross entropy model .",The training curves for DCN + with reinforcement learning and DCN + without reinforcement learning are shown in to illustrate the effectiveness of our proposed mixed objective .,experiment
text_summarization,7,lists the current top system results .,experiment,Experiments,0,122,19,19,0,experiment : Experiments,0.8079470198675497,0.7307692307692307,0.9047619047619048,lists the current top system results ,7,"Thus , we conclude that the WFE sub-model has a positive impact to gain the ABS performance since performance gains were derived only by the effect of incorporating our WFE sub -model .",Our method EncDec + WFE successfully achieved the current best scores on most evaluations .,experiment
natural_language_inference,92,"Note that for DROP num with QANET and WIKISQL , we do not ablate with varying ? and just report the number without annealing .",model,Annealing,0,261,43,7,0,model : Annealing,0.90625,0.6142857142857143,0.875,Note that for DROP num with QANET and WIKISQL we do not ablate with varying and just report the number without annealing ,23,Ablations and chosen ? for each dataset are shown in .,"Note that for DROP num with QANET and WIKISQL , we do not ablate with varying ? and just report the number without annealing .",method
machine-translation,2,"To counteract this effect , we scale the dot products by 1",model,Scaled Dot-Product Attention,0,79,38,15,0,model : Scaled Dot-Product Attention,0.35267857142857145,0.3486238532110092,1.0,To counteract this effect we scale the dot products by 1,11,"We suspect that for large values of d k , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients", ,method
sentiment_analysis,42,We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position .,method,Comparison to Other Methods,1,175,20,20,0,method : Comparison to Other Methods,0.6944444444444444,0.7692307692307693,0.7692307692307693,We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position ,19,It is somewhat dis appointing that incorporating attention model over TDLSTM does not bring any improvement .,"Therefore , the model of TDLSTM + ATT actually selects such mixed semantics of word sequence , which is weird and not an intuitive way to selectively focus on parts of contexts .",method
text_summarization,12,"First , the bidirectional GRU encoder reads the input words x = ( x 1 , x 2 , . . . , x n ) and builds its representation ( h 1 , h 2 , . . . , h n ) .",model,Model,0,82,3,3,0,model : Model,0.35964912280701755,0.061224489795918366,0.5,First the bidirectional GRU encoder reads the input words x x 1 x 2 x n and builds its representation h 1 h 2 h n ,27,"As shown in , our model consists of a sentence encoder using the Gated Recurrent Unit ( GRU ) , a selective gate network and an attention - equipped GRU decoder .",Then the selective gate selects and filters the word representations according to the sentence meaning representation to produce a tailored sentence word representation for abstractive sentence summarization task .,method
machine-translation,2,"To improve computational performance for tasks involving very long sequences , self - attention could be restricted to considering only a neighborhood of sizer in the input sequence centered around the respective output position .",model,Why Self-Attention,0,140,99,14,0,model : Why Self-Attention,0.625,0.908256880733945,0.5833333333333334,To improve computational performance for tasks involving very long sequences self attention could be restricted to considering only a neighborhood of sizer in the input sequence centered around the respective output position ,33,"In terms of computational complexity , self - attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state - of - the - art models in machine translations , such as word - piece and byte - pair representations .",This would increase the maximum path length to O ( n / r ) .,method
natural_language_inference,53,"indicates methods that we trained , other transfer models have been extracted from .",model,Embedding size,0,157,16,6,0,model : Embedding size,0.7548076923076923,0.26229508196721313,0.6,indicates methods that we trained other transfer models have been extracted from ,13,"Underlined are best results for transfer learning approaches , in bold are best results among the models trained in the same way .","For best published supervised methods ( no transfer ) , we consider AdaSent , TF - KLD , Tree - LSTM and Illinois - LH system .",method
natural_language_inference,87,"To verify its effectiveness , the proposed SG - Net is applied to typical pre-trained language model BERT which is right based on a Transformer encoder .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.05113636363636364,0.875,0.875,To verify its effectiveness the proposed SG Net is applied to typical pre trained language model BERT which is right based on a Transformer encoder ,26,Syntax - guided network ( SG - Net ) is then composed of this extra SDOI - SAN and the SAN from the original Transformer encoder through a dual contextual architecture for better linguistics inspired representation .,Extensive experiments on popular benchmarks including SQuAD 2.0 and RACE show that the proposed SG - Net design helps achieve substantial performance improvement over strong baselines .,abstract
part-of-speech_tagging,2,"Even on datasets with relatively abundant labels , multi-task transfer can sometimes achieve improvement over state - of - the - art results .",introduction,introduction,0,18,7,7,0,introduction : introduction,0.10112359550561796,0.3333333333333333,0.3333333333333333,Even on datasets with relatively abundant labels multi task transfer can sometimes achieve improvement over state of the art results ,21,"In these cases , transfer learning can improve performance by taking advantage of more plentiful labels from related tasks .","Recently , a number of approaches based on deep neural networks have addressed the problem of sequence tagging in an end - to - end manner .",introduction
natural_language_inference,97,MCTest is challenging because it is both complicated and small .,dataset,The Dataset,0,212,6,6,0,dataset : The Dataset,0.726027397260274,0.75,0.75,MCTest is challenging because it is both complicated and small ,11,"Despite the elementary level , stories and questions are more natural and more complex than those found in synthetic MC datasets like bAb I and CNN .","As per , "" it is very difficult to train statistical models only on MCTest . """,experiment
natural_language_inference,52,"Using another vector representation of only the unique words in the justification , i.e. , the words that do not occur in either the question or the candidate answer , we also compute sim ( Q , uniqueJ ) and sim ( A , unique J ) .",model,Representation-based features (Emb):,0,98,16,5,0,model : Representation-based features (Emb):,0.3754789272030651,0.2857142857142857,0.5,Using another vector representation of only the unique words in the justification i e the words that do not occur in either the question or the candidate answer we also compute sim Q uniqueJ and sim A unique J ,40,". We then compute sim ( Q , A ) , sim ( Q , J ) , and sim ( A , J ) us - ing cosine similarity .","To create a feature which captures the relationship between the question , answer , and justification , we take inspiration from TransE , a popular relation extraction framework .",method
prosody_prediction,0,"We selected the ' clean ' subsets of LibriTTS for annotation , comprising of 262.5 hours of read speech from 1230 speakers .",dataset,Dataset,0,61,4,4,0,dataset : Dataset,0.3177083333333333,0.17391304347826084,0.17391304347826084,We selected the clean subsets of LibriTTS for annotation comprising of 262 5 hours of read speech from 1230 speakers ,21,"The LibriTTS corpus is a cleaned subset of LibriSpeech corpus , derived from English audiobooks of the LibriVox project .",The transcribed sentences were aligned 1 https://librivox.org :,experiment
machine-translation,8,"The backward states ( ? ? h 1 , , ? ? h Tx ) are computed similarly .",model,DETAILED DESCRIPTION OF THE MODEL,0,259,46,12,0,model : DETAILED DESCRIPTION OF THE MODEL,0.7824773413897281,0.3898305084745763,0.8,The backward states h 1 h Tx are computed similarly ,11,"and n are the word embedding dimensionality and the number of hidden units , respectively . ? ( ) is as usual a logistic sigmoid function .","We share the word embedding matrix E between the forward and backward RNNs , unlike the weight matrices .",method
sentiment_analysis,18,We propose two novel approaches for improving the effectiveness of attention models .,introduction,introduction,1,27,13,13,0,introduction : introduction,0.11297071129707112,0.34210526315789475,0.34210526315789475,We propose two novel approaches for improving the effectiveness of attention models ,13,Our work builds upon this line of research .,The first approach is a new way of encoding a target which better captures the aspect semantics of the target expression .,introduction
sentiment_analysis,28,ransformation ( PCT ) can transform contextual information gathered by the MHA .,methodology,GloVe Embedding,0,88,38,25,0,methodology : GloVe Embedding,0.4888888888888889,0.6785714285714286,0.5813953488372093,ransformation PCT can transform contextual information gathered by the MHA ,11,Point - wise Convolution Transformation,Point - wise means that the kernel sizes are 1 and the same transformation is applied to every single token belonging to the input .,method
natural_language_inference,54,We run our experiments on a machine that contains a single GTX 1080 GPU with 8 GB VRAM .,experiment,Experiment Setting,1,124,12,2,0,experiment : Experiment Setting,0.5414847161572053,0.4615384615384616,0.125,We run our experiments on a machine that contains a single GTX 1080 GPU with 8 GB VRAM ,19, ,All of the models being compared have the same settings on character embedding and word embedding .,experiment
temporal_information_extraction,0,"As expected , running a transitive closure module after the temporal rule - based sieve ( RB + TR ) results in improving recall , but the over all performance is still lacking ( less than .30 F1-score ) .",evaluation,Evaluation Results,1,160,15,15,0,evaluation : Evaluation Results,0.8465608465608465,0.5,0.75,As expected running a transitive closure module after the temporal rule based sieve RB TR results in improving recall but the over all performance is still lacking less than 30 F1 score ,33,"Results are reported in , evaluated on both TempEval - 3 and TimeBank - Dense test data .",Combining rule - based and machine - learned sieves ( RB + ML ) yields a slight improvement compared with enabling only the machine - learned sieve in the system ( ML ) .,result
paraphrase_generation,0,"Though of particular interest to students and enthusiast of international dance and world music , the film is designed to make viewers of all ages , cultural backgrounds and rhythmic ability want to getup and dance .",baseline,157382,0,210,32,2,0,baseline : 157382,0.8860759493670886,0.7619047619047619,0.2857142857142857,Though of particular interest to students and enthusiast of international dance and world music the film is designed to make viewers of all ages cultural backgrounds and rhythmic ability want to getup and dance ,35, ,comic gem with some serious sparkles .,result
text_generation,4,neural language model conditioned on the encoder output hi serves as the decoder .,system description,Skip-Thought Architecture,0,50,23,8,0,system description : Skip-Thought Architecture,0.4587155963302753,0.575,0.6153846153846154,neural language model conditioned on the encoder output hi serves as the decoder ,14,Both update gates take values between zero and one .,"Bias matrices C z , Cr , C are introduced for the update gate , reset gate and hidden state computation by the encoder .",method
natural_language_inference,62,"For model optimization , we apply the Adam ( Kingma and Ba , 2014 ) optimizer with a learning rate of 0.0005 and a minibatch size of 32 .",experiment,Experimental Settings,1,184,12,11,1,experiment : Experimental Settings,0.8214285714285714,0.3157894736842105,0.7857142857142857,For model optimization we apply the Adam Kingma and Ba 2014 optimizer with a learning rate of 0 0005 and a minibatch size of 32 ,26,"For the dense layers and the BiLSTMs , we set the dimensionality unit d to 600 .","For model evaluation , we use Exact Match ( EM ) and F 1 score as evaluation metrics .",experiment
natural_language_inference,81,"An example is the query instance of qilakitsoq , for which the model predicts "" archaeological site "" , which is more specific than the answer "" town "" .",analysis,ERROR ANALYSIS,0,181,28,11,0,analysis : ERROR ANALYSIS,0.42992874109263657,0.9032258064516128,0.7857142857142857,An example is the query instance of qilakitsoq for which the model predicts archaeological site which is more specific than the answer town ,24,The third type ( 22 % of errors ) results from queries that yield multiple correct answers .,The second and third types of errors underscore the difficulty of using distant supervision to create large - scale datasets such as WikiHop .,result
sarcasm_detection,1,"Since CUE - CNN generates its user embeddings using a method similar to the ParagraphVector , we test the importance of personality features being included in our user profiling .",model,Baseline Models,1,279,24,24,0,model : Baseline Models,0.8353293413173652,0.9230769230769232,0.9230769230769232,Since CUE CNN generates its user embeddings using a method similar to the ParagraphVector we test the importance of personality features being included in our user profiling ,28,"We further compare our proposed user -profiling method with that of CUE - CNN , with absolute differences shown in the bottom row of .","As seen in the table , CASCADE without personality features drops in performance to a range similar to CUE - CNN .",method
sentiment_analysis,20,It is inconclusive as to which quantification approach is better .,experiment,Experiments and Results,0,171,9,9,0,experiment : Experiments and Results,0.9193548387096774,0.5294117647058824,0.5294117647058824,It is inconclusive as to which quantification approach is better ,11,"To get a better insight into the quantification approaches , we compare the performance of CC and PCC .",PCC outperformed CC in but underperformed CC in .,experiment
semantic_role_labeling,0,We propose an end - to - end approach for predicting all the predicates and their argument spans in one forward pass .,introduction,introduction,1,13,5,5,0,introduction : introduction,0.1238095238095238,0.35714285714285715,0.35714285714285715,We propose an end to end approach for predicting all the predicates and their argument spans in one forward pass ,21,"They are typically only evaluated with gold predicates , and must be pipelined with error - prone predicate identification models for deployment .","Our model builds on a recent coreference resolution model , by making central use of learned , contextualized span representations .",introduction
natural_language_inference,92,"Examples of the input , answer text ( y ) , f , Z tot and Z. First , in multi-mention reading comprehension , the answer text ' Robert Schumann ' is mentioned six times but only the fourth span is related to the question .",method,Reading Comprehension with Discrete Reasoning (DROPnum),0,130,77,11,0,method : Reading Comprehension with Discrete Reasoning (DROPnum),0.4513888888888889,0.7857142857142857,0.5789473684210527,Examples of the input answer text y f Z tot and Z First in multi mention reading comprehension the answer text Robert Schumann is mentioned six times but only the fourth span is related to the question ,38,Afterwards the Titans would retake the lead with Young and Williams hooking up with each other again on a 41 yard td pass . :,"Second , in reading comprehension with discrete reasoning , many equations yield to the answer 4 , but only ' 40 - 37 ' answers the question .",method
text-classification,5,"While our approach is equally applicable to sequence labeling tasks , we focus on text classification tasks in this work due to their important realworld applications .",experiment,Experiments,0,148,2,2,0,experiment : Experiments,0.5873015873015873,0.6666666666666666,1.0,While our approach is equally applicable to sequence labeling tasks we focus on text classification tasks in this work due to their important realworld applications ,26, , ,experiment
natural_language_inference,24,"Finally , a bilinear function is used to find the begin and endpoint of answer spans at each reasoning step t ? { 0 , 1 , . . . , T ? 1}.",system description,Experiment Setup,0,106,79,16,0,system description : Experiment Setup,0.45299145299145294,0.8681318681318682,0.5714285714285714,Finally a bilinear function is used to find the begin and endpoint of answer spans at each reasoning step t 0 1 T 1 ,25,"Here , x t is computed from the previous state s t?1 and memory M : x t = j ? j M j and ? j = sof tmax ( s t?1 W 5 M ) .","Finally , a bilinear function is used to find the begin and endpoint of answer spans at each reasoning step t ? { 0 , 1 , . . . , T ? 1}.",method
sentiment_analysis,33,The filter - detected m-gram of a sentence is the one whose convolution feature vector has the shortest Euclidean distance to the max - pooled vector .,analysis,Analysis,0,103,14,14,0,analysis : Analysis,0.8240000000000001,0.7777777777777778,0.7777777777777778,The filter detected m gram of a sentence is the one whose convolution feature vector has the shortest Euclidean distance to the max pooled vector ,26,The key phrase hit rate is then defined as the ratio of filter - detected m-grams that fall into the corresponding key phrase sets .,The hit rate results computed on SST dev set are presented in .,result
text-to-speech_synthesis,1,"Each audio is listened by at least 20 testers , who are all native English speakers .",result,Results,0,145,6,6,0,result : Results,0.6621004566210046,0.14285714285714285,0.2727272727272727,Each audio is listened by at least 20 testers who are all native English speakers ,16,"We keep the text content consistent among different models so as to exclude other interference factors , only examining the audio quality .","We compare the MOS of the generated audio samples by our FastSpeech model with other systems , which include 1 ) GT , the ground truth audio ; 2 ) GT ( Mel + WaveGlow ) , where we first convert the ground truth audio into mel-spectrograms , and then convert the mel-spectrograms back to audio using WaveGlow ; 3 ) Tacotron 2 ( Mel + WaveGlow ) ; 4 ) Transformer TTS ( Mel + WaveGlow ) .",result
relation_extraction,4,"This is mainly because : ( 1 ) these datasets are relatively small for effectively training high - capacity models ( see ) , and ( 2 ) they capture very different types of relations .",introduction,introduction,0,73,63,63,0,introduction : introduction,0.3526570048309179,0.7682926829268293,0.7682926829268293,This is mainly because 1 these datasets are relatively small for effectively training high capacity models see and 2 they capture very different types of relations ,27,"However , existing relation extraction datasets such as the SemEval - 2010 Task 8 dataset and the Automatic Content Extraction ( ACE ) dataset are less useful for this purpose .","For example , the SemEval dataset focuses on semantic relations ( e.g. , Cause - Effect , Component - Whole ) between two nominals .",introduction
sentiment_analysis,1,"Our proposed Emotion DL regularizer is inspired by , which applies distribution learning to learn labels with ambiguity in the computer vision domain .",system description,Learning with Noisy Labels,0,119,47,7,0,system description : Learning with Noisy Labels,0.3005050505050505,0.2270531400966184,1.0,Our proposed Emotion DL regularizer is inspired by which applies distribution learning to learn labels with ambiguity in the computer vision domain ,23,Several other approaches include bootstrap that leverages predicted labels to generate training targets and alternatively updating network parameters and labels during training ., ,method
relation_extraction,12,"In practice , we treat the original adjacency matrix as an initialization so that the dependency information can be captured in the node representations for later attention calculation .",system description,Attention Guided Layer,0,112,57,32,0,system description : Attention Guided Layer,0.3404255319148936,0.5757575757575758,0.9696969696969696,In practice we treat the original adjacency matrix as an initialization so that the dependency information can be captured in the node representations for later attention calculation ,28,"Accordingly , the input dependency tree is converted into multiple fully connected edge - weighted graphs .",The attention guided layer is included starting from the second block .,method
natural_language_inference,89,"Here , bi refers to the attended vector from question Q for the i - th word in answer sentence S , and vice versa for c j .",approach,Answer Verifier,0,132,66,29,0,approach : Answer Verifier,0.5076923076923077,0.75,0.5686274509803921,Here bi refers to the attended vector from question Q for the i th word in answer sentence S and vice versa for c j ,26,aij ls k =1 ea kj s i,"Next , in order to separately compare the aligned pairs",method
natural_language_inference,67,This observation also agrees with .,system description,Chunk Representation,0,119,79,17,0,system description : Chunk Representation,0.5833333333333334,0.8404255319148937,0.53125,This observation also agrees with ,6,"8 ) We hypothesize that the hidden states at that two ends can better represent the chunk 's contexts , which is critical for this task , than the states within the chunk .","Ranker Layer Each chunk c m,n is evaluated on its context similarity to the question , by taking the cosine similarity between the chunk context representation ? m ,n acquired from chunk representation layer , and the question representation which is the concatenation of the last hidden state in forward RNN and the first hidden state in backward RNN .",method
sentence_classification,2,The base model used is the convolutional neural network ( CNN ) for sentences .,system description,Problem: Translated Sentences as Context,0,59,7,6,0,system description : Problem: Translated Sentences as Context,0.2341269841269841,0.21212121212121213,0.1875,The base model used is the convolutional neural network CNN for sentences ,13,Base Model : Convolutional Neural Network .,It is a simple variation of the original CNN for texts to be used on sentences .,method
natural_language_inference,90,"In contrast to existing approaches , our approach only relies on alignment and is fully computationally decomposable with respect to the input text .",introduction,introduction,1,24,17,17,0,introduction : introduction,0.16,0.7083333333333334,0.7083333333333334,In contrast to existing approaches our approach only relies on alignment and is fully computationally decomposable with respect to the input text ,23,"We leverage this intuition to build a simpler and more lightweight approach to NLI within a neural framework ; with considerably fewer parameters , our model outperforms more complex existing neural architectures .",An overview of our approach is given in Figure 1 .,introduction
sentiment_analysis,49,"It depends on the information that can be obtained from more than one modality ( e.g. text , visual and acoustic ) for the analysis .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.05533596837944664,0.12121212121212124,0.12121212121212124,It depends on the information that can be obtained from more than one modality e g text visual and acoustic for the analysis ,24,"In contrast , multi-modal sentiment analysis has recently gained attention due to the tremendous growth of many social media platforms such as YouTube , Instagram , Twitter , Facebook etc .",The motivation is to leverage the varieties of ( often distinct ) information from multiple sources for building an efficient system .,introduction
text-classification,1,"LSTM can embed text regions of variable ( and possibly large ) sizes , whereas the region size needs to be fixed in a CNN .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.02734375,0.5555555555555556,0.5555555555555556,LSTM can embed text regions of variable and possibly large sizes whereas the region size needs to be fixed in a CNN ,23,"Under this framework , we explore a more sophisticated region embedding method using Long Short - Term Memory ( LSTM ) .",We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings .,abstract
natural_language_inference,51,"At timestep t , given a query key k pt , the working program is retrieved as follows , where D ( ) is cosine similarity and ? pt is the scalar program strength parameter .",method,Neural Stored-program Memory,0,73,11,10,0,method : Neural Stored-program Memory,0.27547169811320754,0.15492957746478872,0.38461538461538464,At timestep t given a query key k pt the working program is retrieved as follows where D is cosine similarity and pt is the scalar program strength parameter ,30,Let us denote M p ( i ) .k and M p ( i ) .v as the key and the program of the i - th memory slot .,"At timestep t , given a query key k pt , the working program is retrieved as follows , where D ( ) is cosine similarity and ? pt is the scalar program strength parameter .",method
text-classification,7,Investigating Capsule Networks with Dynamic Routing for Text Classification,title,title,1,2,1,1,0,title : title,0.00823045267489712,1.0,1.0,Investigating Capsule Networks with Dynamic Routing for Text Classification,9, , ,title
named-entity-recognition,1,"?Y Xe s ( X , y) .",model,model,0,62,13,13,0,model : model,0.2995169082125604,0.6842105263157895,0.6842105263157895, Y Xe s X y ,7,softmax over all possible tag sequences yields a probability for the sequence y:,"During training , we maximize the log-probability of the correct tag sequence :",method
sentiment_analysis,4,"In this paper , we address the problem of emotion recognition in conversational videos .",introduction,introduction,0,18,9,9,0,introduction : introduction,0.05521472392638037,0.20454545454545456,0.20454545454545456,In this paper we address the problem of emotion recognition in conversational videos ,14,This is due to the presence of intricate dependencies between the affective states of speakers participating in the dialogue .,We specifically focus on dyadic conversations where two entities participate in a dialogue .,introduction
text-classification,1,"In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .",system description,Experiments (supervised),1,143,97,20,0,system description : Experiments (supervised),0.55859375,0.485,0.3333333333333333,In on three out of the four datasets oh 2 LSTMp outperforms SVM and the CNN ,17,"They were obtained by bow - CNN ( whose input to the embedding function is a bow vector of the region ) with region size 20 on RCV1 , and seq -CNN ( with the regular concatenation input ) with region size 3 on the others .","However , on RCV1 , it underperforms both .",method
semantic_parsing,1,"The MRs in the two datasets are defined in ?- calculus logical forms ( e.g. , "" lambda x ( and ( state x ) ( next to x texas ) ) "" and "" lambda x ( and ( flight x dallas ) ( to x baltimore ) ) "" ) .",system description,Semantic Parsing,0,110,4,4,0,system description : Semantic Parsing,0.7692307692307693,0.25,0.6666666666666666,The MRs in the two datasets are defined in calculus logical forms e g lambda x and state x next to x texas and lambda x and flight x dallas to x baltimore ,34,"GEO is a collection of 880 U.S. geographical questions ( e.g. , "" Which states border Texas ? "" ) , and ATIS is a set of 5,410 inquiries of flight information ( e.g. , "" Show me flights from Dallas to Baltimore "" ) .",We use the pre-processed datasets released by .,method
natural_language_inference,65,where each column min Q ? R cM contains features extracted in a context window around the m - th word of q .,system description,Convolution,0,58,17,5,0,system description : Convolution,0.25217391304347825,0.425,0.17857142857142858,where each column min Q R cM contains features extracted in a context window around the m th word of q ,22,The output of the convolution with c filters over the question q is computed as follows :,where each column min Q ? R cM contains features extracted in a context window around the m - th word of q .,method
topic_models,0,"Using the above described scheme , we trained three different classifiers : ( i ) Gaussian linear classifier ( GLC ) , ( ii ) multi-class logistic regression ( LR ) , and , ( iii ) Gaussian linear classifier with uncertainty ( GLCU ) .",system,system,0,280,6,6,0,system : system,0.6796116504854369,0.6666666666666666,0.6666666666666666,Using the above described scheme we trained three different classifiers i Gaussian linear classifier GLC ii multi class logistic regression LR and iii Gaussian linear classifier with uncertainty GLCU ,30,It can then be used to stop the training earlier if needed .,"Note that GLC and LR can not exploit the uncertainty in the document embeddings ; and are trained using only the mean parameter ? of the posterior distributions ; whereas GLCU is trained using the full posterior distribution q ( w ) , i.e. , along with the uncertainties of document embeddings as described in Section IV .",method
natural_language_inference,69,"We start with individual KB facts and transform them into query - answer pairs by leaving the object slot empty , i.e. q = ( s , r , ? ) and a * = o.",dataset,Dataset Assembly,0,54,4,4,0,dataset : Dataset Assembly,0.1565217391304348,0.17391304347826084,0.17391304347826084,We start with individual KB facts and transform them into query answer pairs by leaving the object slot empty i e q s r and a o ,28,"For example , one such fact could be ( Hanging Gardens of Mumbai , country , India ) .","Next , we define a directed bipartite graph , where vertices on one side correspond to documents in D , and vertices on the other side are entities from the KB - see for an example .",experiment
question_generation,0,We extend the s 2s+ att with our feature - rich encoder to build the NQG system .,experiment,Experiments and Results,1,77,15,15,0,experiment : Experiments and Results,0.4230769230769231,0.7142857142857143,0.7142857142857143,We extend the s 2s att with our feature rich encoder to build the NQG system ,17,We implement a seq2seq with attention as the baseline method .,"NQG + Based on NQG , we incorporate copy mechanism to deal with rare words problem .",experiment
sentiment_analysis,9,This section introduces the architecture and methodology of LCF - ATEPC .,methodology,Methodology,0,67,4,4,0,methodology : Methodology,0.24187725631768955,0.5714285714285714,0.6666666666666666,This section introduces the architecture and methodology of LCF ATEPC ,11,"To propose an effective aspect - based sentiment analysis model based on multi-task learning , we adopted domain - adapted BERT model from BERT - ADA and integrated the local context focus mechanism into the proposed model .","This section introduces the methodology of the APC module and the ATE module , respectively .",method
question_answering,1,"Context : "" A piece of paper was later found on which Luther had written his last statement .",analysis,ERROR ANALYSIS,0,264,2,2,0,analysis : ERROR ANALYSIS,0.8328075709779179,0.03636363636363636,1.0,Context A piece of paper was later found on which Luther had written his last statement ,17, , ,result
sarcasm_detection,0,Each element is given as a key to the previous file .,system description,hashtable in JSON format containing all comments,0,50,25,5,0,system description : hashtable in JSON format containing all comments,0.2840909090909091,0.8333333333333334,0.5,Each element is given as a key to the previous file ,12,"An array in CSV format , with each row containing a sequence of comments leading up to a sarcastic comment , the ( sarcastic and non-sarcastic ) responses to the last element in that sequence , and the labels of those responses .",This raw corpus is very large and suitable for both largescale machine learning and statistical analysis as well for deriving smaller benchmark tasks for evaluating sarcasm,method
sentiment_analysis,13,"As there are no existing datasets for RRC and to be consistent with existing research on sentiment analysis , we adopt the laptop and restaurant reviews of SemEval 2016 Task 5 as the source to create datasets for RRC .",experiment,Experiments,0,182,6,6,0,experiment : Experiments,0.6546762589928058,0.15,0.15,As there are no existing datasets for RRC and to be consistent with existing research on sentiment analysis we adopt the laptop and restaurant reviews of SemEval 2016 Task 5 as the source to create datasets for RRC ,39,"We aim to answer the following research questions ( RQs ) in the experiment : RQ1 : what is the performance gain of posttraining for each review - based task , with respect to the state - of - the - art performance ? RQ2 : what is the performance of BERT 's pretrained weights on three review - based tasks without any domain and task adaptation ? RQ3 : upon ablation studies of separate domain knowledge post - training and task - awareness posttraining , what is their respective contribution to the whole post - training performance gain ? End Task Datasets",We do not use SemEval 2014 Task 4 or SemEval 2015 Task 12 because these datasets do not come with the review ( document ) level XML tags to recover whole reviews from review sentences .,experiment
machine-translation,9,"In contrast , a 32 16 compositional coding scheme will produce codes of only 128 bits . :",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,113,17,17,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.3937282229965157,0.2833333333333333,0.6296296296296297,In contrast a 32 16 compositional coding scheme will produce codes of only 128 bits ,16,"For example , a binary code will have a length of 256 bits to support 512 basis vectors .",Comparison of different coding approaches .,method
text-classification,4,"We sample 15 % of the training data as the validation set , to select hyperparameters for our models and perform early stopping .",dataset,dataset,0,145,7,7,0,dataset : dataset,0.6561085972850679,0.5833333333333334,0.5833333333333334,We sample 15 of the training data as the validation set to select hyperparameters for our models and perform early stopping ,22,"DBpedia is extracted from Wikipedia by crowd - sourcing and is categorized into 14 non-overlapping ontology classes , including Company , Athlete , Natural Place , etc .","For sentence matching , we evaluate the AdaQA model on two datasets for open - domain question answering : Wiki QA and SelQA .",experiment
natural_language_inference,67,"An answer candidate for the i - th training example is defined as c m , n i , a sub-sequence in P i , that spans from position m ton ( 1 ? m ? n ? | P i | ) .",system description,Problem Definition,0,42,2,2,0,system description : Problem Definition,0.2058823529411765,0.02127659574468085,0.5,An answer candidate for the i th training example is defined as c m n i a sub sequence in P i that spans from position m ton 1 m n P i ,34, ,"The ground truth answer A i could be included in the set of all candidates Ci = { c m ,n i |? m , n ? N + , subj ( m , n , P i ) and 1 ? m ? n ? | P i |} , where subj ( m , n , P i ) is the constraint put on the candidate chunk for P i , such as , "" c m , n i can have at most 10 tokens "" , or "" c m , n i must have a pre-defined POS pattern "" .",method
sentiment_analysis,47,Left - Center - Right Separated LSTMs,model,Left-Center-Right Separated LSTMs,0,73,3,1,0,model : Left-Center-Right Separated LSTMs,0.3395348837209302,0.06666666666666668,0.08333333333333333,Left Center Right Separated LSTMs,5, , ,method
natural_language_inference,27,We adopt the strategy of to predict the probability of each position in the context being the start or end of an answer span .,model,Model Encoder Layer. Similar to,0,116,72,7,0,model : Model Encoder Layer. Similar to,0.3431952662721893,0.5413533834586466,0.3888888888888889,We adopt the strategy of to predict the probability of each position in the context being the start or end of an answer span ,25,Each example in SQuAD is labeled with a span in the context containing the answer .,"More specifically , the probabilities of the starting and ending position are modeled as",method
sarcasm_detection,1,CASCADE : Contextual Sarcasm Detection in Online Discussion Forums,title,title,1,2,1,1,0,title : title,0.0059880239520958105,1.0,1.0,CASCADE Contextual Sarcasm Detection in Online Discussion Forums,8, , ,title
text_generation,2,"Specifically , we pick the News section from the original dataset .",experiment,Long Text Generation: EMNLP2017 WMT News,0,188,11,4,0,experiment : Long Text Generation: EMNLP2017 WMT News,0.5371428571428571,0.2972972972972973,0.3076923076923077,Specifically we pick the News section from the original dataset ,11,Dataset as the long text corpus .,"The news dataset consists of 646,459 words and 397,726 sentences .",experiment
sentiment_analysis,39,"More concretely , we define the following sparse representations of locations :",baseline,Logistic Regression,1,164,12,4,0,baseline : Logistic Regression,0.6693877551020408,0.35294117647058826,0.25,More concretely we define the following sparse representations of locations ,11,We can think of these features as a sparse representation e l that enter the softmax in equation 1 .,Mask target entity n-grams :,result
text-to-speech_synthesis,1,We conduct experiments on the LJSpeech dataset to test FastSpeech .,introduction,introduction,0,37,24,24,0,introduction : introduction,0.1689497716894977,0.8888888888888888,0.8888888888888888,We conduct experiments on the LJSpeech dataset to test FastSpeech ,11,"The length regulator can easily adjust voice speed by lengthening or shortening the phoneme duration to determine the length of the generated mel-spectrograms , and can also control part of the prosody by adding breaks between adjacent phonemes .","The results show that in terms of speech quality , FastSpeech nearly matches the autoregressive Transformer model .",introduction
machine-translation,6,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity .,experiment,Settings,1,183,27,9,0,experiment : Settings,0.6288659793814433,0.5192307692307693,0.2647058823529412,The goal is to predict the next word conditioned on previous words and the task is evaluated by perplexity ,20,Language Modeling is a basic task in natural language processing .,"We do experiments on two widely used datasets , Penn Treebank ( PTB ) and WikiText - 2 ( WT2 ) .",experiment
text_summarization,2,We investigate the source dependency relations preserved in the summaries in .,system,System,0,260,15,15,0,system : System,0.9352517985611508,0.5172413793103449,0.5172413793103449,We investigate the source dependency relations preserved in the summaries in ,12,"On the other hand , the ground - truth summaries , corresponding to article titles , are judged as less satisfying according to human raters .",source relation is considered preserved if both its words appear in the summary .,method
natural_language_inference,37,"We compute a second vector , v 2 in the same way using the end scores .",method,No-Answer Option,0,129,46,17,0,method : No-Answer Option,0.5019455252918288,0.8518518518518519,0.85,We compute a second vector v 2 in the same way using the end scores ,16,"We compute a soft attention over the span start scores , pi = e s in j =1 e s j , and then take the weighted sum of the hidden states from the GRU used to generate those scores , hi , giving v 1 = n i = 1 hi pi .","Finally , a step of learned attention is performed on the output of the Self - Attention layer that computes :",method
sentiment_analysis,5,"In this experiment , we adopt the same protocols as , and .",experiment,The EEG emotion recognition experiments,0,168,33,3,0,experiment : The EEG emotion recognition experiments,0.6339622641509434,0.3793103448275862,0.08571428571428573,In this experiment we adopt the same protocols as and ,11,) The subject - dependent experiment :,"Namely , for SEED , we use the former nine trails of EEG data per session of each subject as source ( training ) domain data while using the remaining six trials per session as target ( testing ) domain data ; for SEED - IV , we use the first sixteen trials per session of each subject as the training data , and the last eight trials containing all emotions ( each emotion with two trials ) as the testing data ; for MPED , we use twenty - one trials of EEG data as training data and the rest seven trails consist of seven emotions as testing data for each subject .",experiment
natural_language_inference,81,"Iceland is warmed by the Gulf Stream and has a temperate climate , despite a high latitude just outside the Arctic Circle .",APPENDIX,Answer town,0,342,134,54,0,APPENDIX : Answer town,0.8123515439429929,0.6291079812206573,0.40601503759398494,Iceland is warmed by the Gulf Stream and has a temperate climate despite a high latitude just outside the Arctic Circle ,22,"The interior consists of a plateau characterised by sand and lava fields , mountains and glaciers , while many glacial rivers flow to the sea through the lowlands .","Its high latitude and marine influence still keeps summers chilly , with most of the archipelago having a tundra climate .",others
sentiment_analysis,18,We build upon this line of research and propose two novel approaches for improving the effectiveness of attention .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.03347280334728033,0.5,0.5,We build upon this line of research and propose two novel approaches for improving the effectiveness of attention ,19,The mechanism is able to capture the importance of each context word towards a target by modeling their semantic associations .,"First , we propose a method for target representation that better captures the semantic meaning of the opinion target .",abstract
natural_language_inference,77,The dataset is built from CNN news stories that were originally collected by .,experiment,Experimental Setup,0,138,6,6,0,experiment : Experimental Setup,0.5054945054945055,0.8571428571428571,0.8571428571428571,The dataset is built from CNN news stories that were originally collected by ,14,QA dataset 3 contains 100 k answerable questions from a total of 120 k questions .,"Performance on the SQuAD and News QA datasets is measured in terms of exact match ( accuracy ) and a mean , per answer token - based F 1 measure which was originally proposed by to also account for partial matches .",experiment
sentiment_analysis,1,The last step in constructing the adjacency matrix is finding an optimal value of ? to regularize the weights of connections between local channels .,system description,Adjacency Matrix in RGNN,0,201,129,24,0,system description : Adjacency Matrix in RGNN,0.5075757575757576,0.6231884057971014,0.8571428571428571,The last step in constructing the adjacency matrix is finding an optimal value of to regularize the weights of connections between local channels ,24,Note that our adjacency matrix A obtained in ( 10 ) aims to represent the brain network which combines both local anatomical connectivity and emotion - related global functional connectivity .,"Achard and Bullmore observed that sparse f MRI networks , comprising around 20 % of all possible connections , typically maximize the efficiency of the network topology .",method
sentiment_analysis,44,Tree by finding the sub-tree that exactly spans over the sentence .,system description,Corpora,0,98,8,8,0,system description : Corpora,0.44545454545454544,0.5333333333333333,0.5333333333333333,Tree by finding the sub tree that exactly spans over the sentence ,13,The sentence - level Discourse Trees were extracted from the document - level Discourse,"This resulted in a dataset of 6846 sentences with well - formed Discourse Trees , out of which 2239 sentences had only one EDU .",method
named-entity-recognition,4,"In every task considered , simply adding ELMo establishes a new state - of - the - art result , with relative error reductions ranging from 6 - 20 % over strong base models .",model,Pre-trained bidirectional language model architecture,0,103,56,18,0,model : Pre-trained bidirectional language model architecture,0.3786764705882353,0.9655172413793104,0.9,In every task considered simply adding ELMo establishes a new state of the art result with relative error reductions ranging from 6 20 over strong base models ,28,shows the performance of ELMo across a diverse set of six benchmark NLP tasks .,This is a very general result across a diverse set model architectures and language understanding tasks .,method
natural_language_inference,44,More analyses are shown in Appendix B.,system description,SQuAD and NewsQA,0,152,59,49,0,system description : SQuAD and NewsQA,0.5314685314685315,0.6483516483516484,0.6049382716049383,More analyses are shown in Appendix B ,8,"In both examples , Dyn selects the oracle sentence with minimum number of sentences , and subsequently predicts the answer .",Trivia QA and SQuAD - Open,method
named-entity-recognition,6,"In this work , we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system .",introduction,introduction,1,24,12,12,0,introduction : introduction,0.11320754716981132,0.631578947368421,0.631578947368421,In this work we discuss some of the limitations of gazetteer features and propose an alternative lexical representation which is trained offline and that can be added to any neural NER system ,33,Gazetteer features encode the presence of word n-grams in predefined lists of NEs .,"In a nutshell , we embed words and entity types into a joint vector space by leveraging WiFiNE , a ressource which automatically annotates mentions in Wikipedia with 120 entity types .",introduction
natural_language_inference,39,Systems in the competition are ranked by the weighted mean ( the of - Model Pearson 's r Beltagy et al . 0.8300 0.8730 0.8803 0.9090,experiment,Experimental Setup,0,168,38,38,1,experiment : Experimental Setup,0.8155339805825242,0.95,0.95,Systems in the competition are ranked by the weighted mean the of Model Pearson s r Beltagy et al 0 8300 0 8730 0 8803 0 9090,27,"Our model outperforms the work of , which already reports a Pearson 's r score of over 0.9 , STS2014 Results ) .","This work 0.9112 , 3rd systems in the STS2014 competition , all of which are based on heavy feature engineering .",experiment
natural_language_inference,60,"We use a standard bidirectional LSTM encoder with max - pooling ( BiLSTM - Max ) , which computes two sets of s hidden states , one for each direction :",baseline,Naive baseline,0,89,22,22,0,baseline : Naive baseline,0.461139896373057,0.6111111111111112,0.9565217391304348,We use a standard bidirectional LSTM encoder with max pooling BiLSTM Max which computes two sets of s hidden states one for each direction ,25,"We set m = 2 , which makes the contextualization very efficient .","The hidden states are subsequently concatenated for each timestep to obtain the final hidden states , after which a max - pooling operation is applied over their components to get the final sentence representation :",result
sentiment_analysis,20,"For the tasks of spell correction and word segmentation ) we used the Viterbi algorithm , utilizing word statistics ( unigrams and bigrams ) from our unlabeled dataset , to obtain word probabilities .",system description,Text Processor,0,47,22,10,0,system description : Text Processor,0.25268817204301075,0.4313725490196079,0.7692307692307693,For the tasks of spell correction and word segmentation we used the Viterbi algorithm utilizing word statistics unigrams and bigrams from our unlabeled dataset to obtain word probabilities ,29,"This is where we perform spell correction , word normalization and segmentation and decide which tokens to omit , normalize or annotate ( surround or replace with special tags ) .","Moreover , we lowercase all words , and normalize URLs , emails and user handles ( @user ) .",method
machine-translation,5,"Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation ( SMT ) systems , the dominant neural network ( NN ) architectures for NMT have changed on a yearly ( and even more frequent ) basis .",introduction,introduction,0,10,3,3,0,introduction : introduction,0.06993006993006994,0.1875,0.1875,Since 2016 when NMT systems first showed to achieve significantly better results than statistical machine translation SMT systems the dominant neural network NN architectures for NMT have changed on a yearly and even more frequent basis ,37,Neural machine translation ( NMT ) is a rapidly changing research area .,The state - of - the - art in 2016 were shallow attention - based recurrent neural networks ( RNN ) with gated recurrent units ( GRU ) in recurrent layers .,introduction
named-entity-recognition,2,The convolutional operator applied to each token x t with output ct is defined as :,model,Dilated Convolutions,0,73,28,5,0,model : Dilated Convolutions,0.3427230046948357,0.4745762711864407,0.38461538461538464,The convolutional operator applied to each token x t with output ct is defined as ,16,"Here , and throughout the paper , we do not explicitly write the bias terms in affine transformations .",where ? is vector concatenation .,method
question_generation,0,We reuse the attention probability in equation 4 to decide which word to copy .,approach,Copy Mechanism,0,62,37,7,0,approach : Copy Mechanism,0.3406593406593407,1.0,1.0,We reuse the attention probability in equation 4 to decide which word to copy ,15,where ? is sigmoid function ., ,method
question_answering,0,Similar graphical representations were used in previous work .,introduction,introduction,0,22,13,13,0,introduction : introduction,0.07457627118644068,0.3714285714285713,0.3714285714285713,Similar graphical representations were used in previous work ,9,Semantic parses can be deterministically converted to a query to extract the answers from the KB .,"However , the modern semantic parsing approaches usually focus either on the syntactic analysis of the input question or on detecting individual KB relations , whereas the structure of the semantic parse is ignored or only approximately modeled .",introduction
sentiment_analysis,22,"The meaning of the example sentence in the case study is that the "" array of sushi "" is good .",performance,performance,0,201,48,48,0,performance : performance,0.8777292576419214,0.9056603773584906,0.9056603773584906,The meaning of the example sentence in the case study is that the array of sushi is good ,19,"In , we give the visualization of the attention weights ( Source2context ) on this sentence computed by HAPN .","Obviously , the words "" freshest "" and "" most delicious "" play an important role in judging the sentiment polarity of "" array of sushi "" .",result
natural_language_inference,42,"Secondly , it has lower memory requirements , which is attractive to applications that dispose of low computational power .",evaluation,FABIR vs BiDAF,0,253,28,16,0,evaluation : FABIR vs BiDAF,0.8754325259515571,0.5283018867924528,1.0,Secondly it has lower memory requirements which is attractive to applications that dispose of low computational power ,18,"First , it s training time is expected to be shorter , because the number of variables to be updated in every iteration is smaller .", ,result
natural_language_inference,17,"For the MRC tasks , a question Q and a context C are given , our goal is to predict an answer A , which has different forms according to the specific task .",system description,Task Description,0,42,3,2,0,system description : Task Description,0.16153846153846155,0.75,0.6666666666666666,For the MRC tasks a question Q and a context C are given our goal is to predict an answer A which has different forms according to the specific task ,31, ,"In the SQuAD dataset , the answer A is constrained as a segment of text in the context C , nerual networks are designed to model the probability distribution p ( A |C , Q ) .",method
sentiment_analysis,17,"We want the expected rating under the predicted distributionp ? given model parameters ? to be close to the gold rating y ? [ 1 , K ]:? = r Tp ? ? y .",model,Semantic Relatedness of Sentence Pairs,0,141,24,11,0,model : Semantic Relatedness of Sentence Pairs,0.6266666666666667,0.8,0.6470588235294118,We want the expected rating under the predicted distributionp given model parameters to be close to the gold rating y 1 K r Tp y ,26,The multiplicative measure h can be interpreted as an elementwise comparison of the signs of the input representations .,"We want the expected rating under the predicted distributionp ? given model parameters ? to be close to the gold rating y ? [ 1 , K ]:? = r Tp ? ? y .",method
text_summarization,13,"Empirically , we find that while coarse - tofine attention models lag behind state - of the - art baselines , our method achieves the desired behavior of sparsely attending to subsets of the document for generation .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.02621722846441948,1.0,1.0,Empirically we find that while coarse tofine attention models lag behind state of the art baselines our method achieves the desired behavior of sparsely attending to subsets of the document for generation ,33,"While the computation for training standard attention models scales linearly with source sequence length , our method scales with the number of top - level chunks and can handle much longer sequences .", ,abstract
natural_language_inference,100,We propose two neural matching model architectures and compare the effectivenesses of them .,model,Model Overview,0,138,9,9,0,model : Model Overview,0.3770491803278688,0.16071428571428573,0.8181818181818182,We propose two neural matching model architectures and compare the effectivenesses of them ,14,"Finally , we employ a question attention network to learn question term importance and produce the final ranking score .","We firstly describe a basic version of the architecture , which is referred to as a NMM - 1 .",method
sentiment_analysis,11,The activations of this layer form our sentence representation t u .,approach,Textual Features Extraction,0,112,17,9,0,approach : Textual Features Extraction,0.32558139534883723,0.3469387755102041,1.0,The activations of this layer form our sentence representation t u ,12,"Finally , a fully connected layer is used with 100 neurons .", ,method
natural_language_inference,40,Two example passages from the LAMBADA dataset with predictions from the GA and GA + MAGE models .,model,Model,0,246,47,47,0,model : Model,0.8880866425992779,0.7580645161290323,0.7704918032786885,Two example passages from the LAMBADA dataset with predictions from the GA and GA MAGE models ,17,"Cloze - style QA : Lastly , we test our models on the CNN dataset from , which consists of pairs of news articles and a cloze - style question over the contents .","Tokens of the same color represent coreferent mentions of the same entity , as extracted by the preprocessing tool ..",method
text-classification,5,Our method again outperforms the state - of the - art significantly .,result,Results,0,185,14,14,0,result : Results,0.7341269841269841,0.875,0.875,Our method again outperforms the state of the art significantly ,11,"We show the test error rates on the larger AG , DBpedia , Yelp - bi , and Yelp - full datasets in .","On AG , we observe a similarly dramatic error reduction by 23.7 % compared to the state - of - the - art .",result
natural_language_inference,95,"Given the representation of the answer candidates from all passages {r A i } , each answer candidate then attends to other candidates to collect supportive information via attention mechanism :",model,Cross-Passage Answer Verification,0,111,44,6,0,model : Cross-Passage Answer Verification,0.4743589743589744,0.88,0.5,Given the representation of the answer candidates from all passages r A i each answer candidate then attends to other candidates to collect supportive information via attention mechanism ,29,"Therefore , we propose a method to enable the answer candidates to exchange information and verify each other through the cross - passage answer verification process .",Herer A i is the collected verification information from other passages based on the attention weights .,method
natural_language_inference,38,propose iterarively inferring the answer with a dynamic number of steps trained with reinforcement learning .,system description,Attention Based Models for Machine Reading,0,181,8,8,0,system description : Attention Based Models for Machine Reading,0.9576719576719576,0.8,0.8,propose iterarively inferring the answer with a dynamic number of steps trained with reinforcement learning ,16,propose self - aware representation and multi-hop query - sensitive pointer to predict the answer span .,employ gated self - matching attention to obtain the relation between the question and passage .,method
natural_language_inference,2,"In practice , recurrent neural networks fail to remember information when the context is long .",architecture,Multi-factor Attentive Encoding,0,89,44,4,0,architecture : Multi-factor Attentive Encoding,0.33208955223880604,0.4036697247706422,0.125,In practice recurrent neural networks fail to remember information when the context is long ,15,We propose a multi-factor attentive encoding approach using tensor - based transformation .,Our proposed multi-factor attentive encoding approach helps to aggregate meaningful information from along context with fine - grained inference due to the use of multiple factors while calculating attention .,method
semantic_parsing,0,"First , we collected about 70 complex data bases from different college data base courses , SQL tutorial websites , online csv files , and textbook examples .",system description,Data base Collection and Creation,0,88,45,6,0,system description : Data base Collection and Creation,0.31654676258992803,0.42857142857142855,0.3333333333333333,First we collected about 70 complex data bases from different college data base courses SQL tutorial websites online csv files and textbook examples ,24,Our 200 data bases covering 138 different domains are collected from three resources .,"Second , we collected about 40 data bases from the Data baseAnswers 1 where contains over 1,000 data models across different domains .",method
natural_language_inference,89,"Moreover , we introduce two auxiliary losses to help the reader better handle answer extraction as well as noanswer detection , and investigate three different architectures for the answer verifier .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.030769230769230767,0.8571428571428571,0.8571428571428571,Moreover we introduce two auxiliary losses to help the reader better handle answer extraction as well as noanswer detection and investigate three different architectures for the answer verifier ,29,"To address this problem , we propose a novel read - then - verify system , which not only utilizes a neural reader to extract candidate answers and produce noanswer probabilities , but also leverages an answer verifier to decide whether the predicted answer is entailed by the input snippets .","Our experiments on the SQuAD 2.0 dataset show that our system obtains a score of 74.2 F1 on test set , achieving state - of - the - art results at the time of submission ( Aug. 28th , 2018 ) .",abstract
natural_language_inference,83,"For example , in the story in , Wesley gets angry and bites his sister 's hand .",introduction,introduction,0,37,27,27,0,introduction : introduction,0.12091503267973855,0.6,0.6,For example in the story in Wesley gets angry and bites his sister s hand ,16,"The first aspect is motivated from approaches in semantic script induction , and evaluates if events described in an ending - alternative are likely to occur within the sequence of events described in the preceding context .","So , a next likely step might suggest that he would be scolded .",introduction
sarcasm_detection,1,This is also evident from the variances of the distributions where the sarcastic distribution comprises of 10.92 variance as opposed to 5.20 variance of the non-sarcastic distribution .,analysis,User Embedding Analysis,0,307,10,10,0,analysis : User Embedding Analysis,0.9191616766467066,0.3125,0.9090909090909092,This is also evident from the variances of the distributions where the sarcastic distribution comprises of 10 92 variance as opposed to 5 20 variance of the non sarcastic distribution ,31,"However , the sarcastic users have a greater spread than the non-sarcastic ones ( red belt around the green region ) .",We can infer from this observation that the user embeddings belonging to this non-overlapping red-region provide discriminative information regarding the sarcastic tendencies of their users .,result
sentiment_analysis,49,Please refer to in appendix for illustration of attention computation .,methodology,Proposed Methodology,0,74,13,13,0,methodology : Proposed Methodology,0.2924901185770751,0.17567567567567569,0.6190476190476191,Please refer to in appendix for illustration of attention computation ,11,An over all architecture of the proposed MMMU - BA framework is illustrated in .,"For comparison , we also experiment with two other variants of the proposed MMMU - BA framework i.e. a ) .",method
natural_language_inference,37,This requires the model to produce globally correct output even though each paragraph is processed independently .,introduction,introduction,0,33,24,24,0,introduction : introduction,0.12840466926070038,0.8275862068965517,0.8275862068965517,This requires the model to produce globally correct output even though each paragraph is processed independently ,17,"We then use a shared - normalization objective where paragraphs are processed independently , but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document .","We evaluate our work on Trivia QA web , a dataset of questions paired with web documents that contain the answer .",introduction
text_generation,3,They are asked to score the generated responses in terms of fluency and coherence .,result,Results,0,115,22,22,0,result : Results,0.8156028368794326,0.5945945945945946,0.5945945945945946,They are asked to score the generated responses in terms of fluency and coherence ,15,All annotators have linguistic background .,Fluency rep - resents whether each sentence is incorrect grammar .,result
semantic_parsing,0,The average question length and SQL length are about 13 and 21 respectively .,dataset,Dataset Statistics and Comparison,0,156,8,8,0,dataset : Dataset Statistics and Comparison,0.5611510791366906,0.7272727272727273,0.7272727272727273,The average question length and SQL length are about 13 and 21 respectively ,14,"On average , each data base in Spider has 27.6 columns and 8.8 foreign keys .","Our task uses different data bases for training and testing , evaluating the cross - domain performance .",experiment
natural_language_inference,3,SNLI is two orders of magnitude larger than all other existing RTE corpora .,method,Experiment-I: Recognizing Textual Entailment,0,164,11,5,0,method : Experiment-I: Recognizing Textual Entailment,0.7884615384615384,0.7857142857142857,0.625,SNLI is two orders of magnitude larger than all other existing RTE corpora ,14,"This corpus contains 570K sentence pairs , and all of the sentences and labels stem from human annotators .","Therefore , the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper .",method
natural_language_inference,72,"For example , object tracking and coreference both need to maintain the link between objects ; object tracking , which includes establishing set relations and membership , maybe overlaid with the schematic clause relation skill ( subordination ) ; and bridging inference can overlap with coreference resolution .",analysis,Analysis of comprehension skills,0,144,14,14,0,analysis : Analysis of comprehension skills,0.4615384615384616,0.9333333333333332,0.9333333333333332,For example object tracking and coreference both need to maintain the link between objects object tracking which includes establishing set relations and membership maybe overlaid with the schematic clause relation skill subordination and bridging inference can overlap with coreference resolution ,41,"In our experience , the annotation of skills proved quite challenging due to certain confusables .","Nevertheless , we adhered to this classification of skills to increase comparability to other datasets included in .",result
paraphrase_generation,1,"The VAE ( Kingma and Welling 2014 ; Rezende , Mohamed , and Wierstra 2014 ) is a deep generative latent variable model 1 https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs :",methodology,Variational Autoencoder (VAE),0,48,7,2,1,methodology : Variational Autoencoder (VAE),0.2171945701357466,0.25925925925925924,0.09090909090909093,The VAE Kingma and Welling 2014 Rezende Mohamed and Wierstra 2014 is a deep generative latent variable model 1 https data quora com First Quora Dataset Release Question Pairs ,30, ,"macro-view of our model : the paraphrase generation model is also conditioned on the original sentence that allows learning rich , nonlinear representations for highdimensional inputs .",method
text-classification,6,"Given the high cost of annotating supervised training data , very large training sets are usually not available for most research or industry NLP tasks .",introduction,introduction,0,16,4,4,0,introduction : introduction,0.10810810810810813,0.25,0.25,Given the high cost of annotating supervised training data very large training sets are usually not available for most research or industry NLP tasks ,25,This presents a challenge for data hungry deep learning methods .,Many models address the problem by implicitly performing limited transfer learning through the use of pre-trained word embeddings such as those produced by word2vec or Glo Ve .,introduction
natural_language_inference,71,"First , we argue for the advantage of an RNN which can forget some of its past inputs .",system description,The Gated Orthogonal Recurrent Unit,0,67,20,4,0,system description : The Gated Orthogonal Recurrent Unit,0.3116279069767442,0.5882352941176471,0.2352941176470588,First we argue for the advantage of an RNN which can forget some of its past inputs ,18,The Problem of Forgetting in Orthogonal RNNs,"This is desirable because we seek a state representation which can capture the most important elements of the past sequence , and can throwaway irrelevant details or noise .",method
natural_language_inference,100,"We choose Mean Average Precision ( MAP ) and Mean Reciprocal Rank ( MRR ) , which are commonly used in information retrieval and question answering , as the metric to evaluate our model .",evaluation,Evaluation and Metrics,0,251,3,3,0,evaluation : Evaluation and Metrics,0.6857923497267759,0.2307692307692308,0.2307692307692308,We choose Mean Average Precision MAP and Mean Reciprocal Rank MRR which are commonly used in information retrieval and question answering as the metric to evaluate our model ,29,"For evaluation , we rank answer sentences with the predicted score of each method and compare the rank list with the ground truth to compute metrics .",The definition of MRR is as follows :,result
natural_language_inference,13,The model denoted by MRU ( without any prefix ) corresponds to the recurrent MRU model .,method,Our Methods,0,167,17,4,0,method : Our Methods,0.7292576419213974,0.7083333333333334,0.3636363636363637,The model denoted by MRU without any prefix corresponds to the recurrent MRU model ,15,The first is denoted as Sim. MRU which corresponds to the Simple MRU model described earlier .,"Finally , the final variant is the MRU - LSTM which places a MRU encoder layer on top of a BiLSTM layer .",method
question-answering,5,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",training,Training Details,1,118,2,2,1,training : Training Details,0.5339366515837104,0.15384615384615385,0.15384615384615385,To train our model we used stochastic gradient descent with the ADAM optimizer Kingma and Ba 2014 with an initial learning rate of 0 001 ,26, ,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .",experiment
named-entity-recognition,0,Classification Accuracy Performance TED RRSSNie RRSSour ARSSour : Comparisons of four algorithms on Optdigit .,system description,Classifiers,0,24,2,2,0,system description : Classifiers,0.08856088560885607,0.011235955056179777,0.05128205128205128,Classification Accuracy Performance TED RRSSNie RRSSour ARSSour Comparisons of four algorithms on Optdigit ,14, ,Two conclusions can be drawn .,method
natural_language_inference,62,"To avoid overfitting , we apply dropout to the dense layers and the BiLSTMs with a dropout rate of 0.3 .",experiment,Experimental Settings,1,186,14,13,0,experiment : Experimental Settings,0.8303571428571429,0.3684210526315789,0.9285714285714286,To avoid overfitting we apply dropout to the dense layers and the BiLSTMs with a dropout rate of 0 3 ,21,"For model evaluation , we use Exact Match ( EM ) and F 1 score as evaluation metrics .","To boost the performance , we apply exponential moving average with a decay rate of 0.999 .",experiment
sentiment_analysis,34,Governments as well can use it to understand the reaction of people to their policies and actions .,introduction,introduction,0,16,8,8,0,introduction : introduction,0.10810810810810813,0.4,0.4,Governments as well can use it to understand the reaction of people to their policies and actions ,18,Companies can benefit from understanding the feedback of their costumers and their opinions .,"Work on SA started in early 2000s , particularly with the work of , where they studied the sentiment of movies ' reviews .",introduction
natural_language_inference,20,The sequence of the embedded words is then passed onto the sentence encoder which utilizes BiLSTM with max pooling .,model,Fig. 1. Overall NLI Architecture,0,46,11,4,0,model : Fig. 1. Overall NLI Architecture,0.1893004115226337,0.3235294117647059,0.14814814814814814,The sequence of the embedded words is then passed onto the sentence encoder which utilizes BiLSTM with max pooling ,20,For the sentence representations we first embed the individual words with pretrained word embeddings .,"Given a sequence T of words ( w 1 . . . , w T ) , the output of the bi-directional LSTM is a set of vectors ( h 1 , . . . , h T ) , where each ht ? ( h 1 , . . . , h T ) is the concatenation",method
natural_language_inference,47,"Since the two halves of each pair were collected separately , we report some statistics for both .",system description,Data collection,0,85,50,28,0,system description : Data collection,0.3953488372093023,0.6329113924050633,0.9333333333333332,Since the two halves of each pair were collected separately we report some statistics for both ,17,"We observed that while premise sentences varied considerably in length , hypothesis sentences tended to be as : Key statistics for the raw sentence pairs in SNLI .","short as possible while still providing enough information to yield a clear judgment , clustering at around seven words .",method
relation-classification,9,"Recent progress in NLP has been driven by the adoption of deep neural models , but training such models often requires large amounts of labeled data .",introduction,introduction,0,12,3,3,0,introduction : introduction,0.08163265306122447,0.2307692307692308,0.2307692307692308,Recent progress in NLP has been driven by the adoption of deep neural models but training such models often requires large amounts of labeled data ,26,The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large - scale knowledge extraction and machine reading of these documents .,"In general domains , large - scale training data is often possible to obtain through crowdsourcing , but in scientific domains , annotated data is difficult and expensive to collect due to the expertise required for quality annotation .",introduction
relation_extraction,11,RESIDE : Improving Distantly - Supervised Neural Relation Extraction using Side Information,title,title,1,2,1,1,0,title : title,0.008064516129032258,1.0,1.0,RESIDE Improving Distantly Supervised Neural Relation Extraction using Side Information,10, , ,title
text_generation,1,We also conduct human study to evaluate the quality of the generated sentences .,experiment,Results on COCO image captions,0,253,89,14,0,experiment : Results on COCO image captions,0.9233576642335768,0.8640776699029126,0.7,We also conduct human study to evaluate the quality of the generated sentences ,14,The results show that RankGAN is able to learn effective language generator in a large corpus .,We invite 28 participants who are native or proficient English speakers to grade the sentences .,experiment
sentiment_analysis,48,"In ATSA , achieving the sentiment of the aspectterm is semantically complicated and it is nontrivial for a model to capture sentimental similarity of the aspects , which causes the difficulties for semi-supervised learning .",introduction,introduction,0,28,17,17,0,introduction : introduction,0.11864406779661014,0.5151515151515151,0.5151515151515151,In ATSA achieving the sentiment of the aspectterm is semantically complicated and it is nontrivial for a model to capture sentimental similarity of the aspects which causes the difficulties for semi supervised learning ,34,"Therefore , the semi-supervised ATSA is a promising research topic .","In this paper , we proposed a classifier - agnostic framework which named Aspect - term Semi-supervised Variational Autoencoder ( Kingma and Welling , 2014 ) based on Transformer ( ASVAET ) .",introduction
sentiment_analysis,16,IT also outperforms the other baselines while NP has similar performance to BL - MN .,analysis,Result Analysis,0,266,9,9,0,analysis : Result Analysis,0.8364779874213837,0.17307692307692307,0.17307692307692307,IT also outperforms the other baselines while NP has similar performance to BL MN ,15,"Comparing the 1 - hop memory networks ( first nine rows ) , we see significant performance gains achieved by CNP , CI , JCI , and JPI on both datasets , where each of them has p < 0.01 over the strongest baseline ( BL - MN ) from paired t- test using F1 - Macro .","This indicates that TCS interaction is very useful , as BL - MN and NP do not model it .",result
relation-classification,6,"visualizes latent type representation t j? { 1 , 2 } in Equation 3.12",model,Self Attention,0,72,26,15,0,model : Self Attention,0.3977900552486188,0.3376623376623377,0.5769230769230769,visualizes latent type representation t j 1 2 in Equation 3 12,12,"We can see that the using is more highlighted than the assess , because the former represents the relation better .","Since the dimensionality of representation vectors are too large to visualize , we applied the t - SNE , one of the most popular dimensionality reduction methods .",method
part-of-speech_tagging,2,"Our base model is similar to , but in contrast to their model , we employ GRUs for the character - level and word - level networks instead of Long Short - Term Memory ( LSTM ) units , and define the objective function based on the max-margin principle .",implementation,MODEL IMPLEMENTATION,0,124,18,18,0,implementation : MODEL IMPLEMENTATION,0.6966292134831461,0.72,0.72,Our base model is similar to but in contrast to their model we employ GRUs for the character level and word level networks instead of Long Short Term Memory LSTM units and define the objective function based on the max margin principle ,43,"The cost function cost ( y , y ) is added based on the max- margin principle ) that high - cost tags y should be penalized more heavily .","We note that our transfer learning framework does not make assumptions about specific model implementation , and could be applied to other neural architectures , and a Twitter corpus .",experiment
natural_language_inference,15,"Next , the effectiveness of dense connections was tested in models ( 5 - 9 ) .",ablation,ablation,0,160,9,9,0,ablation : ablation,0.7079646017699115,0.2195121951219512,0.2195121951219512,Next the effectiveness of dense connections was tested in models 5 9 ,13,The trainable embedding E tr seems more effective than the fixed embedding E fix .,"In ( 5 - 6 ) , we removed dense connections only over co-attentive or recurrent features , respectively .",result
natural_language_inference,9,We use the hidden state size of 50 by deafult .,model,MODEL DETAILS,1,205,25,25,0,model : MODEL DETAILS,0.6212121212121212,0.3164556962025317,0.7352941176470589,We use the hidden state size of 50 by deafult ,11,We withhold 10 % of the training for development .,"Batch sizes of 32 for bAbI story - based QA 1k , bAb I dialog and DSTC2 dialog , and 128 for bAbI QA 10 k are used .",method
natural_language_inference,70,The KeLP system : an overview,system description,The KeLP system: an overview,0,30,1,1,0,system description : The KeLP system: an overview,0.17142857142857146,0.010752688172043013,0.14285714285714285,The KeLP system an overview,5, , ,method
part-of-speech_tagging,3,"Our end - to - end model outperforms previous stateof - the - art systems , obtaining 97. 55 % accuracy for POS tagging and 91.21 % F1 for NER .",introduction,introduction,0,28,20,20,0,introduction : introduction,0.13793103448275862,0.8695652173913043,0.8695652173913043,Our end to end model outperforms previous stateof the art systems obtaining 97 55 accuracy for POS tagging and 91 21 F1 for NER ,25,"We evaluate our model on two linguistic sequence labeling tasks - POS tagging on Penn Treebank WSJ , and NER on English data from the CoNLL 2003 shared task .",The contributions of this work are ( i ) proposing a novel neural network architecture for linguistic sequence labeling .,introduction
text_summarization,12,"For the test set , we use the same randomly heldout test set of 2000 sentence - summary pairs as .",training,training,0,136,5,5,0,training : training,0.5964912280701754,0.25,0.7142857142857143,For the test set we use the same randomly heldout test set of 2000 sentence summary pairs as ,19,We use the script 1 released by to pre-process and extract the training and development datasets .,"We also find that except for the empty titles , this test set has some invalid lines like the input sentence containing only one word .",experiment
sentiment_analysis,12,"When we replace the CNN of TNet with an attention mechanism , TNet - ATT is slightly inferior to TNet .",result,result,1,176,7,7,0,result : result,0.7857142857142857,0.4666666666666667,0.4666666666666667,When we replace the CNN of TNet with an attention mechanism TNet ATT is slightly inferior to TNet ,19,These results show that our reimplemented baselines are competitive .,"Moreover , when we perform additional K+1 - iteration of training on these models , their performance has not changed significantly , suggesting simply increasing training time is unable to enhance the performance of the neural ASC models .",result
question-answering,8,We use word embeddings from GloVe to initialize the model .,experiment,EXPERIMENT SETTINGS,1,176,12,4,0,experiment : EXPERIMENT SETTINGS,0.7068273092369478,0.5714285714285714,0.3076923076923077,We use word embeddings from GloVe to initialize the model ,11,The resulting vocabulary contains 117K unique words .,Words not found in Glo Ve are initialized as zero vectors .,experiment
natural_language_inference,90,"Finally , the results of these subproblems are merged to produce the final classification .",introduction,introduction,1,28,21,21,0,introduction : introduction,0.18666666666666668,0.875,0.875,Finally the results of these subproblems are merged to produce the final classification ,14,We then use the ( soft ) alignment to decompose the task into subproblems that are solved separately .,"In addition , we optionally apply intra-sentence attention to endow the model with a richer encoding of substructures prior to the alignment step .",introduction
text-classification,8,"Surprisingly , SWEMs exhibit comparable or even superior performance in the majority of cases considered .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.026022304832713755,0.625,0.625,Surprisingly SWEMs exhibit comparable or even superior performance in the majority of cases considered ,15,"In this paper , we conduct a point - by - point comparative study between Simple Word - Embeddingbased Models ( SWEMs ) , consisting of parameter - free pooling operations , relative to word - embedding - based RNN / CNN models .","Based upon this understanding , we propose two additional pooling strategies over learned word embeddings : ( i ) a max - pooling operation for improved interpretability ; and ( ii ) a hierarchical pooling operation , which preserves spatial ( n - gram ) information within text sequences .",abstract
text-to-speech_synthesis,1,"To generate a target melspectrogram sequence in parallel , we design a novel feed - forward structure , instead of using the encoder - attention - decoder based architecture as adopted by most sequence to sequence based autoregressive and non-autoregressive generation .",system description,FastSpeech,0,64,3,3,0,system description : FastSpeech,0.2922374429223744,0.06666666666666668,0.6,To generate a target melspectrogram sequence in parallel we design a novel feed forward structure instead of using the encoder attention decoder based architecture as adopted by most sequence to sequence based autoregressive and non autoregressive generation ,38,"In this section , we introduce the architecture design of FastSpeech .",The over all model architecture of FastSpeech is shown in .,method
question-answering,3,Wiki QA dataset .,model,Comparing with State-of-the-art Models,1,215,45,16,0,model : Comparing with State-of-the-art Models,0.8464566929133859,0.6428571428571429,0.3902439024390244,Wiki QA dataset ,4,"We can see that our model ( the last row of ) got the best MAP among all previous work , and a comparable MRR than dos .",presents the results of our model and several state - of - the - art models .,method
natural_language_inference,31,"We question the necessity of many slow components in text matching approaches presented in previous literature , including complicated multi-way alignment mechanisms , heavy distillations of alignment results , external syntactic features , or dense connections to connect stacked blocks when the model is going deep .",introduction,introduction,0,22,15,15,0,introduction : introduction,0.07971014492753623,0.5172413793103449,0.5172413793103449,We question the necessity of many slow components in text matching approaches presented in previous literature including complicated multi way alignment mechanisms heavy distillations of alignment results external syntactic features or dense connections to connect stacked blocks when the model is going deep ,44,"This paper presents RE2 , a fast and strong neural architecture with multiple alignment processes for general purpose text matching .",These design choices slowdown the model by a large amount and can be replaced by much more lightweight and equally effective ones .,introduction
text-classification,3,"The statistical significance was calculated according to an unpaired t- test at the 5 % significance level . age variability 14 of 2.4 % for the CNN + LSTM model , including a statistical significance gap in seven of the nine datasets ) , which proves the influence of preprocessing on the final results .",experiment,Experiment 1: Preprocessing effect,0,87,26,13,0,experiment : Experiment 1: Preprocessing effect,0.7073170731707317,0.5098039215686274,0.5909090909090909,The statistical significance was calculated according to an unpaired t test at the 5 significance level age variability 14 of 2 4 for the CNN LSTM model including a statistical significance gap in seven of the nine datasets which proves the influence of preprocessing on the final results ,49,Computed by averaging accuracy of two different runs .,It is perhaps not surprising that the lowest variance of results is seen in the datasets with the larger training data ( i.e. RTC and Stanford ) .,experiment
semantic_role_labeling,0,"Recent BIO - tagging - based neural semantic role labeling models are very high performing , but assume gold predicates as part of the input and can not incorporate span - level features .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.0380952380952381,0.3333333333333333,0.3333333333333333,Recent BIO tagging based neural semantic role labeling models are very high performing but assume gold predicates as part of the input and can not incorporate span level features ,30, ,"We propose an endto - end approach for jointly predicting all predicates , arguments spans , and the relations between them .",abstract
relation-classification,2,shows an example of the BIO encoding tags assigned to the tokens of the sentence .,system description,Named entity recognition,0,53,6,6,0,system description : Named entity recognition,0.17966101694915254,0.10344827586206896,0.17647058823529413,shows an example of the BIO encoding tags assigned to the tokens of the sentence ,16,"To do so , we assign the B - type ( beginning ) to the first token of the entity , the I - type ( inside ) to every other token within the entity and the O tag ( outside ) if a token is not part of an entity .","In the CRF layer , one can observe that we assign the B - ORG and I - ORG tags to indicate the beginning and the inside tokens of the entity "" Disease Control Center "" , respectively .",method
natural_language_inference,15,"In ( 5 - 6 ) , we removed dense connections only over co-attentive or recurrent features , respectively .",ablation,ablation,0,161,10,10,0,ablation : ablation,0.7123893805309734,0.24390243902439024,0.24390243902439024,In 5 6 we removed dense connections only over co attentive or recurrent features respectively ,16,"Next , the effectiveness of dense connections was tested in models ( 5 - 9 ) .",The result shows that the dense connections over attentive features are more effective .,result
semantic_role_labeling,0,"Where ? ( p , a , l ) is a scoring function for a possible ( predicate , argument , label ) combination . ? is decomposed into two unary scores on the predicate and the argument ( defined in Section 3 ) , as well as a label - specific score for the relation :",model,Model,0,33,11,11,0,model : Model,0.3142857142857143,0.5,0.5,Where p a l is a scoring function for a possible predicate argument label combination is decomposed into two unary scores on the predicate and the argument defined in Section 3 as well as a label specific score for the relation ,42,"The random variables y p , a are conditionally independent of each other given the input X :","Where ? ( p , a , l ) is a scoring function for a possible ( predicate , argument , label ) combination . ? is decomposed into two unary scores on the predicate and the argument ( defined in Section 3 ) , as well as a label - specific score for the relation :",method
relation_extraction,7,"In , we report P@100 , P@200 , P@300 and the mean of them for each model in the held - out evaluation .",baseline,Comparison with Baselines,0,246,16,16,0,baseline : Comparison with Baselines,0.9498069498069498,0.8,0.8,In we report P 100 P 200 P 300 and the mean of them for each model in the held out evaluation ,23,"Following previous works , we adopt P@N as a quantitative indicator to compare our model with baselines based on various instances under each relational tuple .","We can find : ( 1 ) Compared with baselines , BGRU + STP + EWA + TL achieves the best performance in all test settings , which increases the performance of PCNN + ATT in three settings by 6.3 % , 7.6 % , and 7.7 % respectively .",result
question_similarity,0,The experiments are divided into two sets .,experiment,Experimental Setup,0,86,6,3,0,experiment : Experimental Setup,0.6231884057971014,0.15789473684210525,0.3333333333333333,The experiments are divided into two sets ,8,All experiments discussed in this work have been done on the Google Colab 7 environment using Tesla T4 GPU accelerator with the following hyperparameters :,"The first set aims to explore the effect of the Recurrent Neural Network ( RNN ) cell type , while the second set aims to explore the effect of the data augmentation techniques mentioned in Section 2.2 .",experiment
sentiment_analysis,48,"ACSA is to infer the sentiment polarity with regard to the predefined categories , e.g. , the aspect food , price , ambience .",introduction,introduction,1,14,3,3,0,introduction : introduction,0.059322033898305086,0.09090909090909093,0.09090909090909093,ACSA is to infer the sentiment polarity with regard to the predefined categories e g the aspect food price ambience ,21,"Aspect based sentiment analysis ( ABSA ) has two sub - tasks , namely aspect - term sentiment analysis ( ATSA ) and aspect - category sentiment analysis ( ACSA ) .","On the other hand , ATSA aims at classifying the sentiment polarity of a given aspect word or phrase in the text .",introduction
sentence_classification,2,"Moreover , recent research on neural machine translation ( NMT ) improved the efficiency and even enabled zero - shot translation of models for languages with no parallel data .",introduction,introduction,0,33,23,23,0,introduction : introduction,0.13095238095238096,0.5476190476190477,0.5476190476190477,Moreover recent research on neural machine translation NMT improved the efficiency and even enabled zero shot translation of models for languages with no parallel data ,26,"Thankfully , translation services ( e.g. Google Translate )","This provides an opportunity to leverage on as many languages as possible to any domain , providing a much wider context compared to the limited contexts provided by past studies .",introduction
natural_language_inference,72,"Considering the large effect of hyper - parameter selection on the quality of word embeddings , we optimize the embedding hyper - parameters also using random search .",baseline,Embedding data and pre-training,0,181,35,3,0,baseline : Embedding data and pre-training,0.5801282051282052,1.0,1.0,Considering the large effect of hyper parameter selection on the quality of word embeddings we optimize the embedding hyper parameters also using random search ,25,"We induce the word embeddings on a combination of the CliCR training corpus and PubMed abstracts with open - access PMC articles available until 2015 ( segmented and tokenized ) , amounting to over 9 billion tokens .", ,result
natural_language_inference,69,"The first , WIKIHOP , uses sets of WIKIPEDIA articles where answers to queries about specific properties of an entity can not be located in the entity 's article .",introduction,introduction,1,29,17,17,0,introduction : introduction,0.08405797101449275,0.6071428571428571,0.6071428571428571,The first WIKIHOP uses sets of WIKIPEDIA articles where answers to queries about specific properties of an entity can not be located in the entity s article ,28,We introduce a methodology to induce datasets for this task and derive two datasets .,"In the second dataset , MEDHOP , the goal is to establish drug - drug interactions based on scientific findings about drugs and proteins and their interactions , found across multiple MEDLINE abstracts .",introduction
natural_language_inference,8,The underlying distributional models provide the semantic information necessary for this matching function .,introduction,introduction,0,39,30,30,0,introduction : introduction,0.1848341232227488,0.8571428571428571,0.8571428571428571,The underlying distributional models provide the semantic information necessary for this matching function ,14,"Assuming a set of pre-trained semantic word embeddings , we train a supervised model to learn a semantic matching between question and answer pairs .","We also present an enhanced version of this model , which combines the signal of the distributed matching algorithm with two simple word matching features .",introduction
natural_language_inference,14,The mean and standard deviation of the three trials are reported in .,evaluation,evaluation,0,102,12,12,0,evaluation : evaluation,0.5964912280701754,0.4137931034482759,0.4137931034482759,The mean and standard deviation of the three trials are reported in ,13,"Ablation experiments are conducted in the same way as in the CODAHonly setting above , with the same dataset splits for training .",Fine - tuned on SWAG and evaluated on CODAH .,result
sentiment_analysis,48,"As the classifier can be implemented by various models , the description of the classifier will be omitted .",method,Method Description,0,77,15,15,0,method : Method Description,0.326271186440678,0.2307692307692308,0.8823529411764706,As the classifier can be implemented by various models the description of the classifier will be omitted ,18,"For the unlabeled data , they is regarded as the latent discrete variable and it is induced by maximizing the generative probability .",We present a autoencoder structure based on Transformer .,method
natural_language_inference,91,"Token sequences x are then cropped or padded with empty tokens at the left to match the number of shifts added or removed from a , and can then be padded with empty tokens at the right to meet the desired length N.",implementation,Implementation issues,0,131,37,37,0,implementation : Implementation issues,0.5622317596566524,0.6607142857142857,1.0,Token sequences x are then cropped or padded with empty tokens at the left to match the number of shifts added or removed from a and can then be padded with empty tokens at the right to meet the desired length N ,43,Transition sequences a are cropped at the left or padded at the left with shifts ., ,experiment
natural_language_inference,93,Our model consists of four parts :,introduction,introduction,1,23,13,13,0,introduction : introduction,0.1111111111111111,0.4642857142857143,0.4642857142857143,Our model consists of four parts ,7,"Inspired by , we introduce a gated self - matching network , illustrated in , an end - to - end neural network model for reading comprehension and question answering .",") the recurrent network encoder to build representation for questions and passages separately , 2 ) the gated matching layer to match the question and passage , 3 ) the self - matching layer to aggregate information from the whole passage , and 4 ) the pointernetwork based answer boundary prediction layer .",introduction
text_summarization,3,"In this way , the training pairs are distantly - labelled for training the model .",system description,Distant Supervision (DS) for Model Adaption,0,134,79,6,0,system description : Distant Supervision (DS) for Model Adaption,0.5537190082644629,0.6030534351145038,0.10344827586206896,In this way the training pairs are distantly labelled for training the model ,14,"In answer to this need and also to provide end - to - end functionality in the model , we developed a simple approach for labeling summary - document pairs by calculating the Kullback - Leibler ( KL ) divergence between each training reference summary and a set of testing documents .","Specifically , the representations of the reference summaries and the testing set are computed by summing all the involved word embeddings .",method
natural_language_inference,56,"Denote the digit for cell j d j ( 0 - 9 , 0 if not given ) , and the row and column position row j ( 1 - 9 ) and column j ( 1 - 9 ) respectively ..",experiment,Sudoku experimental details,0,280,3,3,0,experiment : Sudoku experimental details,0.8333333333333334,0.05084745762711865,0.2307692307692308,Denote the digit for cell j d j 0 9 0 if not given and the row and column position row j 1 9 and column j 1 9 respectively ,31,Unless otherwise specified we use 96 hidden units for all hidden layers and all MLPs are 3 ReLU layers followed by a linear layer .,The node features are then,experiment
text_generation,0,"G ? D ? ( s , a ) is the action - value function of a sequence , i.e. the expected accumulative reward starting from state s , taking action a , and then following policy G ? .",system description,SeqGAN via Policy Gradient,0,82,20,5,0,system description : SeqGAN via Policy Gradient,0.25308641975308643,0.18691588785046728,0.078125,G D s a is the action value function of a sequence i e the expected accumulative reward starting from state s taking action a and then following policy G ,31,"Note that the reward is from the discriminator D ? , which we will discuss later .","The rational of the objective function for a sequence is that starting from a given initial state , the goal of the generator is to generate a sequence which would make the discriminator consider it is real .",method
natural_language_inference,66,"While their basic architecture is still based on sentence embeddings for the premise and the hypothesis , a key difference is that the embedding of the premise takes into consideration the alignment between the premise and the hypothesis .",abstract,abstract,0,13,11,11,0,abstract : abstract,0.04659498207885305,0.9166666666666666,0.9166666666666666,While their basic architecture is still based on sentence embeddings for the premise and the hypothesis a key difference is that the embedding of the premise takes into consideration the alignment between the premise and the hypothesis ,38,more recent work by Rocktschel et al. ( 2016 ) improved the performance by applying a neural attention model .,This so - called attention - weighted representation of the premise was shown to help push the accuracy to,abstract
temporal_information_extraction,0,Event - event Rules E - E pairs are finally labelled following two sets of rules .,system,Event-timex Rules,0,58,31,12,0,system : Event-timex Rules,0.3068783068783069,0.476923076923077,0.75,Event event Rules E E pairs are finally labelled following two sets of rules ,15,We define additional rules as follows : ( i ) If T matches TBEGIN then E - T label is if T matches TEND then E - T label is ENDED BY .,"The first set is based on the dependency path possibly existing between the first ( e 1 ) and the second event ( e 2 ) , and the verb information encoded in e 1 .",method
natural_language_inference,57,"Our goal is to predict whether an unseen pair ( u , v ) is ordered .",system description,LEARNING ORDER-EMBEDDINGS,0,38,4,4,0,system description : LEARNING ORDER-EMBEDDINGS,0.22093023255813954,0.07017543859649122,0.4,Our goal is to predict whether an unseen pair u v is ordered ,14,"In partial order completion , we are given a set of positive examples P = { ( u , v ) } of ordered pairs drawn from a partially ordered set ( X , X ) , and a set of negative examples N which we know to be unordered .","Note that hypernym prediction , caption - image retrieval , and textual entailment are all special cases of this task , since they all involve classifying pairs of concepts in the ( partially ordered ) visual - semantic hierarchy .",method
semantic_parsing,2,"The original split has 16 , 000 training , 1 , 000 development , and 1 , 805 test instances .",training,Natural Language to Source Code,0,156,47,4,0,training : Natural Language to Source Code,0.5360824742268041,0.3983050847457627,0.2222222222222222,The original split has 16 000 training 1 000 development and 1 805 test instances ,16,"The dataset contains lines of code paired with natural language expressions ( see the third block in ) and exhibits a variety of use cases , such as iteration , exception handling , and string manipulation .",We used the built - in lexical scanner of Python 1 to tokenize the code and obtain token types .,experiment
natural_language_inference,74,Residual stacked encoders 86.0 74.6 73.6 Gated - Att BiLSTM - 73.2 73.6 100D LSTMs with attention 83.5 -- 300 D re-read LSTM 87.5 --DIIN 88.0 78.8 77.8 Biattentive Classification Network 88.1 --300D CAFE 88.5 78.7 77.9 KIM 88.6 --600D ESIM + 300D,method,SNLI MultiNLI Matched Mismatched,0,150,7,6,0,method : SNLI MultiNLI Matched Mismatched,0.6696428571428571,0.875,0.8571428571428571,Residual stacked encoders 86 0 74 6 73 6 Gated Att BiLSTM 73 2 73 6 100D LSTMs with attention 83 5 300 D re read LSTM 87 5 DIIN 88 0 78 8 77 8 Biattentive Classification Network 88 1 300D CAFE 88 5 78 7 77 9 KIM 88 6 600D ESIM 300D,55,Gumbel TreeLSTM encoders 86.0 -- 600D,Syntactic TreeLSTM 88.8 -- DIIN ( Ensemble ) 88.9 80.0 78.7 KIM ( Ensemble ) 89.1 --300D CAFE ( Ensemble ) 89,method
natural_language_inference,0,"Comparing with prior work , on the WDW dataset the basic version of the GA Reader outperforms all previously published models when trained on the Strict setting .",performance,Performance Comparison,1,142,13,13,0,performance : Performance Comparison,0.7208121827411168,0.5909090909090909,0.5909090909090909,Comparing with prior work on the WDW dataset the basic version of the GA Reader outperforms all previously published models when trained on the Strict setting ,27,This is not surprising given that the latter datasets are larger and less prone to overfitting .,By adding the qecomm feature the performance increases by 3.2 % and 3.5 % on the Strict and Relaxed settings respectively to set a new state of the art on this dataset .,result
relation_extraction,11,The higher performance of BGWA and PCNN + ATT over PCNN shows that attention helps in distant supervised RE .,performance,Performance Comparison,1,222,8,8,0,performance : Performance Comparison,0.8951612903225806,0.8,0.8,The higher performance of BGWA and PCNN ATT over PCNN shows that attention helps in distant supervised RE ,19,RESIDE outperforms PCNN + ATT and BGWA which indicates that incorporating side information helps in improving the performance of the model .,"Following , we also evaluate our method with different number of sentences .",result
natural_language_inference,2,"Moreover , none of the models explicitly focus on question / answer type information for predicting the answer .",introduction,introduction,0,31,20,20,0,introduction : introduction,0.11567164179104478,0.7142857142857143,0.7142857142857143,Moreover none of the models explicitly focus on question answer type information for predicting the answer ,17,"However , most of the models do not focus on synthesizing evidence from multiple sentences and fail to perform well on challenging open - world QA tasks such as New s QA and Triv - ia QA .","In practice , fine - grained understanding of question / answer type plays an important role in QA .",introduction
part-of-speech_tagging,2,The statistics of the datasets are described in .,implementation,MODEL IMPLEMENTATION,0,126,20,20,0,implementation : MODEL IMPLEMENTATION,0.7078651685393258,0.8,0.8,The statistics of the datasets are described in ,9,"We note that our transfer learning framework does not make assumptions about specific model implementation , and could be applied to other neural architectures , and a Twitter corpus .",We construct the POS tagging dataset with the instructions described in .,experiment
natural_language_inference,39,Bi - LSTMs consist of two LSTMs that run in parallel in opposite directions : one ( forward LST M f ) on the input sequence and the other ( backward LST Mb ) on the reverse of the sequence .,model,Context Modeling,0,60,13,13,0,model : Context Modeling,0.2912621359223301,0.8125,0.8125,Bi LSTMs consist of two LSTMs that run in parallel in opposite directions one forward LST M f on the input sequence and the other backward LST Mb on the reverse of the sequence ,35,We use bidirectional LSTMs ( Bi - LSTMs ) for context modeling in this work .,"At time step t , the Bi - LSTMs hidden state h bit is a concatenation of the hidden state hf or t of LST M f and the hidden state h back t of LST Mb , representing the neighbor contexts of input x tin the sequence .",method
natural_language_inference,83,"For our experiments , we have used a publicly available collection of commonsense short stories released by .",dataset,Dataset,0,159,2,2,0,dataset : Dataset,0.5196078431372549,0.15384615384615385,0.15384615384615385,For our experiments we have used a publicly available collection of commonsense short stories released by ,17, ,It consists of about 100K unannotated five - sentences long stories .,experiment
natural_language_inference,12,Natural language inference ( NLI ) or recognizing textual entailment ( RTE ) is a fundamental semantic task in the field of natural language processing .,introduction and background,introduction and background,1,10,2,2,0,introduction and background : introduction and background,0.11363636363636365,0.14285714285714285,0.14285714285714285,Natural language inference NLI or recognizing textual entailment RTE is a fundamental semantic task in the field of natural language processing ,22, ,The problem is to determine whether a given hypothesis sentence can be logically inferred from a given premise sentence .,introduction
natural_language_inference,94,"As a general trend , we see that in the earlier hops , words which are near tokens that occur in both the context and commonsense paths have high activation , but the activation becomes more focused on the passage 's key words w.r.t. the question , as the number of hops increase .",training,training,0,280,15,15,0,training : training,0.7310704960835509,0.1271186440677966,0.1271186440677966,As a general trend we see that in the earlier hops words which are near tokens that occur in both the context and commonsense paths have high activation but the activation becomes more focused on the passage s key words w r t the question as the number of hops increase ,52,"In the following examples ( next page ) , we use shades of blue to represent the average of ( 1 ? z i ) at each word in the context ( normalized within each hop ) , with deeper blue indicating the use of more commonsense information .","maurya has lost her husband , and five of her sons to the sea .",experiment
text_summarization,4,We also show the attention weights of the firm and soft models .,result,Results,0,224,30,30,0,result : Results,0.8682170542635659,0.5172413793103449,0.5172413793103449,We also show the attention weights of the firm and soft models ,13,"Finally , we provide one sample for each dataset in for case study , comparing our final model that uses firm attention ( BASE cn n + s d ) , a variant that uses soft attention ( BASE cn n + soft ) , and the baseline model ( BASE ) .","In the Gigaword example , we find three observations .",result
sentiment_analysis,41,"Sentiment analysis , also known as opinion mining , is a key NLP task that receives much attention these years .",introduction,introduction,0,13,2,2,0,introduction : introduction,0.05829596412556054,0.08,0.08,Sentiment analysis also known as opinion mining is a key NLP task that receives much attention these years ,19, ,Aspect - level sentiment analysis is a fine - grained task that can provide complete and in - depth results .,introduction
named-entity-recognition,6,"For instance , "" iota "" and "" x.org "" are embedded near their respective types , despite the fact that they appear less than 30 times in the version of Wikipedia used to compile WiFiNE .",method,Embedding Words and Entity Types,0,82,31,30,0,method : Embedding Words and Entity Types,0.3867924528301887,0.62,1.0,For instance iota and x org are embedded near their respective types despite the fact that they appear less than 30 times in the version of Wikipedia used to compile WiFiNE ,32,"Last , we also observe the tendency of rare words to cluster around their entity type .", ,method
natural_language_inference,50,This section delves into qualitative analysis of our model and aims to investigate the following research questions :,analysis,DISCUSSION AND ANALYSIS,0,269,46,2,0,analysis : DISCUSSION AND ANALYSIS,0.8485804416403786,0.5822784810126582,0.060606060606060615,This section delves into qualitative analysis of our model and aims to investigate the following research questions ,18, ,1 ) RQ1 : Is there any hierarchical structure learned in the QA embeddings ? How are QA embeddings organized in the final embedding space of Hyper QA ? ( 2 ) RQ2 : What are the impacts of embedding compositional embeddings in hyperbolic space ? Is there an impact on the constituent word embeddings ? ( 3 ) RQ3 : Are we able to derive any insight about how word interaction and matching happens in Hyper QA ?.,result
sentiment_analysis,22,"Therefore , we remove the fusion operation and Source2aspect attention while removing the Source2context attention .",performance,performance,0,194,41,41,0,performance : performance,0.8471615720524017,0.7735849056603774,0.7735849056603774,Therefore we remove the fusion operation and Source2aspect attention while removing the Source2context attention ,15,The output of Source2aspect attention is only used for information fusion .,"And the achieved model is "" Bi - GRU - PE "" reported in the , achieving the accuracies of 80.89 % and 76.02 % on the two datasets respectively , which are 1.34 % and 1.25 % lower than the proposed model .",result
sentiment_analysis,49,The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity ( computed using inter-modality correlations ) with the target utterance .,introduction,introduction,1,37,27,27,0,introduction : introduction,0.14624505928853754,0.8181818181818182,0.8181818181818182,The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity computed using inter modality correlations with the target utterance ,28,"In this case , our model computes the relatedness among the modalities ( for e.g. , Vt and T k ) of Ut and U kin order to produce a richer multi-modal representation for final classification .","Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing correlations among the modalities of the target utterance and the context utterances .",introduction
sentiment_analysis,15,"We initialize all word vectors by randomly sampling each value from a uniform distribution : U (? r , r ) , where r = 0.0001 .",model,Recursive Neural Models,0,101,12,12,0,model : Recursive Neural Models,0.3740740740740741,0.11214953271028036,0.5454545454545454,We initialize all word vectors by randomly sampling each value from a uniform distribution U r r where r 0 0001 ,22,Each word is represented as a d-dimensional vector .,"All the word vectors are stacked in the word embedding matrix L ? R d|V | , where | V | is the size of the vocabulary .",method
sentiment_analysis,23,"First , a sentence is converted to a parsing tree , either a constituency or dependency tree .",system description,Tree-based Convolution,0,78,42,4,0,system description : Tree-based Convolution,0.2671232876712329,0.4,0.14814814814814814,First a sentence is converted to a parsing tree either a constituency or dependency tree ,16,depicts the convolution process on a tree .,The corresponding model variants are denoted as c- TBCNN and d-TBCNN .,method
natural_language_inference,10,the woman is wearing a vest .,analysis,Qualitative Analysis,0,213,48,27,0,analysis : Qualitative Analysis,0.8987341772151899,0.9056603773584906,0.84375,the woman is wearing a vest ,7,woman is playing with her dog .,Tree examples show that two models ( 300D SNLI and SST - 2 ) generate different tree structures given an identical sentence .,result
sentence_compression,2,We go beyond this by suggesting that eye - tracking recordings can be used to induce better models for sentence compression for text simplification .,introduction,introduction,1,12,6,6,0,introduction : introduction,0.1188118811881188,0.4615384615384616,0.4615384615384616,We go beyond this by suggesting that eye tracking recordings can be used to induce better models for sentence compression for text simplification ,24,These two observations recently lead to suggest using eye - tracking measures as metrics in text simplification .,"Specifically , we show how to use existing eye - tracking recordings to improve the induction of Long Short - Term Memory models ( LSTMs ) for sentence compression .",introduction
part-of-speech_tagging,6,"We use a fixed random seed , and we do not utilize pre-trained embeddings in any experiment .",implementation,implementation,0,77,4,4,0,implementation : implementation,0.5167785234899329,0.26666666666666666,0.26666666666666666,We use a fixed random seed and we do not utilize pre trained embeddings in any experiment ,18,"We optimize the objective function using Adam ( Kingma and Ba , 2014 ) with default DYNET parameter settings and no mini-batches .","Following Kiperwasser and Goldberg ( 2016 b ) and , we apply a word dropout rate of 0.25 and Gaussian noise with ? = 0.2 .",experiment
sentiment_analysis,13,Then the hidden representation is passed to two separate dense layers followed by softmax functions :,system description,Review Reading Comprehension (RRC),0,97,21,7,0,system description : Review Reading Comprehension (RRC),0.3489208633093525,0.21,0.4666666666666667,Then the hidden representation is passed to two separate dense layers followed by softmax functions ,16,"We first obtain the hidden representation ash = BERT ( x ) ? R r h * | x | , where | x | is the length of the input sequence and r h is the size of the hidden dimension .",The softmax is applied along the dimension of the sequence .,method
sentiment_analysis,35,"The C2F module makes up these missing information for the source task , which effectively reduces the aspect granularity gap between tasks and facilitates the subsequent feature alignment .",introduction,introduction,0,43,29,29,0,introduction : introduction,0.17338709677419356,0.8285714285714286,0.8285714285714286,The C2F module makes up these missing information for the source task which effectively reduces the aspect granularity gap between tasks and facilitates the subsequent feature alignment ,28,"Actually , more specific aspect terms and their position information are most directly pertinent to the expression of sentiment .","Second , considering that a sentence may contain multiple aspects with different sentiments , thus capturing incorrect sentiment features towards the aspect can mislead feature alignment .",introduction
natural_language_inference,97,"When a question contains the words which and not , we negate the hypothesis ranking scores so that the minimum becomes the maximum .",training,Training and Model Details,0,227,13,13,0,training : Training and Model Details,0.7773972602739726,0.5,0.5,When a question contains the words which and not we negate the hypothesis ranking scores so that the minimum becomes the maximum ,23,"Following earlier methods , we use a heuristic to improve performance on negation questions .",The most important technique for training the model was the training wheels approach .,experiment
relation_extraction,9,"primary attention mechanism , based on diagonal matrices is used to capture the relevance of words with respect to the target entities .",model,The Proposed Model,0,54,7,7,0,model : The Proposed Model,0.2583732057416268,0.07954545454545454,0.5833333333333334,primary attention mechanism based on diagonal matrices is used to capture the relevance of words with respect to the target entities ,22,"The input sentence is first encoded using word vector representations , exploiting the context and a positional encoding to better capture the word order .","To the resulting output matrix , one then applies a convolution operation in order to capture contextual information such as relevant n-grams , followed by max - pooling .",method
sentiment_analysis,50,The key observations are : ( 1 ) Transfer is helpful in all settings .,model,Model Comparison,0,112,14,14,0,model : Model Comparison,0.6913580246913579,0.9333333333333332,0.9333333333333332,The key observations are 1 Transfer is helpful in all settings ,12,"LSTM only "" denotes the setting where only the LSTM layer is transferred , and "" Without LSTM "" denotes the setting where only the embedding and output layers are transferred ( excluding the LSTM layer ) .",Improvements over LSTM + ATT are observed even when only one layer is transferred .,method
sentiment_analysis,16,"To avoid this , we can use the following five alternative techniques .",approach,approach,0,133,11,11,0,approach : approach,0.41823899371069184,0.13414634146341464,0.13414634146341464,To avoid this we can use the following five alternative techniques ,12,"For example , we may have difficulty in tracking how each individual context word ( c i ) affects the final sentiment score s , as all context and target representations are coupled .",Contextual Non-linear Projection ( CNP ) :,method
text_summarization,1,"In particular , we compare with recent diverse search algorithms including Truncated Sampling , Diverse Beam Search , and Mixture Decoder .",baseline,Baselines,0,156,3,3,0,baseline : Baselines,0.65,0.07894736842105263,0.17647058823529413,In particular we compare with recent diverse search algorithms including Truncated Sampling Diverse Beam Search and Mixture Decoder ,19,"For each task , we compare our method with other techniques which promote diversity at the decoding step .",We implement these methods with NQG ++ and PG .,result
natural_language_inference,29,and list performances of all comparisons in terms of two automatic evaluation metrics .,implementation,implementation,0,292,10,10,0,implementation : implementation,0.8,0.2857142857142857,0.2857142857142857,and list performances of all comparisons in terms of two automatic evaluation metrics ,14,"For research question RQ1 , to demonstrate the effectiveness of PAAG , we examine the over all performance in term of BLEU , embedding metrics and human evaluation .",Significant differences are with respect to SNet ( row with shaded background ) .,experiment
natural_language_inference,11,All models are trained end - to - end jointly with the refinement module using a dimensionality of n = 300 for all but the TriviaQA experiments for which we had to reduce n to 150 due to memory constraints .,model,model,1,101,7,7,0,model : model,0.3659420289855073,0.18421052631578946,0.7,All models are trained end to end jointly with the refinement module using a dimensionality of n 300 for all but the TriviaQA experiments for which we had to reduce n to 150 due to memory constraints ,38,We refer the interested reader to the ESIM paper for details of the model .,All baselines operate on the unrefined word embeddings E 0 described in 3.1 .,method
sentence_compression,2,the first sentence .,result,Results and discussion,0,99,9,9,0,result : Results and discussion,0.9801980198019802,0.8181818181818182,0.8181818181818182,the first sentence ,4,"The three columns are , from left to right , results on annotators 1 - 3 .","With the harder datasets , the impact of the gaze information becomes stronger , consistently favouring the cascaded architecture , and with improvements using both first pass duration and regression duration , the late measure associated with interpretation of content .",result
sentiment_analysis,11,CNNs are effective in learning high level abstract representations of sentences from constituting words or n-grams .,approach,Textual Features Extraction,0,106,11,3,0,approach : Textual Features Extraction,0.3081395348837209,0.22448979591836726,0.3333333333333333,CNNs are effective in learning high level abstract representations of sentences from constituting words or n grams ,18,We extract features from the transcript of an utterance video using convolutional neural networks ( CNNs ) .,"To get our sentence representation , we use a simple CNN with one convolutional layer followed by max - pooling .",method
natural_language_inference,56,"We create 100,000 random scenes , and 128 questions for each ( 8 start objects , 0 - 7 jumps , output is color or shape ) , resulting in 12.8 M questions .",experiment,Pretty-CLEVR,0,124,45,16,0,experiment : Pretty-CLEVR,0.3690476190476191,0.4205607476635514,0.4444444444444444,We create 100 000 random scenes and 128 questions for each 8 start objects 0 7 jumps output is color or shape resulting in 12 8 M questions ,29,"Questions with zero jumps are non-relational and correspond to : "" What color is shape X ? "" or "" What shape is color X ? "" .",We also render the scenes as images .,experiment
natural_language_inference,3,And then we sum the outputs of these LSTMs by aggregation layer .,model,Stacked Coupled-LSTMs Layers,0,103,80,5,0,model : Stacked Coupled-LSTMs Layers,0.4951923076923077,0.7079646017699115,0.8333333333333334,And then we sum the outputs of these LSTMs by aggregation layer ,13,We firstly use four directional coupled - LSTMs to model the local interactions with different information flows .,"To increase the learning capabilities of the coupled - LSTMs , we stack the basic block on top of each other .",method
relation_extraction,6,We take the output vector o s as the representation of the relation between the target entities in the sentence .,system description,Model variants,0,78,33,15,0,system description : Model variants,0.6,0.9705882352941176,0.9375,We take the output vector o s as the representation of the relation between the target entities in the sentence ,21,It maps a sequence of n vectors to a fixedsize output vector o s ? R o .,We use the Long Short - Term Memory ( LSTM ) variant of RNN that was successfully applied to information extraction before .,method
part-of-speech_tagging,0,This advantage can also be observed from the AT POS tagger 's notably higher sentence - level accuracy than the baseline ( see left ) .,analysis,analysis,0,191,14,14,0,analysis : analysis,0.7764227642276422,0.2258064516129032,0.9333333333333332,This advantage can also be observed from the AT POS tagger s notably higher sentence level accuracy than the baseline see left ,23,The robustness of our AT POS tagger against rare / unseen words helps to mitigate such an issue .,"Nonetheless , gold POS tags still yield better parsing results as compared to the baseline / AT POS taggers , supporting the claim that POS tagging needs further improvement for downstream tasks .",result
natural_language_inference,61,"In detail , a ESIM also consists of four main parts : encoding layer , local inference modeling layer , decoding layer and classi cation layer .",model,aESIM model,0,76,15,3,0,model : aESIM model,0.4871794871794872,0.22058823529411764,0.09090909090909093,In detail a ESIM also consists of four main parts encoding layer local inference modeling layer decoding layer and classi cation layer ,23,over all architecture of our newly proposed a ention boosted sequential inference model ( named aESIM ) based on ESIM is similar to ESIM .,only difference between ESIM and a ESIM is that we substitute the two Bi - LSTM layers ( LSTM1 and LSTM2 ) in ESIM with two Bi - a LSTM layers in a ESIM .,method
natural_language_inference,93,"It can be applied to variants of RNN , such as GRU and LSTM .",system description,Gated Attention-based Recurrent Networks,0,79,41,16,0,system description : Gated Attention-based Recurrent Networks,0.38164251207729466,0.6507936507936508,0.9411764705882352,It can be applied to variants of RNN such as GRU and LSTM ,14,We call this gated attention - based recurrent networks .,We also conduct experiments to show the effectiveness of the additional gate on both GRU and LSTM .,method
natural_language_inference,81,"Although some evidence indicates honey maybe effective in treating diseases and other medical conditions , such as wounds and burns , the over all evidence for its use in therapy is not conclusive .",APPENDIX,Answer town,0,364,156,76,0,APPENDIX : Answer town,0.8646080760095012,0.7323943661971831,0.5714285714285714,Although some evidence indicates honey maybe effective in treating diseases and other medical conditions such as wounds and burns the over all evidence for its use in therapy is not conclusive ,32,People who have a weakened immune system should not eat honey because of the risk of bacterial or fungal infection .,"Providing 64 calories in a typical serving of one tablespoon ( 15 ml ) equivalent to 1272 kj per 100 g , honey has no significant nutritional value .",others
part-of-speech_tagging,0,"Since our model uses BiLSTM - CRF for that reason , we also study the tagging performance on the neighbors of rare / unseen words , and analyze the effects of AT with the sequence model in mind .",analysis,Word-level Analysis,0,172,21,14,0,analysis : Word-level Analysis,0.6991869918699187,0.8076923076923077,0.7777777777777778,Since our model uses BiLSTM CRF for that reason we also study the tagging performance on the neighbors of rare unseen words and analyze the effects of AT with the sequence model in mind ,35,"One important characteristic of natural language tasks is the sequential nature of inputs ( i.e. , sequence of words ) , where each word influences the function of its neighboring words .","In , we cluster all words in the test set based on their frequency in training again , and consider the tagging accuracy on the neighbors ( left and right ) of these words in the test text .",result
text_generation,3,"However , as there is no target text provided in the testing stage , we propose the customized implementation , which is illustrated in Section 2.5 .",approach,Decoder,0,56,24,3,0,approach : Decoder,0.3971631205673759,0.6857142857142857,0.42857142857142855,However as there is no target text provided in the testing stage we propose the customized implementation which is illustrated in Section 2 5 ,25,"Similar to the encoder , our decoder D ? is also a LSTM - based auto - encoder .","Here in the introduction of the decoder , we do not provide the testing details .",method
question_answering,1,"Despite being a simpler attention mechanism , our proposed static attention outperforms the dynamically computed attention by more than 3 points .",experiment,QUESTION ANSWERING EXPERIMENTS,1,199,36,36,0,experiment : QUESTION ANSWERING EXPERIMENTS,0.6277602523659306,0.4,0.5454545454545454,Despite being a simpler attention mechanism our proposed static attention outperforms the dynamically computed attention by more than 3 points ,21,"This is in contrast with our approach , where the attention is pre-computed before flowing to the modeling layer .",We conjecture that separating out the attention layer results in a richer set of features computed in the first 4 layers which are then incorporated by the modeling layer .,experiment
natural_language_inference,43,We performed 5 random 70 / 30 splits of the training set for development .,experiment,Experiments,0,104,4,4,0,experiment : Experiments,0.6709677419354839,0.1176470588235294,0.1176470588235294,We performed 5 random 70 30 splits of the training set for development ,14,"contains 1,300 training examples and 800 test examples .",We computed POS tags and named entities with Stanford CoreNLP .,experiment
relation-classification,5,"We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks , and train them separately .",result,Main results,1,94,14,14,0,result : Main results,0.831858407079646,0.4827586206896552,0.4827586206896552,We provide in the results of a pipeline approach where we treat our two NER and RC components as independent networks and train them separately ,26,We plan to extend our model with their syntactic integration approach to further improve our model performance in future work .,"Here , the RC network uses gold NER labels when training , and uses predicted labels produced by the NER network when decoding .",result
sentiment_analysis,32,"We suppose that a context consists of n words [ w 1 c , w 2 c , ... , w n c ] and a target has m words [ w 1 t , w 2 t , ... , w mt ].",system description,Interactive Attention Networks,0,58,8,8,0,system description : Interactive Attention Networks,0.25217391304347825,0.17391304347826084,0.17391304347826084,We suppose that a context consists of n words w 1 c w 2 c w n c and a target has m words w 1 t w 2 t w mt ,33,"Specifically , let us first formalize the notation .",denotes a specific word .,method
natural_language_inference,88,"Interestingly , the network learns to associate sentiment important words such as though and fantastic or not and good .",model,Models,0,207,22,22,0,model : Models,0.8380566801619433,0.4150943396226415,1.0,Interestingly the network learns to associate sentiment important words such as though and fantastic or not and good ,19,On the fine - grained and binary classification tasks our 2 - layer LSTMN performs close to the best system T -. shows examples of intra-attention for sentiment words ., ,method
natural_language_inference,86,"Therefore , they constructed a corpus of ( passage , question , answer ) triples by replacing one entity in these bullet points at a time with a placeholder .",introduction,introduction,0,24,11,11,0,introduction : introduction,0.14035087719298245,0.5238095238095238,0.5238095238095238,Therefore they constructed a corpus of passage question answer triples by replacing one entity in these bullet points at a time with a placeholder ,25,"They observed that each news article has a number of bullet points , which summarise aspects of the information in the article .","Then , the MC task is converted into filling the placeholder in the question with an entity within the corresponding passage .",introduction
machine-translation,3,We also find that larger batch size results in better convergence although the improvement is not large .,model,Optimization,0,227,27,16,0,model : Optimization,0.7252396166134185,0.84375,0.7619047619047619,We also find that larger batch size results in better convergence although the improvement is not large ,18,The exact number depends on the sequence lengths and model size .,"However , the largest batch size is constrained by the GPU memory .",method
sentiment_analysis,0,"Using this score at as a weight parameter , the weighted sum of the sequences of the hidden state of the text - RNN , ht , is calculated to generate an attention - application vector Z .",model,Multimodal Dual Recurrent Encoder with Attention (MDREA),0,97,56,7,0,model : Multimodal Dual Recurrent Encoder with Attention (MDREA),0.5449438202247191,0.918032786885246,0.5833333333333334,Using this score at as a weight parameter the weighted sum of the sequences of the hidden state of the text RNN ht is calculated to generate an attention application vector Z ,33,"As seen in equation 5 , during each time step t , the dot product between the context vector e and the hidden state of the text - RNN at each t- th sequence ht is evaluated to calculate a similarity score at .","This attention - application vector is concatenated with the final encoding vector of the audio - RNN A ( equation 4 ) , which will be passed through the softmax function to predict the emotion class .",method
natural_language_inference,82,"Formally , we write forward and backward LSTMs as :",system description,Dynamic Entity Representation,0,43,6,6,0,system description : Dynamic Entity Representation,0.3771929824561404,0.2,0.4,Formally we write forward and backward LSTMs as ,9,"LSTM is a neural cell that outputs a vector h c , t for each token tin the sentence c ; taking the word vector x c , t of the token as input , each h c, t is calculated recurrently from its precedent vector h c ,t ? 1 or h c , t + 1 , depending on the direction of the encoding .","Then , denoting the length of the sentence c as T and the index of the entity e token as ? , we define the dynamic entity representation d e , c as the concatenation of the vectors [ h c , T , h c , 1 , h c , ? , h c, ? ] encoded by a feed - forward layer :",method
sentiment_analysis,40,is the number of hidden cells at the layer l of the forward LSTM .,model,BLSTM for Memory Building,0,88,23,11,0,model : BLSTM for Memory Building,0.3946188340807175,0.3709677419354839,0.6875,is the number of hidden cells at the layer l of the forward LSTM ,15,where ? and tanh are sigmoid and hyperbolic tan -,"The gates i , f , o ? R ? ? d l simulate binary switches that control whether to update the information from the current input , whether to forget the information in the memory cells , and whether to reveal the information in memory cells to the output , respectively .",method
sentiment_analysis,2,"2 ) We propose a position - aware bidirectional attention network ( PBAN ) based on Bi - GRU , which has been proved to be effective to improve the sentiment analysis performance .",introduction,introduction,0,48,32,32,0,introduction : introduction,0.21145374449339208,0.9696969696969696,0.9696969696969696,2 We propose a position aware bidirectional attention network PBAN based on Bi GRU which has been proved to be effective to improve the sentiment analysis performance ,28,1 ) We attempt to explicitly investigate the effectiveness of the position information of aspect term for aspect - level sentiment analysis .,"3 ) We apply a bidirectional attention mechanism , which can enhance the mutual relation between the aspect term and its corresponding sentence , and prevent the irrelevant words from getting more attention .",introduction
machine-translation,7,"Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to ?? ( which causes the corresponding gate values to equal 0 ) .",system description,GATING NETWORK,0,90,47,7,0,system description : GATING NETWORK,0.2412868632707775,0.903846153846154,0.5833333333333334,Before taking the softmax function we add tunable Gaussian noise then keep only the top k values setting the rest to which causes the corresponding gate values to equal 0 ,31,We add two components to the Softmax gating network : sparsity and noise .,"Before taking the softmax function , we add tunable Gaussian noise , then keep only the top k values , setting the rest to ?? ( which causes the corresponding gate values to equal 0 ) .",method
sentiment_analysis,5,"It includes seven refined emotion types , i.e. , joy , funny , neutral , sad , fear , disgust and anger , and each emotion has 4 film clips .",experiment,experiment,0,154,19,19,0,experiment : experiment,0.5811320754716981,0.2183908045977012,0.6333333333333333,It includes seven refined emotion types i e joy funny neutral sad fear disgust and anger and each emotion has 4 film clips ,24,MPED dataset contains 30 subjects and each subject has one session .,"There are totally 28 trails , and each trail has 120 samples .",experiment
topic_models,0,This is true for NVDM or any other model in the VB framework where the true posterior is intractable .,result,result,0,326,17,17,0,result : result,0.7912621359223301,0.3541666666666667,0.3541666666666667,This is true for NVDM or any other model in the VB framework where the true posterior is intractable ,20,"We can only compute L ( q ) , which is a lower bound on log p ( x ) ; thus the resulting perplexity values act as upper bounds .","We estimated L ( q ) from ( 16 ) using 32 samples , i.e. , R = 32 , in order to compute perplexity .",result
semantic_parsing,2,"Suffice it to say that the extraction amounts to stripping off arguments and variable names in logical forms , schema specific information in SQL queries , and substituting tokens with types in source code ( see ) .",system description,Problem Formulation,0,66,11,11,0,system description : Problem Formulation,0.2268041237113402,0.2037037037037037,0.6470588235294118,Suffice it to say that the extraction amounts to stripping off arguments and variable names in logical forms schema specific information in SQL queries and substituting tokens with types in source code see ,34,We defer detailed description of how sketches are extracted to Section 4 .,"As shown in , we first predict sketch a for input x , and then fill in missing details to generate the final meaning representation y by conditioning on both x and a .",method
natural_language_inference,21,"Given input sequence x and a mask M , we compute f ( x i , x j ) according to Eq. ( 15 ) , and follow the standard procedure of multi-dimensional token 2 token self - attention to The final output u ? Rd h n of DiSA is obtained by combining the output sand the input h of the masked multidimensional token 2 token self - attention block .",system description,Directional Self-Attention Network,0,153,106,27,0,system description : Directional Self-Attention Network,0.5275862068965518,0.8346456692913385,0.5625,Given input sequence x and a mask M we compute f x i x j according to Eq 15 and follow the standard procedure of multi dimensional token 2 token self attention to The final output u Rd h n of DiSA is obtained by combining the output sand the input h of the masked multidimensional token 2 token self attention block ,63,We show these three positional masks in .,"Given input sequence x and a mask M , we compute f ( x i , x j ) according to Eq. ( 15 ) , and follow the standard procedure of multi-dimensional token 2 token self - attention to The final output u ? Rd h n of DiSA is obtained by combining the output sand the input h of the masked multidimensional token 2 token self - attention block .",method
natural_language_inference,77,"However , the differences are not substantial , indicating that this extension does not offer any systematic advantage .",model,model,0,173,33,33,0,model : model,0.6336996336996337,0.6346153846153846,1.0,However the differences are not substantial indicating that this extension does not offer any systematic advantage ,17,This shows that a more sophisticated interaction layer can help ., ,method
text_summarization,10,"We selected human annotators that were located in the US , had an ap - proval rate greater than 95 % , and had at least 10,000 approved HITs .",model,Fast Multi-Task Training,0,135,90,15,0,model : Fast Multi-Task Training,0.5133079847908745,0.967741935483871,0.8333333333333334,We selected human annotators that were located in the US had an ap proval rate greater than 95 and had at least 10 000 approved HITs ,27,We used Amazon MTurk to perform human evaluation of summary relevance and readability .,"For the pairwise model comparisons discussed in Sec. 6.2 , we showed the annotators the input article , the ground truth summary , and the two model summaries ( randomly shuffled to anonymize model identities ) - we then asked them to choose the better among the two model summaries or choose ' Not - Distinguishable ' if both summaries are equally good / bad .",method
question_answering,1,"In the first example , Where matches locations and in the second example , many matches quantities and numerical symbols .",experiment,QUESTION ANSWERING EXPERIMENTS,0,215,52,52,0,experiment : QUESTION ANSWERING EXPERIMENTS,0.6782334384858044,0.5777777777777777,0.7878787878787878,In the first example Where matches locations and in the second example many matches quantities and numerical symbols ,19,Finally we visualize the attention matrices for some question - context tuples in the dev data in .,"Also , entities in the question typically attend to the same entities in the context , thus providing a feature for the model to localize possible answers .",experiment
natural_language_inference,79,Tasks and Baselines :,experiment,Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0,155,20,13,0,experiment : Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0.5783582089552238,0.4,0.6842105263157895,Tasks and Baselines ,4,The dataset can be downloaded at : http://nlp.Stanford.edu/sentiment/,"In , the authors propose two ways of benchmarking .",experiment
sentiment_analysis,38,"Image classifiers learn a broadly useful hierarchy of feature detectors rerepresenting raw pixels as edges , textures , and objects .",introduction,introduction,0,18,7,7,0,introduction : introduction,0.10714285714285714,0.16279069767441862,0.16279069767441862,Image classifiers learn a broadly useful hierarchy of feature detectors rerepresenting raw pixels as edges textures and objects ,19,Analysis of the task specific representations learned by these models reveals many fascinating properties .,"In the field of computer vision , it is now commonplace to reuse these representations on a broad suite of related tasks - one of the most successful examples of transfer learning to date .",introduction
sentiment_analysis,2,where ? is the regularization factor and ? contains all the parameters .,training,Model training,0,113,5,5,0,training : Model training,0.4977973568281938,0.625,0.625,where is the regularization factor and contains all the parameters ,11,"We regard the cross-entropy as the loss function , and the formula is as follows :",where ? is the regularization factor and ? contains all the parameters .,experiment
sentiment_analysis,31,"Given a sentence s = [ w 1 , w 2 , ... , w i , ... , w n ] , let vi ? R k be the word vector for word w i .",system description,Convolutional Neural Networks,0,53,9,3,0,system description : Convolutional Neural Networks,0.3333333333333333,0.16071428571428573,0.15789473684210525,Given a sentence s w 1 w 2 w i w n let vi R k be the word vector for word w i ,25,We first briefly describe convolutional neural networks ( CNN ) for general sentiment classification .,"Given a sentence s = [ w 1 , w 2 , ... , w i , ... , w n ] , let vi ? R k be the word vector for word w i .",method
relation_extraction,12,"Through preliminary experiments on the development sets , we find that the combinations ( N = 2 , M = 2 , L 1 = 2 , L 2 = 4 , d hidden = 340 ) 8 and ( N = 3 , M = 2 , L 1 = 2 , L 2 = 4 , d hidden = 300 ) give the best results on cross - sentence n-ary relation extraction and sentence - level relation extraction , respectively .",experiment,Setup,0,175,21,5,0,experiment : Setup,0.5319148936170213,0.6363636363636364,0.5,Through preliminary experiments on the development sets we find that the combinations N 2 M 2 L 1 2 L 2 4 d hidden 340 8 and N 3 M 2 L 1 2 L 2 4 d hidden 300 give the best results on cross sentence n ary relation extraction and sentence level relation extraction respectively ,58,"We choose the number of heads N for attention guided layer from { 1 , 2 , 3 , 4 } , the block number M from { 1 , 2 , 3 } , the number of sub - layers L in each densely connected layer from { 2 , 3 , 4 }.",Glo Ve vectors are used as the initialization for word embeddings .,experiment
natural_language_inference,50,"The relative performance of HyperQA is significantly better on large datasets , e.g. , YahooCQA ( 253K training pairs ) as opposed to smaller ones like WikiQA ( 5.9 K training pairs ) .",analysis,Overall analysis.,0,255,32,5,0,analysis : Overall analysis.,0.8044164037854891,0.4050632911392405,0.3125,The relative performance of HyperQA is significantly better on large datasets e g YahooCQA 253K training pairs as opposed to smaller ones like WikiQA 5 9 K training pairs ,30,Hyper QA outperforms complex models such as MP - CNN and AP - BiLSTM on multiple datasets .,We believe that this is due to the fact that Hyperbolic space is seemingly larger than Euclidean space . Hyper,result
named-entity-recognition,1,"However , we decided to use the IOBES tagging scheme , a variant of IOB commonly used for named entity recognition , which encodes information about singleton entities ( S ) and explicitly marks the end of named entities ( E ) .",training,Tagging Schemes,0,86,18,5,0,training : Tagging Schemes,0.4154589371980677,0.21176470588235294,0.625,However we decided to use the IOBES tagging scheme a variant of IOB commonly used for named entity recognition which encodes information about singleton entities S and explicitly marks the end of named entities E ,36,"Sentences are usually represented in the IOB format ( Inside , Outside , Beginning ) where every token is labeled as B- label if the token is the beginning of a named entity , I-label if it is inside a named entity but not the first token within the named entity , or O otherwise .","Using this scheme , tagging a word as I-label with high - confidence narrows down the choices for the subsequent word to I-label or E-label , however , the IOB scheme is only capable of determining that the subsequent word can not be the interior of another label .",experiment
natural_language_inference,74,"where ? ( l|P , H ) is the previous action policy that predicts the label given P and H , {l * } is the set of annotated labels , and",training,Training,0,125,16,16,0,training : Training,0.5580357142857143,0.8,0.8,where l P H is the previous action policy that predicts the label given P and H l is the set of annotated labels and,25,"To simulate the thought of human being more closely , in this paper , we tackle this problem by using the REINFORCE algorithm to minimize the negative expected reward , which is defined as :",is the reward function defined to measure the distance to all the ideas of the annotators .,experiment
sentiment_analysis,42,"1 , : Examples of attention weights in different hops for aspect level sentiment classification .",method,Effects of Location Attention,0,198,14,9,0,method : Effects of Location Attention,0.7857142857142857,0.6363636363636364,0.5294117647058824,1 Examples of attention weights in different hops for aspect level sentiment classification ,14,"a ) Aspect : service , Answer :",The model also takes into account of the location information ( Model 2 ) .,method
natural_language_inference,79,The neural network based word vectors are usually trained using stochastic gradient descent where the gradient is obtained via backpropagation .,system description,Learning Vector Representation of Words,0,71,25,22,0,system description : Learning Vector Representation of Words,0.26492537313432835,0.7352941176470589,0.7333333333333333,The neural network based word vectors are usually trained using stochastic gradient descent where the gradient is obtained via backpropagation ,21,This use of binary Huffman code for the hierarchy is the same with .,This type of models is commonly known as neural language models .,method
named-entity-recognition,7,The embeddings of POS tags are initialized randomly with dimension 32 .,experiment,Setup,1,125,9,4,0,experiment : Setup,0.7911392405063291,0.6428571428571429,0.4444444444444444,The embeddings of POS tags are initialized randomly with dimension 32 ,12,Glo Ve of dimension 100 are used to initialize the word vectors for all three datasets .,The model is trained using Adam and a gradient clipping of 3.0 .,experiment
part-of-speech_tagging,7,We hence use three different types of taggers : our implementation of a bi -LSTM ; TNT - a second order HMM with suffix trie handling for OOVs .,system description,Tagging with bi-LSTMs,0,61,33,33,0,system description : Tagging with bi-LSTMs,0.488,0.9428571428571428,0.9428571428571428,We hence use three different types of taggers our implementation of a bi LSTM TNT a second order HMM with suffix trie handling for OOVs ,26,We want to compare POS taggers under varying conditions .,We use TNT as it was among the best performing taggers evaluated in .,method
prosody_prediction,0,The results of this experiment are shown in .,result,Results,0,130,26,26,0,result : Results,0.6770833333333334,0.7878787878787878,0.8666666666666667,The results of this experiment are shown in ,9,For this experiment we trained the models using the train - 360 training set ( as above ) replacing only the test set .,The good results 6 from this experiment provide further support for the quality of the new dataset .,result
relation-classification,1,"This separated framework makes the task easy to deal with , and each component can be more flexible .",introduction,introduction,0,14,6,6,0,introduction : introduction,0.05691056910569105,0.14634146341463414,0.14634146341463414,This separated framework makes the task easy to deal with and each component can be more flexible ,18,"Traditional methods handle this task in a pipelined manner , i.e. , extracting the entities first and then recognizing their relations .",But it neglects the relevance between these two sub - tasks and each subtask is an independent model .,introduction
passage_re-ranking,0,"Despite the potential of neural models to match documents at the semantic level for improved ranking , most scalable search engines use exact Researchers are finding that cinnamon reduces blood sugar levels naturally when taken daily ...",introduction,introduction,0,15,5,5,0,introduction : introduction,0.12295081967213115,0.2083333333333333,0.2083333333333333,Despite the potential of neural models to match documents at the semantic level for improved ranking most scalable search engines use exact Researchers are finding that cinnamon reduces blood sugar levels naturally when taken daily ,36,"Continuous vector space representations and neural networks , however , no longer depend on discrete onehot representations , and thus offer an exciting new approach to tackling this challenge .",does cinnamon lower blood sugar ? does cinnamon lower blood sugar ? Researchers are finding that cinnamon reduces blood sugar levels naturally when taken daily ...,introduction
text-classification,5,BPTT for Text Classification ( BPT3C ),system description,Target task classifier fine-tuning,0,138,87,20,0,system description : Target task classifier fine-tuning,0.5476190476190477,0.9157894736842104,0.7142857142857143,BPTT for Text Classification BPT3C ,6,"While discriminative fine - tuning , slanted triangular learning rates , and gradual unfreezing all are beneficial on their own , we show in Section 5 that they complement each other and enable our method to perform well across diverse datasets .",Language models are trained with backpropagation through time ( BPTT ) to enable gradient propagation for large input sequences .,method
natural_language_inference,39,"To enable direct comparison with previous work , we used the same training , development , and test sets as released by .",experiment,Experimental Setup,0,145,15,15,0,experiment : Experimental Setup,0.7038834951456311,0.375,0.375,To enable direct comparison with previous work we used the same training development and test sets as released by ,20,. The TrecQA dataset from the Text Retrieval Conferences has been widely used for the answer selection task during the past decade .,"The TrecQA data consists of 1,229 questions with 53,417 question - answer pairs in the TRAIN - ALL training set , 82 questions with 1,148 pairs in the development set , and 100 questions with 1,517 pairs in the test set .",experiment
text_generation,2,"The experiment consists of three parts : synthetic data experiments , experiments in real - world scenarios and some explanation study .",experiment,Experiment,0,157,2,2,0,experiment : Experiment,0.44857142857142857,0.6666666666666666,0.6666666666666666,The experiment consists of three parts synthetic data experiments experiments in real world scenarios and some explanation study ,19, ,The repeatable experiment code is published for further research 3 .,experiment
sentiment_analysis,11,Gradient clipping is used for regularization with a norm set to 40 .,training,Training Details,1,250,7,7,0,training : Training Details,0.7267441860465116,0.6363636363636364,0.6363636363636364,Gradient clipping is used for regularization with a norm set to 40 ,13,An annealing approach halves the lr every 20 epochs and termination is decided using an early - stop measure with a patience of 12 by monitoring the validation loss .,Hyperparameters are decided using a Random Search .,experiment
text_summarization,0,Abstractive Text Summarization by Incorporating Reader Comments,title,title,1,2,1,1,0,title : title,0.006644518272425249,1.0,1.0,Abstractive Text Summarization by Incorporating Reader Comments,7, , ,title
natural_language_inference,1,We tested additional configurations for our algorithm .,experiment,Experimental setup,0,232,7,7,0,experiment : Experimental setup,0.8498168498168498,0.5384615384615384,0.5384615384615384,We tested additional configurations for our algorithm ,8,"For each configuration of hyperparameters , the F1score on the validation set was computed regularly during learning to perform early stopping .","First , in the Candidates as Negatives setting ( negative facts are sampled from the candidate set , see Section 4 ) , abbreviated CANDS AS NEGS , the experimental protocol is the same as in the default setting but the embeddings are initialized with the best configuration of the default setup .",experiment
natural_language_inference,60,Our analysis showed that LEAR was particularly favored for verbs ( with average weights of 0.75 ) .,model,Specialization,0,173,16,9,0,model : Specialization,0.8963730569948186,0.4444444444444444,0.75,Our analysis showed that LEAR was particularly favored for verbs with average weights of 0 75 ,17,"shows that LEAR embeddings get high weights compared to the original source embeddings ( "" Levy "" in the table ) .","The sentiment - refined embeddings were less useful , with the original GloVe embeddings receiving higher weights .",method
text_summarization,3,"Noted that , this distribution can be sparse since not every word has its upper concept .",system description,Context-aware Conceptualization,0,101,46,17,0,system description : Context-aware Conceptualization,0.4173553719008264,0.3511450381679389,0.6071428571428571,Noted that this distribution can be sparse since not every word has its upper concept ,16,Purple bar represents the concept distribution over the inputs .,Green bar represents the vocabulary distribution generated from seq2seq component .,method
sentiment_analysis,9,"Consistently with CDM , is the SRD between the i - th context token and a targeted aspect .",model,Context-features Dynamic Weighting,0,145,75,6,0,model : Context-features Dynamic Weighting,0.5234657039711191,0.9375,0.75,Consistently with CDM is the SRD between the i th context token and a targeted aspect ,17,where is the constructed weight matrix and is the weight vector for each non-local context words .,is the length of the input sequence .,method
topic_models,0,The vector m known as universal background model represents log unigram probabilities of words .,model,model,0,62,7,7,0,model : model,0.15048543689320387,0.1346153846153846,0.15217391304347827,The vector m known as universal background model represents log unigram probabilities of words ,15,"where {m , T } are parameters of the model .","known as total variability matrix , is a low - rank matrix defining subspace of document - specific distributions .",method
machine-translation,1,We do not reduce the learning rate during training .,model,model,0,163,19,19,0,model : model,0.8109452736318408,0.3877551020408163,0.3877551020408163,We do not reduce the learning rate during training ,10,We apply dropout to the last ReLU layer before the softmax dropping units with a probability of 0.1 .,"At each step we sample a batch of sequences of 500 characters each , use the first 100 characters as the minimum context and predict the latter 400 characters .",method
natural_language_inference,56,"For example , 9 - by - 9 Sudokus can be solved in a fraction of a second with constraint propagation and search or with dancing links .",introduction,introduction,0,34,21,21,0,introduction : introduction,0.1011904761904762,0.75,0.75,For example 9 by 9 Sudokus can be solved in a fraction of a second with constraint propagation and search or with dancing links ,25,Some of the tasks we evaluate on can be efficiently and perfectly solved by hand - crafted algorithms that operate on the symbolic level .,"These symbolic algorithms are superior in every respect but one : they do n't comply with the interface , as they are not differentiable and do n't work with real - valued vector descriptions .",introduction
relation-classification,3,"Larger values of ? ( i.e. , larger perturbations ) lead to consistent performance decrease in our early experiments .",experiment,Experimental setup,0,93,14,14,0,experiment : Experimental setup,0.6788321167883211,0.2978723404255319,0.2978723404255319,Larger values of i e larger perturbations lead to consistent performance decrease in our early experiments ,17,"The scaling parameter ? is selected from { 5 e?2 , 1 e ? 2 , 1 e ? 3 , 1e?4 } .",This can be explained from the fact that adding more noise can change the content of the sentence as also reported by .,experiment
sentiment_analysis,27,"ATAE - LSTM first attaches the aspect embedding to each word embedding to capture aspect - dependent information , and then employs attention mechanism to get the sentence representation for final classification .",method,Comparative methods,1,198,5,5,0,method : Comparative methods,0.7173913043478259,0.21739130434782608,0.21739130434782608,ATAE LSTM first attaches the aspect embedding to each word embedding to capture aspect dependent information and then employs attention mechanism to get the sentence representation for final classification ,30,The last hidden states of the two LSTMs are finally concatenated for predicting the sentiment polarity of the aspect .,Mem Net uses a deep memory network on the context word embeddings for sentence representation to capture the relevance between each context word and the aspect .,method
natural_language_inference,39,In the final model component we use the f ocusCube to compute the final similarity score .,system description,Similarity Classification with Deep,0,116,53,4,0,system description : Similarity Classification with Deep,0.5631067961165048,0.7910447761194029,0.2222222222222222,In the final model component we use the f ocusCube to compute the final similarity score ,17,The f ocus Cube contains focus - weighted finegrained similarity information .,"If we treat the f ocusCube as an "" image "" with 13 channels , then semantic similarity measurement can be converted into a pattern recognition ( image processing ) problem , where we are looking for patterns of strong pairwise word interactions in the "" image "" .",method
natural_language_inference,80,"Finally , we demonstrate how the results can be improved further with an additional preprocessing step .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.027586206896551724,0.8571428571428571,0.8571428571428571,Finally we demonstrate how the results can be improved further with an additional preprocessing step ,16,"We also introduce a sophisticated ensemble strategy to combine our proposed models , which noticeably improves final predictions .",Our evaluation shows that DR - BiLSTM obtains the best single model and ensemble model results achieving the new state - of - the - art scores on the Stanford NLI dataset .,abstract
natural_language_inference,77,Another important observation is the fact that seemingly complex questions might be answerable by simple heuristics .,introduction,introduction,0,22,13,13,0,introduction : introduction,0.08058608058608059,0.5652173913043478,0.5652173913043478,Another important observation is the fact that seemingly complex questions might be answerable by simple heuristics ,17,This gap raises the question whether the complexity of current systems is justified solely by their empirical results .,Let 's consider the following example :,introduction
relation-classification,5,Other neural models employ dependency parsing - based information .,introduction,introduction,0,18,12,12,0,introduction : introduction,0.1592920353982301,0.42857142857142855,0.42857142857142855,Other neural models employ dependency parsing based information ,9,Their approach relies on various manually extracted features .,"In particular , applied bottom - up and top - down tree - structured LSTMs to model dependency paths between entities .",introduction
topic_models,0,"If p ( ) = N ( 0 , I ) , then , where L is the Cholesky factor of ? ? 1",model,model,0,99,44,44,0,model : model,0.2402912621359224,0.8461538461538461,0.9565217391304348,If p N 0 I then where L is the Cholesky factor of 1,14,This is done by introducing a differentiable function g over another random variable .,"If p ( ) = N ( 0 , I ) , then , where L is the Cholesky factor of ? ? 1",method
text_summarization,0,"As for training generation module , we sum up the loss function of denoising module L d , cross entropy between ground truth L g and the result of discriminator L g c , as shown in Equation 27 .",training,Model training,0,195,4,4,0,training : Model training,0.6478405315614618,0.5714285714285714,0.5714285714285714,As for training generation module we sum up the loss function of denoising module L d cross entropy between ground truth L g and the result of discriminator L g c as shown in Equation 27 ,37,"As our model is trained in an adversarial manner , we resplit the parameters in our model into two parts : ( 1 ) generation module including the parameters of summary generator , reader attention module and goal tracker ; ( 2 ) discriminator module including the parameters of CNN classifier .",We use the L to optimize the parameters of generation module .,experiment
natural_language_inference,53,and V are learned linear transformations that project the caption x and the image y to the same embedding space .,evaluation,Paraphrase detection The Microsoft Research,0,133,41,13,0,evaluation : Paraphrase detection The Microsoft Research,0.6394230769230769,0.8913043478260869,0.7222222222222222,and V are learned linear transformations that project the caption x and the image y to the same embedding space ,21,"where ( x , y) consists of an image y with one of its associated captions x , ( y k ) k and ( y k ? ) k ? are negative examples of the ranking loss , ? is the margin and s corresponds to the cosine similarity .",We use a margin ? = 0.2 and 30 contrastive terms .,result
machine-translation,3,Further operations will be used at the interface part to extract the final representation c of the source sequence from e.,system description,Neural Machine Translation,0,50,8,8,0,system description : Neural Machine Translation,0.1597444089456869,0.06666666666666668,0.25,Further operations will be used at the interface part to extract the final representation c of the source sequence from e ,22,"In the encoding part , the source sequence is processed and transformed into a group of vectors e = {e 1 , , em } for each time step .","At the decoding step , the target sequence is generated from the representation c.",method
natural_language_inference,69,"Similar to our qualitative analysis of MED - HOP , annotators were shown the query - answer pair as a fact and the chain of relevant documents leading to the answer .",analysis,Crowdsourced Human Annotation,0,186,33,3,0,analysis : Crowdsourced Human Annotation,0.5391304347826087,0.6111111111111112,0.2,Similar to our qualitative analysis of MED HOP annotators were shown the query answer pair as a fact and the chain of relevant documents leading to the answer ,29,We asked human annotators on Amazon Mechanical Turk to evaluate samples of the WIKIHOP development set .,"They were then instructed to answer ( 1 ) whether they knew the fact before ; ( 2 ) whether the fact follows from the texts ( with options "" fact follows "" , "" fact is likely "" , and "" fact does not follow "" ) ; and ; whether a single or several of the documents are required .",result
natural_language_inference,19,"Let an input sentence be a sequence of discrete words x = [ x 1 , x 2 , , x n ] , where x i ? RN is a one - hot representation of the word i , and N is the vocabulary size .",architecture,Word Embedding Layer,0,85,10,2,0,architecture : Word Embedding Layer,0.3333333333333333,0.12345679012345676,0.25,Let an input sentence be a sequence of discrete words x x 1 x 2 x n where x i RN is a one hot representation of the word i and N is the vocabulary size ,37, ,"Let an input sentence be a sequence of discrete words x = [ x 1 , x 2 , , x n ] , where x i ? RN is a one - hot representation of the word i , and N is the vocabulary size .",method
part-of-speech_tagging,3,State - of - the - art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.019704433497536943,0.3333333333333333,0.3333333333333333,State of the art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre processing ,25, ,"In this paper , we introduce a novel neutral network architecture that benefits from both word - and character - level representations automatically , by using combination of bidirectional LSTM , CNN and CRF .",abstract
natural_language_inference,80,Here stands for element - wise product while W p ? R 8 dd and b p ? Rd are the trainable weights and biases of the projector layer respectively .,model,model,0,97,41,41,0,model : model,0.3344827586206897,0.640625,0.640625,Here stands for element wise product while W p R 8 dd and b p Rd are the trainable weights and biases of the projector layer respectively ,28,This helps the model to capture deeper dependencies between the sentences besides lowering the complexity of vector representations .,Here stands for element - wise product while W p ? R 8 dd and b p ? Rd are the trainable weights and biases of the projector layer respectively .,method
sentiment_analysis,40,"For example , for the Restaurant dataset , our model with 4 attention layers performs the best .",result,Effects of Attention Layers,0,184,21,5,0,result : Effects of Attention Layers,0.8251121076233184,0.40384615384615385,0.625,For example for the Restaurant dataset our model with 4 attention layers performs the best ,16,"In general , our model with 2 or 3 attention layers works better , but the advantage is not always therefor different datasets .","Using 1 attention is always not as good as using more , which shows that one - time attention might not be sufficient to capture the sentiment features in complicated cases .",result
natural_language_inference,66,This can be done using an RNN which at each position takes in both a k and ht k as its input and determines how well the over all matching of the two sentences is up to the current position .,model,Our Model,0,97,5,5,0,model : Our Model,0.3476702508960573,0.1724137931034483,0.1724137931034483,This can be done using an RNN which at each position takes in both a k and ht k as its input and determines how well the over all matching of the two sentences is up to the current position ,41,"We speculate that if we instead use each of the attention - weighted representations of the premise for matching , i.e. , use a k at position k to match the hidden state ht k of the hypothesis while we go through the hypothesis , we could achieve better matching results .",In the end the RNN will produce a single vector representing the matching of the two entire sentences .,method
natural_language_inference,79,"The paragraph vectors for the 25,000 labeled instances are then fed through a neural network with one hidden layer with 50 units and a logistic classifier to learn to predict the sentiment .",experiment,Experimental protocols:,1,164,29,3,0,experiment : Experimental protocols:,0.6119402985074627,0.58,0.2,The paragraph vectors for the 25 000 labeled instances are then fed through a neural network with one hidden layer with 50 units and a logistic classifier to learn to predict the sentiment ,34,"We learn the word vectors and paragraph vectors using 75,000 training documents ( 25,000 labeled and 50,000 unlabeled instances ) .","At test time , given a test sentence , we again freeze the rest of the network and learn the paragraph vectors for the test reviews by gradient descent .",experiment
sentiment_analysis,50,We find that the benefits brought by document - level knowledge are typically shown in four ways .,analysis,Analysis,0,130,9,9,0,analysis : Analysis,0.8024691358024691,0.25,0.25,We find that the benefits brought by document level knowledge are typically shown in four ways ,17,"To better understand in which conditions the proposed method is helpful , we analyze a subset of test examples thatare correctly classified by PRET + MULT but are misclassified by LSTM + ATT .","First of all , to our surprise , LSTM + ATT made obvious mistakes on some instances with common opinion words .",result
question_generation,1,k= 5 performs the best and hence we use this value for the results in main paper .,Supplementary Material,33:,0,243,14,6,0,Supplementary Material : 33:,0.6198979591836735,0.6666666666666666,0.75,k 5 performs the best and hence we use this value for the results in main paper ,18,All these experiment are for the differential image network .,Each image in the dataset contains 5 natural questions .,others
part-of-speech_tagging,0,"Major neural character - level models include the character - level CNN and ( Bi ) LSTM . A Bi-directional LSTM ( BiLSTM ) processes each sequence both forward and backward to capture sequential information , while preventing the vanishing / exploding gradient problem .",model,model,0,104,5,5,0,model : model,0.4227642276422765,0.25,0.25,Major neural character level models include the character level CNN and Bi LSTM A Bi directional LSTM BiLSTM processes each sequence both forward and backward to capture sequential information while preventing the vanishing exploding gradient problem ,37,Prior work has shown that incorporating character - level representations of words can boost POS tagging accuracy by capturing morphological information present in each language .,"We observed that the character - level BiLSTM outperformed the CNN by 0.1 % on the PTB - WSJ development set , and hence in all of our experiments we use the character - level BiLSTM .",method
natural_language_inference,100,"However , for larger training data sets than TREC QA data , a NMM - 2 could have better performance since it has more model parameters and is suitable for fitting larger training data set .",result,Learning without Combining Additional Features,0,310,49,14,0,result : Learning without Combining Additional Features,0.8469945355191257,0.5975609756097561,0.7,However for larger training data sets than TREC QA data a NMM 2 could have better performance since it has more model parameters and is suitable for fitting larger training data set ,33,be trained much faster with good prediction accuracy .,We leave the study of impact of the number of hidden layers in a NMM to future work .,result
sentiment_analysis,17,"The N - ary Tree - LSTM can be used on tree structures where the branching factor is at most N and where children are ordered , i.e. , they can be indexed from 1 to N .",system description,-ary Tree-LSTMs,0,98,59,2,0,system description : -ary Tree-LSTMs,0.4355555555555555,0.7564102564102564,0.09523809523809523,The N ary Tree LSTM can be used on tree structures where the branching factor is at most N and where children are ordered i e they can be indexed from 1 to N ,35, ,"For any node j , write the hidden state and memory cell of its kth child ash jk and c jk respectively .",method
relation_extraction,3,"Note that the system ( Rotsztejn et al. , 2018 ) integrates many techniques like feature - engineering , model combination , pretraining embeddings on in - domain data , and artificial data generation , while our model is almost a direct adaption from the ACE architecture .",training,Results on SemEval 2018 Task 7,0,119,16,6,1,training : Results on SemEval 2018 Task 7,0.8686131386861314,0.8888888888888888,0.8571428571428571,Note that the system Rotsztejn et al 2018 integrates many techniques like feature engineering model combination pretraining embeddings on in domain data and artificial data generation while our model is almost a direct adaption from the ACE architecture ,39,"When predicting multiple relations in one - pass , we have 0.9 % drop on Macro - F1 , but a further 0.8 % improvement on Micro - F1 .","On the other hand , compared to the top singlemodel result , which makes use of additional word and entity embeddings pretrained on in - domain data , our methods demonstrate clear advantage as a single model .",experiment
question_answering,0,Any entity in the KB that can take the place of the q - node will be apart of the answer .,introduction,introduction,0,18,9,9,0,introduction : introduction,0.061016949152542375,0.2571428571428571,0.2571428571428571,Any entity in the KB that can take the place of the q node will be apart of the answer ,21,The depicted graph structure consists of entities and relations from the KB and the special q-node .,"In this paper , we describe a semantic parsing approach to the problem of KB QA .",introduction
sarcasm_detection,1,"Here , d sis the embedding size and V represents the size of the vocabulary .",method,Stylometric features,0,120,51,16,0,method : Stylometric features,0.3592814371257485,0.3377483443708609,0.5925925925925926,Here d sis the embedding size and V represents the size of the vocabulary ,15,"Every user- document and all words within them are first mapped to unique vectors such that each vector is represented by a column in matrix D ? R ds Nu and W s ? R ds V , respectively .",Continuous - bag - of - words approach is then performed where a target word is predicted given the word vectors from its context - window .,method
natural_language_inference,24,"The learning algorithm for reading comprehension is to learn a function f ( Q , P ) ? A .",system description,Proposed model: SAN,0,32,5,5,0,system description : Proposed model: SAN,0.13675213675213674,0.054945054945054944,0.07936507936507936,The learning algorithm for reading comprehension is to learn a function f Q P A ,16,"Here , m and n denote the number of tokens in Q and P , respectively .","The learning algorithm for reading comprehension is to learn a function f ( Q , P ) ? A .",method
text_summarization,1,Each focus m z = ( m z 1 . . . m z S ) is encoded as embeddings and concatenated with the input embeddings of the source sequence x = ( x 1 . . . x S ) .,analysis,Select and Generate,0,116,37,31,0,analysis : Select and Generate,0.4833333333333333,0.9487179487179488,0.9393939393939394,Each focus m z m z 1 m z S is encoded as embeddings and concatenated with the input embeddings of the source sequence x x 1 x S ,30,This gives us a list of focus m 1 . . . m K coming from K experts .,"An off - the - shelf generation function such as encoder - decoder can be used for modeling p ( y |m , x ) , as long as it accepts a stream of input embeddings .",result
text_generation,0,The generative model is treated as an agent of reinforcement learning ( RL ) ; the state is the generated tokens so far and the action is the next token to be generated .,introduction,introduction,1,33,22,22,0,introduction : introduction,0.10185185185185183,0.7586206896551724,0.7586206896551724,The generative model is treated as an agent of reinforcement learning RL the state is the generated tokens so far and the action is the next token to be generated ,31,"In this paper , to address the above two issues , we follow ) and consider the sequence generation procedure as a sequential decision making process .","Unlike the work in ) that requires a task - specific sequence score , such as BLEU in machine translation , to give the reward , we employ a discriminator to evaluate the sequence and feedback the evaluation to guide the learning of the generative model .",introduction
text_summarization,0,"In last year , Softbank also invested into Uber with a valuation of $ 48 billion .",introduction,introduction,0,27,12,12,0,introduction : introduction,0.08970099667774087,0.2352941176470588,0.2352941176470588,In last year Softbank also invested into Uber with a valuation of 48 billion ,15,"In March of this year , a Uber driverless car hit a woman and caused her death .",comments Toyota 's investment in Uber is a wise choice .,introduction
natural_language_inference,22,The typical context length spans from 50 tokens to 250 tokens .,system description,Question Answering,0,43,9,9,0,system description : Question Answering,0.19545454545454544,0.8181818181818182,0.8181818181818182,The typical context length spans from 50 tokens to 250 tokens ,12,"SQuAD contains 107,785 question - answer pairs .",The typical length of a question is around 10 tokens .,method
part-of-speech_tagging,0,The normalization is performed every time we feed input embeddings into the LSTMs and generate adversarial examples .,training,Adversarial Training,0,78,21,21,0,training : Adversarial Training,0.3170731707317073,0.5384615384615384,0.5384615384615384,The normalization is performed every time we feed input embeddings into the LSTMs and generate adversarial examples ,18,"To prevent this issue , we normalize word / character embeddings so that they have mean 0 and variance 1 for every entry , as in .","To ensure a fair comparison , we also normalize input embeddings in our baseline model .",experiment
sentiment_analysis,15,"Intuitively , we can interpret each slice of the tensor as capturing a specific type of composition .",model,Standard,0,161,72,5,0,model : Standard,0.5962962962962963,0.6728971962616822,0.7142857142857143,Intuitively we can interpret each slice of the tensor as capturing a specific type of composition ,17,"The main advantage over the previous RNN model , which is a special case of the RNTN when V is set to 0 , is that the tensor can directly relate input vectors .",An alternative to RNTNs would be to make the compositional function more powerful by adding a second neural network layer .,method
natural_language_inference,84,"To simplify the notation , the rest of the paper assumes that there is only one definition for each word .",system description,ON THE FLY EMBEDDINGS,0,88,21,21,0,system description : ON THE FLY EMBEDDINGS,0.3826086956521739,0.1891891891891892,0.6774193548387096,To simplify the notation the rest of the paper assumes that there is only one definition for each word ,20,We include all definitions whose headwords match w or any possible lemma of a lower - cased w 2 .,"While the primary purpose of definition embeddings ed ( w ) is to inform the network about the rare words , they might also contain useful information for the words in V train .",method
natural_language_inference,90,"We first obtain unnormalized attention weights e ij , computed by a function F , which decomposes as :",approach,Aggregate,0,57,22,4,0,approach : Aggregate,0.38,0.3055555555555556,0.14814814814814814,We first obtain unnormalized attention weights e ij computed by a function F which decomposes as ,17,"Finally , aggregate the sets {v 1 , i } a i =1 and {v 2 , j } b j=1 from the previous step and use the result to predict the label ?.",This decomposition avoids the quadratic complexity that would be associated with separately applying F a b times .,method
sentiment_analysis,37,"Ds to a GRU ( GRU a ) of size D o ( kindly refer to for the value ) to propagate aspect information among the aspect - aware sentence representations and obtain Q = GRU a ( R ) , where Q ? R M Do and GRU a has the following parameters :",model,Inter-Aspect Dependency Modeling,0,75,15,3,0,model : Inter-Aspect Dependency Modeling,0.2964426877470356,0.20270270270270271,0.1,Ds to a GRU GRU a of size D o kindly refer to for the value to propagate aspect information among the aspect aware sentence representations and obtain Q GRU a R where Q R M Do and GRU a has the following parameters ,45,"We feed R = [ r a 1 , r a 2 , . . . , r a M ] ? R M","Ds to a GRU ( GRU a ) of size D o ( kindly refer to for the value ) to propagate aspect information among the aspect - aware sentence representations and obtain Q = GRU a ( R ) , where Q ? R M Do and GRU a has the following parameters :",method
natural_language_inference,2,where softmax is used for normalization .,architecture,Question-focused Attentional Pointing,0,127,82,10,0,architecture : Question-focused Attentional Pointing,0.4738805970149254,0.7522935779816514,0.3333333333333333,where softmax is used for normalization ,7,We define k ? R U as the normalized max - attentional weights :,The maxattentional question representation q ma ? R H is :,method
sentence_classification,0,"To this end , we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers .",system description,Model,1,23,7,7,0,system description : Model,0.08614232209737828,0.6363636363636364,0.6363636363636364,To this end we propose a neural multitask learning framework to incorporate knowledge into citations from the structure of scientific papers ,22,"In this paper we argue that better representations can be obtained directly from data , sidestepping problems associated with external features .","In particular , we propose two auxiliary tasks as structural scaffolds to improve citation intent prediction : 1 ( 1 ) predicting the section title in which the citation occurs and ( 2 ) predicting whether a sentence needs a citation .",method
machine-translation,3,"However , our experimental results suggest that deep models are less prone to the problem of over-fitting .",analysis,Over-fitting,0,292,21,3,0,analysis : Over-fitting,0.9329073482428116,0.7241379310344828,0.2727272727272727,However our experimental results suggest that deep models are less prone to the problem of over fitting ,18,"Deep models have more parameters , and thus have a stronger ability to fit the large data set .","In , we show three results from models with a different depth on the English - to - French task .",result
natural_language_inference,37,"Therefore , the probability that token a from paragraph p starts an answer span is computed as :",method,Shared-Normalization,0,104,21,4,0,method : Shared-Normalization,0.4046692607003891,0.3888888888888889,0.5,Therefore the probability that token a from paragraph p starts an answer span is computed as ,17,"However , a modified objective function is used where the normalization factor in the softmax operation is shared between all paragraphs from the same context .","where P is the set of paragraphs thatare from the same context asp , and s ij is the score given to token i from paragraph j.",method
semantic_parsing,0,"In addition , we include Seq2Seq + Copying by adding an attention - based 34.1 19.6 11.7 3.3 18.3 18.4 TypeSQL 47.5 38.4 24.1 14.4 33.0 34.4 Data base Split Seq2 Seq 11.9 1.9 1.3 0.5 3.7 1.9 Seq2Seq+ Attention 26.2 12.6 6.6 1.3 12.4 10.9 TypeSQL 19.6 7.6 3.8 0.8 8.2 8.0 copying operation similar to ) .",method,SELECT COUNT(*) FROM cars_data WHERE cylinders > 4,0,227,16,10,0,method : SELECT COUNT(*) FROM cars_data WHERE cylinders > 4,0.8165467625899281,0.64,0.5263157894736842,In addition we include Seq2Seq Copying by adding an attention based 34 1 19 6 11 7 3 3 18 3 18 4 TypeSQL 47 5 38 4 24 1 14 4 33 0 34 4 Data base Split Seq2 Seq 11 9 1 9 1 3 0 5 3 7 1 9 Seq2Seq Attention 26 2 12 6 6 6 1 3 12 4 10 9 TypeSQL 19 6 7 6 3 8 0 8 8 2 8 0 copying operation similar to ,85,"Then , we also explore Seq2Seq + Attention from by adding an attention mechanism .",The original model does not take the schema into account because it has the same schema in both train and test .,method
question_answering,4,"On this dataset , DECAPROP outperforms the existing state - of - the - art , i.e. , the recent AMANDA model by ( + 4.7 % EM / + 2.6 % F1 ) .",result,Results,1,194,4,4,0,result : Results,0.7548638132295721,0.19047619047619047,0.4444444444444444,On this dataset DECAPROP outperforms the existing state of the art i e the recent AMANDA model by 4 7 EM 2 6 F1 ,25,Our re-implementation ) 71.9 79.6 DECAPROP ( This paper ) 72.9 81.4 QANet 73.6 82.7 News QA reports the results on News QA .,"Notably , AMANDA is a strong neural baseline that also incorporates gated self - attention layers , along with question - aware pointer layers .",result
natural_language_inference,0,"When tested with the feature it still gives an improvement , but the improvement is significant only with 100 % training data .",analysis,GA Reader Analysis,0,160,9,9,0,analysis : GA Reader Analysis,0.8121827411167513,0.375,0.375,When tested with the feature it still gives an improvement but the improvement is significant only with 100 training data ,21,"On CNN when tested without feature engineering , we observe that GA provides a significant boost in performance compared to without GA .","On WDW - Strict , which is a third of the size of CNN , without the feature we see an improvement when using GA versus without using GA , which becomes significant as the training set size increases .",result
text_generation,2,The embedding size of the LSTM - RNNs is set to 32 .,experiment,Experiment Settings,0,288,4,4,0,experiment : Experiment Settings,0.8228571428571428,0.5,0.5,The embedding size of the LSTM RNNs is set to 32 ,12,The goal dimension size k is set to 16 .,"For the discriminative model , we set the hyperparameters of the CNN as For synthetic data with length 40 , the learning rate for MANAGER and WORKER is set to 0.0005 .",experiment
sentiment_analysis,29,We train and evaluate our model on these two SemEval datasets separately .,model,Model Comparisons,0,130,2,2,0,model : Model Comparisons,0.7471264367816092,0.125,0.125,We train and evaluate our model on these two SemEval datasets separately ,13, ,We use accuracy metric to measure the performance .,method
text_summarization,2,"We want to investigate the effectiveness of these techniques on sentence summarization , which has not been explored in previous work .",approach,Learning Objective and Beam Search,0,154,103,3,0,approach : Learning Objective and Beam Search,0.5539568345323741,0.7862595419847328,0.0967741935483871,We want to investigate the effectiveness of these techniques on sentence summarization which has not been explored in previous work ,21,"We next describe our learning objective , including a coverage - based regularizer , and a beam search with reference mechanism .",Our training proceeds by minimizing a per-target - word cross - entropy loss function .,method
natural_language_inference,89,"Model - III outperforms all of other competitors , achieving a no-answer accuracy of 76.2 .",ablation,Ablation Study,0,198,9,9,0,ablation : Ablation Study,0.7615384615384615,0.39130434782608703,0.39130434782608703,Model III outperforms all of other competitors achieving a no answer accuracy of 76 2 ,16,details the results of various architectures for the answer verifier .,This illustrates that the combination of two different architectures can bring in further improvement .,result
sentiment_analysis,10,We evaluated our model primarily on textual modality .,system description,Modalities,0,166,20,2,0,system description : Modalities,0.6459143968871596,0.9523809523809524,0.6666666666666666,We evaluated our model primarily on textual modality ,9, ,"However , to substantiate efficacy of our model in multimodal scenario , we also experimented with multimodal features .",method
relation-classification,6,"Description Value We use pre-trained weights of the publicly available Glo Ve model to initialize word embeddings in our model , and other weights are randomly initialized from zero-mean Gaussian distribution .",implementation,Hyperparameter,0,147,5,2,0,implementation : Hyperparameter,0.8121546961325967,0.3333333333333333,0.16666666666666666,Description Value We use pre trained weights of the publicly available Glo Ve model to initialize word embeddings in our model and other weights are randomly initialized from zero mean Gaussian distribution ,33, ,compares our Entity - aware Attention LSTM model with state - of - theart models on this relation classification dataset .,experiment
natural_language_inference,26,The input data flow is depicted in .,model,Semantic Role Labeling,0,72,38,7,0,model : Semantic Role Labeling,0.33962264150943394,0.6333333333333333,1.0,The input data flow is depicted in ,8,"To disclose the multidimensional semantics , we group the semantic labels and integrate them with text embeddings in the next encoding component .", ,method
natural_language_inference,54,"Here x pt is the t th node in the syntactic sequence of the word p , which is a vector that uniquely identifies each syntactic node .",system description,Syntactic Sequence Encoding,0,100,45,6,0,system description : Syntactic Sequence Encoding,0.4366812227074236,0.7894736842105263,0.3333333333333333,Here x pt is the t th node in the syntactic sequence of the word p which is a vector that uniquely identifies each syntactic node ,27,"We can use a Bi-directional LSTM as our RNN encoder , where the hidden state v pt is updated according to Eq.","We obtain the structural embedding of the given word p , v p Bi - LSTM = v p T to be the final hidden state .",method
text_summarization,3,We implemented this baseline without its coverage mechanism since this is not our focus .,system description,Distant Supervision (DS) for Model Adaption,0,184,129,56,0,system description : Distant Supervision (DS) for Model Adaption,0.7603305785123967,0.9847328244274808,0.9655172413793104,We implemented this baseline without its coverage mechanism since this is not our focus ,15,Pointer - generator is an integrated pointer network and seq2seq model .,Baseline models also include two pointer - generator based extensions .,method
text_generation,1,"LSTM iteratively takes the embedded features of the current token wt plus the information in the hidden state h t?1 and the cell state c t?1 from previous stages , and updates the current states ht and ct .",architecture,Overall architecture,0,85,19,19,0,architecture : Overall architecture,0.3102189781021898,0.4130434782608696,0.8260869565217391,LSTM iteratively takes the embedded features of the current token wt plus the information in the hidden state h t 1 and the cell state c t 1 from previous stages and updates the current states ht and ct ,40,"In this paper , we design the generative model with the long short - term memory networks ( LSTMs ) .","Additionally , the subsequent word w t + 1 is conditionally sampled subjects to the probability distribution p ( w t + 1 |h t ) which is determined by the value of the current hidden state ht .",method
text-classification,4,The corresponding results are shown in ( c ) .,Filter visualization,Filter visualization,0,216,3,3,0,Filter visualization : Filter visualization,0.9773755656108596,0.375,0.375,The corresponding results are shown in c ,8,"To better understand what information has been encoded into our contextsensitive filters , we visualize one of the filters for sentences within the test set ( on the DBpedia dataset ) with t- SNE .","It can be observed that the filters for documents with the same label ( ontology ) are grouped into clusters , indicating that for different types of document , ACNN has leveraged distinct convolutional filters for better feature extraction .",others
natural_language_inference,0,"Given a token w from the document or query , its vector space representation is computed as x = L ( w ) | | C ( w ) .",model,Further Enhancements,0,98,37,4,0,model : Further Enhancements,0.4974619289340101,0.7708333333333334,0.26666666666666666,Given a token w from the document or query its vector space representation is computed as x L w C w ,22,"During the training phase , model parameters of GA are updated w.r.t. a cross - entropy loss between the predicted probabilities and the true answers .","( w ) retrieves the word - embedding for w from a lookup table L ? R | V |n l , whose rows hold a vector for each unique token in the vocabulary .",method
natural_language_inference,18,Experiments on Document Modelling,experiment,Experiments on Document Modelling,1,141,15,1,0,experiment : Experiments on Document Modelling,0.5164835164835165,0.1875,0.07142857142857142,Experiments on Document Modelling,4, , ,experiment
natural_language_inference,81,"We address the first subtask using BiDAF ++ , a competitive span extraction question answering model by and the second subtask using the CFC .",system description,RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA,0,138,5,5,0,system description : RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA,0.32779097387173395,0.2631578947368421,0.2631578947368421,We address the first subtask using BiDAF a competitive span extraction question answering model by and the second subtask using the CFC ,23,"GRUs , and the replacement of encoder GRUs with projection over word embeddings .","To compute the candidate list for reranking , we obtain the top 50 answer candidates from BiDAF ++.",method
natural_language_inference,47,Transfer learning with SICK,analysis,Transfer learning with SICK,0,195,27,1,0,analysis : Transfer learning with SICK,0.9069767441860463,0.675,0.07142857142857142,Transfer learning with SICK,4, , ,result
relation_extraction,9,"We conduct our experiments on the commonly used SemEval - 2010 Task 8 dataset , which contains 10,717 sentences for nine types of annotated relations , together with an additional "" Other "" type .",experiment,Experimental Setup,0,141,4,3,0,experiment : Experimental Setup,0.6746411483253588,0.14814814814814814,0.12,We conduct our experiments on the commonly used SemEval 2010 Task 8 dataset which contains 10 717 sentences for nine types of annotated relations together with an additional Other type ,31,Dataset and Metric .,"The nine types are : Cause - Effect , Component - Whole , Content - Container , Entity - Destination , Entity - Origin , Instrument - Agency , Member - Collection , Message - Topic , and Product - Producer , while the relation type "" Other "" indicates that the relation expressed in the sentence is not among the nine types .",experiment
sentiment_analysis,39,Price Safety Transit General LR - Mask ( n-gram + POS ) 0.940 0.960 0.879 0.864 LSTM - Final 0.875 0.932 0.836 0.869,model,Model,0,224,2,2,0,model : Model,0.9142857142857144,1.0,1.0,Price Safety Transit General LR Mask n gram POS 0 940 0 960 0 879 0 864 LSTM Final 0 875 0 932 0 836 0 869,27, , ,method
text-classification,1,"If a word embedding is appropriately pre-trained with unlabeled data , its inclusion is a form of semi-supervised learning and could be useful .",system description,Elimination of the word embedding layer,0,88,42,8,0,system description : Elimination of the word embedding layer,0.34375,0.21,0.8,If a word embedding is appropriately pre trained with unlabeled data its inclusion is a form of semi supervised learning and could be useful ,25,"Instead , demerits were evident - more meta-parameters to tune , poor accuracy with lowdimensional word vectors , and slow training / testing with high - dimensional word vectors as they are dense .","We will show later , however , that this type of approach falls behind our approach of learning region embeddings through training one - hot LSTM on unlabeled data .",method
sentiment_analysis,19,"Notably , the encoding of the suffix in the forward direction is biased towards tokens sequentially farther away to the right oft .",introduction,introduction,0,32,16,16,0,introduction : introduction,0.20915032679738568,0.4444444444444444,0.4444444444444444,Notably the encoding of the suffix in the forward direction is biased towards tokens sequentially farther away to the right oft ,22,"For each token t , we encode both its prefix and suffix in both the forward and reverse direction .","Similarly , the encoding of the prefix in the reverse direction is biased towards tokens sequentially farther away to the left oft .",introduction
natural_language_inference,9,Then the sentence representation x t is obtained by Position Encoder .,model,MODEL DETAILS,0,188,8,8,0,model : MODEL DETAILS,0.5696969696969697,0.10126582278481013,0.2352941176470588,Then the sentence representation x t is obtained by Position Encoder ,12,We use a trainable embedding matrix A ? R d V to encode the one - hot vector of each word x tj in each sentence x t into a d-dimensional vector x tj ? Rd .,The same encoder with the same embedding matrix is also used to obtain the question vector q from q.,method
natural_language_inference,64,"For example , the Transformer can be pre-trained on a large amount of data obtaining a powerful language model , which can then be specialized for solving specific NLP tasks by just adding new layers and training them on the target data .",system description,Related Work,0,27,8,8,0,system description : Related Work,0.10756972111553784,0.2424242424242425,0.2424242424242425,For example the Transformer can be pre trained on a large amount of data obtaining a powerful language model which can then be specialized for solving specific NLP tasks by just adding new layers and training them on the target data ,42,"Interestingly , the resulting models can be easily applied to solve different NLP applications by just fine - tuning them on the training data of the target tasks .","Although the approach is simple , the procedure for fine - tuning a Transformer - based model is not completely understood , and can result in high accuracy variance , with a possible on - off behavior , e.g. , models may always predict a single label .",method
sentiment_analysis,47,It illustrates target2 context attention is more important than context2target attention .,performance,The Effect of Rotatory Attention,0,196,44,10,0,performance : The Effect of Rotatory Attention,0.9116279069767442,0.7586206896551724,0.4166666666666667,It illustrates target2 context attention is more important than context2target attention ,12,"When the attention is further reduced , from , we can see that the performance of No-Attention drops significantly .","By comparing LCR - Rot and Attention - Reverse , we find that reversing the order of attention will cause 0.5 % - 1.5 % performance decrease on the datasets which proves the advantage of our rotatory attention .",result
temporal_information_extraction,0,"In particular , ( i ) TLINK labels for event - event ( E -E ) pairs , resulting from the rule - based sieve + temporal reasoner modules , are used as features for the CLINK classifier ; and ( ii ) CLINK labels ( i.e. CLINK and are used as a post-editing method for correcting the wrong labelled event pairs by the TLINK classifier .",architecture,architecture,0,25,8,8,0,architecture : architecture,0.13227513227513227,0.8,0.8,In particular i TLINK labels for event event E E pairs resulting from the rule based sieve temporal reasoner modules are used as features for the CLINK classifier and ii CLINK labels i e CLINK and are used as a post editing method for correcting the wrong labelled event pairs by the TLINK classifier ,55,"Although some steps can be run in parallel , the two modules interact , based on the assumption that the notion of causality is tightly connected with the temporal dimension and that information from one module can be used to improve or check the consistency of the other .","This step relies on a set of rules based on the temporal constraint of causality , i.e. ( i ) CLINK ( e 1 , e 2 ) ? BEFORE ( e 1 , e 2 ) and ( ii ) CLINK - R ( e 1 , e 2 ) ? AFTER ( e 1 , e 2 ) .",method
text-to-speech_synthesis,0,"It can be seen from that our method can still reach high accuracy with 1 - 1 layer of encoder - decoder , which can significantly reduce the model size by nearly 6 times and the time cost by nearly 4 times compared with the baseline model , but still achieving higher accuracy in terms of WER .",result,Achieving State-Of-The-Art Accuracy,0,142,13,12,0,result : Achieving State-Of-The-Art Accuracy,0.8765432098765432,0.9285714285714286,0.9230769230769232,It can be seen from that our method can still reach high accuracy with 1 1 layer of encoder decoder which can significantly reduce the model size by nearly 6 times and the time cost by nearly 4 times compared with the baseline model but still achieving higher accuracy in terms of WER ,54,"To compare the inference speed , we use the time consumed by generating the outputs of the test set ( 12855 words ) on a single M40 GPU with 12000 max tokens in one mini-batch .",The reduction in model size and inference time cost demonstrate the effectiveness of our method for online deployment .,result
natural_language_inference,52,C ) A pine tree knocked sideways in a landslide grows upward in a bend .,introduction,introduction,0,17,8,8,0,introduction : introduction,0.06513409961685822,0.2222222222222222,0.2222222222222222,C A pine tree knocked sideways in a landslide grows upward in a bend ,15,B ) A cucumber tendril wraps around a wire .,D ) Guard cells of a tomato plant leaf close when there is little water in the roots .,introduction
sentiment_analysis,2,"Here , d p denotes the dimension of position embedding and N indicates the length of the sentence .",model,Position Representation,0,61,12,9,0,model : Position Representation,0.2687224669603524,0.2033898305084746,0.6923076923076923,Here d p denotes the dimension of position embedding and N indicates the length of the sentence ,18,"And its corresponding position embedding are obtained by looking up a position embedding matrix P ? R dpN , which is randomly initialized , and updated during the training process .","After the position index is converted to the embedding , the position embedding can model the different weights of words with different distance .",method
sentiment_analysis,48,"To verify if sharing embedding will benefit , we also conducted experiments with sharing embedding , as illustrated in 4 .",ablation,Effect of Sharing Embeddings,0,215,13,7,0,ablation : Effect of Sharing Embeddings,0.9110169491525424,0.52,0.7777777777777778,To verify if sharing embedding will benefit we also conducted experiments with sharing embedding as illustrated in 4 ,19,This implementation guarantees that the improvement does not come from learning to generate .,The results indicate that the joint training for the embedding layer is negative for improving the performance in this task .,result
part-of-speech_tagging,5,"To keep our results comparable with the Shared Task , we use the provided precomputed word embeddings .",data set,Data Sets,0,128,7,7,0,data set : Data Sets,0.6336633663366337,0.5384615384615384,0.5384615384615384,To keep our results comparable with the Shared Task we use the provided precomputed word embeddings ,17,We use the training treebank for training only and the development sets for hyperparameter tuning and early stopping .,We excluded Gothic from our experiments as the available downloadable content did not include embeddings for this language .,experiment
sentiment_analysis,50,"have experienced no problems , [ works ] pos as anticipated .",analysis,Analysis,0,148,27,27,0,analysis : Analysis,0.9135802469135802,0.75,0.75,have experienced no problems works pos as anticipated ,9,"Thirdly , we find that LSTM + ATT made a number of errors on sentences with negation words :",Service ] neg not the friendliest to our party !,result
relation_extraction,11,Relevant side information can be effective for improving RE .,introduction,introduction,0,28,17,17,0,introduction : introduction,0.1129032258064516,0.5,0.5,Relevant side information can be effective for improving RE ,10,"However , all the above models rely only on the noisy instances from distant supervision for RE .","For instance , in the sentence , Microsoft was started by Bill Gates. , the type information of Bill Gates ( person ) and Microsoft ( organization ) can be helpful in predicting the correct relation founder OfCompany .",introduction
semantic_role_labeling,2,"We independently predict whether each word in the sentence is a predicate , using a binary softmax over the outputs of a bidirectional LSTM trained to maximize the likelihood of the gold labels .",model,Predicate Detection,0,79,43,5,0,model : Predicate Detection,0.35267857142857145,1.0,1.0,We independently predict whether each word in the sentence is a predicate using a binary softmax over the outputs of a bidirectional LSTM trained to maximize the likelihood of the gold labels ,33,Then each predicate in v is used as an input to argument prediction ., ,method
sentiment_analysis,1,"In addition , our Emotion DL can be easily adopted by other deep learning models .",ablation,Ablation Study,0,359,14,14,0,ablation : Ablation Study,0.9065656565656566,1.0,1.0,In addition our Emotion DL can be easily adopted by other deep learning models ,15,This performance gain validates our assumption that participants are not always generating the intended emotions when watching emotion - eliciting stimuli ., ,result
natural_language_inference,21,method used in is applied to the input sequence to encode temporal information .,experiment,Word Embedding with s2t self-attention: DiSAN with,0,210,36,7,0,experiment : Word Embedding with s2t self-attention: DiSAN with,0.7241379310344828,0.7058823529411765,0.3181818181818182,method used in is applied to the input sequence to encode temporal information ,14,The positional encoding 3 https://nlp.stanford.edu/projects/snli/,"We find our experiments show that multi-head attention is sensitive to hyperparameters , so we adjust keep probability of dropout from 0.7 to 0.9 with step 0.05 and report the best result .",experiment
natural_language_inference,9,"Then we obtain z t , rt ? Rd ( instead of z t , rt ? R ) , and these can be element - wise multiplied ( ) instead of being broadcasted in Equation 3 and 6 .",model,EXTENSIONS,0,122,82,15,0,model : EXTENSIONS,0.3696969696969697,0.8541666666666666,0.9375,Then we obtain z t rt Rd instead of z t rt R and these can be element wise multiplied instead of being broadcasted in Equation 3 and 6 ,30,"For vector gates , we modify the row dimension of weights and biases in Equation 1 and 5 from 1 to d .","Then we obtain z t , rt ? Rd ( instead of z t , rt ? R ) , and these can be element - wise multiplied ( ) instead of being broadcasted in Equation 3 and 6 .",method
sentiment_analysis,45,"Then we add a classification layer whose parameter matrix is W ? R KH , where K is the number of categories .",methodology,Fine-tuning procedure,0,77,43,5,0,methodology : Fine-tuning procedure,0.5347222222222222,0.7962962962962963,0.7142857142857143,Then we add a classification layer whose parameter matrix is W R KH where K is the number of categories ,21,We denote the vector as C ? R H .,"Then we add a classification layer whose parameter matrix is W ? R KH , where K is the number of categories .",method
question_answering,4,"The key goals of this module are to ( 1 ) connect any two layers of P/Q in the network , returning a residual feature that can be propagated 1 to deeper layers , ( 2 ) model cross-hierarchical interactions between P/Q and ( 3 ) minimize any costs incurred to other network components such that this component maybe executed multiple times across all layers .",system description,Bidirectional Attention Connectors (BAC),0,47,4,4,0,system description : Bidirectional Attention Connectors (BAC),0.1828793774319066,0.038461538461538464,0.10256410256410256,The key goals of this module are to 1 connect any two layers of P Q in the network returning a residual feature that can be propagated 1 to deeper layers 2 model cross hierarchical interactions between P Q and 3 minimize any costs incurred to other network components such that this component maybe executed multiple times across all layers ,61,The BAC module can bethought of as a connector component that connects two sequences / layers .,Let P ? R p d and Q ? R q d be inputs to the BAC module .,method
text-classification,7,for all capsule i in layer land capsule j in,model,Coefficients Amendment,0,97,59,4,0,model : Coefficients Amendment,0.3991769547325103,0.6276595744680851,0.3076923076923077,for all capsule i in layer land capsule j in,10,"Algorithm 1 : Dynamic Routing Algorithm 1 procedure ROUTING ( j|i , j|i , r , l ) 2 Initialize the logits of coupling coefficients b j|i = 0 3 for r iterations do 4 for all capsule i in layer land capsule j in layer l + 1 :","Given each prediction vector j|i and its probability of existence j|i , where j|i = i , each iterative coupling coefficient of connection strength c j|i is updated by",method
relation-classification,8,This section describes the proposed one - pass encoding MRE solution .,approach,Proposed Approach,1,29,2,2,0,approach : Proposed Approach,0.2116788321167883,0.057142857142857134,0.4,This section describes the proposed one pass encoding MRE solution ,11, ,"The solution is built upon BERT with a structured prediction layer to enable BERT to predict multiple relations with onepass encoding , and an entity - aware self - attention mechanism to infuse the relational information with regard to multiple entities at each layer of hidden states .",method
semantic_role_labeling,4,"Finally , using h s , f label calculates the score for the span s = ( i , j ) with a label r. Each function in Eqs. 4 , 5 and 6 can arbitrarily be defined .",system description,Scoring Function,0,70,35,11,0,system description : Scoring Function,0.2333333333333333,0.660377358490566,0.3793103448275862,Finally using h s f label calculates the score for the span s i j with a label r Each function in Eqs 4 5 and 6 can arbitrarily be defined ,32,"Function F ? Function F ? in Eq. 2 consists of three types of functions ; the base feature function f base , the span feature function f span and the labeling function f label as follows , Firstly , f base calculates a base feature vector ht for each word wt ? w 1:T . Then , from a sequence of the base feature vectors h 1:T , f span calculates a span feature vector h s for a span s = ( i , j ) .","In Section 3 , we describe our functions used in this paper .",method
sentiment_analysis,1,"Specifically , given source / training data XS ? RN nd ( in this subsection , we denote X by XS for better clarity ) and unlabelled target / testing data X T ? RN nd , wherein practice X T can be either oversampled or donwsampled to have the same number of samples as XS , the domain classifier aims to minimize the sum of the following two binary cross - entropy losses :",system description,Node-wise Domain Adversarial Training,0,227,155,7,0,system description : Node-wise Domain Adversarial Training,0.5732323232323232,0.7487922705314008,0.3684210526315789,Specifically given source training data XS RN nd in this subsection we denote X by XS for better clarity and unlabelled target testing data X T RN nd wherein practice X T can be either oversampled or donwsampled to have the same number of samples as XS the domain classifier aims to minimize the sum of the following two binary cross entropy losses ,64,"During optimization , our model aims to confuse the domain classifier by learning domain - invariant representations for each node .","Specifically , given source / training data XS ? RN nd ( in this subsection , we denote X by XS for better clarity ) and unlabelled target / testing data X T ? RN nd , wherein practice X T can be either oversampled or donwsampled to have the same number of samples as XS , the domain classifier aims to minimize the sum of the following two binary cross - entropy losses :",method
named-entity-recognition,6,The two systems that have slightly higher F 1 scores on the CONLL dataset both use embeddings obtained from a forward and a backward Language Model trained on the One Billion Word Benchmark .,training,Training and Implementation,0,161,24,24,0,training : Training and Implementation,0.7594339622641509,0.7272727272727273,0.7741935483870968,The two systems that have slightly higher F 1 scores on the CONLL dataset both use embeddings obtained from a forward and a backward Language Model trained on the One Billion Word Benchmark ,34,They report a performance of 90.43 when using an architecture similar to ours .,"They report gains between 0.8 and 1.2 points by using such LM embeddings , which suggests that LS vectors are indeed efficient .",experiment
relation_extraction,9,"However , for each of the aforementioned relation types , the two entities can also appear in inverse order , which implies that the sentence needs to be regarded as expressing a different relation , namely the respective inverse one .",experiment,Experimental Setup,0,143,6,5,0,experiment : Experimental Setup,0.6842105263157895,0.2222222222222222,0.2,However for each of the aforementioned relation types the two entities can also appear in inverse order which implies that the sentence needs to be regarded as expressing a different relation namely the respective inverse one ,37,"The nine types are : Cause - Effect , Component - Whole , Content - Container , Entity - Destination , Entity - Origin , Instrument - Agency , Member - Collection , Message - Topic , and Product - Producer , while the relation type "" Other "" indicates that the relation expressed in the sentence is not among the nine types .","For example , Cause - Effect ( e 1 , e 2 ) and Cause - Effect ( e 2 ,e 1 ) can be considered two distinct relations , so the total number | Y | of relation types is 19 .",experiment
sentiment_analysis,20,We pass the representation r to a Maxout layer to make the final comparison between the tweet aspects and the topic aspects .,model,TSA Model (topic-based),0,124,48,22,0,model : TSA Model (topic-based),0.6666666666666666,0.6956521739130435,0.88,We pass the representation r to a Maxout layer to make the final comparison between the tweet aspects and the topic aspects ,23,"where W h , b hand uh are jointly learned weights .",We selected Maxout as it amplifies the effects of dropout ( Section 3.3 ) .,method
sentiment_analysis,32,Our IAN model takes a further step towards emphasizing the importance of targets through learning target and context representation interactively .,model,Model Comparisons,0,152,23,23,0,model : Model Comparisons,0.6608695652173913,0.7419354838709677,0.7419354838709677,Our IAN model takes a further step towards emphasizing the importance of targets through learning target and context representation interactively ,21,"Compared with AE - LSTM , ATAE - LSTM especially enhance the interaction between the context words and target and thus has a better performance than AE - LSTM .",We can see that IAN achieves the best performance among all baselines .,method
sentiment_analysis,38,The performance of our model is noticeably lopsided .,experiment,Experimental Setup and Results,0,94,18,18,0,experiment : Experimental Setup and Results,0.5595238095238095,0.8181818181818182,0.8181818181818182,The performance of our model is noticeably lopsided ,9,shows the results of our model on 4 standard text classification datasets .,On the MR and CR ) sentiment analysis datasets we improve the state of the art by a significant margin .,experiment
natural_language_inference,92,"Finally , we experiment on the weakly - supervised setting of WIKISQL , in which only the question & answer pair is used and the SQL query ( z ) is treated as a latent variable .",method,SQL Query Generation,0,141,88,3,0,method : SQL Query Generation,0.4895833333333333,0.8979591836734694,0.2307692307692308,Finally we experiment on the weakly supervised setting of WIKISQL in which only the question answer pair is used and the SQL query z is treated as a latent variable ,31,"We can see that one equation is correct , while the others are false positives which coincidentally lead to the correct answer .","(z i | Q , H ) can be computed by any query generation or semantic parsing models .",method
natural_language_inference,69,chain textual background resources for science exam QA and provide multisentence answer explanations .,system description,Compositional Knowledge Base Inference,0,310,15,15,0,system description : Compositional Knowledge Base Inference,0.8985507246376812,0.625,0.75,chain textual background resources for science exam QA and provide multisentence answer explanations ,14,have demonstrated that exploit - ing information from other related documents based on lexical semantic similarity is beneficial for reranking answers in open - domain non-factoid QA .,"Beyond , a rich collection of neural models tailored towards multi-step RC has been developed .",method
natural_language_inference,15,"Thus , we could cautiously interpret the classification results using our attentive weights and max - pooled positions .",ablation,ablation,0,176,25,25,0,ablation : ablation,0.7787610619469026,0.6097560975609756,0.6097560975609756,Thus we could cautiously interpret the classification results using our attentive weights and max pooled positions ,17,Word Alignment and Importance Our denselyconnected recurrent and co-attentive features are connected to the classification layer through the max pooling operation such that all max - valued features of every layer affect the loss function and perform a kind of deep supervision .,The attentive weights contain information on how two sentences are aligned and the numbers of max - pooled positions in each dimension play an important role in classification .,result
text_generation,0,The vanishing and exploding gradient problem in backpropagation through time ( BPTT ) issues a challenge of learning longterm dependencies to recurrent neural network .,implementation,Model Implementations,0,277,8,8,0,implementation : Model Implementations,0.8549382716049383,0.16326530612244894,0.21621621621621626,The vanishing and exploding gradient problem in backpropagation through time BPTT issues a challenge of learning longterm dependencies to recurrent neural network ,23,where the parameters are a bias vector c and a weight matrix V .,"To address such problems , gated RNNs have been designed based on the basic idea of creating paths through time that have derivatives that neither vanish nor explode .",experiment
natural_language_inference,11,"We first compute a weighted , n-dimensional representationq of the processed questionQ ( Eq. 8 ) .",implementation,Recognizing Textual Entailment,0,217,20,7,0,implementation : Recognizing Textual Entailment,0.7862318840579711,0.2531645569620253,0.1891891891891892,We first compute a weighted n dimensional representationq of the processed questionQ Eq 8 ,15,Our prediction - or answer layer is similar to .,"Result variations were small , that is within less than a percentage point in all experiments .",experiment
sentiment_analysis,12,"Besides , CPT layers are also equipped with a Context - Preserving Mechanism ( CPM ) to preserve the context information and learn more abstract word - level features .",system description,Our Approach,0,58,4,4,0,system description : Our Approach,0.25892857142857145,0.3636363636363637,0.3636363636363637,Besides CPT layers are also equipped with a Context Preserving Mechanism CPM to preserve the context information and learn more abstract word level features ,25,"It contains another Bi - LSTM for generating v( t ) via an attention mechanism , and then incorporates v( t ) into the word representations .","In the end , we obtain the word - level semantic representations",method
natural_language_inference,62,"We name each semantic relation in a semantic relation chain as a hop , therefore the above semantic relation chain is a 3 - hop chain .",method,method,0,59,14,14,0,method : method,0.26339285714285715,0.28,0.9333333333333332,We name each semantic relation in a semantic relation chain as a hop therefore the above semantic relation chain is a 3 hop chain ,25,"For example , the synset "" keratin .n.01 "" is related to the synset "" feather .n.01 "" through the semantic relation "" substance holonym "" , the synset "" feather .n.01 "" is related to the synset "" bird .n.01 "" through the semantic relation "" part holonym "" , and the synset "" bird .n.01 "" is related to the synset "" parrot .n.01 "" through the semantic relation "" hyponym "" , thus "" substance holonym ? part holonym ? hyponym "" is a semantic relation chain , which links the synset "" keratin .n.01 "" to the synset "" parrot .n.01 "" .","By the way , each single semantic relation is equivalent to a 1 - hop chain .",method
sentiment_analysis,22,Hierarchical Attention Based Fusion Layer,system description,system description,0,91,1,1,0,system description : system description,0.3973799126637553,0.038461538461538464,0.038461538461538464,Hierarchical Attention Based Fusion Layer,5, , ,method
text_summarization,0,"Then , we treat the decoder attention weights as the focused aspect of the generated summary , a.k.a. , "" decoder focused aspect "" .",introduction,introduction,1,54,39,39,0,introduction : introduction,0.17940199335548174,0.7647058823529411,0.7647058823529411,Then we treat the decoder attention weights as the focused aspect of the generated summary a k a decoder focused aspect ,22,"We first calculate alignment between the reader comments words and document words , and this alignment information is regarded as reader attention representing the "" reader focused aspect "" .","After each decoding step , a supervisor is designed to measure the distance between the reader focused aspect and the decoder focused aspect .",introduction
natural_language_inference,50,"Additionally , it is good to consider that the base neural encoder employed also contributes to the computational cost of these neural ranking models , e.g. , LSTM networks are known to be over-parameterized and also incur a parameter and runtime cost of quadratic scale .",abstract,abstract,0,22,20,20,0,abstract : abstract,0.0694006309148265,0.43478260869565216,0.43478260869565216,Additionally it is good to consider that the base neural encoder employed also contributes to the computational cost of these neural ranking models e g LSTM networks are known to be over parameterized and also incur a parameter and runtime cost of quadratic scale ,45,"Notably , these new innovations come with trade - offs such as huge computational cost that lead to significantly longer training times and also a larger memory footprint .","It also seems to be a well - established fact that a neural encoder ( such as the LSTM , Gated Recurrent Unit ( GRU ) , CNN , etc. ) must be first selected for learning individual representations of questions and answers and is generally treated as mandatory for good performance .",abstract
natural_language_inference,73,"Specifically , a hard attention mechanism is used to encode rich structural information about the contextual dependencies and trims along sequence into a much shorter one for a soft attention mechanism to process .",introduction,introduction,0,31,19,19,0,introduction : introduction,0.1183206106870229,0.5588235294117647,0.5588235294117647,Specifically a hard attention mechanism is used to encode rich structural information about the contextual dependencies and trims along sequence into a much shorter one for a soft attention mechanism to process ,33,"However , soft and hard attention mechanisms might be integrated into a single model to benefit each other in overcoming their inherent dis advantages , and this notion motivates our study .","Conversely , the soft one is used to provide a stable environment and strong reward signals to help in training the hard one .",introduction
named-entity-recognition,4,for each biLSTM layer .,model,ELMo,0,63,16,5,0,model : ELMo,0.23161764705882354,0.27586206896551724,0.4166666666666667,for each biLSTM layer ,5,"where h LM k ,0 is the token layer and h LM","For inclusion in a downstream model , ELMo collapses all layers in R into a single vector , ELMo k = E(R k ; ? e ) .",method
machine-translation,1,We aim at building neural language and translation models that capture the desiderata set out in Sect. 2.1 .,model,Desiderata,0,73,23,12,0,model : Desiderata,0.3631840796019901,0.3770491803278688,0.24,We aim at building neural language and translation models that capture the desiderata set out in Sect 2 1 ,20,Shorter paths whose length is largely decoupled from the sequence distance between the two tokens have the potential to better propagate the signals and to let the network learn long - range dependencies more easily .,The proposed ByteNet architecture is composed of a decoder that is stacked on an encoder ( Sect. 3.1 ) and generates variable - length outputs via dynamic unfolding ( Sect. 3.2 ) .,method
sentiment_analysis,48,IAN : IAN adopts two LSTMs to derive the representations of the context and the target phrase interactively and the concatenation is fed to the softmax layer .,system description,Model Configuration & Classifiers,1,166,18,18,0,system description : Model Configuration & Classifiers,0.7033898305084746,0.5625,0.5625,IAN IAN adopts two LSTMs to derive the representations of the context and the target phrase interactively and the concatenation is fed to the softmax layer ,27,"MemNet : It uses the attention mechanism over the word embedding over multiple rounds to aggregate the information in the sentence , the vector of the final round is used for the prediction .",http://nlp.stanford.edu/data/glove.8B.300d.zip,method
topic_models,0,Relation between SMM and sparse additive generative model ( SAGE ) was explained in .,model,model,0,254,5,5,0,model : model,0.616504854368932,0.2,0.2,Relation between SMM and sparse additive generative model SAGE was explained in ,13,"regularization over T for SMM ( 1 SMM ) was observed to yield better results when compared to LDA , STC and 2 regularized SMM ( 2 SMM ) .","In , the authors proposed an algorithm to obtain sparse document embeddings ( called sparse composite document vector ( SCDV ) ) from pre-trained word embeddings .",method
relation_extraction,12,"In fact , such strategies can also be viewed as a form of hard attention , where edges that connect nodes not on the resulting subtree will be directly assigned zero weights ( not attended ) .",system description,Attention Guided Layer,0,87,32,7,0,system description : Attention Guided Layer,0.2644376899696049,0.3232323232323233,0.21212121212121213,In fact such strategies can also be viewed as a form of hard attention where edges that connect nodes not on the resulting subtree will be directly assigned zero weights not attended ,33,"They prune the full tree into a subtree , based on which the adjacency matrix is constructed .",Such strategies might eliminate relevant information from the original dependency tree .,method
part-of-speech_tagging,3,For many sequence labeling tasks it is beneficial to have access to both past ( left ) and future ( right ) contexts .,architecture,BLSTM,0,55,24,13,0,architecture : BLSTM,0.270935960591133,0.4897959183673469,0.7647058823529411,For many sequence labeling tasks it is beneficial to have access to both past left and future right contexts ,20,It should be noted that we do not include peephole connections in the our LSTM formulation .,"However , the LSTM 's hidden state ht takes information only from past , knowing nothing about the future .",method
sentiment_analysis,35,"1 ) To prevent early overfitting of the target domain , the source network S is individually trained on the source domain by optimizing L s sen + L aux + ?L s reg .",training,training,0,165,8,8,0,training : training,0.6653225806451613,0.8,0.8,1 To prevent early overfitting of the target domain the source network S is individually trained on the source domain by optimizing L s sen L aux L s reg ,31,The whole training procedure consists of two stages :,"Then , Sand the BiLSTM , C2A , and PaS modules of S are used to initialize the source and target networks of the MGAN , respectively .",experiment
text_summarization,4,": correct and relevant , B : correct and somewhat relevant , C : correct but irrelevant , and D : incorrect .",system description,Possible issues,0,69,30,14,0,system description : Possible issues,0.26744186046511625,0.967741935483871,0.9333333333333332, correct and relevant B correct and somewhat relevant C correct but irrelevant and D incorrect ,17,"In our experimental data , we randomly select 100 data instances and tag the correctness and relevance of extracted entities into one of four labels :","Results show that 29.4 % , 13.7 % , 30.0 % , and 26.9 % are tagged with A , B , C , and D , respectively , which shows that there is a large amount of incorrect and irrelevant entities .",method
natural_language_inference,28,One way to interpret the importance of the diagonal contrast is that each neuron in the hidden state plays an important role for learning since each element on the diagonal activates a distinct neuron .,analysis,VISUAL ANALYSIS,0,227,9,9,0,analysis : VISUAL ANALYSIS,0.8376383763837638,0.2,0.5625,One way to interpret the importance of the diagonal contrast is that each neuron in the hidden state plays an important role for learning since each element on the diagonal activates a distinct neuron ,35,"hh , but it is contrasted less ) .","Therefore , it seems that RUM utilizes the capacity of the hidden state almost completely .",result
question_answering,5,"We then treat the new context as one sequence to represent the answer , and build an attention - based match - LSTM model between the sequence and the question to measure how well the new aggregated context could entail the question .",introduction,introduction,0,46,32,32,0,introduction : introduction,0.16666666666666666,0.8648648648648649,0.8648648648648649,We then treat the new context as one sequence to represent the answer and build an attention based match LSTM model between the sequence and the question to measure how well the new aggregated context could entail the question ,40,The result is a new context that aggregates all the evidence necessary to entail the answer for the question .,"Overall , our contributions are as follows :",introduction
prosody_prediction,0,"Briefly , the method consists of 1 ) the extraction of pitch and energy signals from the speech data and duration from the word level alignments , 2 ) filling the unvoiced gaps in extracted signals by interpolation followed by smoothing and normalizing , 3 ) combining the normalized signals by summing or multiplication , and 4 ) performing a continuous wavelet transform ( CWT ) on the composite signal and extracting continuous prominence values as lines of maximum amplitude across wavelet scales ( see ) .",dataset,Dataset,0,69,12,12,0,dataset : Dataset,0.359375,0.5217391304347826,0.5217391304347826,Briefly the method consists of 1 the extraction of pitch and energy signals from the speech data and duration from the word level alignments 2 filling the unvoiced gaps in extracted signals by interpolation followed by smoothing and normalizing 3 combining the normalized signals by summing or multiplication and 4 performing a continuous wavelet transform CWT on the composite signal and extracting continuous prominence values as lines of maximum amplitude across wavelet scales see ,75,"For the annotation , we used the Wavelet Prosody Analyzer toolkit 2 , which implements the method described in .","Essentially , the method assumes that the louder , the longer , and the higher , the more prominent .",experiment
paraphrase_generation,1,"Due to the importance of the paraphrase generation task in QA system , we perform a comprehensive evaluation of our proposed model on the recently released Quora questions dataset 1 , and demonstrates its effectiveness for the task of question paraphrase generation through both quantitative metrics , as well as qualitative analysis .",introduction,introduction,0,40,29,29,0,introduction : introduction,0.1809954751131221,0.9666666666666668,0.9666666666666668,Due to the importance of the paraphrase generation task in QA system we perform a comprehensive evaluation of our proposed model on the recently released Quora questions dataset 1 and demonstrates its effectiveness for the task of question paraphrase generation through both quantitative metrics as well as qualitative analysis ,50,"We compare our framework with various sophisticated sequence - to - sequence models including the state - of - the - art stacked residual model for paraphrase generation , and show its efficacy on benchmark datasets , on which it outperforms the stateof - the - art by significant margins .","Human evaluation indicate that the paraphrases generated by our system are well - formed , and grammatically correct for the most part , and are able to capture new concepts related to the input sentence .",introduction
natural_language_inference,83,"Given a sequence of frames evoked in the context , such a trained language model can then be used to get the conditional probabilities of the frame ( s ) evoked in each of the two ending - options .",system description,Measuring Consistency,0,80,25,19,0,system description : Measuring Consistency,0.26143790849673204,0.4545454545454545,0.8636363636363636,Given a sequence of frames evoked in the context such a trained language model can then be used to get the conditional probabilities of the frame s evoked in each of the two ending options ,36,We train this language model using a log bilinear language model ) on a collection of unannotated short stories ( see Sec. 3.1 ) and also 20 years of New York Times data 1 .,The option with more probable frame ( s ) is likely to be the appropriate ending .,method
sentiment_analysis,47,"IAN , GRNN - G3 and Mem - Net show different advantages in different datasets .",performance,System Performance Comparison,0,164,12,12,0,performance : System Performance Comparison,0.7627906976744186,0.206896551724138,0.7058823529411765,IAN GRNN G3 and Mem Net show different advantages in different datasets ,13,"Because of the introduction of attention mechanism , AE - LSTM and ATAE - LSTM achieve better results than TD - LSTM , and ATAE - LSTM is slightly better among the two .","MemNet achieves better results than other models on the Restaurant dataset , since it considers not only the contexts of targets but also the position of each context word related to the target .",result
natural_language_inference,94,"With these in mind , we propose the Multi - Hop Pointer - Generator Model ( MHPGM ) baseline , a novel combination of previous works with the following major components :",method,method,0,74,6,6,0,method : method,0.1932114882506528,0.047244094488188976,0.047244094488188976,With these in mind we propose the Multi Hop Pointer Generator Model MHPGM baseline a novel combination of previous works with the following major components ,26,It would also need to be able to generate coherent statements to answer complex questions while having the ability to copy rare words such as specific entities from the reading context .,Embedding Layer : The tokens are embedded into both learned word embeddings and pretrained context - aware embeddings ( ELMo ) .,method
natural_language_inference,95,"Second , we model the meanings of the answer candidates extracted from those passages and use the content scores to measure the quality of the candidates from a second perspective .",introduction,introduction,1,53,45,45,0,introduction : introduction,0.2264957264957265,0.7758620689655172,0.7758620689655172,Second we model the meanings of the answer candidates extracted from those passages and use the content scores to measure the quality of the candidates from a second perspective ,30,"First , we follow the boundary - based MRC models to find an answer candidate for each passage by identifying the start and end position of the answer ( .","Third , we conduct the answer verification by enabling each answer candidate to attend to the other candidates based on their representations .",introduction
text-classification,1,"Comparing the two types of LSTM in , we see that our one - hot bidirectional LSTM with pooling ( oh - 2 LSTMp ) outperforms word - vector LSTM ( wv - LSTM ) on all the datasets , confirming the effectiveness of our approach .",system description,Experiments (supervised),1,139,93,16,0,system description : Experiments (supervised),0.54296875,0.465,0.26666666666666666,Comparing the two types of LSTM in we see that our one hot bidirectional LSTM with pooling oh 2 LSTMp outperforms word vector LSTM wv LSTM on all the datasets confirming the effectiveness of our approach ,37,"The pooling settings chosen based on the performance on the development data are the same as JZ15a , which are max - pooling with k= 1 on IMDB and Elec and average - pooling with k=10 on RCV1 ; on 20 NG , max - pooling with k = 10 was chosen .",Now we review the non -LSTM baseline methods .,method
natural_language_inference,81,"Sites may range from those with few or no remains visible above ground , to buildings and other structures still in use .",APPENDIX,Answer town,0,315,107,27,0,APPENDIX : Answer town,0.7482185273159145,0.5023474178403756,0.20300751879699247,Sites may range from those with few or no remains visible above ground to buildings and other structures still in use ,22,"An archaeological site is a place ( or group of physical sites ) in which evidence of past activity is preserved ( either prehistoric or historic or contemporary ) , and which has been , or maybe , investigated using the discipline of archaeology and represents apart of the archaeological record .","Nuussuaq Peninsula ( old spelling : "" Ngssuaq "" ) is a large ( 180 x 48 km ) peninsula in western Greenland .",others
sentiment_analysis,40,"Finally , it takes the hidden states of LSTM at last time step to represent the context for prediction .",method,Compared Methods,0,156,10,10,0,method : Compared Methods,0.6995515695067265,0.5882352941176471,0.5882352941176471,Finally it takes the hidden states of LSTM at last time step to represent the context for prediction ,19,TD- LSTM : It uses a forward LSTM and a backward LSTM to abstract the information before and after the target .,"We reproduce its results on the tweet dataset with our embeddings , and also run it for the other three datasets .",method
natural_language_inference,58,"With a question in mind , ReasoNets read a document repeatedly , each time focusing on di erent parts of the document until a satisfying answer is found or formed .",introduction,introduction,1,35,26,26,0,introduction : introduction,0.1044776119402985,0.6666666666666666,0.6666666666666666,With a question in mind ReasoNets read a document repeatedly each time focusing on di erent parts of the document until a satisfying answer is found or formed ,29,which tries to mimic the inference process of human readers .,"This reminds us of a Chinese proverb : "" The meaning of a book will become clear if you read it hundreds of times . "" .",introduction
natural_language_inference,65,"Next , the attention vectors are used to perform pooling .",introduction,introduction,1,18,10,10,0,introduction : introduction,0.0782608695652174,0.30303030303030304,0.30303030303030304,Next the attention vectors are used to perform pooling ,10,"The main idea in AP consists of learning a similarity measure over projected segments ( e.g. trigrams ) of the two items in the input pair , and using the similarity scores between the segments to compute attention vectors in both directions .",There are a few key benefits of our model .,introduction
sentiment_analysis,1,"One subtle difference between our model and other models is that our model performs consistently better in gamma band than beta band , whereas other models perform comparably in both bands , indicating that gamma band maybe the most discriminative band for our model .",evaluation,Performance Comparison of Frequency Bands,1,335,19,9,0,evaluation : Performance Comparison of Frequency Bands,0.8459595959595959,0.7037037037037037,1.0,One subtle difference between our model and other models is that our model performs consistently better in gamma band than beta band whereas other models perform comparably in both bands indicating that gamma band maybe the most discriminative band for our model ,43,"This observation is consistent with the literature , .", ,result
natural_language_inference,94,"We calculate a selection distribution p sel ? R 2 , where p sel 1 is the probability of generating a token from P gen and p sel 2 is the probability of copying a word from the context :",method,method,0,114,46,46,0,method : method,0.2976501305483029,0.3622047244094488,0.3622047244094488,We calculate a selection distribution p sel R 2 where p sel 1 is the probability of generating a token from P gen and p sel 2 is the probability of copying a word from the context ,38,We utilize a pointer mechanism that allows the decoder to directly copy tokens from the context based on ? i .,"We calculate a selection distribution p sel ? R 2 , where p sel 1 is the probability of generating a token from P gen and p sel 2 is the probability of copying a word from the context :",method
question-answering,1,"This pooling has different mechanism as in the 1D case , for it selects not only among compositions on different segments but also among different local matchings .",model,Training,0,100,41,12,0,model : Training,0.5181347150259067,0.8913043478260869,0.7058823529411765,This pooling has different mechanism as in the 1D case for it selects not only among compositions on different segments but also among different local matchings ,27,where ? concatenates the corresponding vectors from its 2 D receptive field in Layer -?1 .,"This pooling strategy resembles the dynamic pooling in in a similarity learning context , but with two distinctions :",method
natural_language_inference,18,The Neural Answer Selection Model ( ) is a supervised model that learns the question and answer representations and predicts their relatedness .,model,Neural Answer Selection Model,0,100,27,5,0,model : Neural Answer Selection Model,0.3663003663003663,0.5094339622641509,0.16129032258064516,The Neural Answer Selection Model is a supervised model that learns the question and answer representations and predicts their relatedness ,21,"This is a classification task where we treat each training data point as a triple ( q , a , y ) while predicting y for the unlabelled question - answer pair ( q , a ) .","It employs two different LSTMs to embed raw question inputs q and answer inputs a. Let sq ( j ) and s a ( i ) be the state outputs of the two LSTMs , and i , j be the positions of the states .",method
natural_language_inference,30,"Besides , evaluation is broad since , in ReVerb , most entities actually appear many times under different names as explained in Section 3 .",evaluation,Evaluation on WebQuestions,0,250,13,13,0,evaluation : Evaluation on WebQuestions,0.9689922480620154,0.9285714285714286,0.9285714285714286,Besides evaluation is broad since in ReVerb most entities actually appear many times under different names as explained in Section 3 ,22,"Still , for a system trained with almost no manual annotation nor prior information on another dataset , with another - very noisy - KB , the results can be seen as particularly promising .","Hence , there might be higher ranked answers but they are missed by our evaluation script .",result
natural_language_inference,97,Training and Model Details,training,Training and Model Details,0,215,1,1,0,training : Training and Model Details,0.7363013698630136,0.038461538461538464,0.038461538461538464,Training and Model Details,4, , ,experiment
semantic_role_labeling,4,"As Table 5 shows , the span - based models outperformed the CRF - based models .",performance,performance,0,197,4,4,0,performance : performance,0.6566666666666666,0.08163265306122447,0.18181818181818185,As Table 5 shows the span based models outperformed the CRF based models ,14,"For labeled spans whose boundaries match the gold annotation , we evaluate the label accuracies .","Also , interestingly , the performance gap between SENNA and ELMO was not so big as that for span boundary identification .",result
natural_language_inference,100,Two different architectures will be presented and analyzed .,system description,Extensive experimental evaluation and promising results.,0,62,10,8,0,system description : Extensive experimental evaluation and promising results.,0.16939890710382513,0.8333333333333334,0.8,Two different architectures will be presented and analyzed ,9,"We will review related work in Section 2 . In Section 3 , we will present the proposed a NMM model with two components : value - shared weights and question attention network with gating functions .",Section 4 is a systematic experimental analysis using the TREC QA benchmark dataset .,method
machine-translation,5,"Then , we used the best - performing models ( based on translation quality on the vali -dation set ) , which were the Transformer models ( see , and back - translated monolingual data .",system,Full Workflow,0,40,17,4,0,system : Full Workflow,0.2797202797202797,0.2297297297297297,0.3333333333333333,Then we used the best performing models based on translation quality on the vali dation set which were the Transformer models see and back translated monolingual data ,28,"For baseline models , we used the MLSTM and transf configurations ( see ) .","As mentioned before , for the unconstrained systems , we back - translated the monolingual data using pre-existing MLSTM - based NMT systems .",method
natural_language_inference,29,"On this website , users can post a question about the product .",system description,EXPERIMENTAL SETUP 5.1 Research questions,0,248,153,14,0,system description : EXPERIMENTAL SETUP 5.1 Research questions,0.6794520547945205,0.95625,0.6666666666666666,On this website users can post a question about the product ,12,This dataset is available at https://github.com/gsh199449/,Most questions are asking for an experience of user who has already bought the product .,method
text_summarization,1,"We speculate that our SELECTOR improves accuracy by focusing on more modes of the output distribution ( diversity ? ) , whereas Mixture Decoder tries to improve the accuracy by concentrating on fewer modalities of the output distribution ( diversity ?) .",result,Diversity vs. Number of Mixtures,0,222,16,9,0,result : Diversity vs. Number of Mixtures,0.925,0.6153846153846154,0.4736842105263158,We speculate that our SELECTOR improves accuracy by focusing on more modes of the output distribution diversity whereas Mixture Decoder tries to improve the accuracy by concentrating on fewer modalities of the output distribution diversity ,36,"The abstractive summarization task on CNN - DM has a target distribution with more modalities than question generation task on SQuAD , which is more difficult to model .",show the upper bound performance of SELECTOR by feeding focus guide to generator during test time .,result
text_summarization,6,"The source texts , golden summaries , and the generated summaries are shown in .",analysis,Summary Case Analysis,0,236,3,3,0,analysis : Summary Case Analysis,0.900763358778626,0.375,0.375,The source texts golden summaries and the generated summaries are shown in ,13,"In order to analyze the reasons of improving the performance , we compare the generated summaries by DRGD and the standard decoders StanD used in some other works such as .",From the cases we can observe that DRGD can indeed capture some latent structures which are consistent with the golden summaries .,result
natural_language_inference,28,"Perhaps , the best way to demonstrate this unique property , among other RNN models , is to test RUM on real world character level NLP tasks .",model,CHARACTER LEVEL LANGUAGE MODELING,0,195,5,3,0,model : CHARACTER LEVEL LANGUAGE MODELING,0.7195571955719557,0.1851851851851852,1.0,Perhaps the best way to demonstrate this unique property among other RNN models is to test RUM on real world character level NLP tasks ,25,The rotational unit of memory is a natural architecture that can learn long - term structure in data while avoiding significant overfitting ., ,method
natural_language_inference,86,"The predictions of Pr ( a b | Q , P ) and Pr ( a e | Q , P ) only differentiate at the last prediction layer .",system description,Multi-Perspective Context Matching Model,0,48,14,4,0,system description : Multi-Perspective Context Matching Model,0.2807017543859649,0.2222222222222222,0.6666666666666666,The predictions of Pr a b Q P and Pr a e Q P only differentiate at the last prediction layer ,22,shows the architecture of our MPCM model .,And all other layers below the prediction layer are shared .,method
sentiment_analysis,20,"In Subtasks D , E ( quantification ) we are given a set of messages about a set of topics and must estimate the distribution of the tweets across 2 - point scale ( Subtask D ) and a 5 - point scale ( Subtask E ) .",system description,Overview,0,31,6,6,0,system description : Overview,0.16666666666666666,0.1176470588235294,0.5,In Subtasks D E quantification we are given a set of messages about a set of topics and must estimate the distribution of the tweets across 2 point scale Subtask D and a 5 point scale Subtask E ,39,"In Subtasks B , C ( topic - based sentiment polarity classification ) we are given a message and a topic and must classify the message on 2 - poi nt scale ( Subtask B ) and a 5 - point scale ( Subtask C ) .","We collected a big dataset of 330M English Twitter messages , gathered from 12/2012 to 07/2016 , which is used ( 1 ) for calculating words statistics needed by our text processor and ( 2 ) for training our word embeddings .",method
text_generation,0,"However , in many other practical applications , such as poem generation and chatbot ( Hingston 2009 ) , a task specific loss may not be directly available to score a generated sequence accurately .",introduction,introduction,0,22,11,11,1,introduction : introduction,0.06790123456790123,0.3793103448275862,0.3793103448275862,However in many other practical applications such as poem generation and chatbot Hingston 2009 a task specific loss may not be directly available to score a generated sequence accurately ,30,"For instance , in the application of machine translation , a task specific sequence score / loss , bilingual evaluation understudy ( BLEU ) , can be adopted to guide the sequence generation .",General adversarial net ( GAN ) proposed by is a promising framework for alleviating the above problem .,introduction
sentiment_analysis,13,"For the restaurant domain , we use Yelp reviews from restaurant categories that the SemEval reviews also belong to .",experiment,Experiments,0,204,28,28,0,experiment : Experiments,0.7338129496402878,0.7,0.7,For the restaurant domain we use Yelp reviews from restaurant categories that the SemEval reviews also belong to ,19,"This gives us 1,151,863 posttraining examples for laptop domain knowledge .",We choose 700K reviews to ensure it is large enough to generate training examples ( with a duplicate factor of 1 ) to cover all post - training steps that we can afford ( discussed in Section 5.3 ),experiment
natural_language_inference,46,"While the relevant paragraph depicting the injury appears early on , it is not until the next snippet ( which appears at the end of the narrative ) that the lethal consequences of the injury are revealed .",analysis,FRANK (to Dana),0,252,22,11,0,analysis : FRANK (to Dana),0.8484848484848485,0.8461538461538461,0.7333333333333333,While the relevant paragraph depicting the injury appears early on it is not until the next snippet which appears at the end of the narrative that the lethal consequences of the injury are revealed ,35,"In particular , the examples show that larger parts of the story are required to answer questions .",This illustrates an iterative reasoning process as well as extremely long temporal dependencies we encountered during manual annotation .,result
sentiment_analysis,37,"For IAN , we ran our own experiments for this scenario .",experiment,Experimental Settings,0,188,6,6,0,experiment : Experimental Settings,0.7430830039525692,0.6666666666666666,0.6666666666666666,For IAN we ran our own experiments for this scenario ,11,"In this setup , samples with single aspect and multi aspect sentences are tested independently on the trained model .","Here , the model trained for restaurant domain is tested with the test set for laptop domain and vice versa .",experiment
semantic_role_labeling,4,Semantic Role Labeling ( SRL ) is a shallow semantic parsing task whose goal is to recognize the predicate - argument structure of each predicate .,introduction,introduction,0,10,2,2,0,introduction : introduction,0.03333333333333333,0.07407407407407407,0.07407407407407407,Semantic Role Labeling SRL is a shallow semantic parsing task whose goal is to recognize the predicate argument structure of each predicate ,23, ,"Given a sentence and a target predicate , SRL systems have to predict semantic arguments of the predicate .",introduction
natural_language_inference,3,"Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence , such as ABCNN , Attention LSTM .",introduction,introduction,0,22,13,13,0,introduction : introduction,0.10576923076923077,0.9285714285714286,0.9285714285714286,Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence such as ABCNN Attention LSTM ,28,"Some improved methods focus on utilizing multi-granularity representation ( word , phrase and sentence level ) , such as MultiGranCNN and Multi - Perspective .","These models can alleviate the weak interaction problem , but are still insufficient to model the contextualized interaction on the word as well as phrase level .",introduction
sentiment_analysis,30,"Following , we also report strict accuracy for aspect detection , as the fraction of sentences where all aspects are detected correctly .",experiment,Experimental Setup,0,108,29,28,0,experiment : Experimental Setup,0.84375,0.935483870967742,0.9333333333333332,Following we also report strict accuracy for aspect detection as the fraction of sentences where all aspects are detected correctly ,21,"Following , we consider the top 4 aspects only ( GENERAL , PRICE , TRANSIT - LOCATION , and SAFETY ) and employ the following evaluation metrics : macro -average F 1 and AUC for aspect detection ignoring the none class , and accuracy and macro -average AUC for sentiment classification .","tied vs. all free ) , this hybrid setup results in the best performance on the validation set . :",experiment
natural_language_inference,83,"For this task , we wanted to model events at an abstraction level that would be generalizable and yet semantically meaningful .",system description,Measuring Consistency,0,72,17,11,0,system description : Measuring Consistency,0.2352941176470588,0.3090909090909091,0.5,For this task we wanted to model events at an abstraction level that would be generalizable and yet semantically meaningful ,21,"For instance , in Wesley got angry ? Wesley bit her hand ? Wesley was scolded describes a more coherent sequence of events , as compared to Wesley got angry ? Wesley bit her hand ? Wesley got a cookie Prior work in script - learning attempts to model such prototypical sequence of events ( usually captured through verbs ) .","recently proposed a neural SemLM approach , to model such sequence of events using a language model of FrameNet frames thatare evoked in the given text .",method
natural_language_inference,4,Our mixed objective brings two benefits : ( i ) the reinforcement learning objective encourages answers thatare textually similar to the ground truth answer and discourages those thatare not ; ( ii ) the cross entropy objective significantly facilitates policy learning by encouraging trajectories thatare known to be correct .,introduction,introduction,0,20,11,11,0,introduction : introduction,0.101010101010101,0.6875,0.6875,Our mixed objective brings two benefits i the reinforcement learning objective encourages answers thatare textually similar to the ground truth answer and discourages those thatare not ii the cross entropy objective significantly facilitates policy learning by encouraging trajectories thatare known to be correct ,44,We obtain the latter objective using self - critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer .,The resulting objective is one that is both faithful to the evaluation metric and converges quickly in practice .,introduction
sentiment_analysis,27,"Besides the last fully connect layer , all the weight matrices are randomly initialized by a uniform distribution U ( ? 0.01 , 0.01 ) .",experiment,Data sets and experimental settings,1,188,19,18,0,experiment : Data sets and experimental settings,0.6811594202898551,0.7916666666666666,0.7826086956521741,Besides the last fully connect layer all the weight matrices are randomly initialized by a uniform distribution U 0 01 0 01 ,23,"The weight matrix of last fully connect layer is randomly initialized by a normal distribution N ( 0 , 1 ) .","In addition , we add L2-regularization to the last fully connect layer with a weight of 0.01 .",experiment
semantic_parsing,2,"Furthermore , due to the formulaic nature of the SQL query , we only use our decoder to generate the WHERE clause ( with the help of sketches ) .",training,Natural Language to SQL,0,183,74,13,0,training : Natural Language to SQL,0.6288659793814433,0.6271186440677966,0.52,Furthermore due to the formulaic nature of the SQL query we only use our decoder to generate the WHERE clause with the help of sketches ,26,"We therefore modify our input encoder in order to render it table - aware , so to speak .","The SELECT clause has a fixed number of slots ( i.e. , aggregation operator agg op and column agg col ) , which we straightforwardly predict with softmax classifiers ( conditioned on the input ) .",experiment
text-to-speech_synthesis,1,"As described in Section 4.3 , we leverage sequence - level knowledge distillation for FastSpeech .",system,1D Convolution in FFT Block,0,194,10,6,0,system : 1D Convolution in FFT Block,0.8858447488584474,0.8333333333333334,0.75,As described in Section 4 3 we leverage sequence level knowledge distillation for FastSpeech ,15,Sequence - Level Knowledge Distillation,"We conduct CMOS evaluation to compare the performance of FastSpeech with and without sequence - level knowledge distillation , as shown in .",method
question-answering,3,"By increasing the window size , the local -l function acquired progressive improvements when the window size is smaller than 4 .",model,Model Properties,0,180,10,10,0,model : Model Properties,0.7086614173228346,0.14285714285714285,0.3448275862068966,By increasing the window size the local l function acquired progressive improvements when the window size is smaller than 4 ,21,We found that the max function worked better than the global function on both MAP and MRR .,"But after we enlarged the window size to 4 , the performance dropped .",method
sentiment_analysis,35,Not all aspect words contribute equally to the semantic of the aspect .,system description,Context2Aspect (C2A) Attention,0,94,35,2,0,system description : Context2Aspect (C2A) Attention,0.3790322580645161,0.35714285714285715,0.030769230769230767,Not all aspect words contribute equally to the semantic of the aspect ,13, ,"For example , in the aspect term "" techs at HP "" , the sentiment is usually expressed over the headword "" techs "" but seldom over modifiers like the brand name "" HP "" .",method
part-of-speech_tagging,0,"Adversarial training ( AT ) 1 is a powerful regularization method for neural networks , aiming to achieve robustness to input perturbations .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.016260162601626018,0.25,0.25,Adversarial training AT 1 is a powerful regularization method for neural networks aiming to achieve robustness to input perturbations ,20, ,"Yet , the specific effects of the robustness obtained from AT are still unclear in the context of natural language processing .",abstract
text-classification,0,"This simplification of engineering could be crucial for a single system that can work for different languages , since characters always constitute a necessary construct regardless of whether segmentation into words is possible .",introduction,introduction,0,26,15,15,0,introduction : introduction,0.1145374449339207,0.9375,0.9375,This simplification of engineering could be crucial for a single system that can work for different languages since characters always constitute a necessary construct regardless of whether segmentation into words is possible ,33,from previous research that ConvNets do not require the knowledge about the syntactic or semantic structure of a language .,Working on only characters also has the advantage that abnormal character combinations such as misspellings and emoticons maybe naturally learnt .,introduction
relation_extraction,13,This approach is analogous to previous work where positional embeddings have been applied to relation extraction .,model,Entity span identification,0,81,16,11,0,model : Entity span identification,0.38028169014084506,0.3018867924528302,0.6470588235294118,This approach is analogous to previous work where positional embeddings have been applied to relation extraction ,17,"To address the STANDARD representation 's lack of explicit entity identification , we introduce two new segmentation embeddings , one that is added to all tokens in the span s 1 , while the other is added to all tokens in the span s 2 .","Finally , we augment x with four reserved word pieces to mark the begin and end of each entity mention in the relation statement .",method
sentiment_analysis,0,"As emotional dialogue is composed of sound and spoken content , researchers have also investigated the combination of acoustic features and language information , built belief network - based methods of identifying emotional key phrases , and assessed the emotional salience of verbal cues from both phoneme sequences and words .",introduction,introduction,0,40,31,31,0,introduction : introduction,0.2247191011235955,0.96875,0.96875,As emotional dialogue is composed of sound and spoken content researchers have also investigated the combination of acoustic features and language information built belief network based methods of identifying emotional key phrases and assessed the emotional salience of verbal cues from both phoneme sequences and words ,47,"Another researcher investigated transfer learning methods , leveraging external data from related domains .","However , none of these studies have utilized information from speech signals and text sequences simultaneously in an end - to - end learning neural network - based model to classify emotions .",introduction
sentiment_analysis,38,We train on a very large corpus picked to have a similar distribution as our task of interest .,introduction,introduction,1,53,42,42,0,introduction : introduction,0.3154761904761905,0.9767441860465116,0.9767441860465116,We train on a very large corpus picked to have a similar distribution as our task of interest ,19,We are also interested in evaluating this approach as it is not immediately clear whether such a low - level training objective supports the learning of high - level representations .,We also benchmark on a wider range of tasks to quantify the sensitivity of the learned represen -tation to various degrees of out - of - domain data and tasks .,introduction
sentiment_analysis,11,It is an open - source software which provides high dimensional audio vectors .,approach,Audio Feature Extraction,0,116,21,4,0,approach : Audio Feature Extraction,0.3372093023255814,0.42857142857142855,0.3636363636363637,It is an open source software which provides high dimensional audio vectors ,13,To extract audio features we use open,"These vectors comprise of features like loudness , Mel-spectra , MFCC , pitch , etc .",method
natural_language_inference,100,Some methods employ CNNs for question / answer matching .,introduction,introduction,0,36,26,26,0,introduction : introduction,0.09836065573770493,0.6190476190476191,0.6190476190476191,Some methods employ CNNs for question answer matching ,9,Architectures not specifically designed for question / answer matching :,"However , CNNs are originally designed for computer vision ( CV ) , which uses position - shared weights with local perceptive filters , to learn spatial regularities in many CV tasks .",introduction
natural_language_inference,22,"Thus we have a context character representation M ? Rf C and a query representation N ? Rf Q , where C is the sequence length of the context , Q is the sequence length of the query and f is the number of 1D convolutional neural network filters .",model,Ruminating Reader,0,54,9,8,0,model : Ruminating Reader,0.24545454545454545,0.13636363636363635,0.5,Thus we have a context character representation M Rf C and a query representation N Rf Q where C is the sequence length of the context Q is the sequence length of the query and f is the number of 1D convolutional neural network filters ,46,It does so using a convolutional neural network with max pooling over learned character vectors .,"Thus we have a context character representation M ? Rf C and a query representation N ? Rf Q , where C is the sequence length of the context , Q is the sequence length of the query and f is the number of 1D convolutional neural network filters .",method
natural_language_inference,50,"In our implementation , we initialize this layer with pretrained word embeddings .",approach,Embedding Layer,0,103,11,6,0,approach : Embedding Layer,0.3249211356466877,0.14666666666666667,0.75,In our implementation we initialize this layer with pretrained word embeddings ,12,"As such , this layer is a look - up layer that converts each word into a low - dimensional vector by indexing onto the word embedding matrix .",Note that this layer is not updated during training .,method
natural_language_inference,88,Long Short - Term Memory,system description,Long Short-Term Memory,0,78,6,1,0,system description : Long Short-Term Memory,0.3157894736842105,0.09090909090909093,0.2,Long Short Term Memory,4, , ,method
sentiment_analysis,24,"We set the diagonal elements in A to zeros , as we only consider context words for inferring the sentiment of the target token .",method,Aspect-Level Tasks,0,98,43,25,0,method : Aspect-Level Tasks,0.3151125401929261,0.3208955223880597,0.8333333333333334,We set the diagonal elements in A to zeros as we only consider context words for inferring the sentiment of the target token ,24,"In this way , AS is directly influenced by the predictions of AE .",The self - attention layer outputs h as i = n j=1 A ij h as j .,method
named-entity-recognition,6,"Still , LS vectors seem to encode a large portion of the information needed to model the NER task .",ablation,Ablation Results,0,185,8,8,0,ablation : Ablation Results,0.8726415094339622,0.8888888888888888,0.8888888888888888,Still LS vectors seem to encode a large portion of the information needed to model the NER task ,19,"The difference is not has high as we first expected , especially since the SSKIP model is adjusted during training , while our representation is not .","Also , it is worth mentioning that our embeddings are trained on 1.3B words compared to 42B for SSKIP .",result
sentiment_analysis,26,"Deep convolutional neural networks excel at sentiment polarity classification , but tend to require substantial amounts of training data , which moreover differs quite significantly between domains .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.0163265306122449,0.4,0.4,Deep convolutional neural networks excel at sentiment polarity classification but tend to require substantial amounts of training data which moreover differs quite significantly between domains ,26, ,"In this work , we present an approach to feed generic cues into the training process of such networks , leading to better generalization abilities given limited training data .",abstract
natural_language_inference,79,amaas/data/sentiment/index.html,model,Dataset:,0,205,20,10,0,model : Dataset:,0.7649253731343284,0.31746031746031744,1.0,amaas data sentiment index html,5,The dataset can be downloaded at http://ai.Stanford.edu/, ,method
text-classification,8,The extracted features from all windows are further down - sampled with a global max - pooling operation on top of the representations for every window .,system description,Simple Word-Embedding Model,0,96,49,31,0,system description : Simple Word-Embedding Model,0.35687732342007433,0.7424242424242424,0.6458333333333334,The extracted features from all windows are further down sampled with a global max pooling operation on top of the representations for every window ,25,"Let v i:i+n?1 refer to the local window consisting of n consecutive words words , First , an average - pooling is performed on each local window , v i:i+n?1 .",We call this approach SWEM - hier due to its layered pooling .,method
relation_extraction,7,"For instance , in Among the parse tree , the SDP has been widely used by and to help models focus on relational words .",methodology,Sub-Tree Parse,0,81,20,3,0,methodology : Sub-Tree Parse,0.3127413127413127,0.17543859649122806,0.375,For instance in Among the parse tree the SDP has been widely used by and to help models focus on relational words ,23,"In order to reduce inner-sentence noise and extract relational words , we propose the STP method which intercepts the subtree of each instance under the parent of entities ' lowest common ancestor .","However , in our observation , the SDP is not appropriate in the condition that key relation words are not in the SDP .",method
sentiment_analysis,29,"With a column - wise softmax and row - wise softmax , we get target - to - sentence attention ? and sentence - to - target attention ?.",method,Problem Definition,0,95,34,33,0,method : Problem Definition,0.5459770114942529,0.6415094339622641,0.8461538461538461,With a column wise softmax and row wise softmax we get target to sentence attention and sentence to target attention ,21,"Given the target representation ht ? R m 2 d hand sentence representation h s ? R n2d h , we first calculate a pair - wise interaction matrix I = h s h T t , where the value of each entry represents the correlation of a word pair among sentence and target .","With a column - wise softmax and row - wise softmax , we get target - to - sentence attention ? and sentence - to - target attention ?.",method
sentiment_analysis,28,Experimental results on three benchmark datasets show that the proposed model achieves competitive performance and is a lightweight alternative of the best RNN based models .,introduction,introduction,0,30,20,20,0,introduction : introduction,0.16666666666666666,0.6896551724137931,0.6896551724137931,Experimental results on three benchmark datasets show that the proposed model achieves competitive performance and is a lightweight alternative of the best RNN based models ,26,We also apply pre-trained BERT to this task and show our model enhances the performance of basic BERT model .,The main contributions of this work are presented as follows :,introduction
natural_language_inference,66,"We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ? 1 set to 0.9 and ? 2 set to 0.999 for optimization .",experiment,Experiment Settings,1,142,10,9,1,experiment : Experiment Settings,0.5089605734767025,0.3448275862068966,0.4736842105263158,We use the Adam method Kingma and Ba 2014 with hyperparameters 1 set to 0 9 and 2 set to 0 999 for optimization ,25,We perform three - class classification and use accuracy as our evaluation metric .,"We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ? 1 set to 0.9 and ? 2 set to 0.999 for optimization .",experiment
named-entity-recognition,6,"To confirm that the gains came from our feature vector and not from increasing the number of hidden units , we tested several SSKIP models by increasing the LSTM hidden layer dimension so that number of parameters is the same as the model with LS vectors .",model,Model,0,175,3,3,0,model : Model,0.8254716981132075,0.6,0.6,To confirm that the gains came from our feature vector and not from increasing the number of hidden units we tested several SSKIP models by increasing the LSTM hidden layer dimension so that number of parameters is the same as the model with LS vectors ,46,We also observe that models that use both feature sets significantly outperform other configurations .,"We observed a degradation of performance on both datasets , mostly due to overfitting on the training set .",method
natural_language_inference,85,The difference between ESIM and each of the other models listed in is statistically significant under the one - tailed paired t- test at the 99 % significance level .,ablation,Ablation analysis,0,210,11,11,0,ablation : Ablation analysis,0.8936170212765957,0.5238095238095238,0.5238095238095238,The difference between ESIM and each of the other models listed in is statistically significant under the one tailed paired t test at the 99 significance level ,28,"To provide some detailed comparison with , replacing bidirectional LSTMs in inference composition and also input encoding with feedforward neural network reduces the accuracy to 87.3 % and 86.3 % respectively .",The difference between model and is also significant at the same level .,result
sentence_classification,0,"We selected a sample of papers from the Semantic Scholar corpus , 7 consisting of papers in general computer science and medicine domains .",system description,Data collection and annotation,0,129,3,3,0,system description : Data collection and annotation,0.4831460674157304,0.1153846153846154,0.15789473684210525,We selected a sample of papers from the Semantic Scholar corpus 7 consisting of papers in general computer science and medicine domains ,23,Citation intent of sentence extractions was labeled through the crowdsourcing platform .,Citation contexts were extracted using science - parse .,method
text-classification,7,"Base lending rates even less likely in the near term , dealers said .",ablation,Interest Rates,0,200,50,8,0,ablation : Interest Rates,0.8230452674897121,0.8928571428571429,0.6666666666666666,Base lending rates even less likely in the near term dealers said ,13,Operators feel the foreign exchanges are likely to test sterling on the downside and that this seems to make a fall in U.K .,"The feeling remains in the market , however , that fundamental factors have not really changed and that arise in U.K .",result
natural_language_inference,49,"To show that dense interaction tensor contains more semantic information , we replace the dense interaction tensor with dot product similarity matrix between the encoded representation of premise and hypothesis .",ablation,ablation,0,208,23,23,0,ablation : ablation,0.8188976377952756,0.8518518518518519,0.8518518518518519,To show that dense interaction tensor contains more semantic information we replace the dense interaction tensor with dot product similarity matrix between the encoded representation of premise and hypothesis ,30,"By comparing the base model and the model the in experiment 6 , we show that the fuse gate not only well serves as a skip connection , but also makes good decision upon which information the fuse for both representation .",The result shows that the dot product similarity matrix has an inferior capacity of semantic information .,result
natural_language_inference,94,"We speculate that the improvement is smaller on Wikihop because only approximately 11 % of WikiHop data points require commonsense and because WikiHop data requires more fact - based commonsense ( e.g. , from Freebase ) as opposed to semantics - based commonsense ( e.g. , from Con-ceptNet ( Speer and Havasi , 2012 ) ) .",experiment,experiment,1,217,6,6,1,experiment : experiment,0.5665796344647521,0.8571428571428571,0.8571428571428571,We speculate that the improvement is smaller on Wikihop because only approximately 11 of WikiHop data points require commonsense and because WikiHop data requires more fact based commonsense e g from Freebase as opposed to semantics based commonsense e g from Con ceptNet Speer and Havasi 2012 ,48,"We also see that our model performs reasonably well on WikiHop , and further achieves promising initial improvements via the addition of commonsense , hinting at the generalizability of our approaches .", ,experiment
sentiment_analysis,35,The Bi - LSTM transforms the input e into the con-,system description,Bi-directional LSTM layer,0,88,29,8,0,system description : Bi-directional LSTM layer,0.3548387096774194,0.2959183673469388,0.6666666666666666,The Bi LSTM transforms the input e into the con ,11,"To capture phrase - level sentiment features in the context ( e.g. , "" not satisfactory "" ) , we employ a Bi-directional LSTM ( Bi - LSTM ) to preserve the contextual information for each word of the input sentence .",hidden states of Bi - LSTM ) .,method
natural_language_inference,80,"For n number of models , the best performance on the development set is used as the criteria to determine the final ensemble .",experiment,experiment,0,136,10,10,0,experiment : experiment,0.4689655172413793,0.2325581395348837,0.2325581395348837,For n number of models the best performance on the development set is used as the criteria to determine the final ensemble ,23,Our observation indicates that using larger batch sizes hurts the performance of our model .,The best performance on development set ( 89.22 % ) is observed using 6 models and is henceforth considered as our final DR - BiLSTM ( Ensemble ) model .,experiment
natural_language_inference,34,"We evaluate DFGN on HotpotQA , a public TBQA dataset requiring multi-hop reasoning .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.030508474576271188,0.7777777777777778,0.7777777777777778,We evaluate DFGN on HotpotQA a public TBQA dataset requiring multi hop reasoning ,14,"Inspired by human 's step - by - step reasoning behavior , DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query , explores along the entity graph dynamically built from the text , and gradually finds relevant supporting entities from the given documents .",DFGN achieves competitive results on the public board .,abstract
natural_language_inference,95,where y vi is the index of the correct answer in all the answer candidates of the i th instance .,model,Cross-Passage Answer Verification,0,117,50,12,0,model : Cross-Passage Answer Verification,0.5,1.0,1.0,where y vi is the index of the correct answer in all the answer candidates of the i th instance ,21,And the loss function can be formulated as the negative log probability of the correct answer :, ,method
sentiment_analysis,42,"We can find that LSTM based recurrent models are indeed computationally expensive , which is caused by the complex operations in each LSTM unit along the word sequence .",method,Method,0,187,3,3,0,method : Method,0.7420634920634921,0.13636363636363635,0.6,We can find that LSTM based recurrent models are indeed computationally expensive which is caused by the complex operations in each LSTM unit along the word sequence ,28,Time The training time of each iteration on the restaurant dataset is given in .,"Instead , the memory network approach is simpler and evidently faster because it does not need recurrent calculators of sequence length .",method
sentiment_analysis,39,"t=0 , where p is the polarity expressed for the aspect a of entity l.",system description,Task,0,139,3,3,0,system description : Task,0.5673469387755102,0.3,0.3,t 0 where p is the polarity expressed for the aspect a of entity l ,16,"We define the task of targeted aspect - based sentiment analysis as follows : given a unit of text s ( for example , a sentence ) , provide a list of tuples ( labels ) { ( l , a , p ) }",Each sentence can have zero to T number of labels associated with it .,method
natural_language_inference,50,"In our work , we mainly adopt the hyperbolic 1 distance function to model the relationships between questions and answers .",approach,Hyperbolic Representations of QA Pairs,0,122,30,13,0,approach : Hyperbolic Representations of QA Pairs,0.38485804416403785,0.4,0.5,In our work we mainly adopt the hyperbolic 1 distance function to model the relationships between questions and answers ,20,Neural ranking models are mainly characterized by the interaction function between question and answer representations .,"be the open d-dimensional unit ball , our model corresponds to the Riemannian manifold ( B d , ? x ) and is equipped with the Riemannian metric tensor given as follows :",method
natural_language_inference,53,The BiLSTM - 4096 with the max - pooling operation performs best on both SNLI and transfer tasks .,model,model,1,144,3,3,0,model : model,0.6923076923076923,0.04918032786885247,0.3,The BiLSTM 4096 with the max pooling operation performs best on both SNLI and transfer tasks ,17,We observe in that different models trained on the same NLI corpus lead to different transfer tasks results .,"Looking at the micro and macro averages , we see that it performs significantly better than the other models LSTM , GRU , BiGRU - last , BiLSTM - Mean , inner-attention and the hierarchical - ConvNet. also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM - Mean for instance .",method
sentiment_analysis,9,"Meanwhile , the ABSA datasets commonly benchmarked are generally small with the domain - specific characteristic , the effect of BERT - BASE model on the most ABSA datasets can be further improved through domain - adaption .",model,Model,0,226,5,5,0,model : Model,0.8158844765342961,0.38461538461538464,0.38461538461538464,Meanwhile the ABSA datasets commonly benchmarked are generally small with the domain specific characteristic the effect of BERT BASE model on the most ABSA datasets can be further improved through domain adaption ,33,"The BERT - BASE model is trained on a large - scale general corpus , so the fine - tuning during process during training process is significant and inevitable for BERT - based models .",Domain adaption is a effective technique while integrating the pre-trained BERT - BASE model .,method
natural_language_inference,70,In this task the data distribution among classes is quite balanced and the accuracy is also a good performance indicator .,result,Subtask A,0,141,19,9,0,result : Subtask A,0.8057142857142857,0.3584905660377358,0.9,In this task the data distribution among classes is quite balanced and the accuracy is also a good performance indicator ,21,"The good results on the 10 fold cross validations are confirmed on the official test set : the model is very accurate and achieved the first position among 12 systems , with the best MAP .",In this case we achieved the second position .,result
natural_language_inference,36,"For the following evaluation , the default dimension of SRL embeddings is 5 and the case study concerning the dimension is shown in the subsection dimension of SRL Embedding .",implementation,implementation,0,137,6,6,0,implementation : implementation,0.6523809523809524,0.5454545454545454,0.5454545454545454,For the following evaluation the default dimension of SRL embeddings is 5 and the case study concerning the dimension is shown in the subsection dimension of SRL Embedding ,29,"At test time , we perform Viterbi decoding to enforce valid spans using BIO constraints 5 .",The model is run forward for every verb in the sentence .,experiment
sentence_compression,1,It is probably even more common to operate on syntactic trees directly ( dependency or constituency ) and generate compressions by pruning them .,introduction,introduction,0,13,6,6,0,introduction : introduction,0.062200956937799035,0.35294117647058826,0.35294117647058826,It is probably even more common to operate on syntactic trees directly dependency or constituency and generate compressions by pruning them ,22,common approach is to use only some syntactic information or use syntactic features as signals in a statistical model .,"Unfortunately , this makes such systems vulnerable to error propagation as there is noway to recover from an incorrect parse tree .",introduction
natural_language_inference,10,The dropout probability is set to 0.2 and word embeddings are not updated during training .,experiment,Natural Language Inference,1,153,19,15,0,experiment : Natural Language Inference,0.6455696202531646,0.6129032258064516,0.5555555555555556,The dropout probability is set to 0 2 and word embeddings are not updated during training ,17,"In the 600D experiment , we set D x = 300 , D h = 600 , and an MLP with three hidden layers ( D c = 1024 ) is used .","The size of mini-batches is set to 128 in all experiments , and hyperparameters are tuned using the validation split .",experiment
sentiment_analysis,29,"Unlike document level sentiment classification task , aspect level sentiment classification is a more fine - grained classification task .",introduction,introduction,0,10,2,2,0,introduction : introduction,0.05747126436781608,0.03773584905660377,0.03773584905660377,Unlike document level sentiment classification task aspect level sentiment classification is a more fine grained classification task ,18, ,"It aims at identifying the sentiment polarity ( e.g. positive , negative , neutral ) of one specific aspect in its context sentence .",introduction
natural_language_inference,74,"According to the property of the datasets , we incorporate reinforcement learning to optimize a new objective function to make full use of the labels ' information .",introduction,introduction,0,41,29,29,0,introduction : introduction,0.18303571428571427,0.8529411764705882,0.8529411764705882,According to the property of the datasets we incorporate reinforcement learning to optimize a new objective function to make full use of the labels information ,26,We propose the Discourse Marker Augmented Network to combine the learned encoder of the sentences with the integrated NLI model .,We conduct extensive experiments on two large - scale datasets to show that our method achieves better performance than other stateof - the - art solutions to the problem .,introduction
natural_language_inference,79,", the vector for "" powerful "" is the same for all paragraphs .",model,model,0,97,17,17,0,model : model,0.3619402985074627,0.3090909090909091,0.5, the vector for powerful is the same for all paragraphs ,12,"The word vector matrix W , however , is shared across paragraphs .",The paragraph vectors and word vectors are trained using stochastic gradient descent and the gradient is obtained via backpropagation .,method
natural_language_inference,98,"However , we do n't use SNLI as an additional training / development data in our experiments .",experiment,Experimental Setup,0,79,6,6,0,experiment : Experimental Setup,0.6694915254237288,1.0,1.0,However we do n t use SNLI as an additional training development data in our experiments ,17,"SNLI corpus can be used as an additional training / development set , which includes content from the single genre of image captions .", ,experiment
sentiment_analysis,0,"We use a max encoder step of 750 for the audio input , based on the implementation choices presented in and 128 for the text input because it covers the maximum length of the transcripts .",implementation,Implementation details,1,124,3,3,0,implementation : Implementation details,0.6966292134831461,0.5,0.5,We use a max encoder step of 750 for the audio input based on the implementation choices presented in and 128 for the text input because it covers the maximum length of the transcripts ,35,"Among the variants of the RNN function , we use GRUs as they yield comparable performance to that of the LSTM and include a smaller number of weight parameters .","The vocabulary size of the dataset is 3,747 , including the "" UNK "" token , which represents unknown words , and the "" PAD "" token , which is used to indicate padding information added while preparing mini-batch data .",experiment
machine-translation,9,The English texts are tokenized by moses toolkit whereas the Japanese texts are tokenized by kytea .,analysis,MACHINE TRANSLATION,0,219,42,10,0,analysis : MACHINE TRANSLATION,0.7630662020905923,0.4772727272727273,0.3125,The English texts are tokenized by moses toolkit whereas the Japanese texts are tokenized by kytea ,17,We only use the first 150M pairs for training the models .,The vocabulary size for each language is reduced to 40K using byte - pair encoding .,result
sentiment_analysis,26,"In any case , it enables us to compare the ability of different model variants to learn to recognize pertinent words .",experiment,Experimental Setup,0,117,12,10,0,experiment : Experimental Setup,0.4775510204081633,0.2926829268292683,0.2564102564102564,In any case it enables us to compare the ability of different model variants to learn to recognize pertinent words ,21,"As the number of entities per sentence is often one or very low , this process is reasonably precise .","From TripAdvisor ( TA ) , we crawled German , Russian , Italian , Czech , and Japanese reviews of restaurants and hotels .",experiment
text-classification,4,We investigate the effectiveness of the proposed ACNN framework on both document classification and text sequence matching tasks .,dataset,dataset,0,140,2,2,0,dataset : dataset,0.6334841628959276,0.16666666666666666,0.16666666666666666,We investigate the effectiveness of the proposed ACNN framework on both document classification and text sequence matching tasks ,19, ,"Specifically , we consider two large - scale document classification datasets :",experiment
sentiment_analysis,47,"Meanwhile , in sub -figure ( b ) , given the target "" windows 8 operation system "" , although both left and right contexts contain sentiment word ( "" pleased "" in the left context and "" bad "" in the right context ) , the correct sentiment word "" bad "" in the right context is selected as the most important one .",performance,The Effect of Rotatory Attention,0,205,53,19,0,performance : The Effect of Rotatory Attention,0.9534883720930232,0.913793103448276,0.7916666666666666,Meanwhile in sub figure b given the target windows 8 operation system although both left and right contexts contain sentiment word pleased in the left context and bad in the right context the correct sentiment word bad in the right context is selected as the most important one ,49,"For example , in sub-figure ( a ) , the word "" pleased "" has the biggest attention weight for the target "" the life of battery "" .","Secondly , in sub-figure ( a ) and ( b ) , we can see that attention weights of left - aware target phrase and right - aware target phrase are very different .",result
relation-classification,3,The character embeddings are fed to a bidirectional LSTM ( BiLSTM ) to obtain the character - based representation of the word .,model,Joint learning as head selection,1,40,7,6,0,model : Joint learning as head selection,0.291970802919708,0.15217391304347827,0.1875,The character embeddings are fed to a bidirectional LSTM BiLSTM to obtain the character based representation of the word ,20,"We use character level embeddings to implicitly capture morphological features ( e.g. , prefixes and suffixes ) , representing each character by a vector ( embedding ) .",We also use pre-trained word embeddings .,method
relation_extraction,5,"To make use of these word representations for relation extraction , we first obtain a sentence representation as follows ( see also left ) :",model,Contextualized GCN,0,69,34,5,0,model : Contextualized GCN,0.2623574144486692,0.5074626865671642,0.2083333333333333,To make use of these word representations for relation extraction we first obtain a sentence representation as follows see also left ,22,"After applying an L-layer GCN over word vectors , we obtain hidden representations of each token thatare directly influenced by its neighbors no more than L edges apart in the dependency tree .","where h ( l ) denotes the collective hidden representations at layer l of the GCN , and f : R dn ? Rd is a max pooling function that maps from n output vectors to the sentence vector .",method
question-answering,3,models the sentence similarity by enriching LSTMs with a latent stochastic attention mechanism .,model,Comparing with State-of-the-art Models,0,219,49,20,0,model : Comparing with State-of-the-art Models,0.8622047244094488,0.7,0.4878048780487805,models the sentence similarity by enriching LSTMs with a latent stochastic attention mechanism ,14,The best performance ( shown at the second row of ) was acquired by a bigram CNN model combining with the word overlap features .,The corresponding performance is given at the fourth row of .,method
natural_language_inference,30,"For each question q from the WikiAn - swers + ReVerb test set , we take the provided candidate triples t and rerank them by sorting by the score S ( q , t ) or S ft ( q , t ) of our model , depending whether we use fine - tuning or not .",evaluation,Test Set,0,190,8,6,0,evaluation : Test Set,0.7364341085271318,0.5714285714285714,0.8571428571428571,For each question q from the WikiAn swers ReVerb test set we take the provided candidate triples t and rerank them by sorting by the score S q t or S ft q t of our model depending whether we use fine tuning or not ,46,We first evaluated different versions of our model against the paralex system in a reranking setting .,"As in , we then compute the precision , recall and F1 -score of the highest ranked answer as well as the mean average precision ( MAP ) of the whole output , which measures the average precision over all levels of recall .",result
natural_language_inference,34,"Since each entity is recognized via the NER tool , the text spans associated with the entities are utilized to compute entity embeddings ( Doc2 Graph ) .",system description,Reasoning with the Fusion Block,0,142,39,9,0,system description : Reasoning with the Fusion Block,0.4813559322033898,0.38613861386138615,0.16363636363636366,Since each entity is recognized via the NER tool the text spans associated with the entities are utilized to compute entity embeddings Doc2 Graph ,25,Document to Graph Flow .,"To this end , we construct a binary matrix M , where M i , j is 1 if i - th token in the context is within the span of the j - th entity .",method
named-entity-recognition,8,We differentiate the sentences in two ways .,model,model,0,98,20,20,0,model : model,0.2532299741602067,0.2702702702702703,0.3225806451612903,We differentiate the sentences in two ways ,8,Sentence pairs are packed together into a single sequence .,"First , we separate them with a special token ( [ SEP ] ) .",method
text_summarization,1,Content Selection in NLP,system description,Diversity-Promoting Regularization,0,71,5,5,0,system description : Diversity-Promoting Regularization,0.2958333333333333,0.5555555555555556,0.5555555555555556,Content Selection in NLP,4,Our work is orthogonal to these methods and can potentially benefit from adding these regularization terms to our objective function .,Selecting important parts of the context has been an important step in NLP applications .,method
sentence_compression,3,"Each part - ofspeech tag was mapped into a vector representation , ( p ( w 1 ) , p ( w 2 ) , ... , p (w n ) ) through the parameter matrix P , while each dependency relation was mapped into a vector representation , ( d ( w 1 ) , d ( w 2 ) , ... , d ( w n ) ) through the parameter matrix D .",methodology,Task and Framework,0,32,7,6,0,methodology : Task and Framework,0.25806451612903225,0.2058823529411765,0.35294117647058826,Each part ofspeech tag was mapped into a vector representation p w 1 p w 2 p w n through the parameter matrix P while each dependency relation was mapped into a vector representation d w 1 d w 2 d w n through the parameter matrix D ,49,"We first converted the word sequence into a dense vector representation through the parameter matrix E. Except for word embedding , ( e ( w 1 ) , e ( w 2 ) , ... , e ( w n ) ) , we also considered the part - of - speech tag and the dependency relation between w i and its headword as extra features .","Three vector representations are concatenated , [ e ( w i ) ; p ( w i ) ; d ( w i ) ] as the input to the next part , policy network .",method
negation_scope_resolution,0,Word embeddings were the first attempt at using Transfer Learning in NLP .,approach,Deep Learning Approaches,0,162,85,26,0,approach : Deep Learning Approaches,0.7043478260869566,0.934065934065934,0.8125,Word embeddings were the first attempt at using Transfer Learning in NLP ,13,"They preprocessed the text first , and then used the Gensim implementation of word2vec to generate embeddings for the text .","More recently , used a shared encoder and 2 separate decoders to get the entities and negations respectively .",method
relation-classification,2,ADE : There are two types of entities ( drugs and diseases ) in this dataset and the aim of the task is to identify the types of entities and relate each drug with a disease ( adverse drug events ) .,dataset,Datasets and evaluation metrics,0,197,26,26,0,dataset : Datasets and evaluation metrics,0.6677966101694915,0.8387096774193549,0.8387096774193549,ADE There are two types of entities drugs and diseases in this dataset and the aim of the task is to identify the types of entities and relate each drug with a disease adverse drug events ,37,"Also , we report results using the strict evaluation for future reference .","There are 6,821 sentences in total and similar to previous work , we remove ? 130 relations with overlapping entities ( e.g. , "" lithium "" is a drug which is related to "" lithium intoxication "" ) .",experiment
natural_language_inference,46,"Furthermore , we want annotators to privilege writing answers expressed in their own words , and consider higher - level relations between entities , places , and events , rather than copy short spans of the document .",system description,FRANK (to the baby),0,97,81,75,0,system description : FRANK (to the baby),0.3265993265993266,0.9642857142857144,0.9615384615384616,Furthermore we want annotators to privilege writing answers expressed in their own words and consider higher level relations between entities places and events rather than copy short spans of the document ,32,"The questions and answers should be natural , unconstrained , and human generated , and answering questions should frequently require reference to several parts or a larger span of the context document rather than superficial representations of local context .","Furthermore , we want to evaluate models both on the fluency and correctness of generated free - form answers , and as an answer selection problem , which requires the provision of sensible distractors to the correct answer .",method
natural_language_inference,56,For more givens the accuracy ( fraction of test puzzles solved ) quickly approaches 100 % .,experiment,Sudoku,0,155,76,11,0,experiment : Sudoku,0.4613095238095238,0.7102803738317757,0.3235294117647059,For more givens the accuracy fraction of test puzzles solved quickly approaches 100 ,14,"We only consider a puzzled solved if all the digits are correct , i.e. no partial credit is given forgetting individual digits correct .","Since the network outputs a probability distribution for each step , we can visualize how the network arrives at the solution step by step .",experiment
natural_language_inference,95,contaminated culture contains organisms that derived from someplace . . .,introduction,introduction,0,38,30,30,0,introduction : introduction,0.1623931623931624,0.5172413793103449,0.5172413793103449,contaminated culture contains organisms that derived from someplace ,9,mixed culture is taken from a source and may contain multiple strains or species .,It will beat that time when we can truly obtain a pure culture .,introduction
part-of-speech_tagging,1,The filters are computed with different kernels by the initial convolutional layers are concatenated :,architecture,architecture,0,106,27,27,0,architecture : architecture,0.424,0.3375,0.3375,The filters are computed with different kernels by the initial convolutional layers are concatenated ,15,Equation 2 produces m filters with different kernel sizes .,"where h is the number of kernels , g 0 is the output for the initial convolutional layer which feeds into the next convolutional block .",method
natural_language_inference,70,"The proposed tree kernel combinations extend such reasoning to text pairs , and can capture emerging pairwise patterns .",system description,Inter-pair kernel methods,0,73,44,8,0,system description : Inter-pair kernel methods,0.4171428571428572,0.4731182795698925,0.8888888888888888,The proposed tree kernel combinations extend such reasoning to text pairs and can capture emerging pairwise patterns ,18,"Tree kernels , computing the shared substructures between parse trees , are effective in evaluating the syntactic similarity between two texts .","Therefore this method can be effective in recognizing valid question / answer pairs , or similar questions , even in those cases in which the two texts have few words in common that would cause the failure of any intra-pair approach .",method
question_answering,3,"However , since our gating function is learned via reasoning over multi-granular sequence blocks , it captures more compositionality and long range context .",system description,Simple Encoding,0,91,44,7,0,system description : Simple Encoding,0.325,0.8,0.875,However since our gating function is learned via reasoning over multi granular sequence blocks it captures more compositionality and long range context ,23,Note that this formulation is in similar spirit to highway networks .,Note that an optional and additional projection maybe applied tow t but we found that it did not yield much empirical benefit .,method
natural_language_inference,81,"France , officially the French Republic , is a country with territory in western Europe and several overseas regions and territories .",APPENDIX,ATTENTION MAPS,0,265,57,42,0,APPENDIX : ATTENTION MAPS,0.6294536817102138,0.2676056338028169,0.7,France officially the French Republic is a country with territory in western Europe and several overseas regions and territories ,20,"The primarily physiographic term "" continent "" as applied to Europe also incorporates cultural and political elements whose discontinuities are not always reflected by the continents current overland boundaries .","The European , or metropolitan , are a of France extends from the Mediterranean Sea to the English Channel and the North Sea , and from the Rhine to the Atlantic Ocean .",others
natural_language_inference,76,"Our benchmark LSTM achieves an accuracy of 80.9 % on SNLI , outperforming a simple lexicalized classifier tailored to RTE by 2.7 percentage points .",introduction,introduction,0,27,14,14,0,introduction : introduction,0.18493150684931506,0.9333333333333332,0.9333333333333332,Our benchmark LSTM achieves an accuracy of 80 9 on SNLI outperforming a simple lexicalized classifier tailored to RTE by 2 7 percentage points ,25,"Our contributions are threefold : ( i ) We present a neural model based on LSTMs that reads two sentences in one go to determine entailment , as opposed to mapping each sentence independently into a semantic space ( 2.2 ) , ( ii ) We extend this model with a neural word - by - word attention mechanism to encourage reasoning over entailments of pairs of words and phrases ( 2.4 ) , and ( iii ) We provide a detailed qualitative analysis of neural attention for RTE ( 4.1 ) .","An extension with word - by - word neural attention surpasses this strong benchmark LSTM result by 2.6 percentage points , setting a new state - of - the - art accuracy of 83.5 % for recognizing entailment on SNLI .",introduction
natural_language_inference,11,"First , we evaluated models trained with knowledge on our tasks while not providing any knowledge at test time .",implementation,Further Analysis of Knowledge Utilization in RTE,0,260,63,4,0,implementation : Further Analysis of Knowledge Utilization in RTE,0.9420289855072465,0.7974683544303798,0.2,First we evaluated models trained with knowledge on our tasks while not providing any knowledge at test time ,19,"Is additional knowledge used ? To verify whether and how our models make use of additional knowledge , we conducted several experiments .","This ablation drops performance by 3.7 - 3.9 % accuracy on MultiNLI , and by 4 % F1 on SQuAD .",experiment
natural_language_inference,33,We train on a collection of about 1 million sentence pairs from the SNLI and MultiNLI corpora .,system description,Neural Machine Translation,0,102,45,12,0,system description : Neural Machine Translation,0.3908045977011495,0.6716417910447762,0.375,We train on a collection of about 1 million sentence pairs from the SNLI and MultiNLI corpora ,18,This is the same classification strategy adopted by .,use periodic task alternations with equal training ratios for every task .,method
natural_language_inference,21,The experimental results of DiSAN and existing methods are shown in .,model,Sentence Classifications,0,259,33,6,0,model : Sentence Classifications,0.8931034482758621,0.5689655172413793,0.5454545454545454,The experimental results of DiSAN and existing methods are shown in ,12,") CR : Customer review of various products ( cameras , etc. ) , which is to predict whether the review is positive or negative ; 2 ) MPQA : Opinion polarity detection subtask of the MPQA dataset ; 3 ) SUBJ : Subjectivity dataset ( Pang and Lee 2004 ) whose labels indicate whether each sentence is subjective or objective ; 4 ) TREC : TREC question - type classification dataset .","particular , we will focus primarily on the probability in forward / backward DiSA blocks ( ) , forward / backward fusion gates F in Eq. ( 19 ) , and the probability in multi-dimensional source2 token self - attention block .",method
natural_language_inference,72,"We see that answer prediction based on contextual representation of queries and passages ( sim -entity ) achieves a strong base performance that is only outperformed by GA 7 In precision , the number of correct words is divided by the number of all predicted words .",analysis,Results and analysis,1,208,3,3,0,analysis : Results and analysis,0.6666666666666666,0.13043478260869565,0.13043478260869565,We see that answer prediction based on contextual representation of queries and passages sim entity achieves a strong base performance that is only outperformed by GA 7 In precision the number of correct words is divided by the number of all predicted words ,44,We show the results in .,"In recall , the former is divided by the number of words in the ground - truth answer .",result
natural_language_inference,81,"Like all living organisms , seed plants have a single major goal : to pass their genetic information onto the next generation .",APPENDIX,Answer town,0,409,201,121,0,APPENDIX : Answer town,0.9714964370546318,0.943661971830986,0.9097744360902256,Like all living organisms seed plants have a single major goal to pass their genetic information onto the next generation ,21,"Pollination is the process by which pollen is transferred to the female reproductive organs of a plant , thereby enabling fertilization to take place .","The reproductive unit is the seed , and pollination is an essential step in the production of seeds in all spermatophytes ( seed plants ) .",others
natural_language_inference,96,"As a result , models trained on such datasets with hu-man biases run the risk of over-estimating the actual performance on the underlying task , and are vulnerable to adversarial or out - of - domain examples .",introduction,introduction,0,34,24,24,0,introduction : introduction,0.08717948717948718,0.6857142857142857,0.6857142857142857,As a result models trained on such datasets with hu man biases run the risk of over estimating the actual performance on the underlying task and are vulnerable to adversarial or out of domain examples ,36,"However , recent work has shown that human - written datasets are susceptible to annotation artifacts : unintended stylistic patterns that give out clues for the gold labels .","In this paper , we introduce Adversarial Filtering ( AF ) , a new method to automatically detect and reduce stylistic artifacts .",introduction
sentiment_analysis,19,"In fact , the representation of a sentence in this case is simply the max - pooling of all the intermediate representations of the SuBiLSTM .",system description,Paraphrase Detection,0,111,59,4,0,system description : Paraphrase Detection,0.7254901960784313,0.9672131147540984,0.6666666666666666,In fact the representation of a sentence in this case is simply the max pooling of all the intermediate representations of the SuBiLSTM ,24,"Note that unlike the BCN and ESIM models , we use a simple Siamese architecture without any attention mechanism .","Even in this case , we observe gains over both single layer and 2 - layer BiLSTMs , although slightly lesser than the attention based models .",method
natural_language_inference,62,"Then following , we construct the passage - attended question summaries R Q and the question - attended passage summaries RP :",architecture,Knowledge Aided Mutual Attention,0,153,58,19,0,architecture : Knowledge Aided Mutual Attention,0.6830357142857143,0.7532467532467533,0.8260869565217391,Then following we construct the passage attended question summaries R Q and the question attended passage summaries RP ,19,"Based on the modified similarity function and the enhanced context embeddings , to perform knowledge aided mutual attention , first we construct a knowledge aided similarity matrix A ? R nm , where each element A i , j = f * ( c pi , c q j ) .",where softmax r represents softmax along the row dimension and softmax c along the column dimension .,method
sentiment_analysis,11,This constraint of sharing parameters adjacently between layers is added for reduction in total parameters and ease of training .,system description,Multiple Layers,0,207,63,7,0,system description : Multiple Layers,0.6017441860465116,0.6363636363636364,0.2916666666666667,This constraint of sharing parameters adjacently between layers is added for reduction in total parameters and ease of training ,20,"At a particular hop r , the output memory of the previous hop M ? .","At every hop , the query utterance u i 's representation q i is updated as : We perform experiments on the IEMOCAP dataset",method
named-entity-recognition,6,"Similarly to previous works , we use capitalization features for characterizing certain categories of capitalization patterns : all Upper , allLower , upperFirst , upperNotFirst , numeric or noAlphaNum .",model,Capitalization Features,0,120,16,2,0,model : Capitalization Features,0.5660377358490566,0.8,0.6666666666666666,Similarly to previous works we use capitalization features for characterizing certain categories of capitalization patterns all Upper allLower upperFirst upperNotFirst numeric or noAlphaNum ,24, ,"We define a random lookup table for these features , and learn its parameters during training .",method
natural_language_inference,56,"Toward generally realizing the ability to methodically reason about objects and their interactions over many steps , this paper introduces a composite function , the recurrent relational network .",introduction,introduction,1,25,12,12,0,introduction : introduction,0.0744047619047619,0.42857142857142855,0.42857142857142855,Toward generally realizing the ability to methodically reason about objects and their interactions over many steps this paper introduces a composite function the recurrent relational network ,27,"Looking beyond relational networks , there is a rich literature on logic and reasoning in artificial intelligence and machine learning , which we discuss in section 5 .",It serves as a modular component for many - step relational reasoning in end - to - end differentiable learning systems .,introduction
natural_language_inference,60,In each case we show state - of - the - art performance within the class of single sentence encoder models .,evaluation,evaluation,0,39,9,9,0,evaluation : evaluation,0.2020725388601036,0.9,0.9,In each case we show state of the art performance within the class of single sentence encoder models ,19,"We apply the technique in a BiLSTM - max sentence encoder and evaluate it on wellknown tasks in the field : natural language inference ( SNLI and MultiNLI ; 4 ) , sentiment analysis ( SST ; 5 ) , and image - caption retrieval ( Flickr30k ; 6 ) .","Furthermore , we include an extensive analysis ( 7 ) to highlight the general usefulness of our technique and to illustrate how it can lead to new insights .",result
text_generation,5,"While the KL term is critical for training VAEs , historically , instability on text has been evidenced by the KL term becoming vanishingly small during training , as observed by .",system description,Training Collapse with Textual VAEs,0,75,7,7,0,system description : Training Collapse with Textual VAEs,0.2542372881355932,0.09210526315789473,0.4117647058823529,While the KL term is critical for training VAEs historically instability on text has been evidenced by the KL term becoming vanishingly small during training as observed by ,29,"It is the KL - divergence term , KL ( q ? ( z|x ) ||p ? ( z ) ) , that discourages the VAE memorizing each x as a single latent point .","When the training procedure collapses in this way , the result is an encoder that has duplicated the Gaussian prior ( instead of a more interesting posterior ) , a decoder that completely ignores the latent variable z , and a learned model that reduces to a simpler language model .",method
natural_language_inference,35,This also enables our model to give an answer in the target style .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.038022813688212934,0.8,0.8,This also enables our model to give an answer in the target style ,14,"Second , whereas previous studies built a specific model for each answer style because of the difficulty of acquiring one general model , our approach learns multi-style answers within a model to improve the NLG capability for all styles involved .",Experiments show that our model achieves state - of - the - art performance on the Q&A task and the Q & A + NLG task of MS MARCO 2.1 and the summary task of Nar-rative QA .,abstract
question_similarity,0,"With a single parameter , w a , the attention mechanism can be described as follows :",model,Sequence Representation Extractor,0,70,22,12,0,model : Sequence Representation Extractor,0.5072463768115942,0.6875,0.8,With a single parameter w a the attention mechanism can be described as follows ,15,"The attention mechanism ( inspired by ) allows the model to learn to decide the importance of each word and build the final question representation vector based on important words only , while tuning out less important words .",The weight matrix w a is the only new trainable parameter which learns the attention mechanism over the outputs of the second ON - LSTM layer .,method
sarcasm_detection,0,"As with Twitter hashtags , using these markers as indicators of sarcasm is noisy , especially since many users do not use the marker , do not know about it , or only use it where sarcastic intent is not otherwise obvious .",system description,Reddit Structure and Annotation,0,32,7,6,0,system description : Reddit Structure and Annotation,0.18181818181818185,0.2333333333333333,0.8571428571428571,As with Twitter hashtags using these markers as indicators of sarcasm is noisy especially since many users do not use the marker do not know about it or only use it where sarcastic intent is not otherwise obvious ,39,"Reddit users have adopted a common method for sarcasm annotation consisting of adding the marker "" / s "" to the end of sarcastic statements ; this originates from the HTML text delineation < sarcasm >... < / sarcasm >.",We discuss the extent of this noise in Section 4.1 .,method
natural_language_inference,89,"Take for example , after comparing the passage snippet "" Normandy , a region in France "" with the question , we can easily determine that no answer exists since the question asks for an impossible condition 1 .",introduction,introduction,0,37,28,28,0,introduction : introduction,0.1423076923076923,0.7777777777777778,0.7777777777777778,Take for example after comparing the passage snippet Normandy a region in France with the question we can easily determine that no answer exists since the question asks for an impossible condition 1 ,34,This is based on the observation that the core phenomenon of unanswerable questions usually occurs between a few passage words and question words .,"This observation is even more obvious when antonym or mutual exclusion occurs , such as the question asks for "" the decline of rainforests "" but the passage mentions that "" the rainforests spread out "" .",introduction
sentiment_analysis,16,"If we defin ? c i = tanh ( c i + v t ) , c i can be viewed as the target - aware context representation of context xi and the final sentiment score is calculated based on the aggregation of suchc i .",approach,approach,0,138,16,16,0,approach : approach,0.4339622641509434,0.1951219512195122,0.1951219512195122,If we defin c i tanh c i v t c i can be viewed as the target aware context representation of context xi and the final sentiment score is calculated based on the aggregation of suchc i ,39,"From Eq. 5 , we can see that this approach can keep the linearity of attention - weighted context aggregation while taking into account the aspect information with non-linear projection , which works in a different way compared to NP .","If we defin ? c i = tanh ( c i + v t ) , c i can be viewed as the target - aware context representation of context xi and the final sentiment score is calculated based on the aggregation of suchc i .",method
sentiment_analysis,17,We therefore turn to ordersensitive sequential or tree - structured models .,introduction,introduction,0,26,18,18,0,introduction : introduction,0.11555555555555555,0.5806451612903226,0.5806451612903226,We therefore turn to ordersensitive sequential or tree structured models ,11,"Order-insensitive models are insufficient to fully capture the semantics of natural language due to their inability to account for differences in meaning as a result of differences in word order or syntactic structure ( e.g. , "" cats climb trees "" vs. "" trees climb cats "" ) .","In particular , tree - structured models are a linguistically attractive option due to their relation to syntactic interpretations of sentence structure .",introduction
natural_language_inference,6,"There are 1,000 training and development documents and 4,000 test documents for each language , divided in 4 different genres .",training,MLDoc: cross-lingual classification,0,128,59,4,0,training : MLDoc: cross-lingual classification,0.5161290322580645,0.6413043478260869,0.5,There are 1 000 training and development documents and 4 000 test documents for each language divided in 4 different genres ,22,"In order to evaluate our sentence embeddings in this task , we use the MLDoc dataset of Schwenk and Li , which is an improved version of the Reuters benchmark with uniform class priors and a wider language coverage .","Just as with the XNLI evaluation , we consider the zero - shot transfer scenario : we train a classifier on top of our multilingual encoder using the English training data , optimizing hyperparameters on the English development set , and evaluating the resulting system in the remaining languages .",experiment
sentence_compression,0,But the Traditional ILP method performs worse in the in - domain setting than both the LSTM and LSTM + methods and our methods .,evaluation,Automatic Evaluation,1,206,22,22,0,evaluation : Automatic Evaluation,0.7383512544802867,0.5,0.6666666666666666,But the Traditional ILP method performs worse in the in domain setting than both the LSTM and LSTM methods and our methods ,23,"This is probably because the Traditional ILP method relies heavily on syntax , which is less domain - dependent compared with lexical patterns .","Overall , shows that our proposed method combines both the strength of neural network models in the in - domain setting and the strength of the syntax - based methods in the crossdomain setting .",result
natural_language_inference,17,"where E ij indicates the similarity between i - th question word and j- th context word , and f is a scalar function .",architecture,Alignment Architecture for MRC,0,54,11,11,0,architecture : Alignment Architecture for MRC,0.2076923076923077,0.08029197080291971,0.08461538461538462,where E ij indicates the similarity between i th question word and j th context word and f is a scalar function ,23,"Aligning Rounds Attention Interactive Self Type and U = {u j } m j=1 , representing question and context respectively , a similarity matrix E ? R nm is computed as","Different methods are proposed to normalize the matrix , resulting in variants of attention such as bi-attention and coattention .",method
question-answering,7,The write module then transforms the composition output to the encoding memory space and writes the resulting new representation into the slot location of the memory .,training,"Read, Compose and Write",0,87,14,6,0,"training : Read, Compose and Write",0.31636363636363635,0.2857142857142857,0.21428571428571427,The write module then transforms the composition output to the encoding memory space and writes the resulting new representation into the slot location of the memory ,27,The compose module implements a composition operation that combines the memory slot with the current input .,"Instead of composing the raw embedding vector x t , we use the hidden state o t produced by the read module at time t",experiment
natural_language_inference,21,The goal of sentence classification is to correctly predict the class label of a given sentence in various scenarios .,model,Sentence Classifications,0,255,29,2,0,model : Sentence Classifications,0.8793103448275862,0.5,0.18181818181818185,The goal of sentence classification is to correctly predict the class label of a given sentence in various scenarios ,20, ,"We evaluate the models on four sentence classification benchmarks of various NLP tasks , such as sentiment analysis and question - type classification .",method
natural_language_inference,44,We observed that 98 % of questions are answerable given the document .,system description,Human studies,0,35,22,3,0,system description : Human studies,0.12237762237762236,0.5238095238095238,0.3333333333333333,We observed that 98 of questions are answerable given the document ,12,"First , we randomly sample 50 examples from the SQuAD development set , and analyze the minimum number of sentences required to answer the question , as shown in .",The remaining 2 % of questions are not answerable even given the entire document .,method
natural_language_inference,58,"Motivated by , we tackle this challenge by proposing a reinforcement learning approach , which utilizes an instance - dependent reward baseline , to successfully train ReasoNets .",introduction,introduction,1,42,33,33,0,introduction : introduction,0.1253731343283582,0.8461538461538461,0.8461538461538461,Motivated by we tackle this challenge by proposing a reinforcement learning approach which utilizes an instance dependent reward baseline to successfully train ReasoNets ,24,This prohibits canonical back - propagation method being directly applied to train ReasoNets .,"Finally , by accounting for a dynamic termination state during inference and applying proposed deep reinforcement learning optimization method , ReasoNets achieve the state - of - the - art results in machine comprehension datasets , including unstructured CNN and Daily Mail datasets , and the proposed structured Graph Reachability dataset , when the paper is rst publicly available on arXiv .",introduction
part-of-speech_tagging,0,The evaluation results are summarized in Table 6 .,analysis,Effects on Representation Learning,0,202,25,10,0,analysis : Effects on Representation Learning,0.8211382113821138,0.4032258064516129,0.3125,The evaluation results are summarized in Table 6 ,9,We also report the average tightness across all the clusters .,"We report the tightness scores for the four major clusters : noun , verb , adjective , and adverb ( from left to right ) .",result
natural_language_inference,10,"We evaluate the proposed model on natural language inference and sentiment analysis , and show that our model outperforms or is at least comparable to previous models .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.033755274261603366,0.8571428571428571,0.8571428571428571,We evaluate the proposed model on natural language inference and sentiment analysis and show that our model outperforms or is at least comparable to previous models ,27,Our model uses Straight - Through Gumbel - Softmax estimator to decide the parent node among candidates dynamically and to calculate gradients of the discrete decision .,We also find that our model converges significantly faster than other models .,abstract
natural_language_inference,28,hh for the target ? on the Penn Treebank task .,analysis,VISUAL ANALYSIS,0,233,15,15,0,analysis : VISUAL ANALYSIS,0.8597785977859779,0.3333333333333333,0.9375,hh for the target on the Penn Treebank task ,10,"For example , in ( b ) we observe a particular W","The way we interpret the meaning of the diagonal structure , combined with the off - diagonal activations , is that probably they encode grammar and vocabulary , as well as the links between various components of language .",result
machine-translation,7,introduce the idea of using multiple,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,61,18,9,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.16353887399463807,0.3461538461538461,0.6,introduce the idea of using multiple,6,The mixture of experts is the whole model .,MoEs with their own gating networks as parts of a deep model .,method
sentiment_analysis,8,"We then merged examples from "" happy "" and "" excited "" classes as "" happy "" was under-represented and the two emotions closely resemble each other .",methodology,methodology,0,54,8,8,0,methodology : methodology,0.2307692307692308,0.0898876404494382,0.5714285714285714,We then merged examples from happy and excited classes as happy was under represented and the two emotions closely resemble each other ,23,"The emotions "" fear "" and "" surprise "" were under-represented and use upsampling techniques to alleviate the issue .","In addition to that , we discard examples classified as "" others "" ; they corresponded to examples that were labeled ambiguous even for a human .",method
natural_language_inference,39,Forward Pass : Similarity Focus Layer 1 : Input : simCube ? R 13|sent1 | |sen t 2 | 2 : Initialize : mask ? R 13 | sent1 | |se n t 2 | to all 0.1 3 : Initialize : s1tag ? R | sent1 | to all zeros 4 : Initialize : s2tag ? R | sent2 | to all zeros 5 : sortIndex 1 = sort ( sim Cube [ 10 ] ) 6 : for each id = 1 ...| sent 1 | + | sent 2 | do teractions should be maximized .,system description,Similarity Focus Layer,0,96,33,12,0,system description : Similarity Focus Layer,0.4660194174757282,0.4925373134328358,0.42857142857142855,Forward Pass Similarity Focus Layer 1 Input simCube R 13 sent1 sen t 2 2 Initialize mask R 13 sent1 se n t 2 to all 0 1 3 Initialize s1tag R sent1 to all zeros 4 Initialize s2tag R sent2 to all zeros 5 sortIndex 1 sort sim Cube 10 6 for each id 1 sent 1 sent 2 do teractions should be maximized ,66,We also aim for the goal that similarity values of all found important word in - Algorithm,Forward Pass : Similarity Focus Layer 1 : Input : simCube ? R 13|sent1 | |sen t 2 | 2 : Initialize : mask ? R 13 | sent1 | |se n t 2 | to all 0.1 3 : Initialize : s1tag ? R | sent1 | to all zeros 4 : Initialize : s2tag ? R | sent2 | to all zeros 5 : sortIndex 1 = sort ( sim Cube [ 10 ] ) 6 : for each id = 1 ...| sent 1 | + | sent 2 | do teractions should be maximized .,method
natural_language_inference,74,"where Glove ( w ) is the embedding vector of the word w from the Glove lookup table , | S t | is the length of the sentence St .",model,model,0,56,6,6,0,model : model,0.25,0.5,0.5,where Glove w is the embedding vector of the word w from the Glove lookup table S t is the length of the sentence St ,26,We first use Glove to transform { S t } 2 t=1 into vectors word byword and subsequently input them to a bi-directional LSTM :,"We apply max pooling on the concatenation of the hidden states from both directions , which provides regularization and shorter back - propagation paths , to extract the features of the whole sequences of vectors :",method
sentiment_analysis,29,There are various approaches proposed for this research question .,introduction,introduction,0,35,27,27,0,introduction : introduction,0.20114942528735635,0.5094339622641509,0.5094339622641509,There are various approaches proposed for this research question ,10,"Compared to previous methods , our model performs better on the laptop and restaurant datasets from SemEval 2014 2 Related work Sentiment Classification Sentiment classification aims at detecting the sentiment polarity for text .",Most existing works use machine learning algorithms to classify texts in a supervision fashion .,introduction
passage_re-ranking,0,Predicted Query : weather in washington dc what is the temperature in washington,result,result,0,91,9,9,0,result : result,0.7459016393442623,0.28125,0.28125,Predicted Query weather in washington dc what is the temperature in washington,12,The wettest month is May with an average of 100 mm of rain .,Input Document : The Delaware River flows through Philadelphia into the Delaware Bay .,result
natural_language_inference,27,"Besides , we only need one fifth of the training time to achieve BiDAF 's best F1 score ( 77.0 ) on dev set . :",result,Speedup over RNNs.,0,242,31,20,0,result : Speedup over RNNs.,0.7159763313609467,0.96875,0.9523809523809524,Besides we only need one fifth of the training time to achieve BiDAF s best F1 score 77 0 on dev set ,23,The result is shown in which shows that our model is 4.3 and 7.0 times faster than BiDAF in training and inference speed .,Speed comparison between our model and BiDAF on SQuAD dataset .,result
machine-translation,4,"where ? D l represents the parameters of the local discriminator and f ? {s , t}.",training,Unsupervised Training,0,122,25,25,0,training : Unsupervised Training,0.5104602510460251,0.5681818181818182,0.5681818181818182,where D l represents the parameters of the local discriminator and f s t ,15,The local discriminator is trained to predict the language by minimizing the following crossentropy loss :,"where ? D l represents the parameters of the local discriminator and f ? {s , t}.",experiment
sentiment_analysis,48,"When decoding left part ( or right part ) , the aspect will first get processed by the decoder and hence the decoder is aware of the aspect - terms .",method,Transformer Decoders,0,126,64,12,0,method : Transformer Decoders,0.5338983050847458,0.9846153846153848,0.9230769230769232,When decoding left part or right part the aspect will first get processed by the decoder and hence the decoder is aware of the aspect terms ,27,It is equivalent to generate two sequences using two decoders .,The position tag is also used in the decoder .,method
question-answering,9,"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03932584269662921,0.38461538461538464,0.38461538461538464,In this paper we focus on this answer extraction task presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network ,33,"However , Rajpurkar et al . ( 2016 ) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text .",We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers .,abstract
text_generation,0,"where , we see that when no intermediate reward , the function is iteratively defined as the next - state value starting from state s = Y 1:t and rolling out to the end .",system description,SeqGAN via Policy Gradient,0,99,37,22,0,system description : SeqGAN via Policy Gradient,0.3055555555555556,0.3457943925233645,0.34375,where we see that when no intermediate reward the function is iteratively defined as the next state value starting from state s Y 1 t and rolling out to the end ,32,"Thus , we have :",benefit of using the discriminator D ? as a reward function is that it can be dynamically updated to further improve the generative model iteratively .,method
text_summarization,5,We use mean value standard deviation to illustrate this item .,evaluation,Evaluation Metrics,0,126,13,13,0,evaluation : Evaluation Metrics,0.5,0.5416666666666666,0.5416666666666666,We use mean value standard deviation to illustrate this item ,11,The absolute value of the length difference between the generated summaries and the actual summaries .,"The average value partially reflects the readability and informativeness , while the standard deviation links to stability .",result
relation-classification,8,This is a more straightforward way to achieve MRE in one - pass derived from previous works using position embeddings .,method,Methods,0,80,6,6,0,method : Methods,0.583941605839416,0.206896551724138,0.6666666666666666,This is a more straightforward way to achieve MRE in one pass derived from previous works using position embeddings ,20,BERT SP with position embedding on the final attention layer .,"In this method , the BERT model encode the paragraph to the last attention - layer .",method
sentiment_analysis,12,3 ) We apply our approach to two dominant neural ASC models : Memory Network ( MN ) and Transformation Network ( TNet ) .,introduction,introduction,0,43,32,32,0,introduction : introduction,0.19196428571428573,0.9696969696969696,0.9696969696969696,3 We apply our approach to two dominant neural ASC models Memory Network MN and Transformation Network TNet ,19,"To the best of our knowledge , our work is the first attempt to explore automatic attention supervision information mining for ASC .",Experimental results on several benchmark datasets demonstrate the effectiveness of our approach .,introduction
natural_language_inference,79,"Suppose that there are N paragraphs in the corpus , M words in the vocabulary , and we want to learn paragraph vectors such that each paragraph is mapped top dimensions and each word is mapped to q dimensions , then the model has the total of N p + M q parameters ( excluding the softmax parameters ) .",model,model,0,103,23,23,0,model : model,0.3843283582089552,0.41818181818181815,0.6764705882352942,Suppose that there are N paragraphs in the corpus M words in the vocabulary and we want to learn paragraph vectors such that each paragraph is mapped top dimensions and each word is mapped to q dimensions then the model has the total of N p M q parameters excluding the softmax parameters ,54,"In this step , the parameters for the rest of the model , the word vectors W and the softmax weights , are fixed .","Even though the number of parameters can be large when N is large , the updates during training are typically sparse and thus efficient ..",method
question_answering,1,We also show the performance of BIDAF with several different definitions of ? and ? functions ( Equation and 2 ) in Appendix B. Visualizations .,experiment,QUESTION ANSWERING EXPERIMENTS,0,201,38,38,0,experiment : QUESTION ANSWERING EXPERIMENTS,0.6340694006309149,0.4222222222222222,0.5757575757575758,We also show the performance of BIDAF with several different definitions of and functions Equation and 2 in Appendix B Visualizations ,22,We conjecture that separating out the attention layer results in a richer set of features computed in the first 4 layers which are then incorporated by the modeling layer .,We also show the performance of BIDAF with several different definitions of ? and ? functions ( Equation and 2 ) in Appendix B. Visualizations .,experiment
text_generation,3,It shows that the proposed model is sensitive to the unseen utterance representations .,analysis,Error Analysis,0,135,5,5,0,analysis : Error Analysis,0.9574468085106383,0.7142857142857143,0.7142857142857143,It shows that the proposed model is sensitive to the unseen utterance representations ,14,"For instance , given "" Bonjour "" as the input , it generates "" Stay out of here "" as the output .","Therefore , we would like to explore more approaches to address this problem in the future work .",result
natural_language_inference,19,"We , however , use the gated sum of SF and HF which resulted in a significant increase in accuracy .",architecture,Fusion Gate,0,132,57,12,0,architecture : Fusion Gate,0.5176470588235295,0.7037037037037037,0.75,We however use the gated sum of SF and HF which resulted in a significant increase in accuracy ,19,It is common in many papers including to use raw Sand H in gated sum .,Multi-head attention is fast and efficient because it is based on dot -product attention .,method
question_answering,0,relation types that allow dates as values .,system description,Semantic graph construction,0,89,40,20,0,system description : Semantic graph construction,0.3016949152542373,0.4444444444444444,0.9523809523809524,relation types that allow dates as values ,8,"Finally , the action a m : Encoding a graph into initial hidden states","Our semantic graph construction process allows to effectively search the space of possible graphs for a given question through an iterative application of the defined actions A on the last state St ( see , for example , ) .",method
text_summarization,9,For the decoder we experimented with both the Elman RNN and the Long - Short Term Memory ( LSTM ) architecture ( as discussed in 3.1 ) .,system description,Architectural Choices,1,99,5,5,0,system description : Architectural Choices,0.6470588235294118,0.4545454545454545,0.4545454545454545,For the decoder we experimented with both the Elman RNN and the Long Short Term Memory LSTM architecture as discussed in 3 1 ,24,"During training we measure the perplexity of the summaries in the validation set and adjust our hyper - parameters , such as the learning rate , based on this number .",We chose hyper - parameters based on a grid search and picked the one which gave the best perplexity on the validation set .,method
natural_language_inference,61,Acc Unlexicalized + Unigram and bigram features 78.2 300D LSTM encoders 80.6 300D NTI - SLSTM - LSTM encoders 83.4 4096D,model,Models,0,144,2,2,0,model : Models,0.9230769230769232,0.25,0.25,Acc Unlexicalized Unigram and bigram features 78 2 300D LSTM encoders 80 6 300D NTI SLSTM LSTM encoders 83 4 4096D,21, ,Bi-LSTM with max-pooling 84.5 300D,method
natural_language_inference,79,The above method considers the concatenation of the paragraph vector with the word vectors to predict the next word in a text window .,model,Paragraph Vector without word ordering: Distributed bag of words,0,125,45,3,0,model : Paragraph Vector without word ordering: Distributed bag of words,0.4664179104477612,0.8181818181818182,0.2307692307692308,The above method considers the concatenation of the paragraph vector with the word vectors to predict the next word in a text window ,24,Distributed bag of words,"Another way is to ignore the context words in the input , but force the model to predict words randomly sampled from the paragraph in the output .",method
text-classification,7,"In addition , we explore two capsule frameworks to integrate these four components in different ways .",model,Our Model,1,42,4,4,0,model : Our Model,0.1728395061728395,0.0425531914893617,0.8,In addition we explore two capsule frameworks to integrate these four components in different ways ,16,"It consists of four layers : ngram convolutional layer , primary capsule layer , convolutional capsule layer , and fully connected capsule layer .","In the rest of this section , we elaborate the key components in detail .",method
sentiment_analysis,37,"Also , shows the count of the samples with single aspect sentence and multi-aspect sentence .",dataset,Dataset Details,0,159,5,5,0,dataset : Dataset Details,0.6284584980237155,1.0,1.0,Also shows the count of the samples with single aspect sentence and multi aspect sentence ,16,shows the distribution of these samples by class labels ., ,experiment
relation_extraction,11,"We observe that on removing different components from RESIDE , the performance of the model degrades drastically .",ablation,Ablation Results,0,229,5,5,0,ablation : Ablation Results,0.9233870967741936,0.2777777777777778,0.7142857142857143,We observe that on removing different components from RESIDE the performance of the model degrades drastically ,17,The experimental results are presented in .,The results validate that GCNs are effective at encoding syntactic information .,result
text-classification,1,We use the tv-embeddings obtained from unlabeled data to produce additional input to LSTM by replacing and ( 3 ) by the following :,system description,Experiments (supervised),0,183,137,60,0,system description : Experiments (supervised),0.71484375,0.685,1.0,We use the tv embeddings obtained from unlabeled data to produce additional input to LSTM by replacing and 3 by the following ,23,"To reduce undesirable relations between the views such as syntactic relations , JZ15 b performed vocabulary control to remove function words from ( and only from ) the vocabulary of the target view , which we found useful also for LSTM .", ,method
natural_language_inference,1,"The default policy to obtain a non-supporting fact is to corrupt the answer fact by exchanging it s subject , its relationship or its object ( s ) with that of another fact chosen uniformly at random from the KB .",training,Generating negative examples,0,201,43,7,0,training : Generating negative examples,0.7362637362637363,0.9347826086956522,0.7,The default policy to obtain a non supporting fact is to corrupt the answer fact by exchanging it s subject its relationship or its object s with that of another fact chosen uniformly at random from the KB ,39,"For question / supporting fact pairs , we use two policies .","In this policy , the element of the fact to corrupt is chosen randomly , with a small probability ( 0.3 ) of corrupting more than one element of the answer fact .",experiment
natural_language_inference,20,Trainable initialization does not seem to add to the model 's capacity and indicates that the hierarchical initialization that we propose is indeed beneficial .,model,Fig. 1. Overall NLI Architecture,0,65,30,23,0,model : Fig. 1. Overall NLI Architecture,0.2674897119341564,0.8823529411764706,0.8518518518518519,Trainable initialization does not seem to add to the model s capacity and indicates that the hierarchical initialization that we propose is indeed beneficial ,25,Ensembling information from three separate BiLSTM layers ( with independent parameters ) improves the performance as we can see in the comparison between BiLSTM - Ens and BiLSTM - Ens - Tied .,"Finally , feeding the same input embeddings to all Bi - LSTMs of HBMP leads to an improvement over the stacked model that does not re-read the input information .",method
natural_language_inference,21,"Nonetheless , DiSAN still outperforms these fancy models , such as NCSL ( + 0.62 % ) and LR- Bi- LSTM ( + 1.12 % ) . :",model,Model,1,240,14,14,0,model : Model,0.8275862068965517,0.2413793103448276,0.6666666666666666,Nonetheless DiSAN still outperforms these fancy models such as NCSL 0 62 and LR Bi LSTM 1 12 ,19,https://nlp.stanford.edu/sentiment/,Fine - grained sentiment analysis accuracy vs. sentence length .,method
sentiment_analysis,40,"After that , we pay multiple attentions on the position - weighted memory and nonlinearly combine the attention results with a recurrent network , i.e. GRUs .",introduction,introduction,1,33,24,24,0,introduction : introduction,0.14798206278026907,0.7058823529411765,0.7058823529411765,After that we pay multiple attentions on the position weighted memory and nonlinearly combine the attention results with a recurrent network i e GRUs ,25,"The memory slices are then weighted according to their relative positions to the target , so that different targets from the same sentence have their own tailor - made memories .","Finally , we apply softmax on the output of the GRU network to predict the sentiment on the target .",introduction
sentiment_analysis,35,Fine-tuning ( FT ) : It advances SO with further finetuning the target network on D t .,baseline,baseline,1,206,11,11,0,baseline : baseline,0.8306451612903226,0.6470588235294118,0.6470588235294118,Fine tuning FT It advances SO with further finetuning the target network on D t ,16,It uses a source network trained on D s to initialize a target network and then tests it on D t .,M- DAN : It is a multi-adversarial version of Domain Adversarial Network ( DAN ) ) based on multiple domain discriminators .,result
natural_language_inference,74,We first propose a sentence encoder model that learns the representations of the sentences from the DMP task and then inject the encoder to the NLI network .,introduction,introduction,1,35,23,23,0,introduction : introduction,0.15625,0.6764705882352942,0.6764705882352942,We first propose a sentence encoder model that learns the representations of the sentences from the DMP task and then inject the encoder to the NLI network ,28,"In this paper , we propose a Discourse Marker Augmented Network for natural language inference , where we transfer the knowledge from the existing supervised task : Discourse Marker Prediction ( DMP ) , to an integrated NLI model .","Moreover , because our NLI datasets are manually annotated , each example from the datasets might get several different labels from the annotators although they will finally come to a consensus and also provide a certain label .",introduction
natural_language_inference,59,The Decomposable Attention Model,model,The Decomposable Attention Model,0,46,1,1,0,model : The Decomposable Attention Model,0.34074074074074073,0.030303030303030307,0.04,The Decomposable Attention Model,4, , ,method
natural_language_inference,94,We also provide manual analysis on the effectiveness of our commonsense selection algorithm .,introduction,introduction,0,42,20,20,0,introduction : introduction,0.10966057441253264,0.9523809523809524,0.9523809523809524,We also provide manual analysis on the effectiveness of our commonsense selection algorithm ,14,"With these additions , we further improve performance on the Narrative QA dataset , achieving 44.16 Rouge - L and 19.03 METEOR ( also verified via human evaluation ) .","Finally , to show the generalizability of our multi-hop reasoning and commonsense methods , we show some promising initial results via the addition of commonsense information over the baseline on , an extractive dataset for multi-hop reasoning from a different domain .",introduction
natural_language_inference,58,We also apply dropout with probability 0.2 to the embedding layer .,training,CNN and Daily Mail Datasets,1,152,21,17,0,training : CNN and Daily Mail Datasets,0.4537313432835821,0.2692307692307692,0.3035714285714285,We also apply dropout with probability 0 2 to the embedding layer ,13,"We choose 300 - dimensional word embeddings , and use the 300 - dimensional pretrained Glove word embeddings for initialization .",We apply bidirectional GRU for encoding query and passage into vector representations .,experiment
sentiment_analysis,49,"We found that the proposed tri-modal MMMU - BA model predicts the labels of all the nine instances correctly , whereas other models make at least one misclassification .",analysis,Analysis of Attention Mechanism,0,171,6,6,0,analysis : Analysis of Attention Mechanism,0.6758893280632411,0.08571428571428573,0.18181818181818185,We found that the proposed tri modal MMMU BA model predicts the labels of all the nine instances correctly whereas other models make at least one misclassification ,28,The gold sentiments are positive for all the utterances except u 3 & u 4 .,Elements of the rows of N 1 & N 2 matrices signify different weights across multiple utterances .,result
natural_language_inference,12,"Dropout layer is also applied on the output of each layer of MLP , with dropout rate set to 0.1 .",model,Parameter Settings,1,55,33,5,0,model : Parameter Settings,0.625,0.9428571428571428,0.7142857142857143,Dropout layer is also applied on the output of each layer of MLP with dropout rate set to 0 1 ,21,The number of hidden units for MLP in classifier is 1600 .,We used pre-trained 300D Glove 840B vectors to initialize the word embeddings .,method
text-classification,0,We refer to traditional methods as those that using a hand - crafted feature extractor and a linear classifier .,method,Traditional Methods,0,104,2,2,0,method : Traditional Methods,0.4581497797356828,0.01680672268907563,0.1111111111111111,We refer to traditional methods as those that using a hand crafted feature extractor and a linear classifier ,19, ,The classifier used is a multinomial logistic regression in all these models .,method
smile_recognition,0,"This is feasible because all experiments were run on a Tesla K40c GPU , allowing a speedup of factor 10 over traditional computations on a CPU .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.07777777777777778,1.0,1.0,This is feasible because all experiments were run on a Tesla K40c GPU allowing a speedup of factor 10 over traditional computations on a CPU ,26,"The novelty of this approach includes a comprehensive model selection of the architecture parameters , allowing to find an appropriate architecture for each expression such as smile .", ,abstract
text_generation,2,"Another real dataset we use is the COCO Image Captions Dataset , a dataset which contains groups of image - description pairs .",experiment,Middle Text Generation: COCO Image Captions,0,199,22,2,0,experiment : Middle Text Generation: COCO Image Captions,0.5685714285714286,0.5945945945945946,0.18181818181818185,Another real dataset we use is the COCO Image Captions Dataset a dataset which contains groups of image description pairs ,21, ,We take the image captions as the text to generate .,experiment
text_summarization,1,"The diversification stage leverages content selection to map the source to multiple sequences , where each mapping is modeled by focusing on different tokens in the source ( oneto - many mapping ) .",introduction,introduction,1,35,25,25,0,introduction : introduction,0.14583333333333334,0.7142857142857143,0.7142857142857143,The diversification stage leverages content selection to map the source to multiple sequences where each mapping is modeled by focusing on different tokens in the source oneto many mapping ,30,"In this paper , we present a method for diverse generation that separates diversification and generation stages .",The generation stage uses a standard encoder - decoder model to generate a target sequence given each selected content from the source ( one - to - one mapping ) .,introduction
machine-translation,9,"In this case , the total size of the embedding layer is 1.23 MB , which is equivalent to a compression rate of 98.4 % .",analysis,SENTIMENT ANALYSIS,0,207,30,30,0,analysis : SENTIMENT ANALYSIS,0.7212543554006968,0.3409090909090909,0.9375,In this case the total size of the embedding layer is 1 23 MB which is equivalent to a compression rate of 98 4 ,25,"For our proposed methods , the maximum loss - free compression rate is achieved by a 16 32 coding scheme .",We also found the classification accuracy can be substantially improved with a slightly lower compression rate .,result
natural_language_inference,84,Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.017391304347826087,0.2857142857142857,0.2857142857142857,Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare ,18, ,"Learning representations for words in the "" long tail "" of this distribution requires enormous amounts of data .",abstract
text_summarization,12,provides an example .,system description,Problem Formulation,0,74,8,8,0,system description : Problem Formulation,0.32456140350877194,0.6153846153846154,0.8888888888888888,provides an example ,4,"If | y | | x | , which means not all words in summary come from input sentence , we denote this as abstractive sentence summarization .",We focus on abstracive sentence summarization task in this paper .,method
natural_language_inference,63,"Then , st and the subset of memory vectors obtained from the memory matrix are concatenated to generate an input vector x t for the second layer , where sis the number of read vectors .",model,Memory Controller,0,79,39,9,0,model : Memory Controller,0.4388888888888889,0.5131578947368421,0.36,Then st and the subset of memory vectors obtained from the memory matrix are concatenated to generate an input vector x t for the second layer where sis the number of read vectors ,34,The contextual representations are fed into the first layer to capture interactions between contexts .,"Then , st and the subset of memory vectors obtained from the memory matrix are concatenated to generate an input vector x t for the second layer , where sis the number of read vectors .",method
text_summarization,13,We use dropout between stacked LSTM hidden states and before the final word generator layer to regularize ( with dropout probability 0.3 ) .,training,training,1,191,8,8,0,training : training,0.7153558052434457,0.6666666666666666,0.6666666666666666,We use dropout between stacked LSTM hidden states and before the final word generator layer to regularize with dropout probability 0 3 ,23,Positional embeddings have dimension 25 .,"At test time , we run beam search to produce the summary with a beam size of 5 .",experiment
natural_language_inference,52,"These consisted of short , flash - card style texts gathered from two online resources : about 700K sentences from StudyStack 7 and 25K sentences from Quizlet 8 .",baseline,Corpora,0,154,9,3,0,baseline : Corpora,0.5900383141762452,0.6,0.3333333333333333,These consisted of short flash card style texts gathered from two online resources about 700K sentences from StudyStack 7 and 25K sentences from Quizlet 8 ,26,For our pool of candidate justifications ( as well as the scores for our IR baselines ) we used the corpora that were cited as being most helpful to the top - performing systems of the Kaggle challenge .,"From these corpora , we use the top 50 sentences retrieved by the IR model as our set of candidate justifications .",result
text_summarization,7,"Let X = ( x i ) I i=1 and Y = ( y j ) J j= 1 be input and output sequences , respectively , where xi and y j are one - hot vectors , which correspond to the i - th word in the input and the j - th word in the output .",system description,Baseline RNN-based EncDec Model,0,28,7,7,0,system description : Baseline RNN-based EncDec Model,0.1854304635761589,0.08536585365853659,0.28,Let X x i I i 1 and Y y j J j 1 be input and output sequences respectively where xi and y j are one hot vectors which correspond to the i th word in the input and the j th word in the output ,48,The following are the necessary parts for explaining our proposed method .,Vt denote the vocabulary ( set of words ) of output .,method
natural_language_inference,7,"The previously defined probability distribution depends on the answer span representations , ha .",model,RASOR: RECURRENT SPAN REPRESENTATION,0,72,21,2,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.4044943820224719,0.3962264150943397,0.2222222222222222,The previously defined probability distribution depends on the answer span representations ha ,13, ,"When computing ha , we assume access to representations of individual passage words that have been augmented with a representation of the question .",method
named-entity-recognition,7,"In this case , each word is shifted ( n ) and involved in a unary action ( n ) .",model,Action Constraints,0,76,33,3,0,model : Action Constraints,0.4810126582278481,0.4852941176470588,0.2727272727272727,In this case each word is shifted n and involved in a unary action n ,16,"To make sure that each action sequence is valid , we need to make some hard constraints on the ac - 5",Then all elements are reduced to a single node ( n ? 1 ) .,method
temporal_information_extraction,1,We used the same training set and test set as CAEVO in the S +I systems .,baseline,Comparison with CAEVO,0,241,45,3,0,baseline : Comparison with CAEVO,0.9377431906614786,0.8181818181818182,0.2307692307692308,We used the same training set and test set as CAEVO in the S I systems ,17,"The proposed structured learning approach was further compared to a recent system , a CAscading EVent Ordering architecture ( CAEVO ) proposed in ( lines 10 - 13 ) .","Again , we added the E - T TLINKs predicted by CAEVO to both S+I systems .",result
sentiment_analysis,4,Termination of the training - phase is decided by early - stopping with a patience of 10 d = 100 dv = 512 dem = 100 K = 40 R = 3 epochs .,training,Training Details,1,221,4,4,0,training : Training Details,0.6779141104294478,0.3076923076923077,0.3076923076923077,Termination of the training phase is decided by early stopping with a patience of 10 d 100 dv 512 dem 100 K 40 R 3 epochs ,27,"We use the Adam optimizer ( Kingma and Ba , 2014 ) for training the parameters starting with an initial learning rate of 0.001 .",The network is subjected to regularization in the form of Dropout and Gradient - clipping for a norm of 40 .,experiment
natural_language_inference,6,Scaling up to almost one hundred languages calls for an encoder with sufficient capacity .,method,Proposed method,0,65,12,12,0,method : Proposed method,0.2620967741935484,0.75,0.75,Scaling up to almost one hundred languages calls for an encoder with sufficient capacity ,15,"In contrast , the decoder takes a language ID embedding that specifies the language to generate , which is concatenated to the input and sentence embeddings at every time step .","In this paper , we limit our study to a stacked BiLSTM with 1 to 5 layers , each 512 - dimensional .",method
sentiment_analysis,8,"Here , we describe the three different settings we conducted our experiments in :",experiment,experiment,0,157,2,2,0,experiment : experiment,0.6709401709401709,0.3333333333333333,0.3333333333333333,Here we describe the three different settings we conducted our experiments in ,13, ,"Audio- only : In this setting , we train all the classifiers using only the audio feature vectors described earlier .",experiment
sentiment_analysis,42,"However , these neural models ( e.g. LSTM ) are computationally expensive , and could not explicitly reveal the importance of context evidences with regard to an aspect .",system description,Aspect Level Sentiment Classification,0,143,7,7,0,system description : Aspect Level Sentiment Classification,0.5674603174603174,0.7,0.7,However these neural models e g LSTM are computationally expensive and could not explicitly reveal the importance of context evidences with regard to an aspect ,26,"In recent years , neural network approaches are of growing attention for their capacity to learn powerful text representation from data .","Instead , we develop simple and fast approach that explicitly encodes the context importance towards a given aspect .",method
natural_language_inference,1,"where q ?? is a random question of the dataset , not belonging to the cluser of q .",training,Generating negative examples,0,198,40,4,0,training : Generating negative examples,0.7252747252747253,0.8695652173913043,0.4,where q is a random question of the dataset not belonging to the cluser of q ,17,"For paraphrases , given a pair ( q , q ? ) , a nonparaphrase pair is generated as ( q , q ?? )","where q ?? is a random question of the dataset , not belonging to the cluser of q .",experiment
sentiment_analysis,1,"The selection of global channels is supported by prior studies showing that the asymmetry in neuronal activities between the left and right hemispheres is informative in valence and arousal predictions , , .",system description,Adjacency Matrix in RGNN,0,197,125,20,0,system description : Adjacency Matrix in RGNN,0.4974747474747474,0.6038647342995169,0.7142857142857143,The selection of global channels is supported by prior studies showing that the asymmetry in neuronal activities between the left and right hemispheres is informative in valence and arousal predictions ,31,depicts the global connections in both SEED and SEED - IV .,"To leverage the differential asymmetry information , we initialize the global inter-channel relations in A to [ ?1 , 0 ] as follows :",method
natural_language_inference,30,"By mapping answers into the same space one can query any knowledge base independent of its schema , without requiring any grammar or lexicon .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.03100775193798449,0.75,0.75,By mapping answers into the same space one can query any knowledge base independent of its schema without requiring any grammar or lexicon ,24,"In this paper , we instead take the radical approach of learning to map questions to vectorial feature representations .",Our method is trained with a new optimization procedure combining stochastic gradient descent followed by a fine - tuning step using the weak supervision provided by blending automatically and collaboratively generated resources .,abstract
sentence_classification,2,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,experiment,Experimental Setting,1,155,17,16,0,experiment : Experimental Setting,0.6150793650793651,0.6071428571428571,0.5925925925925926,We use dropout on all nonlinear connections with a dropout rate of 0 5 ,15,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .","We also use an l 2 constraint of 3 , following for accurate comparisons .",experiment
relation_extraction,9,The remainder of this section will provide further details about this architecture .,model,The Proposed Model,0,57,10,10,0,model : The Proposed Model,0.2727272727272727,0.11363636363636365,0.8333333333333334,The remainder of this section will provide further details about this architecture ,13,secondary attention pooling layer is used to determine the most useful convolved features for relation classification from the output based on an attention pooling matrix .,provides an overview of the notation we will use for this .,method
natural_language_inference,25,Contextualized Word Representations for Reading Comprehension,title,title,1,2,1,1,0,title : title,0.019230769230769232,1.0,1.0,Contextualized Word Representations for Reading Comprehension,6, , ,title
natural_language_inference,9,"In , the question Where is apple ? is transformed into",model,RESULTS.,0,247,67,33,0,model : RESULTS.,0.7484848484848485,0.8481012658227848,0.7333333333333333,In the question Where is apple is transformed into,9,This helps us understand the flow of information in the networks .,"In , the question Where is apple ? is transformed into",method
natural_language_inference,39,We define the unpack operation below :,model,Context Modeling,0,62,15,15,0,model : Context Modeling,0.3009708737864077,0.9375,0.9375,We define the unpack operation below ,7,"At time step t , the Bi - LSTMs hidden state h bit is a concatenation of the hidden state hf or t of LST M f and the hidden state h back t of LST Mb , representing the neighbor contexts of input x tin the sequence .","Context modeling with Bi - LSTMs allows all the following components to be built over word contexts , rather than over individual words .",method
named-entity-recognition,8,"Each downstream task has separate fine - tuned models , even though they are initialized with the same pre-trained parameters .",system description,BERT,0,75,36,6,0,system description : BERT,0.1937984496124031,0.9230769230769232,0.6666666666666666,Each downstream task has separate fine tuned models even though they are initialized with the same pre trained parameters ,20,"For finetuning , the BERT model is first initialized with the pre-trained parameters , and all of the parameters are fine - tuned using labeled data from the downstream tasks .",The question - answering example in will serve as a running example for this section .,method
sentence_compression,1,"The operator denotes element - wise multiplication , the matrices W 1 , . . . , W 8 and the vector h 0 are the parameters of the model , and all the nonlinearities are computed element - wise .",baseline,Baseline,0,87,50,50,0,baseline : Baseline,0.4162679425837321,0.5813953488372093,0.5813953488372093,The operator denotes element wise multiplication the matrices W 1 W 8 and the vector h 0 are the parameters of the model and all the nonlinearities are computed element wise ,32,"Then , given a sequence of inputs ( x 1 , . . . , x T ) , the LSTM computes the h-sequence ( h 1 , . . . , h T ) and the m-sequence ( m 1 , . . . , m T ) as follows",Stochastic gradient descent is used to maximize the training objective ( Eq. ( 1 ) ) w.r.t. all the LSTM parameters .,result
natural_language_inference,23,is the common history - of - word for both context and question .,DETAILED CONFIGURATIONS IN THE ABLATION STUDY,DETAILED CONFIGURATIONS IN THE ABLATION STUDY,0,309,10,10,0,DETAILED CONFIGURATIONS IN THE ABLATION STUDY : DETAILED CONFIGURATIONS IN THE ABLATION STUDY,0.6023391812865497,0.10638297872340426,0.30303030303030304,is the common history of word for both context and question ,12,This is as simple as changing,All other places remains the same as High - Level .,result
temporal_information_extraction,1,We want to construct a temporal graph as in for the events in boldface in Ex1 .,introduction,introduction,0,23,14,14,0,introduction : introduction,0.08949416342412451,0.3181818181818182,0.3181818181818182,We want to construct a temporal graph as in for the events in boldface in Ex1 ,17,"For example , consider the following snippet taken from the training set provided in the TE3 workshop .","Ex1 . . . tons of earth cascaded down a hillside , ripping two houses from their foundations .",introduction
sentiment_analysis,3,"Finally , the concept - enriched word representationt can be obtained via a linear transformation :",model,Dynamic Context-Aware Affective Graph Attention,0,148,83,42,0,model : Dynamic Context-Aware Affective Graph Attention,0.5068493150684932,0.680327868852459,0.9130434782608696,Finally the concept enriched word representationt can be obtained via a linear transformation ,14,The analysis of ? k is discussed in Section 5.2 .,where [ ; ] denotes concatenation and W ? R d2 d denotes a model parameter .,method
named-entity-recognition,6,"Also , it is worth mentioning that our embeddings are trained on 1.3B words compared to 42B for SSKIP .",ablation,Ablation Results,0,186,9,9,0,ablation : Ablation Results,0.8773584905660378,1.0,1.0,Also it is worth mentioning that our embeddings are trained on 1 3B words compared to 42B for SSKIP ,20,"Still , LS vectors seem to encode a large portion of the information needed to model the NER task .", ,result
natural_language_inference,99,"Here , It is rather remarkable that we do not immediately put the Bi - GRU on the passage words .",model,Contextual Encoding,0,89,12,3,0,model : Contextual Encoding,0.3531746031746032,0.125,0.05555555555555555,Here It is rather remarkable that we do not immediately put the Bi GRU on the passage words ,19,We use Gated Recurrent Unit ( Cho et al. 2014 ) with bi-directions to model the contextual representations .,"Here , It is rather remarkable that we do not immediately put the Bi - GRU on the passage words .",method
text-classification,9,"3 ) MCFA moves the vectors inside the same space , thus preserves the meaning of vector dimensions .",introduction,introduction,0,51,41,41,0,introduction : introduction,0.2023809523809524,0.9761904761904762,0.9761904761904762,3 MCFA moves the vectors inside the same space thus preserves the meaning of vector dimensions ,17,2 ) MCFA is extensible and improves the accuracy as the number of translated sentences increases .,"Results show that a convolutional neural network ( CNN ) attached with MCFA significantly improves the classification performance of CNN , achieving state of the 1 Hereon , we mean to "" fix "" as to "" correct , repair , or alter . "" art performance over multiple data sets .",introduction
sentiment_analysis,23,"Kernel machines , e.g. , SVM , are exploited in and by specifying a certain measure of similarity between sentences , without explicit feature representation .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.051369863013698634,0.2083333333333333,0.2083333333333333,Kernel machines e g SVM are exploited in and by specifying a certain measure of similarity between sentences without explicit feature representation ,23,"Feature engineering - for example , n-gram features , dependency subtree features , or more dedicated ones - can play an important role in modeling sentences .","Recent advances of neural networks bring new techniques in understanding natural languages , and have exhibited considerable potential .",introduction
question-answering,9,These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .,experiment,EXPERIMENTAL SETUP,0,107,3,3,0,experiment : EXPERIMENTAL SETUP,0.6011235955056179,0.3333333333333333,0.3333333333333333,These embeddings cover 200 k words and all out of vocabulary OOV words are projected onto one of 1 m randomly initialized 300d embeddings ,25,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .",experiment
sentiment_analysis,18,Impact of Target Representation,analysis,Impact of Target Representation,0,204,17,1,0,analysis : Impact of Target Representation,0.8535564853556485,0.3695652173913043,0.06666666666666668,Impact of Target Representation,4, , ,result
sarcasm_detection,0,This particular technique of constructing word sequence featurizations has been previously studied and established as a strong baseline for a multitude of supervised NLP prediction tasks .,method,Sentence Embeddings,0,154,14,3,0,method : Sentence Embeddings,0.875,0.5384615384615384,0.75,This particular technique of constructing word sequence featurizations has been previously studied and established as a strong baseline for a multitude of supervised NLP prediction tasks ,27,"Given a document , taking the elementwise sum of embeddings of its words provides a simple low - dimensional document representation .","We use 1600 - dimensional GloVe representations trained on the Amazon product corpus , which is used instead of Common Crawl because of the semantic closeness between sentiment and sarcasm .",method
sentiment_analysis,5,"The obtained experimental results are shown in , from which we can see the left hemisphere is superior to the right for EEG emotion recognition , especially in the experiments on SEED - IV and MPED datasets .",The activity maps of the paired EEG electrodes,The performance based on single hemispheric EEG data,0,251,27,5,0,The activity maps of the paired EEG electrodes : The performance based on single hemispheric EEG data,0.9471698113207548,0.75,0.625,The obtained experimental results are shown in from which we can see the left hemisphere is superior to the right for EEG emotion recognition especially in the experiments on SEED IV and MPED datasets ,35,"Therefore , in this section , we focus on this problem and conduct the same experiments by separately feeding our BiHDM with the left and right hemispheric data .","Besides , comparing it with the results in ( a ) , which are based on feeding the model with less symmetric electrodes ' data , we can observe that results are comparable or even better than this experiment based on single hemisphere data .",others
natural_language_inference,69,Note that the document lengths in WIKIHOP correspond to the lengths of the first paragraphs of WIKIPEDIA articles .,Appendix: Versions,Appendix: Versions,0,333,6,6,0,Appendix: Versions : Appendix: Versions,0.9652173913043478,0.3333333333333333,0.42857142857142855,Note that the document lengths in WIKIHOP correspond to the lengths of the first paragraphs of WIKIPEDIA articles ,19,shows the distribution of document lengths for both datasets .,"MED - HOP on the other hand reflects the length of research paper abstracts , which are generally longer .",others
sentence_classification,0,"For each task , we output the class with the highest probability in y .",model,Structural scaffolds,0,76,49,30,0,model : Structural scaffolds,0.2846441947565543,0.98,0.967741935483871,For each task we output the class with the highest probability in y ,14,"We are only interested in the output y ( 1 ) and the rest of outputs ( y ( 2 ) , ... , y ( n ) ) are regarding the scaffold tasks and only used in training to inform the model of knowledge in the structure of the scientific documents .",An alternative inference method is to sample from the output distribution .,method
part-of-speech_tagging,3,"To test the effectiveness of pretrained word embeddings , we experimented with randomly initialized embeddings with 100 dimensions , where embeddings are uni -",training,Parameter Initialization,0,90,10,5,0,training : Parameter Initialization,0.4433497536945813,0.2857142857142857,0.5,To test the effectiveness of pretrained word embeddings we experimented with randomly initialized embeddings with 100 dimensions where embeddings are uni ,22,"We also run experiments on two other sets of published embeddings , namely Senna 50 dimensional embeddings 2 trained on Wikipedia and Reuters RCV - 1 corpus , and Google 's Word2 Vec 300 - dimensional embeddings 3 trained on 100 billion words from Google News .",where dim is the dimension of embeddings .,experiment
semantic_role_labeling,3,So it is crucial to encode positions of each input words .,system description,Position Encoding,0,121,83,3,0,system description : Position Encoding,0.4583333333333333,0.7614678899082569,0.42857142857142855,So it is crucial to encode positions of each input words ,12,The attention mechanism itself can not distinguish between different positions .,"There are various ways to encode positions , and the simplest one is to use an additional position embedding .",method
sentiment_analysis,27,"Comparing the results of SDGCN - A w/o position and SDGCN - G w/o position , SDGCN - A and SDGCN - G , respectively , we observe that the GCN built with global - relation is slightly higher than built with adjacent - relation in both accuracy and Macro - F1 measure .",result,Overall results,1,227,11,11,0,result : Overall results,0.822463768115942,0.22,0.6470588235294118,Comparing the results of SDGCN A w o position and SDGCN G w o position SDGCN A and SDGCN G respectively we observe that the GCN built with global relation is slightly higher than built with adjacent relation in both accuracy and Macro F1 measure ,46,It indicates that the discard of the recurrent neural networks can reduce the size of model while lead to the loss of performance .,This may indicate that the adjacent relationship is not sufficient to capture the interactive information among multiple aspects due to the neglect of the long - distance relation of aspects .,result
natural_language_inference,63,"q :,j is attention weights of questions to j th word in the document .",model,Contextual Representation,0,66,26,18,0,model : Contextual Representation,0.3666666666666665,0.34210526315789475,0.8181818181818182,q j is attention weights of questions to j th word in the document ,15,We can get the attention weights of question words to each document word by applying column - wise softmax to S.,We can get attended question vectors for the document words by multiplying entire attention matrix A q to the contextual embedding of question C q .,method
natural_language_inference,50,"Additionally and due to longstanding nature of this dataset , there have been a huge number of works based on traditional feature engineering approaches which we also report .",baseline,Compared Baselines,0,193,9,9,0,baseline : Compared Baselines,0.6088328075709779,0.8181818181818182,0.8181818181818182,Additionally and due to longstanding nature of this dataset there have been a huge number of works based on traditional feature engineering approaches which we also report ,28,These three baselines are reported in the original Wik - iQA paper .,"For the clean version of this dataset , we also compare with AP - CNN and QA - BiLSTM / CNN .",result
sentiment_analysis,10,"Hence , we capture context ct relevant to the utterance u t as follows :",model,Our Model,0,91,31,31,0,model : Our Model,0.3540856031128405,0.5254237288135594,0.5535714285714286,Hence we capture context ct relevant to the utterance u t as follows ,14,"Speaker Update ( Speaker GRU ) : Speaker usually frames the response based on the context , which is the preceding utterances in the conversation .","2 ) , we calculate attention scores ? over the previous global states representative of the previous utterances .",method
relation_extraction,8,"Similar to , we use PFs to specify entity pairs .",methodology,Position Embeddings,0,109,25,3,0,methodology : Position Embeddings,0.4052044609665427,0.2427184466019417,0.5,Similar to we use PFs to specify entity pairs ,10,"In relation extraction , we focus on assigning labels to entity pairs .",PF is defined as the combination of the relative distances from the current word toe 1 and e 2 .,method
sentiment_analysis,19,We propose a general and effective improvement to the BiLSTM model which encodes each suffix and prefix of a sequence of tokens in both forward and reverse directions .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.0457516339869281,0.35714285714285715,0.35714285714285715,We propose a general and effective improvement to the BiLSTM model which encodes each suffix and prefix of a sequence of tokens in both forward and reverse directions ,29,"However , BiLSTMs are known to suffer from sequential bias the contextual representation of a token is heavily influenced by tokens close to it in a sentence .",We call our model Suffix Bidirectional LSTM or SuBiLSTM .,abstract
natural_language_inference,44,"Also , by controlling the threshold , the number of sentences can be dynamically controlled during the inference .",model,Sentence Selector,0,89,30,27,0,model : Sentence Selector,0.3111888111888112,0.967741935483871,0.9642857142857144,Also by controlling the threshold the number of sentences can be dynamically controlled during the inference ,17,"This method allows the model to select a variable number of sentences for each question , as opposed to a fixed number of sentences for all questions .","We define Dyn ( for Dynamic ) as this method , and define Top k as the method which simply selects the top -k sentences for each question .",method
natural_language_inference,68,The match - LSTM essentially sequentially aggregates the matching of the attention - weighted premise to each token of the hypothesis and uses the aggregated matching result to make a final prediction . :,method,MATCH-LSTM,0,61,11,7,0,method : MATCH-LSTM,0.24497991967871485,0.09649122807017543,0.4375,The match LSTM essentially sequentially aggregates the matching of the attention weighted premise to each token of the hypothesis and uses the aggregated matching result to make a final prediction ,31,"This weighted premise is then to be combined with a vector representation of the current token of the hypothesis and fed into an LSTM , which we call the match - LSTM .",An overview of our two models .,method
natural_language_inference,22,"On the remaining 15.9 % , Ruminate Reader got only partial matches ( i.e. , answers that partially overlapped with reference answers ) , with an average F1 score of 56.0 % .",performance,performance,0,213,5,5,0,performance : performance,0.968181818181818,0.8333333333333334,0.8333333333333334,On the remaining 15 9 Ruminate Reader got only partial matches i e answers that partially overlapped with reference answers with an average F1 score of 56 0 ,29,"On 70.6 % of examples , Ruminate Reader achieves a perfect F1 score .","Comparing to the jNet ) whose success answers occupy 69.1 % of all answers , failure score answers 14.9 % and partial success 16.01 % with an average F1 score of 58.0 % , our model works better on increasing successes and reducing failures .",result
natural_language_inference,34,"As we can see in , regarding the supporting sentences as the ground truth of reasoning chains , our framework can predict reliable information flow .",result,ESP EM (Exact Match),0,263,43,11,0,result : ESP EM (Exact Match),0.8915254237288136,0.5733333333333334,0.3666666666666665,As we can see in regarding the supporting sentences as the ground truth of reasoning chains our framework can predict reliable information flow ,24,"We choose k as 1 , 2 , 5 , 10 to compute ESP EM and ESP Recall scores .",The most informative flow can cover the supporting facts and help produce reliable reasoning results .,result
natural_language_inference,78,Alignment between sentences has become a staple technique in NLI research and many recent state - of - the - art models such as the Enhanced Sequential Inference Model ( ESIM ) also incorporate the alignment strategy .,introduction,introduction,0,22,11,11,0,introduction : introduction,0.07971014492753623,0.28205128205128205,0.28205128205128205,Alignment between sentences has become a staple technique in NLI research and many recent state of the art models such as the Enhanced Sequential Inference Model ESIM also incorporate the alignment strategy ,33,Standard feed - forward neural networks are commonly used to model similarity between aligned ( decomposed ) sub-phrases and then aggregated into the final prediction layers .,"The difference here is that ESIM considers a nonparameterized comparison scheme , i.e. , concatenating the subtraction and element - wise product of aligned sub-phrases , along with two original sub-phrases , into the final comparison vector .",introduction
sentiment_analysis,11,Separate histories are created for both speakers .,introduction,introduction,0,34,24,24,0,introduction : introduction,0.09883720930232558,0.5106382978723404,0.5106382978723404,Separate histories are created for both speakers ,8,"In order to detect the emotion of a particular utterance , say u i , it gathers its histories by collecting previous utterances within a context window .",These histories are then modeled into memory cells using gated recurrent units ( GRUs ) .,introduction
natural_language_inference,20,The third model ( BiLSTM - Ens - Tied ) connects the three layers by tying parameters to each other .,model,Fig. 1. Overall NLI Architecture,0,59,24,17,0,model : Fig. 1. Overall NLI Architecture,0.24279835390946505,0.7058823529411765,0.6296296296296297,The third model BiLSTM Ens Tied connects the three layers by tying parameters to each other ,17,The second model ( BiLSTM - Ens - Train ) adds a trainable initialization to each layer to study the impact of the hierarchical initialization that we propose in our architecture .,"Finally , the fourth model ( BiLSTM - Stack ) implements a standard hierarchical network with stacked layers that do not re-read the original input .",method
relation-classification,2,The final results are displayed in F 1 metric as a macro -average across the folds .,dataset,Datasets and evaluation metrics,0,200,29,29,0,dataset : Datasets and evaluation metrics,0.6779661016949152,0.935483870967742,0.935483870967742,The final results are displayed in F 1 metric as a macro average across the folds ,17,"Since there are no official sets , we evaluate our model using 10 - fold cross- validation where 10 % of the data was used as validation and 10 % for test set similar to .","The dataset consists of 10,652 entities and 6,682 relations .",experiment
relation_extraction,5,"However , pruning too aggressively ( e.g. , keeping only the dependency path ) could lead to loss of crucial information and conversely hurt robustness .",model,Incorporating Off-path Information with Path-centric Pruning,0,93,58,5,0,model : Incorporating Off-path Information with Path-centric Pruning,0.35361216730038025,0.8656716417910447,0.35714285714285715,However pruning too aggressively e g keeping only the dependency path could lead to loss of crucial information and conversely hurt robustness ,23,It is therefore desirable to combine our GCN models with tree pruning strategies to further improve performance .,"For instance , the negation in is neglected when a model is restricted to only looking at the dependency path between the entities .",method
natural_language_inference,79,It has 11855 sentences taken from the movie review site Rotten Tomatoes .,experiment,Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0,146,11,4,0,experiment : Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0.5447761194029851,0.22,0.21052631578947367,It has 11855 sentences taken from the movie review site Rotten Tomatoes ,13,This dataset was first proposed by and subsequently extended by as a benchmark for sentiment analysis .,"The dataset consists of three sets : 8544 sentences for training , 2210 sentences for test and 1101 sentences for validation ( or development ) .",experiment
relation_extraction,7,"Although the Shortest Dependency Path ( SDP ) proposed by tries to get rid of irrelevant words for relation extraction , it is not suitable to handle such informal sentences .",introduction,introduction,0,28,17,17,0,introduction : introduction,0.10810810810810813,0.5,0.5,Although the Shortest Dependency Path SDP proposed by tries to get rid of irrelevant words for relation extraction it is not suitable to handle such informal sentences ,28,"To be more detail , there are about 12 noisy words in each sentence on average , and 99.4 % of sentences in the NYT - 10 dataset have noise .","Moreover , word - level attention has been leveraged to alleviate the impact of noisy words , but it weakens the importance of entity features for relation extraction .",introduction
natural_language_inference,33,"In this section , we describe our approach to evaluate the quality of our learned representations , present the results of our evaluation and discuss our findings .",system description,"EVALUATION STRATEGIES, EXPERIMENTAL RESULTS & DISCUSSION",0,124,67,2,0,"system description : EVALUATION STRATEGIES, EXPERIMENTAL RESULTS & DISCUSSION",0.475095785440613,1.0,1.0,In this section we describe our approach to evaluate the quality of our learned representations present the results of our evaluation and discuss our findings ,26, , ,method
sentiment_analysis,0,This internal hidden state is updated at each time step with the input data x t and the hidden state of the previous time step h t?1 as follows :,model,Audio Recurrent Encoder (ARE),0,49,8,4,0,model : Audio Recurrent Encoder (ARE),0.2752808988764045,0.13114754098360654,0.21052631578947367,This internal hidden state is updated at each time step with the input data x t and the hidden state of the previous time step h t 1 as follows ,31,"Once MFCC features have been extracted from an audio signal , a subset of the sequential features is fed into the RNN ( i.e. , gated recurrent units ( GRUs ) ) , which leads to the formation of the network 's internal hidden state ht to model the time series patterns .","where f ? is the RNN function with weight parameter ? , ht represents the hidden state at t-th time step , and x t represents the t - th MFCC features in x = {x 1:ta }.",method
text_summarization,5,"Retrieve , Rerank and Rewrite : Soft Template Based Neural Summarization",title,title,1,2,1,1,0,title : title,0.007936507936507936,1.0,1.0,Retrieve Rerank and Rewrite Soft Template Based Neural Summarization,9, , ,title
sentiment_analysis,5,The over all optimization process follows the rules below,model,The optimization of BiHDM,0,131,67,7,0,model : The optimization of BiHDM,0.4943396226415094,0.943661971830986,0.7,The over all optimization process follows the rules below,9,GRL acts as an identity transform in the forwardpropagation but reverses the gradient sign while performing the back - propagation operation .,where ? is the learning rate .,method
sentiment_analysis,24,"We employ a fully - connected layer with softmax activation as the decoding layer , which maps ho to ? o .",method,Document-Level Tasks,0,114,59,11,0,method : Document-Level Tasks,0.3665594855305466,0.44029850746268656,0.9166666666666666,We employ a fully connected layer with softmax activation as the decoding layer which maps ho to o ,19,The final document representation is computed as,"We employ a fully - connected layer with softmax activation as the decoding layer , which maps ho to ? o .",method
natural_language_inference,88,The dataset contains approximately 1 million tokens and a vocabulary size of 10K .,model,Language Modeling,0,149,4,4,0,model : Language Modeling,0.6032388663967612,0.12121212121212124,0.12121212121212124,The dataset contains approximately 1 million tokens and a vocabulary size of 10K ,14,"Following common practice , we trained on sections 0 - 20 ( 1M words ) , used sections 21 - 22 for validation ( 80 K words ) , and sections 23 - 24 ( 90 K words for testing ) .",The average sentence length is 21 .,method
relation_extraction,12,Direct connections are introduced from any layer to all its preceding layers .,system description,Densely Connected Layer,0,119,64,6,0,system description : Densely Connected Layer,0.3617021276595745,0.6464646464646465,0.75,Direct connections are introduced from any layer to all its preceding layers ,13,Dense connectivity is shown in .,"Mathematically , we first define g ( l ) j as the concatenation of the initial node representation and the node representations produced in layers 1 , , l ? 1 :",method
named-entity-recognition,9,Availability and implementation :,abstract,abstract,0,14,12,12,0,abstract : abstract,0.07035175879396985,0.9230769230769232,0.9230769230769232,Availability and implementation ,4,Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts .,"We make the pre-trained weights of BioBERT freely available at https://github. com/naver/biobert-pretrained , and the source code for fine - tuning",abstract
text_generation,5,Jang et al. ; propose a continuous approximation to sampling from a categorical distribution .,system description,Semi-supervised VAE,0,128,60,15,1,system description : Semi-supervised VAE,0.43389830508474575,0.7894736842105263,0.4838709677419355,Jang et al propose a continuous approximation to sampling from a categorical distribution ,14,where ? controls the trade off between generative and discriminative terms .,"Let u be a categorical distribution with probabilities ? 1 , ? 2 , ... , ? c .",method
sentiment_analysis,2,"Since a simple and effective method to learn distributed representation was proposed , neural networks enhance target - dependent sentiment analysis significantly .",system description,Neural Network for Target-dependent Sentiment Analysis,0,210,7,2,0,system description : Neural Network for Target-dependent Sentiment Analysis,0.9251101321585904,0.3684210526315789,0.2857142857142857,Since a simple and effective method to learn distributed representation was proposed neural networks enhance target dependent sentiment analysis significantly ,21, ,"split a tweet into a left context and aright context according to a given target , using distributed word representations and neural pooling functions to extract features .",method
text_summarization,6,"As shown in Section 3.3 , both the recurrent deterministic decoder and the recurrent generative decoder are designed based on neural networks .",system description,Learning,0,159,3,3,0,system description : Learning,0.6068702290076335,0.3333333333333333,0.3333333333333333,As shown in Section 3 3 both the recurrent deterministic decoder and the recurrent generative decoder are designed based on neural networks ,23,"Although the proposed model contains a recurrent generative decoder , the whole framework is fully differentiable .","Therefore , all the parameters in our model can be optimized in an end - to - end paradigm using back - propagation .",method
text-to-speech_synthesis,1,"The regulator is built on a phoneme duration predictor , which predicts the duration of each phoneme .",introduction,introduction,1,31,18,18,0,introduction : introduction,0.1415525114155251,0.6666666666666666,0.6666666666666666,The regulator is built on a phoneme duration predictor which predicts the duration of each phoneme ,17,"Since a mel-spectrogram sequence is much longer than its corresponding phoneme sequence , in order to solve the problem of length mismatch between the two sequences , FastSpeech adopts a length regulator that up - samples the phoneme sequence according to the phoneme duration ( i.e. , the number of mel- spectrograms that each phoneme corresponds to ) to match the length of the mel-spectrogram sequence .",Our proposed FastSpeech can address the above - mentioned three challenges as follows :,introduction
natural_language_inference,46,"This is a challenge as dialogue is typically non-descriptive , whereas the questions were asked based on descriptive summaries , requiring models to "" read between the lines "" .",analysis,FRANK (to Dana),0,255,25,14,0,analysis : FRANK (to Dana),0.8585858585858586,0.9615384615384616,0.9333333333333332,This is a challenge as dialogue is typically non descriptive whereas the questions were asked based on descriptive summaries requiring models to read between the lines ,27,"As shown in , reading comprehension on movie scripts requires understanding of written dialogue .",We expect that understanding narratives as complex as those presented in Narrative QA will require transferring text understanding capability from other supervised learning tasks .,result
natural_language_inference,41,"For sequence classification tasks , the same input is fed into the encoder and decoder , and the final hidden state of the final decoder token is fed into new multi -class linear classifier .",architecture,Token Classification Tasks,0,68,30,2,0,architecture : Token Classification Tasks,0.264591439688716,0.33707865168539325,0.3333333333333333,For sequence classification tasks the same input is fed into the encoder and decoder and the final hidden state of the final decoder token is fed into new multi class linear classifier ,33, ,This approach is related to the CLS token in BERT ; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input ( ) .,method
named-entity-recognition,1,"However , even systems that have relied extensively on unsupervised features have used these to augment , rather than replace , hand - engineered features ( e.g. , knowledge about capitalization patterns and character classes in a particular language ) and specialized knowledge resources ( e.g. , gazetteers ) .",introduction,introduction,0,16,8,8,0,introduction : introduction,0.07729468599033816,0.4705882352941176,0.4705882352941176,However even systems that have relied extensively on unsupervised features have used these to augment rather than replace hand engineered features e g knowledge about capitalization patterns and character classes in a particular language and specialized knowledge resources e g gazetteers ,42,Unsupervised learning from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision .,"In this paper , we present neural architectures for NER that use no language - specific resources or features beyond a small amount of supervised training data and unlabeled corpora .",introduction
natural_language_inference,62,"First we use knowledge aided mutual attention ( introduced later ) to fuse C Q into C P , the outputs of which are represented as G ? R dn .",architecture,Overall Architecture,0,113,18,18,0,architecture : Overall Architecture,0.5044642857142857,0.23376623376623376,0.4615384615384616,First we use knowledge aided mutual attention introduced later to fuse C Q into C P the outputs of which are represented as G R dn ,27,This layer maps the context embeddings to the coarse memories .,"First we use knowledge aided mutual attention ( introduced later ) to fuse C Q into C P , the outputs of which are represented as G ? R dn .",method
natural_language_inference,16,shows the performances of the state - of - the - art models and our models .,experiment,experiment,0,182,21,21,0,experiment : experiment,0.8544600938967136,0.65625,0.65625,shows the performances of the state of the art models and our models ,14,"We design the ensemble model by simply averaging the probability distributions of four "" BiMPM "" models , and each of the "" BiMPM "" model has the same architecture , but is initialized with a different seed .","First , we can see that "" Only P ? Q "" works significantly better than "" Only P ? Q "" , which tells us that , for natural language inference , matching the hypothesis against the premise is more effective than the other way around .",experiment
natural_language_inference,64,"All these experiments using NAD for training and accurate datasets for testing , show that TANDA is robust to real - world noise of NAD as it always provides significantly large gains oversimple FT .",result,Results,0,237,7,7,0,result : Results,0.9442231075697212,1.0,1.0,All these experiments using NAD for training and accurate datasets for testing show that TANDA is robust to real world noise of NAD as it always provides significantly large gains oversimple FT ,33,Using TANDA with RoBERTa produces an even higher improvement than with BERT ., ,result
sentiment_analysis,24,"Furthermore , IMN allows AE and AS to be trained together with related document - level tasks , exploiting the knowledge from larger document - level corpora .",introduction,introduction,1,21,12,12,0,introduction : introduction,0.06752411575562701,0.6,0.6,Furthermore IMN allows AE and AS to be trained together with related document level tasks exploiting the knowledge from larger document level corpora ,24,"In this work , we propose an interactive multitask learning network ( IMN ) , which solves both tasks simultaneously , enabling the interactions between both tasks to be better exploited .",IMN introduces a novel message passing mechanism that allows informative interactions between tasks .,introduction
natural_language_inference,82,"Another technique , called "" Byway "" , is based on the observation that the attention mechanism ( 5 ) must always promote some entity occurrences ( since all weights sum to 1 ) , which could be difficult if the entity does not answer the query .",result,Results,0,81,3,3,0,result : Results,0.7105263157894737,0.2727272727272727,0.2727272727272727,Another technique called Byway is based on the observation that the attention mechanism 5 must always promote some entity occurrences since all weights sum to 1 which could be difficult if the entity does not answer the query ,39,"As shown in , Max - pooling described in Section 2.2 drastically improves performance , showing the effect of accumulating information on entities .","To counter this , we make an artificial occurrence for each entity with no contexts , which serves as a byway to attend when no other occurrences can be reasonably related to the query .",result
text_summarization,11,seq2seq : fatah officially officially elects abbas as candidate for candidate .,introduction,introduction,0,15,8,8,0,introduction : introduction,0.10416666666666667,0.3636363636363637,0.3636363636363637,seq2seq fatah officially officially elects abbas as candidate for candidate ,11,", # ### , the official wafa news agency reported .",Gold : fatah officially elects abbas as candidate for presidential election :,introduction
sentiment_analysis,44,"This paper focuses on studying two fundamental NLP tasks , Discourse Parsing and Sentiment Analysis .",introduction,introduction,0,13,2,2,0,introduction : introduction,0.05909090909090909,0.06451612903225806,0.06451612903225806,This paper focuses on studying two fundamental NLP tasks Discourse Parsing and Sentiment Analysis ,15, ,"The importance of these tasks and their wide applications ( e.g. , , ) has initiated much interest in studying both , but no method yet exists that can come close to human performance in solving them .",introduction
natural_language_inference,26,"Note that this part is basically the same as the implementation in BERT without any modification , to avoid extra influence and focus on the intrinsic performance of SemBERT .",implementation,Task-specific Fine-tuning,0,112,18,6,0,implementation : Task-specific Fine-tuning,0.5283018867924528,0.6666666666666666,0.4,Note that this part is basically the same as the implementation in BERT without any modification to avoid extra influence and focus on the intrinsic performance of SemBERT ,29,We transform the fused contextual semantic and LM representations h to a lower dimension and obtain the prediction distributions .,We outline hereto keep the completeness of the implementation .,experiment
text_summarization,3,"In this way , the samples in the training corpus are distantly annotated as either relevant or irrelevant for model adaption , noting that the model is pre-trained with the MLE loss before switching to distantly - supervised training .",system description,Distant Supervision (DS) for Model Adaption,0,145,90,17,0,system description : Distant Supervision (DS) for Model Adaption,0.5991735537190083,0.6870229007633588,0.29310344827586204,In this way the samples in the training corpus are distantly annotated as either relevant or irrelevant for model adaption noting that the model is pre trained with the MLE loss before switching to distantly supervised training ,38,"The divergences are averaged within the testing set , which indicates the over all distances between testing set and each of the reference summarydocument pairs .","To evaluate the effectiveness of our proposed model , we conducted training and testing on two popular datasets .",method
passage_re-ranking,0,We then index and rank the expanded documents exactly as in the BM25 method above .,experiment,experiment,1,72,12,12,0,experiment : experiment,0.5901639344262295,0.5454545454545454,0.5454545454545454,We then index and rank the expanded documents exactly as in the BM25 method above ,16,We first expand the documents using the proposed Doc2query method .,https://github.com/dfcf93/MSMARCO /,experiment
natural_language_inference,68,"To build this reverse match - LSTM , we first define",method,Answer Pointer Layer,0,126,76,24,0,method : Answer Pointer Layer,0.5060240963855421,0.6666666666666666,0.3870967741935484,To build this reverse match LSTM we first define,9,The purpose is to obtain a representation that encodes the contexts from both directions for each token in the passage .,"Note that the parameters here ( W q , W p , W r , b p , wand b ) are the same as used in Eqn .",method
sentiment_analysis,35,Note that we are the first to explore transfer techniques and achieve the best performances in this task .,analysis,analysis,0,215,3,3,0,analysis : analysis,0.8669354838709677,0.0967741935483871,0.0967741935483871,Note that we are the first to explore transfer techniques and achieve the best performances in this task ,19,Comparison with Non - Transfer,"Thus , it is necessary to show our improvements over current superior non -transfer methods .",result
relation_extraction,12,Models are evaluated using the same metrics as previous work .,experiment,Setup,0,177,23,7,0,experiment : Setup,0.5379939209726444,0.696969696969697,0.7,Models are evaluated using the same metrics as previous work ,11,Glo Ve vectors are used as the initialization for word embeddings .,We report the test accuracy averaged over five cross validation folds for the cross - sentence n-ary relation extraction task .,experiment
natural_language_inference,56,"Where is the milk ? "" is preceded by a number of facts in the form of short sentences , e.g.",experiment,Experiments,0,84,5,5,0,experiment : Experiments,0.25,0.04672897196261682,0.5,Where is the milk is preceded by a number of facts in the form of short sentences e g ,20,"Each question , e.g.",Daniel journeyed to the garden .,experiment
natural_language_inference,3,basic block consists of five layers .,model,Stacked Coupled-LSTMs Layers,0,101,78,3,0,model : Stacked Coupled-LSTMs Layers,0.4855769230769231,0.6902654867256637,0.5,basic block consists of five layers ,7,"After the embedding layer , we use our proposed coupled - LSTMs to capture the strong interactions between two sentences .",We firstly use four directional coupled - LSTMs to model the local interactions with different information flows .,method
sentiment_analysis,9,The bottom and top of the figures represent the feature input and output positions ( POS ) corresponding to each token .,model,Figure 4:,0,122,52,7,0,model : Figure 4:,0.4404332129963899,0.65,0.7,The bottom and top of the figures represent the feature input and output positions POS corresponding to each token ,20,"and are two implementations of the local context focus mechanism , the context - feature dynamic mask ( CDM ) layer and context - feature dynamic weighting ( CDW ) layer , respectively .","The self - attention mechanism treats all tokens equally , so that each token can generate the self - attention score with other tokens through parallel matrix operation .",method
text_summarization,4,"Second , the soft model produced a factually wrong summary , saying that "" guadalajara "" is a mexican state , while actually it is a city .",result,Results,0,227,33,33,0,result : Results,0.8798449612403101,0.5689655172413793,0.5689655172413793,Second the soft model produced a factually wrong summary saying that guadalajara is a mexican state while actually it is a city ,23,"First , the base model generated a less informative summary , not mentioning "" mexico state "" and "" first edition "" .","Third , the firm model is able to solve the problem by focusing only on the five most important entities , eliminating possible noise such as "" Unk "" and less crucial entities such as "" Country club "" .",result
named-entity-recognition,6,"For both datasets , we convert the IOB encoding to BILOU , since found the latter to perform better .",evaluation,Data and Evaluation,0,130,5,5,0,evaluation : Data and Evaluation,0.6132075471698113,0.4166666666666667,0.4166666666666667,For both datasets we convert the IOB encoding to BILOU since found the latter to perform better ,18,"As we can see , ONTONOTES is much larger .","In keeping with others , we report mention - level F 1 score using the conlleval script 2 .",result
natural_language_inference,21,"Hence , the gate tends to use their context features given by masked self - attention .",model,Case Study,0,277,51,13,0,model : Case Study,0.9551724137931036,0.8793103448275862,0.65,Hence the gate tends to use their context features given by masked self attention ,15,"The stop words themselves can not contribute important information , so only their semantic relations to other words might help to understand the sentence .","In , we show the two multi-dimensional source2token self - attention score vectors of the same word in the two sentences , by their heatmaps .",method
natural_language_inference,56,Otherwise both networks are identical to our network .,experiment,Sudoku relational network baseline details,0,297,20,7,0,experiment : Sudoku relational network baseline details,0.8839285714285714,0.3389830508474576,0.875,Otherwise both networks are identical to our network ,9,The graph centric model has larger hidden states of 256 in all layers to compensate somewhat for the sum squashing the entire graph into a fixed size vector .,The graph centric has over 4 times as many parameters as our model but performs worse than the node centric .,experiment
natural_language_inference,99,"From human 's perspective , each time we reread the passage , we get some new ideas or more solid comprehension on the basis of the former understanding .",model,Answer Selection with Checking Mechanism,0,155,78,15,0,model : Answer Selection with Checking Mechanism,0.6150793650793651,0.8125,0.4545454545454545,From human s perspective each time we reread the passage we get some new ideas or more solid comprehension on the basis of the former understanding ,27,"For each hop , the reader dynamically collects evidence from former passage representation and encodes the evidence to the new iteration .",The self - alignment is computed by,method
text_generation,0,As mentioned in SEQUENCE GENERATIVE ADVERSARIAL,Proof for Eq. (6),Proof for Eq. (6),0,260,3,3,0,Proof for Eq. (6) : Proof for Eq. (6),0.8024691358024691,0.25,0.25,As mentioned in SEQUENCE GENERATIVE ADVERSARIAL,6,"For readability , we provide the detailed derivation of Eq. ( 6 ) hereby following .","NETS section , the state transition is deterministic after an action has been chosen , i.e. ? a s , s = 1 for the next state s =",others
natural_language_inference,99,The attention weight is calculated by a j = sof tmax ( S :j ) ? Rn .,model,Contextual Encoding,0,121,44,35,0,model : Contextual Encoding,0.4801587301587302,0.4583333333333333,0.6481481481481481,The attention weight is calculated by a j sof tmax S j Rn ,14,Leta j ? Rn represent the normalized attention distribution on the question words by tth passage word .,The attention weight is calculated by a j = sof tmax ( S :j ) ? Rn .,method
natural_language_inference,58,"To better grasp when ReasoNets stop reasoning , we show the distribution of termination steps in ReasoNets on the test set .",system description,Internal State Controller:,0,321,88,30,0,system description : Internal State Controller:,0.9582089552238806,0.8888888888888888,0.7317073170731707,To better grasp when ReasoNets stop reasoning we show the distribution of termination steps in ReasoNets on the test set ,21,All of these demonstrate the dynamic termination characteristic and potential reasoning capability of ReasoNets .,"The termination step is chosen with the maximum termination probability p ( k ) = t k k ?1 i =1 ( 1 ? ti ) , where ti is the termination probability at step i. shows the termination step distribution of Rea - soNets in the graph reachability dataset .",method
natural_language_inference,53,"This work combines two research directions , which we describe in what follows .",approach,Approach,0,45,2,2,0,approach : Approach,0.21634615384615385,0.4,0.4,This work combines two research directions which we describe in what follows ,13, ,"First , we explain how the NLI task can be used to train universal sentence encoding models using the SNLI task .",method
temporal_information_extraction,1,"ClearTK was designed mainly for TE3 , aiming for high precision , which is reflected by its high precision online 10 , but it does not have enough flexibility to cope with two very different annotation schemes .",baseline,Comparison with CAEVO,0,245,49,7,0,baseline : Comparison with CAEVO,0.953307392996109,0.8909090909090909,0.5384615384615384,ClearTK was designed mainly for TE3 aiming for high precision which is reflected by its high precision online 10 but it does not have enough flexibility to cope with two very different annotation schemes ,35,"ClearTK was reported to be outperformed by CAEVO on TD - Test , but we observe that ClearTK online 10 was much worse even than itself online 7 ( trained on TB + VC ) and online 1 ( trained on TB + AQ + VC + TD ) due to the annotation scheme difference between TD and TB / AQ / VC .","Therefore , we have chosen CAEVO as the baseline system to evaluate the significance of the proposed ones .",result
natural_language_inference,75,The addressing and reading of the memory involves three steps :,model,Model Description,0,58,9,9,0,model : Model Description,0.2857142857142857,0.05844155844155844,0.3103448275862069,The addressing and reading of the memory involves three steps ,11,"In KV - MemNNs we define the memory slots as pairs of vectors ( k 1 , v 1 ) . . . , ( k M , v M ) and denote the question x .",Key Hashing : the question can be used to preselect a small subset of the possibly large array .,method
natural_language_inference,79,It 's better to cross validate the window size .,model,Some further observations,0,244,59,11,0,model : Some further observations,0.9104477611940298,0.9365079365079364,0.7333333333333333,It s better to cross validate the window size ,10,"Perhaps , this is because the model loses the ordering information .",good guess of window size in many applications is between 5 and 12 .,method
machine-translation,7,"They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation .",system description,RELATED WORK ON MIXTURES OF EXPERTS,0,64,21,12,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.17158176943699732,0.40384615384615385,0.8,They also allude in their conclusion to the potential to introduce sparsity turning MoEs into a vehicle for computational computation ,21,"It is intuitive that the latter approach is more powerful , since complex problems may contain many sub-problems each requiring different experts .",Our work builds on this use of MoEs as a general purpose neural network component .,method
named-entity-recognition,5,"In addition , it can also be used as a global sentence - level representation for classification tasks .",introduction,introduction,0,29,20,20,0,introduction : introduction,0.13875598086124402,0.9090909090909092,0.9090909090909092,In addition it can also be used as a global sentence level representation for classification tasks ,17,"Being connected with every word , the sentence - level state vector serves to exchange non-local information with each word .","Results on both classification and sequence labelling show that S - LSTM gives better accuracies compared to BiLSTM using the same number of parameters , while being faster .",introduction
natural_language_inference,26,number of studies have found deep learning models might not really understand the natural language texts ) and vulnerably suffer from adversarial attacks .,introduction,introduction,0,19,9,9,0,introduction : introduction,0.08962264150943396,0.39130434782608703,0.39130434782608703,number of studies have found deep learning models might not really understand the natural language texts and vulnerably suffer from adversarial attacks ,23,"To this end , there is a recent trend of incorporating extra knowledge to pre-trained language models .","Through their observation , deep learning models pay great attention to non-significant words and ignore important ones .",introduction
named-entity-recognition,4,What information is captured by the biLM 's representations ? Since adding,analysis,Alternate layer weighting schemes,0,148,28,22,0,analysis : Alternate layer weighting schemes,0.5441176470588235,0.4117647058823529,0.3548387096774194,What information is captured by the biLM s representations Since adding,11,"In the SRL case , . . } they were actors who had been handed fat roles in a successful play , and had tale nt enough to fill the roles competently , with nice understatement . the task - specific context representations are likely more important than those from the biLM .",What information is captured by the biLM 's representations ? Since adding,result
sentiment_analysis,34,"Both ASTD and Ar SAS datasets have no specific splitting of the data to test and train ; thus , we applied random sampling to split both datasets to 80 / 20 % for train / test respectively .",experiment,Experimental Setup,0,108,12,12,0,experiment : Experimental Setup,0.7297297297297297,1.0,1.0,Both ASTD and Ar SAS datasets have no specific splitting of the data to test and train thus we applied random sampling to split both datasets to 80 20 for train test respectively ,34,"After this step , we end up with 17,784 tweets in the ArSAS dataset labelled with three sentiment labels .", ,experiment
machine-translation,4,Model Hyper - parameters and Evaluation,system description,Model Hyper-parameters and Evaluation,0,167,1,1,0,system description : Model Hyper-parameters and Evaluation,0.6987447698744771,0.09090909090909093,0.09090909090909093,Model Hyper parameters and Evaluation,5, , ,method
natural_language_inference,42,"where ? model ? R 1d model , ? input ? R 1 dinput are the embedding vectors , and W Reduction ? Rd model dinput is a weight matrix , to which we refer as "" reduction matrix "" .",system description,Reduction Layer,0,177,137,7,0,system description : Reduction Layer,0.6124567474048442,0.7828571428571428,0.2692307692307692,where model R 1d model input R 1 dinput are the embedding vectors and W Reduction Rd model dinput is a weight matrix to which we refer as reduction matrix ,31,straightforward method to reduce the input embedding size is to multiply it by a matrix with the required dimensions :,"where ? model ? R 1d model , ? input ? R 1 dinput are the embedding vectors , and W Reduction ? Rd model dinput is a weight matrix , to which we refer as "" reduction matrix "" .",method
natural_language_inference,89,"The passage and the question are described as sequences of word tokens , denoted as P = {x pi } lp i=1 and Q = {x q j } lq j=1 respectively , where l p is the passage length and l q is the question length .",system description,Task Description,0,53,3,3,0,system description : Task Description,0.20384615384615384,0.1875,0.75,The passage and the question are described as sequences of word tokens denoted as P x pi lp i 1 and Q x q j lq j 1 respectively where l p is the passage length and l q is the question length ,44,"Given a context passage and a question , the machine needs to not only find answers to answerable questions but also detect unanswerable cases .","Our goal is to predict an answer A , which is constrained as a segment of text in the passage : A = {x pi } lb i=la , or return an empty string if there is no answer , where la and lb indicate the answer boundary .",method
natural_language_inference,25,Results on SQuAD 's development set .,analysis,Importance of context,0,72,12,10,0,analysis : Importance of context,0.6923076923076923,0.6666666666666666,0.625,Results on SQuAD s development set ,7,"In we plot the average gate value gt for each word - type , where the average is taken across entries of the gate vector and across all occurrences of the word in both passages :",The EM metric measures an exact - match between a predicted answer and a correct one and the F1 metric measures the overlap between their bag of words .,result
sentiment_analysis,10,Similar trend across other utterances establish inter-dependence between emotional states of future and past utterances .,result,Case Studies,0,221,54,16,0,result : Case Studies,0.8599221789883269,0.84375,0.8421052631578947,Similar trend across other utterances establish inter dependence between emotional states of future and past utterances ,17,"Interestingly , turn 5 attends to both past ( turn 3 ) and future ( turn 8 ) utterances .","The beneficial consideration of future utterances through GRU E is also apparent through turns 6 , 9 . These utterances focus on the distant future ( turn 49 , 50 ) where the man is at an enraged state , thus capturing emotional correlations across time .",result
sentiment_analysis,26,"However , unlike the VADER data , our transfer learning approach results in multi-dimensional sentiment embeddings that can more easily capture multiple domains right from the start , thus making it possible to use them even without further fine - tuning .",analysis,Inspection of the DM-MCNN-learned Deep Sentiment Information.,0,224,78,13,0,analysis : Inspection of the DM-MCNN-learned Deep Sentiment Information.,0.9142857142857144,1.0,1.0,However unlike the VADER data our transfer learning approach results in multi dimensional sentiment embeddings that can more easily capture multiple domains right from the start thus making it possible to use them even without further fine tuning ,39,"Hence , the figure confirms that our DM - MCNN approach is able to exploit and customize the provided sentiment weights for the target domain .", ,result
natural_language_inference,6,We extend our experiments to 29 languages without any training data ( see ) .,training,Tatoeba: results for unseen languages,0,245,57,2,0,training : Tatoeba: results for unseen languages,0.9879032258064516,0.95,0.4,We extend our experiments to 29 languages without any training data see ,13, ,"Many of them are recognized minority languages spoken in specific regions ( e.g. Asturian , Faroese , Frisian , Kashubian , North Moluccan Malay , Piemontese , Swabian or Sorbian ) .",experiment
natural_language_inference,1,Our Paraphrases dataset is made of 15M clusters containing 2 or more paraphrases each .,training,Training,0,167,9,9,0,training : Training,0.6117216117216118,0.1956521739130435,1.0,Our Paraphrases dataset is made of 15M clusters containing 2 or more paraphrases each ,15,We used a subset of the large set of paraphrases extracted from WIKIANSWERS and introduced in ., ,experiment
natural_language_inference,36,The details are 1 https://spacy.io/,model,Downstream Model,0,112,12,12,0,model : Downstream Model,0.5333333333333333,0.3870967741935484,0.3870967741935484,The details are 1 https spacy io ,8,"Additionally , during inference , Viterbi decoding is applied to accommodate valid BIO sequences .","Actually , the easiest way to deal with segmentation or sequence labeling problems is to transform them into raw labeling problems .",method
natural_language_inference,53,"For the classifier , we use a multi - layer perceptron with 1 hidden - layer of 512 hidden units .",training,Training details,1,91,5,5,0,training : Training details,0.4375,0.8333333333333334,0.8333333333333334,For the classifier we use a multi layer perceptron with 1 hidden layer of 512 hidden units ,18,We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10 ?5 .,We use opensource GloVe vectors trained on Common Crawl 840B with 300 dimensions as fixed word embeddings .,experiment
natural_language_inference,62,"In this paper , we discard the existing implicit way and instead explore an explicit ( i.e. understandable and controllable ) way to utilize general knowledge .",system description,Passage Question,0,39,13,13,0,system description : Passage Question,0.17410714285714285,0.6842105263157895,0.7222222222222222,In this paper we discard the existing implicit way and instead explore an explicit i e understandable and controllable way to utilize general knowledge ,25,"However , this is an implicit way to utilize general knowledge , since in this way we can neither understand nor control the functioning of general knowledge .",The contribution of this paper is two - fold .,method
natural_language_inference,17,"We then finetune this model with Eq. 11 , until the F 1 score on the development set no longer improves .",implementation,implementation,0,189,8,8,0,implementation : implementation,0.7269230769230769,0.5714285714285714,0.5714285714285714,We then finetune this model with Eq 11 until the F 1 score on the development set no longer improves ,21,We first train the model until convergence by optimizing Eq. 7 .,"We use the Adam optimizer [ Kingma and Ba , 2014 ] for both ML and DCRL training .",experiment
natural_language_inference,86,We define those multiperspective matching functions in following two directions :,system description,Context Representation Layer,0,77,43,12,0,system description : Context Representation Layer,0.4502923976608187,0.6825396825396826,0.375,We define those multiperspective matching functions in following two directions ,11,The goal of this layer is to compare each contextual embedding of the passage with the question with multi-perspectives .,"First , dimensional weighted matchings with",method
natural_language_inference,64,"For each question in ASNQ , the positive candidate answers are those sentences that occur in the long answer paragraphs in NQ and contain annotated short answers .",system description,Answer-Sentence Natural Questions,0,124,53,8,0,system description : Answer-Sentence Natural Questions,0.4940239043824701,0.8153846153846154,0.4,For each question in ASNQ the positive candidate answers are those sentences that occur in the long answer paragraphs in NQ and contain annotated short answers ,27,"long answer can contain multiple sentences , thus NQ is not directly applicable for AS2 .",The remaining sentences from the document are labeled as negative for the target question .,method
question_answering,0,"Notably , GGNN also has the best performance on the most simple semantic parses that only have one edge .",result,result,0,242,31,31,0,result : result,0.8203389830508474,0.9393939393939394,0.9393939393939394,Notably GGNN also has the best performance on the most simple semantic parses that only have one edge ,19,The complete correct graph would also include temporal constraints .,"In these cases , two nodes in the graph interact with each other through the single edge in both directions .",result
natural_language_inference,99,The outputs of the three steps ( a ) ( b ) ( c ) are calculated as follows :,model,Contextual Encoding,0,96,19,10,0,model : Contextual Encoding,0.3809523809523809,0.19791666666666666,0.1851851851851852,The outputs of the three steps a b c are calculated as follows ,14,This can be explained that people often reread the question to confirm whether they have thoroughly understand it .,where E qt ? Rd is the lexical representation from the input layer .,method
relation_extraction,12,"Up to N matrices are constructed , where N is a hyper - parameter .",system description,Attention Guided Layer,0,109,54,29,0,system description : Attention Guided Layer,0.331306990881459,0.5454545454545454,0.8787878787878788,Up to N matrices are constructed where N is a hyper parameter ,13,The projections are parameter matrices W Q i ? R dd and W K i ? R dd . ( t ) is the t - th attention guided adjacency matrix corresponding to the t- th head .,shows an example that the original adjacency matrix is transformed into multiple attention guided adjacency matrices .,method
natural_language_inference,98,Modeling inference in human language is very challenging but is a basic problem in natural language understanding .,introduction,introduction,0,15,5,5,0,introduction : introduction,0.1271186440677966,0.625,0.625,Modeling inference in human language is very challenging but is a basic problem in natural language understanding ,18,"Task aims to evaluate language understanding models for sentence representation with natural language inference ( NLI ) tasks , where a sentence is represented as a fixedlength vector .","Specifically , NLI is concerned with determining whether a hypothesis sentence h can be inferred from a premise sentence p.",introduction
named-entity-recognition,3,New York is located :,model,Baseline sequence tagging model,0,40,14,14,0,model : Baseline sequence tagging model,0.21621621621621626,0.32558139534883723,0.32558139534883723,New York is located ,5,Step 3 : Use both word embeddings and LM embeddings in the sequence tagging model .,"The main components in TagLM , our language - model - augmented sequence tagging system .",method
text_generation,2,We conduct extensive experiments based on synthetic and real data .,introduction,introduction,0,42,30,30,0,introduction : introduction,0.12,0.7894736842105263,0.7894736842105263,We conduct extensive experiments based on synthetic and real data ,11,"As such , the guiding signals from D are not only available to G at the end in terms of the scalar reward signals , but also available in terms of a goal embedding vector during the generation process to guide G how to get improved .","For synthetic data , Leak GAN obtains much lower negative log - likelihood than previous models with sequence length set to 20 and 40 .",introduction
relation_extraction,0,"We assign each token in the entity with the tag B appended with the entity type if it is the beginning of the entity , I for inside of an entity , L for the end of the entity or U if there is only one token in the entity .",model,Entity detection,0,77,23,3,0,model : Entity detection,0.30078125,0.323943661971831,0.1875,We assign each token in the entity with the tag B appended with the entity type if it is the beginning of the entity I for inside of an entity L for the end of the entity or U if there is only one token in the entity ,49,We formulate entity detection as a sequence labeling task using BILOU scheme similar to and .,shows an example of the entity tag sequence assigned to the sentence .,method
natural_language_inference,9,"According to the ablation results , we infer that : ( a ) When the number of layers is only one , the model lacks reasoning capability .",model,RESULTS.,1,229,49,15,0,model : RESULTS.,0.6939393939393941,0.620253164556962,0.3333333333333333,According to the ablation results we infer that a When the number of layers is only one the model lacks reasoning capability ,23,We show a subset of combinations of the ablations for bAbI QA in ; other combinations performed poorly and / or did not give interesting observations .,"In the case of 1 k dataset , when there are too many layers ( 6 ) , it seems correctly training the model becomes increasingly difficult .",method
machine-translation,3,"After special handling of unknown words and model ensembling , we obtain the best score reported to date on this task with BLEU = 40.4 .",abstract,abstract,0,11,9,9,0,abstract : abstract,0.03514376996805112,0.9,0.9,After special handling of unknown words and model ensembling we obtain the best score reported to date on this task with BLEU 40 4 ,25,We can still achieve BLEU = 36.3 even without using an attention mechanism .,Our models are also validated on the more difficult WMT ' 14 English - to - German task .,abstract
named-entity-recognition,6,"Interestingly , this mention does not have its page in English Wikipedia .",method,Word,0,92,41,8,0,method : Word,0.4339622641509434,0.82,0.7272727272727273,Interestingly this mention does not have its page in English Wikipedia ,12,"For instance , "" dammstadt "" , which appears only 5 times in WiFiNE , and which refers to the Damm city in Germany , is most similar to / location / city and / location / railway .","Furthermore , we observe that non-entity context words have a strong similarity to types they precede or succeed .",method
natural_language_inference,33,The sentence representation h x is the concatenation of the final hidden vectors from a 2 - layer bidirectional GRU with 2048 - dimensional hidden vectors and a 1 - layer bidirectional GRU with 2048 - dimensional hidden vectors trained without STP .,evaluation,MULTI-TASK MODEL DETAILS,0,207,83,13,0,evaluation : MULTI-TASK MODEL DETAILS,0.7931034482758621,0.6058394160583942,0.7222222222222222,The sentence representation h x is the concatenation of the final hidden vectors from a 2 layer bidirectional GRU with 2048 dimensional hidden vectors and a 1 layer bidirectional GRU with 2048 dimensional hidden vectors trained without STP ,39,STN + Fr + De + NLI +2L + STP :,STN + Fr + De + NLI +L + STP + Par :,result
natural_language_inference,66,The second hypothesis is a contradiction because it mentions a completely different event .,result,ID sentence label Premise,0,179,18,13,0,result : ID sentence label Premise,0.6415770609318996,0.1935483870967742,0.5416666666666666,The second hypothesis is a contradiction because it mentions a completely different event ,14,Three examples of sentence pairs with different relationship labels .,"The third hypothesis is neutral to the premise because the phrase "" with his owner "" can not be inferred from the premise .",result
relation-classification,5,Our model is implemented using DYNET v 2.0 .,experiment,Experimental setup,1,72,15,14,0,experiment : Experimental setup,0.6371681415929203,0.6521739130434783,0.6363636363636364,Our model is implemented using DYNET v 2 0 ,10,"Following , we use the 64%/16%/20 % training / development / test presplit available from Adel and Schtze ( 2017 ) , in which the test set was previously also used by .","We optimize the objective loss using Adam , no mini-batches and run for 100 epochs .",experiment
natural_language_inference,99,"Specifically , we first introduce the Smarnet framework that exploits fine - grained word understanding with various attribution discriminations , like humans recite words with corresponding properties .",introduction,introduction,1,30,20,20,0,introduction : introduction,0.11904761904761905,0.625,0.625,Specifically we first introduce the Smarnet framework that exploits fine grained word understanding with various attribution discriminations like humans recite words with corresponding properties ,25,We design the structure in the viewpoint of imitating how humans take the reading comprehension test .,We then develop the interactive attention with memory network to mimic human reading procedure .,introduction
natural_language_inference,33,Natural Language Inference Natural language inference ( NLI ) is a 3 - way classification problem .,system description,Neural Machine Translation,0,96,39,6,0,system description : Neural Machine Translation,0.3678160919540229,0.582089552238806,0.1875,Natural Language Inference Natural language inference NLI is a 3 way classification problem ,14,Gold parses are duplicated 5 times and shuffled in with the weakly labeled parses to have a roughly 1:5 ratio of gold to noisy parses .,"Given a premise and a hypothesis sentence , the objective is to classify their relationship as either entailment , contradiction , or neutral .",method
semantic_role_labeling,2,"new state - of - the - art deep network for endto - end SRL , supported by publicly available code and models .",introduction,introduction,0,20,13,13,0,introduction : introduction,0.08928571428571429,0.8125,0.8125,new state of the art deep network for endto end SRL supported by publicly available code and models ,19,"In summary , our main contributions incluede :","An in - depth error analysis indicating where the model works well and where it still struggles , including discussion of structural consistency and long - distance dependencies .",introduction
text_summarization,4,"Second , linked entities can be used to represent the topic of the summary , defined as a multinomial distribution over entities , as graphically shown in the example , where the probabilities refer to the relevance of the entities .",system description,Observations,0,50,11,8,0,system description : Observations,0.1937984496124031,0.3548387096774194,0.6153846153846154,Second linked entities can be used to represent the topic of the summary defined as a multinomial distribution over entities as graphically shown in the example where the probabilities refer to the relevance of the entities ,37,"We report that 77.1 % and 75.1 % of the noun phrases on the Gigaword and CNN datasets , respectively , contain at least one linked entity , which confirms our observation .","Entities have been previously used to represent topics , as they can be utilized as a controlled vocabulary of the main topics in a document .",method
natural_language_inference,29,"Unfortunately , many productaware questions lack of proposer answers .",introduction,introduction,0,18,6,6,0,introduction : introduction,0.04931506849315069,0.14634146341463414,0.14634146341463414,Unfortunately many productaware questions lack of proposer answers ,9,"As a convenience of users , more and more e-commerce portals provide community question - answering services that allow users to pose product - aware questions to other consumers who purchased the same product before .","Under the circumstances , users have to read the product 's reviews to find the answer by themselves .",introduction
natural_language_inference,38,"In multiple layers , the integrated hierarchical matching module M can be regarded as the input {r Pt } n t = 1 of next layer after a dimensionality reduction processing .",model,Query-Based Similarity Matching,0,98,57,20,0,model : Query-Based Similarity Matching,0.5185185185185185,0.8636363636363636,0.9523809523809524,In multiple layers the integrated hierarchical matching module M can be regarded as the input r Pt n t 1 of next layer after a dimensionality reduction processing ,29,"Finally , the integrated memory M is passed into a bi-directional LSTM , and the output will captures the interaction among the context words and the query words :",We call this memory networks of full - orientation matching .,method
natural_language_inference,57,"Note that some images have been slightly cropped for easier viewing , but no relevant objects have been removed .",SUPPLEMENTARY MATERIAL,SUPPLEMENTARY MATERIAL,0,172,4,4,0,SUPPLEMENTARY MATERIAL : SUPPLEMENTARY MATERIAL,1.0,1.0,1.0,Note that some images have been slightly cropped for easier viewing but no relevant objects have been removed ,19,Multimodal regularities found with embeddings learned for the caption - image retrieval task ., ,others
natural_language_inference,25,This maybe due to context being most strongly represented in those hidden states as the representations of LM ( emb ) are non-contextual by definition and those of LM ( L2 ) were optimized ( during LM training ) to be similar to parameter vectors that correspond to word - types and not to word - tokens .,model,Incorporating language model representations,0,85,7,7,0,model : Incorporating language model representations,0.8173076923076923,0.7,0.7,This maybe due to context being most strongly represented in those hidden states as the representations of LM emb are non contextual by definition and those of LM L2 were optimized during LM training to be similar to parameter vectors that correspond to word types and not to word tokens ,51,"Besides a crosscutting boost in results , we note that the performance due to utilizing the LM hidden states of the first LSTM layer significantly surpasses the other two variants .","In we list the top - scoring single - model published results on SQuAD 's test set , where we observe RaSoR + TR + LM ( L1 ) ranks second in EM , despite having only minimal questionpassage interaction which is a core component of other works .",method
natural_language_inference,25,The hidden state outputs of each LSTM layer are projected down to a lower dimension via a bottleneck layer .,system description,LM-augmented token re-embedding (TR+LM),0,43,25,9,0,system description : LM-augmented token re-embedding (TR+LM),0.4134615384615384,0.925925925925926,0.8181818181818182,The hidden state outputs of each LSTM layer are projected down to a lower dimension via a bottleneck layer ,20,"It consists of an initial layer which produces character - based word representations , followed by two stacked LSTM layers and a softmax prediction layer .","We set {o 1 , . . . , o k } to either the projections of the first layer , referred to as TR + LM ( L1 ) , or those of the second one , referred to as TR + LM ( L2 ) .",method
natural_language_inference,9,"In fact , the parallelizability of QRN implies that QRN does not suffer from the vanishing gradient problem of RNN , hence effectively addressing the long - term dependency .",introduction,introduction,0,39,31,31,0,introduction : introduction,0.11818181818181818,0.96875,0.96875,In fact the parallelizability of QRN implies that QRN does not suffer from the vanishing gradient problem of RNN hence effectively addressing the long term dependency ,27,"Third , unlike most RNN - based models , QRN can be parallelized overtime by computing candidate reduced queries ( h t ) directly from local input queries ( q t ) and context sentence vectors ( x t ) .",We experimentally demonstrate these contributions by achieving the state - of - the - art results on story - based QA and interactive dialog datasets .,introduction
semantic_role_labeling,3,"The former step involves assigning either a semantic argument or nonargument for a given predicate , while the latter includes labeling a specific semantic role for the identified argument .",system description,Semantic Role Labeling,0,44,6,6,0,system description : Semantic Role Labeling,0.16666666666666666,0.05504587155963303,0.4615384615384616,The former step involves assigning either a semantic argument or nonargument for a given predicate while the latter includes labeling a specific semantic role for the identified argument ,29,"Generally , semantic role labeling consists of two steps : identifying and classifying arguments .",It is also common to prune obvious non-candidates before the first step and to apply post-processing procedure to fix inconsistent predictions after the second step .,method
relation_extraction,11,"RESIDE consists of three components for learning a representation of a given bag , which is fed to a softmax classifier .",system description,RESIDE Overview,0,99,32,4,0,system description : RESIDE Overview,0.3991935483870968,0.25,0.2,RESIDE consists of three components for learning a representation of a given bag which is fed to a softmax classifier ,21,"In multi-instance learning paradigm , we are given a bag of sentences ( or instances ) {s 1 , s 2 , ...s n } for a given entity pair , the task is to predict the relation between them .",We briefly present the components of RESIDE below .,method
named-entity-recognition,6,"For both models , the character embedding size was set to 25 , and the hidden dimension of the forward and backward character LSTMs are set to 50 .",training,Training and Implementation,0,143,6,6,0,training : Training and Implementation,0.6745283018867925,0.18181818181818185,0.1935483870967742,For both models the character embedding size was set to 25 and the hidden dimension of the forward and backward character LSTMs are set to 50 ,27,Our system uses a single Bi - LSTM layer at the word level whose hidden dimensions are set to 128 and 256 for CONLL and ONTONOTES respectively .,"To mitigate overfitting , we apply a dropout mask with a probability of 0.5 on the input and output vectors of the Bi - LSTM layer .",experiment
machine-translation,7,The Adam optimizer keeps first and second moment estimates of the perparameter gradients .,APPENDICES A LOAD-BALANCING LOSS,Computationally-Matched Baselines:,0,305,83,33,0,APPENDICES A LOAD-BALANCING LOSS : Computationally-Matched Baselines:,0.8176943699731903,0.5496688741721855,0.7021276595744681,The Adam optimizer keeps first and second moment estimates of the perparameter gradients ,14,"Secondly , we modify the optimizer on the expert parameters to require less auxiliary storage :",This triples the required memory .,others
named-entity-recognition,0,"Equivalently , ( 2 ) is rewritten in the matrix format :",system description,Subset Selection via Self-Representation,0,82,60,21,0,system description : Subset Selection via Self-Representation,0.3025830258302583,0.33707865168539325,0.5675675675675675,Equivalently 2 is rewritten in the matrix format ,9,"As the representation loss is accumulated via the 1 - norm among samples , compared with ( 1 ) , the robustness against outlier samples is enhanced .","Since the objective is convex in A , the global minimum maybe found by differentiating ( 3 ) and setting the derivative to zero , resulting in a linear system 1",method
named-entity-recognition,2,We define the dilated convolution operator :,model,Dilated Convolutions,0,79,34,11,0,model : Dilated Convolutions,0.3708920187793428,0.576271186440678,0.8461538461538461,We define the dilated convolution operator ,7,"Dilated convolutions perform the same operation , except rather than transforming adjacent in - puts , the convolution is defined over a wider effective input width by skipping over ? inputs at a time , where ? is the dilation width .",dilated convolution of width 1 is equivalent to a simple convolution .,method
text_summarization,0,We design a supervisor as well as a goal tracker to guide the generator to focus on the main aspect of the document .,introduction,introduction,0,63,48,48,0,introduction : introduction,0.20930232558139536,0.9411764705882352,0.9411764705882352,We design a supervisor as well as a goal tracker to guide the generator to focus on the main aspect of the document ,24,"To solve this task , we propose an end - to - end learning framework to conduct the reader attention modeling and reader - aware summary generation .","To reduce the noisy information introduced by the reader comments , we propose a denoising module to identify which comments are helpful for summary generation auto-matically .",introduction
natural_language_inference,85,The same is applied tob j :,model,Input Encoding,0,61,16,8,0,model : Input Encoding,0.2595744680851064,0.15384615384615385,0.2857142857142857,The same is applied tob j ,7,"To bookkeep the notations for later use , we write as ? i the hidden ( output ) state generated by the BiLSTM at time i over the input sequence a .","Due to the space limit , we will skip the description of the basic chain LSTM and readers can refer to for details .",method
relation-classification,5,"This is reasonable because those 3 factors are part of the RC component only , thus helpful in predicting relations .",result,Main results,0,108,28,28,0,result : Main results,0.9557522123893806,0.9655172413793104,0.9655172413793104,This is reasonable because those 3 factors are part of the RC component only thus helpful in predicting relations ,20,"However , they significantly decrease the RC score .","More specifically , using the Biaffine attention produces about 1.5 % significant improvements to a common Linear transformation mechanism in relation classification , i.e. , "" w / o Bilinear "" results against the full results in : 65.4 % vs. 66.9 % and 72.0 % vs. 73.3 % ( although using Biaffine increases training time over using Linear by 35 % , relatively ) .",result
question_answering,1,"At the word embedding layer , query words such as When , Where and Who are not well aligned to possible answers in the context , but this dramatically changes in the contextual embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer .",experiment,QUESTION ANSWERING EXPERIMENTS,1,207,44,44,0,experiment : QUESTION ANSWERING EXPERIMENTS,0.6529968454258676,0.4888888888888889,0.6666666666666666,At the word embedding layer query words such as When Where and Who are not well aligned to possible answers in the context but this dramatically changes in the contextual embedding layer which has access to context from surrounding words and is just 1 layer below the attention layer ,50,"To visualize the embeddings , we choose a few frequent query words in the dev data and look at the context words that have the highest cosine similarity to the query words .","When begins to match years , Where matches locations , and Who matches names .",experiment
relation_extraction,9,"Consider the example in , where the non-entity word "" cause "" is of particular significance in determining the relation .",model,Figure 2: Input and Primary Attention,0,102,55,7,0,model : Figure 2: Input and Primary Attention,0.4880382775119617,0.625,0.3043478260869565,Consider the example in where the non entity word cause is of particular significance in determining the relation ,19,Contextual Relevance Matrices .,"Fortunately , we can exploit the fact that there is a salient connection between the words "" cause "" and "" diabetes "" also in terms of corpus cooccurrences .",method
natural_language_inference,20,"The sentence embeddings have hidden size of 600 for both direction ( except for SentEval test , where we test models with 600D and 1200D per direction ) and the 3 - layer multilayer perceptron ( MLP ) have the size of 600 dimensions .",training,Training Details,1,80,11,11,0,training : Training Details,0.3292181069958848,0.8461538461538461,0.8461538461538461,The sentence embeddings have hidden size of 600 for both direction except for SentEval test where we test models with 600D and 1200D per direction and the 3 layer multilayer perceptron MLP have the size of 600 dimensions ,39,"We use pre-trained Glo Ve word embeddings of size 300 dimensions ( Glo Ve 840B 300D ; , which were fine - tuned during training .",We use a dropout of 0.1 between the MLP layers ( except just before the final layer ) .,experiment
machine-translation,8,"More specifically , his alignment was restricted to predict the location such that the location increases monotonically .",system description,LEARNING TO ALIGN,0,183,5,5,0,system description : LEARNING TO ALIGN,0.552870090634441,0.2631578947368421,0.5,More specifically his alignment was restricted to predict the location such that the location increases monotonically ,17,"In his work , he used a mixture of Gaussian kernels to compute the weights of the annotations , where the location , width and mixture coefficient of each kernel was predicted from an alignment model .","The main difference from our approach is that , in , the modes of the weights of the annotations only move in one direction .",method
sentiment_analysis,25,"Finally , a softmax layer uses the vector to predict the sentiment polarity of the input sentence .",system description,Gated Convolutional Network with Aspect Embedding,0,98,54,31,0,system description : Gated Convolutional Network with Aspect Embedding,0.44144144144144143,0.675,0.6458333333333334,Finally a softmax layer uses the vector to predict the sentiment polarity of the input sentence ,17,"For each convolutional filter , the max - over - time pooling layer takes the maximal value among the generated convolutional features , resulting in a fixed - size vector whose size is equal to the number of filters n k .",illustrates our model architecture .,method
natural_language_inference,57,"In partial order completion , we are given a set of positive examples P = { ( u , v ) } of ordered pairs drawn from a partially ordered set ( X , X ) , and a set of negative examples N which we know to be unordered .",system description,LEARNING ORDER-EMBEDDINGS,0,37,3,3,0,system description : LEARNING ORDER-EMBEDDINGS,0.21511627906976746,0.05263157894736842,0.3,In partial order completion we are given a set of positive examples P u v of ordered pairs drawn from a partially ordered set X X and a set of negative examples N which we know to be unordered ,40,"To unify our treatment of various tasks , we introduce the problem of partial order completion .","Our goal is to predict whether an unseen pair ( u , v ) is ordered .",method
natural_language_inference,61,"In the literature , the task of NLI is usually viewed as a relation classi cation .",introduction,introduction,1,12,5,5,0,introduction : introduction,0.07692307692307693,0.09803921568627452,0.09803921568627452,In the literature the task of NLI is usually viewed as a relation classi cation ,16,shows several samples of natural language inference from SNLI ( Stanford Natural Language Inference ) corpus .,It learns the relation between a premise Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro tor commercial advantage and that copies bear this notice and the full citation on the rst page .,introduction
natural_language_inference,85,"Due to the space limit , we will skip the description of the basic chain LSTM and readers can refer to for details .",model,Input Encoding,0,62,17,9,0,model : Input Encoding,0.2638297872340425,0.16346153846153846,0.32142857142857145,Due to the space limit we will skip the description of the basic chain LSTM and readers can refer to for details ,23,The same is applied tob j :,"Briefly , when modeling a sequence , an LSTM employs a set of soft gates together with a memory cell to control message flows , resulting in an effective modeling of tracking long - distance information / dependencies in a sequence .",method
natural_language_inference,34,"Vt is a linear projection matrix , and ? is the sigmoid function .",system description,Reasoning with the Fusion Block,0,157,54,24,0,system description : Reasoning with the Fusion Block,0.5322033898305085,0.5346534653465347,0.4363636363636363,Vt is a linear projection matrix and is the sigmoid function ,12,"We use an attention network between the query embeddings and the entity embeddings to predict a soft mask mt , which aims to signify the start entities in the t - th reasoning step :","Vt is a linear projection matrix , and ? is the sigmoid function .",method
named-entity-recognition,1,"Recurrent models like RNNs and LSTMs are capable of encoding very long sequences , however , they have a representation biased towards their most recent inputs .",training,Character-based models of words,0,138,70,12,0,training : Character-based models of words,0.6666666666666666,0.8235294117647058,0.75,Recurrent models like RNNs and LSTMs are capable of encoding very long sequences however they have a representation biased towards their most recent inputs ,25,"In all our experiments , the hidden dimension of the forward and backward character LSTMs are 25 each , which results in our character - based representation of words being of dimension 50 .","As a result , we expect the final representation of the forward LSTM to bean accurate representation of the suffix of the word , and the final state of the backward LSTM to be a better representation of its prefix .",experiment
natural_language_inference,42,Convolution kernels in attention sublayers had spatial dimensions 1 5 .,experiment,experiment,0,225,10,10,0,experiment : experiment,0.7785467128027682,1.0,1.0,Convolution kernels in attention sublayers had spatial dimensions 1 5 ,11,"We set processing layers dimension d model to 100 , the number of heads n heads in each attention sublayer to 4 , the feed - forward hidden size to 200 in processing layers and 400 in the reduction layer .", ,experiment
natural_language_inference,69,"It is worth pointing out that by introducing alternative candidates we counterbalance a type - consistency bias , in contrast to and who instead rely on entity masking .",dataset,Dataset Assembly,0,71,21,21,0,dataset : Dataset Assembly,0.20579710144927527,0.9130434782608696,0.9130434782608696,It is worth pointing out that by introducing alternative candidates we counterbalance a type consistency bias in contrast to and who instead rely on entity masking ,27,Models could otherwise identify a * in the documents by simply relying on type - consistency heuristics .,"To determine entities which are type - consistent for a query q , we consider all entities which are observed as object in a fact with r as relation type - including the correct answer .",experiment
relation_extraction,10,Their usage of the BILOU token - tagging scheme makes modelling overlapping entities impossible .,introduction,introduction,0,26,18,18,0,introduction : introduction,0.13541666666666666,0.5806451612903226,0.5806451612903226,Their usage of the BILOU token tagging scheme makes modelling overlapping entities impossible ,14,"Tree - LSTMs , CRFs , Beam Search and Pointer Networks .","In general , these tokenlevel models are sequential in nature and hence have cascading errors .",introduction
semantic_parsing,2,"As shown in , we first predict sketch a for input x , and then fill in missing details to generate the final meaning representation y by conditioning on both x and a .",system description,Problem Formulation,0,67,12,12,0,system description : Problem Formulation,0.2302405498281787,0.2222222222222222,0.7058823529411765,As shown in we first predict sketch a for input x and then fill in missing details to generate the final meaning representation y by conditioning on both x and a ,32,"Suffice it to say that the extraction amounts to stripping off arguments and variable names in logical forms , schema specific information in SQL queries , and substituting tokens with types in source code ( see ) .",The sketch is encoded into vectors which in turn guide and constrain the decoding of y .,method
natural_language_inference,30,"To train our model , we need positive and negative examples of ( q , t ) pairs .",architecture,architecture,0,136,17,17,0,architecture : architecture,0.5271317829457365,0.27419354838709675,0.5,To train our model we need positive and negative examples of q t pairs ,15,"We also enforce a constraint on the norms of the columns of V and W , i.e. ? i , | |v i | | 2 ? 1 and ? j , ||w j || 2 ? 1 .","However , D only contains positive samples , for which the triple actually corresponds to the question .",method
relation-classification,0,Removing label embeddings did not affect 6 https://github.com/clab/cnn,experiment,End-to-end Relation Extraction Results,0,169,17,6,0,experiment : End-to-end Relation Extraction Results,0.7477876106194691,0.3953488372093023,0.1875,Removing label embeddings did not affect 6 https github com clab cnn,12,"This is reasonable because the model can only create relation instances when both of the entities are found and , without these enhancements , it may get too late to find some relations .",http://nlp.stanford.edu/software/,experiment
natural_language_inference,74,"To simulate the thought of human being more closely , in this paper , we tackle this problem by using the REINFORCE algorithm to minimize the negative expected reward , which is defined as :",training,Training,0,124,15,15,0,training : Training,0.5535714285714286,0.75,0.75,To simulate the thought of human being more closely in this paper we tackle this problem by using the REINFORCE algorithm to minimize the negative expected reward which is defined as ,32,The final label 's confidence of this example is obviously lower than an example that all of its labels are the same .,"where ? ( l|P , H ) is the previous action policy that predicts the label given P and H , {l * } is the set of annotated labels , and",experiment
question-answering,1,"For any given sentence input x , the output of type - f filter for location i in the th layer is given by",model,Length Variability,0,50,24,5,0,model : Length Variability,0.2590673575129533,0.7272727272727273,0.38461538461538464,For any given sentence input x the output of type f filter for location i in the th layer is given by,22,"To eliminate the boundary effect caused by the great variability of sentence lengths , we add to the convolutional unit agate which sets the output vectors to all - zeros if the input is all zeros .","2 ) where g ( v ) = 0 if all the elements in vector v equals 0 , otherwise g ( v ) = 1 . This gate , working with max - pooling and positive activation function ( e.g. , Sigmoid ) , keeps away the artifacts from padding in all layers .",method
natural_language_inference,55,"Instead , we create a candidate set A ( q ) for each question .",training,Inference,0,114,9,6,0,training : Inference,0.7755102040816326,0.36,0.2727272727272727,Instead we create a candidate set A q for each question ,12,This candidate set could be the whole KB but this has both speed and potentially precision issues .,We recall that each question contains one identified Freebase entity .,experiment
sentence_classification,0,We next investigate errors made by our best model plots classification errors ) .,analysis,Analysis,0,215,15,15,0,analysis : Analysis,0.8052434456928839,0.75,0.8333333333333334,We next investigate errors made by our best model plots classification errors ,13,"Therefore , suggesting the effectiveness the scaffolds in informing the main task of relevant signals for citation intent classification .",One general error pattern is that the model has more tendency to make false positive errors in the BACKGROUND category likely due to this category dominating both datasets .,result
sentiment_analysis,10,We use two emotion detection datasets IEMOCAP ) and AVEC ) to evaluate Dia - logueRNN .,dataset,Datasets Used,0,143,2,2,0,dataset : Datasets Used,0.556420233463035,0.4,0.4,We use two emotion detection datasets IEMOCAP and AVEC to evaluate Dia logueRNN ,14, ,We partition both datasets into train and test sets with roughly 80 20 ratio such that the partitions do not share any speaker . ) .,experiment
natural_language_inference,40,"Since its release , the dataset has recieved much attention and several deep learning architectures have been introduced with impressive results .",model,Model,0,253,54,54,0,model : Model,0.9133574007220217,0.8709677419354839,0.8852459016393442,Since its release the dataset has recieved much attention and several deep learning architectures have been introduced with impressive results ,21,Results marked with are cf and with from of the article 5 .,An interesting property of the dataset is that in each article named entities and expressions referring to them have been anonymized by replacing with placeholders ( such as @entity24 ) to make the task purely a comprehension one .,method
question_answering,3,"The scoring function is the bilinear product of the nonlinearly transformed input , i.e. , F ( x ) i MF ( y ) i .",model,Multiple Choice Models,0,125,23,17,0,model : Multiple Choice Models,0.44642857142857145,0.4259259259259259,0.6071428571428571,The scoring function is the bilinear product of the nonlinearly transformed input i e F x i MF y i ,21,"This layer models the interactions between P , Q and A. Let B ( . ) be a standard bidirectional attention that utilizes meanpooling aggregation .","We first apply B ( P , Q ) to form bi-attentive P q , Q p representations .",method
sentiment_analysis,9,The LCFG unit extracts the features of the local context by a local context focus layer and a MHSA encoder .,model,Model Architecture,0,79,9,9,0,model : Model Architecture,0.2851985559566787,0.1125,0.75,The LCFG unit extracts the features of the local context by a local context focus layer and a MHSA encoder ,21,"Both context feature generator units contain an independent pre-trained BERT layer , and respectively .",The GCFG unit deploys only one MHSA encoder to learn the global context feature .,method
sentiment_analysis,43,"Previous methods only adopt coarsegrained attentions , which simply use the averaged aspect / context vector as the guide to learn the attention weights on context / aspect .",approach,Multi-grained Attention Layer,0,84,29,3,0,approach : Multi-grained Attention Layer,0.35,0.3333333333333333,0.2727272727272727,Previous methods only adopt coarsegrained attentions which simply use the averaged aspect context vector as the guide to learn the attention weights on context aspect ,26,Attention mechanism is a common way to capture the interactions between the aspect and context words .,"However , the simple average pooling in generating the guide vector might bring some information loss , especially for the aspect with multiple words or larger context .",method
question_generation,1,"In Multimodel differential network , we use exemplars and train them using a triplet loss .",ablation,How are exemplars improving Embedding,0,314,64,2,0,ablation : How are exemplars improving Embedding,0.8010204081632653,0.4507042253521127,0.032786885245901634,In Multimodel differential network we use exemplars and train them using a triplet loss ,15, ,"It is known that using a triplet network , we can learn a representation that accentuates how the image is closer to a supporting exemplar as against the opposing exemplar ( Hoffer and .",result
machine-translation,9,"Ideally , similar words should have similar codes .",introduction,introduction,0,35,22,22,0,introduction : introduction,0.12195121951219512,0.4150943396226415,0.4150943396226415,Ideally similar words should have similar codes ,8,Ci w is an integer number in .,"For example , we may desire C dog = ( 3 , 2 , 4 , 1 ) and C dogs = ( 3 , 2 , 4 , 2 ) .",introduction
natural_language_inference,42,"Note that , to facilitate training , every sublayer is followed by normalization .",system description,Layers,0,168,128,5,0,system description : Layers,0.5813148788927336,0.7314285714285714,0.7142857142857143,Note that to facilitate training every sublayer is followed by normalization ,12,"This standard layer is called "" processing layer "" and is illustrated in .",FABIR is formed by stacking layers on top of each other as shown in .,method
natural_language_inference,17,"However , the convergence can be suppressed when the baseline is better than the reward .",introduction,introduction,0,24,15,15,0,introduction : introduction,0.09230769230769233,0.5,0.5,However the convergence can be suppressed when the baseline is better than the reward ,15,"Specifically , an estimated baseline is utilized to normalize the reward and reduce variances .","This is harmful if the inferior reward is partially overlapped with the ground truth , as the normalized objective will discourage the prediction of ground truth positions .",introduction
sentiment_analysis,24,We compute the F 1 score of the integrated task denoted as F1 - I for measuring the over all performance .,method,Learning,0,187,132,59,0,method : Learning,0.6012861736334405,0.9850746268656716,0.9672131147540984,We compute the F 1 score of the integrated task denoted as F1 I for measuring the over all performance ,21,"Since we are solving the integrated task without assuming that gold aspect terms are given , the two metrics are computed based on the correctly extracted aspect terms from AE .","To compute F1 - I , an extracted aspect term is taken as correct only when both the span and the sentiment are correctly identified .",method
question_generation,1,"The uni-gram , bi-gram and tri-gram feature are computed by applying convolution filter of size 1 , 2 and 3 respectability .",ablation,Tag net,0,281,31,19,0,ablation : Tag net,0.7168367346938775,0.2183098591549296,0.76,The uni gram bi gram and tri gram feature are computed by applying convolution filter of size 1 2 and 3 respectability ,23,"After this , we apply temporal convolution on the word embedding vector .","Finally , we applied max - pooling on this to get a vector representation of the tags as shown figure 12 .",result
text-classification,8,"The store is small , but it carries specialties thatare difficult to find in Pittsburgh .",model,model,0,183,53,53,0,model : model,0.6802973977695167,0.7361111111111112,0.7361111111111112,The store is small but it carries specialties thatare difficult to find in Pittsburgh ,15,Makes me wonder why everyone likes food fight so much .,was particularly excited to find middle eastern chili sauce and chocolate covered turkish delights .,method
machine-translation,0,An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence .,introduction,introduction,0,31,23,23,0,introduction : introduction,0.1415525114155251,0.8518518518518519,0.8518518518518519,An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence ,22,maybe as simple as an elementwise logistic sigmoid function and as complex as along short - term memory ( LSTM ) unit ) .,"In that case , the output at each timestep t is the conditional distribution p ( x t | x t?1 , . . . , x 1 ) .",introduction
natural_language_inference,70,"In this subtask , accuracy is not a reliable measure , as the data is significantly imbalanced .",result,Subtask C Model:,0,173,51,30,0,result : Subtask C Model:,0.9885714285714284,0.9622641509433962,0.9375,In this subtask accuracy is not a reliable measure as the data is significantly imbalanced ,16,It should be also noted that the F 1 our system is the best among 10 primary submissions .,"In a future work we would like to change the learning paradigm from classification , e.g. , demonstrated in for several NLP applications , to a learning to rank problem .",result
relation_extraction,2,"Besides the masked language model , BERT also trains a "" next sentence prediction "" task that jointly pre-trains text - pair representations .",methodology,Pre-trained Model BERT,0,57,11,10,0,methodology : Pre-trained Model BERT,0.4222222222222222,0.9166666666666666,0.9090909090909092,Besides the masked language model BERT also trains a next sentence prediction task that jointly pre trains text pair representations ,21,"Unlike left - to - right language model pre-training , the MLM objective can help a state output to utilize both the left and the right context , which allows a pre-training system to apply a deep bidirectional Transformer .",shows the architecture of our approach .,method
natural_language_inference,23,Other recommendations included that the panel employ a full - time staff and remove government oversight from its processes to avoid political interference .,model,FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0,477,84,61,0,model : FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0.9298245614035088,0.7,0.7176470588235294,Other recommendations included that the panel employ a full time staff and remove government oversight from its processes to avoid political interference ,23,"They suggested a range of new organizational options , from tightening the selection of lead authors and contributors , to dumping it in favor of a small permanent body , or even turning the whole climate science assessment process into a moderated "" living "" Wikipedia - IPCC .",It was suggested that the panel learn to avoid nonpolitical problems .,method
natural_language_inference,50,We discover several interesting properties of QA embeddings in Hyperbolic space .,abstract,abstract,0,46,44,44,0,abstract : abstract,0.14511041009463724,0.9565217391304348,0.9565217391304348,We discover several interesting properties of QA embeddings in Hyperbolic space ,12,We conduct extensive qualitative analysis of both the learned QA embeddings and word embeddings .,"Due to its compositional nature , we find that our model learns to self - organize not only at the QA level but also at the word - level .",abstract
sentiment_analysis,18,"Therefore , we add an unsupervised objective function to ensure the quality of the aspect embeddings , which is jointly trained with the attention - based LSTM .",model,Target Representation,0,99,33,13,0,model : Target Representation,0.41422594142259417,0.4459459459459459,0.5652173913043478,Therefore we add an unsupervised objective function to ensure the quality of the aspect embeddings which is jointly trained with the attention based LSTM ,25,It is difficult to obtain coherent aspect embeddings if we only rely on the training of the sentiment classifier .,"Indeed , we can understand the process shown by Eq. as an autoencoder , where we first reduce c s from d dimensions to K dimensions with softmax non-linearity .",method
question_answering,1,Another important difference from SQuAD is that the answer entity might appear more than once in the context paragraph .,experiment,CLOZE TEST EXPERIMENTS,0,242,79,13,0,experiment : CLOZE TEST EXPERIMENTS,0.7634069400630915,0.8777777777777778,0.5416666666666666,Another important difference from SQuAD is that the answer entity might appear more than once in the context paragraph ,20,"Also , we mask out all non-entity words in the final classification layer so that they are forced to be excluded from possible answers .","To address this , we follow a similar strategy from .",experiment
sentiment_analysis,12,"Our method is able to automatically and incrementally mine attention supervision information from a training corpus , which can be exploited to guide the training of attention mechanisms in ASC models .",introduction,introduction,1,31,20,20,0,introduction : introduction,0.13839285714285715,0.6060606060606061,0.6060606060606061,Our method is able to automatically and incrementally mine attention supervision information from a training corpus which can be exploited to guide the training of attention mechanisms in ASC models ,31,"In this paper , we propose a novel progressive self - supervised attention learning approach for neural ASC models .",The basic idea behind our approach roots in the following fact : the context word with the maximum attention weight has the greatest impact on the sentiment prediction of an input sentence .,introduction
text_summarization,4,We use two widely used summarization datasets with different text lengths .,dataset,dataset,0,165,2,2,0,dataset : dataset,0.6395348837209303,0.16666666666666666,0.16666666666666666,We use two widely used summarization datasets with different text lengths ,12, ,"First , we use the Annotated English Gigaword dataset as used in .",experiment
natural_language_inference,2,"In recent years , several MC datasets have been released .",introduction,introduction,0,14,3,3,0,introduction : introduction,0.05223880597014925,0.10714285714285714,0.10714285714285714,In recent years several MC datasets have been released ,10,"In machine comprehension - based ( MC ) question answering ( QA ) , a machine is expected to provide an answer for a given question by understanding texts .",released a multiple - choice question answering dataset .,introduction
text_summarization,13,"Intuitively , k mul is the number of top - level chunks we select to produce the context .",model,model,0,166,97,97,0,model : model,0.6217228464419475,0.9797979797979798,0.9797979797979798,Intuitively k mul is the number of top level chunks we select to produce the context ,17,"In our results , we label this as + MULTI .","With higher k mul , the hard attention model more closely approximates the soft attention model , and hence should lead to better performance .",method
relation-classification,6,"First , the SVM , Non-Neural Model , was top of the SemEval - 2010 task , during the official competition period .",implementation,Hyperparameter,0,150,8,5,0,implementation : Hyperparameter,0.8287292817679558,0.5333333333333333,0.4166666666666667,First the SVM Non Neural Model was top of the SemEval 2010 task during the official competition period ,19,"We divide the models into three groups , Non-Neural Model , SDP - based Model , and End - to - End Model .",They used many handcraft feature and SVM classifier .,experiment
natural_language_inference,27,"First , we use L2 weight decay on all the trainable variables , with parameter ? = 3 10 ?7 .",dataset,DATASET AND EXPERIMENTAL SETTINGS,1,202,20,20,0,dataset : DATASET AND EXPERIMENTAL SETTINGS,0.5976331360946746,0.6896551724137931,0.6896551724137931,First we use L2 weight decay on all the trainable variables with parameter 3 10 7 ,17,We employ two types of standard regularizations .,"We additionally use dropout on word , character embeddings and between layers , where the word and character dropout rates are 0.1 and 0.05 respectively , and the dropout rate between every two layers is 0.1 .",experiment
natural_language_inference,72,"To check the quality of the recognized entities , we carried out a small manual analysis on 250 entities .",system description,Dataset design,0,85,9,9,0,system description : Dataset design,0.2724358974358974,0.20454545454545456,0.8181818181818182,To check the quality of the recognized entities we carried out a small manual analysis on 250 entities ,19,"For each entity , a concept unique identifier ( CUI ) is also available , which links it to the UMLS R Metathesaurus R.","We found that in 89 % of cases , the boundaries were correct and defined a true entity .",method
text_summarization,5,where y t?1 is the previous output summary word .,system description,Rewrite,0,91,32,8,0,system description : Rewrite,0.3611111111111111,0.6808510638297872,0.8,where y t 1 is the previous output summary word ,11,The combined hidden states are fed into the prevailing attentional RNN decoder to generate the decoding hidden state at the position t:,"Finally , a sof tmax layer is introduced to predict the current summary word :",method
question_answering,1,BI - DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION,title,title,1,2,1,1,0,title : title,0.006309148264984227,1.0,1.0,BI DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION,7, , ,title
text-to-speech_synthesis,0,"In WER calculation , considering the multiple pronunciations , word error is counted only when the output differs from all the references , following .",training,Training and Evaluation,0,129,11,11,0,training : Training and Evaluation,0.7962962962962963,1.0,1.0,In WER calculation considering the multiple pronunciations word error is counted only when the output differs from all the references following ,22,Edit distance is used in PER calculation ., ,experiment
sentiment_analysis,4,"Formally , for each ? ? { a , b} , H ? is created as , Table 1 provides a sample conversation with a context - window of size K =",system description,Problem Setting,0,102,18,18,0,system description : Problem Setting,0.3128834355828221,0.8571428571428571,0.8571428571428571,Formally for each a b H is created as Table 1 provides a sample conversation with a context window of size K ,23,"Considering a context - window of size K , the preceding utterances of Pa and Pb ( starting with the most recent ) within this context - window can be represented by Ha and H b , respectively .","Formally , for each ? ? { a , b} , H ? is created as , Table 1 provides a sample conversation with a context - window of size K =",method
sentiment_analysis,9,"is the SRD threshold . "" . "" denotes the vector dot -product operation .",model,Context-features Dynamic Weighting,0,147,77,8,0,model : Context-features Dynamic Weighting,0.5306859205776173,0.9625,1.0,is the SRD threshold denotes the vector dot product operation ,11,is the length of the input sequence .,"LCF - ATEPC does not only rely on local context features for sentiment polarity classification , but combines and learns the local context features and the global context features to conduct polarity classification .",method
text-classification,6,The Skip - Thought task replaces the LSTM used in the original formulation with a model based on the Transformer architecture .,model,Transformer,0,51,23,9,0,model : Transformer,0.34459459459459457,0.71875,0.8181818181818182,The Skip Thought task replaces the LSTM used in the original formulation with a model based on the Transformer architecture ,21,The supported tasks include : a Skip - Thought like task for the unsupervised learning from arbitrary running text ; a conversational input - response task for the inclusion of parsed conversational data ; and classification tasks for training on supervised data .,"As will be shown in the experimental results below , the transformer based encoder achieves the best over all transfer task performance .",method
sentiment_analysis,42,"Among all our models from single hop to nine hops , we can observe that using more computational layers could generally lead to better performance , especially when the number of hops is less than six .",method,Comparison to Other Methods,1,179,24,24,0,method : Comparison to Other Methods,0.7103174603174603,0.9230769230769232,0.9230769230769232,Among all our models from single hop to nine hops we can observe that using more computational layers could generally lead to better performance especially when the number of hops is less than six ,35,"We can also find that the performance of Contex - tAVG is very poor , which means that assigning the same weight / importance to all the context words is not an effective way .","The best performances are achieved when the model contains seven and nine hops , respectively .",method
text_generation,5,of this class : the variational autoencoder 1 ( VAE ) .,introduction,introduction,0,19,8,8,0,introduction : introduction,0.06440677966101695,0.2424242424242425,0.2424242424242425,of this class the variational autoencoder 1 VAE ,9,Copyright 2017 by the author ( s ) .,The generative story behind the VAE ( to be described in detail in the next section ) is simple :,introduction
machine-translation,2,"Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .",abstract,abstract,0,14,12,12,0,abstract : abstract,0.0625,0.7058823529411765,0.7058823529411765,Ashish with Illia designed and implemented the first Transformer models and has been crucially involved in every aspect of this work ,22,Jakob proposed replacing RNNs with self - attention and started the effort to evaluate this idea .,"Noam proposed scaled dot-product attention , multi-head attention and the parameter - free position representation and became the other person involved in nearly every detail .",abstract
natural_language_inference,28,"Firstly , we test RUM 's memorization capacity on the Copying Memory Task .",experiment,EXPERIMENTS,0,141,2,2,0,experiment : EXPERIMENTS,0.5202952029520295,0.0392156862745098,0.3333333333333333,Firstly we test RUM s memorization capacity on the Copying Memory Task ,13, ,"Secondly , we signify the superiority of RUM by obtaining a state - of - the - art result in the Associative Recall Task .",experiment
natural_language_inference,2,"Two even more challenging open - world QA datasets , TriviaQA and SearchQA , have recently been released .",introduction,introduction,0,25,14,14,0,introduction : introduction,0.09328358208955224,0.5,0.5,Two even more challenging open world QA datasets TriviaQA and SearchQA have recently been released ,16,"As a result , a significant proportion of questions require reasoning beyond simple word matching .",QA consists of question - answer pairs authored by trivia enthusiasts and independently gathered evidence documents from Wikipedia as well as Bing Web search .,introduction
natural_language_inference,80,Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.017241379310344827,0.42857142857142855,0.42857142857142855,Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis ,17,We present a novel deep learning architecture to address the natural language inference ( NLI ) task .,"Instead , we propose a novel dependent reading bidirectional LSTM network ( DR - BiLSTM ) to efficiently model the relationship between a premise and a hypothesis during encoding and inference .",abstract
relation-classification,1,"If ? is too large , it will affect the accuracy of prediction and if ? is too small , the recall will decline .",analysis,Single E1,0,222,25,4,0,analysis : Single E1,0.902439024390244,0.5952380952380952,0.5714285714285714,If is too large it will affect the accuracy of prediction and if is too small the recall will decline ,21,"Single Besides , we also change the Bias Parameter ? from 1 to 20 , and the predicted results are shown in .","If ? is too large , it will affect the accuracy of prediction and if ? is too small , the recall will decline .",result
natural_language_inference,69,"In addition , the number of document paths is ?3 x larger , which along with the complexity of the documents greatly increases the annotation time .",analysis,Validated Test Sets,0,206,53,8,0,analysis : Validated Test Sets,0.5971014492753624,0.9814814814814816,0.8888888888888888,In addition the number of document paths is 3 x larger which along with the complexity of the documents greatly increases the annotation time ,25,"While desirable , crowdsourcing is not feasible for MEDHOP since it requires specialist knowledge .",We thus manually annotated 20 % of the MEDHOP test set and identified the samples for which the text implies the correct answer and where multiple documents are required .,result
natural_language_inference,88,Long Short - Term Memory - Networks for Machine Reading,title,title,1,2,1,1,0,title : title,0.008097165991902834,1.0,1.0,Long Short Term Memory Networks for Machine Reading,8, , ,title
natural_language_inference,68,In what language were the classes given ? German,introduction,introduction,0,29,19,19,0,introduction : introduction,0.11646586345381528,0.475,0.475,In what language were the classes given German,8,"He finished a four - year term in three years , graduating in 1873 .",In what language were the classes given ? German,introduction
natural_language_inference,29,"At the beginning , we use an embedding matrix e to map one - hot representation of each word in the question X q , reviews X r , and attributes A to a high - dimensional vector space .",system description,Review reader,0,125,30,2,0,system description : Review reader,0.3424657534246575,0.1875,0.08333333333333333,At the beginning we use an embedding matrix e to map one hot representation of each word in the question X q reviews X r and attributes A to a high dimensional vector space ,35, ,We denote e ( x ) as the embedding representation of word x .,method
text_summarization,3,"Although generating longer summaries or less UNKs is not our focus , our model still showed improvements in this regard .",model,Models,0,200,3,3,0,model : Models,0.8264462809917356,0.2307692307692308,0.2307692307692308,Although generating longer summaries or less UNKs is not our focus our model still showed improvements in this regard ,20,Gigaword DUC-2004 RG-1 RG - 2 RG- L RG- 1 RG- 2 RG- L ABS + 29.76 11.88 26.96 28.18 8.46 23.81 Luong - NMT 33.10 14.45 30.71 28.55 8.79 24.43 RAS - Elman 33.78 15.97 31.15 28.97 8.26 24.06 lvt5 k-lsent 35.30 16.64 32.62 28.61 9.42 25.24 SEASS 36.15 17.54 33.63 29.21 9.56 25.51 Seq2seq + att ( our impl. ) 33.11 14.67 31.06 28.57 9.31 24.81 Pointer - generator ( our impl. ) rization models .,We counted the number of UNKs and all generated summary words and measured the proportions in both testing sets .,method
natural_language_inference,95,mixed culture is taken from a source and may contain multiple strains or species .,introduction,introduction,0,37,29,29,0,introduction : introduction,0.1581196581196581,0.5,0.5,mixed culture is taken from a source and may contain multiple strains or species ,15,pure culture comprises a single species or strains .,contaminated culture contains organisms that derived from someplace . . .,introduction
natural_language_inference,30,The use of KBs simplifies the problem by separating the issue of collecting and organizing information ( i.e. information extraction ) from the one of searching through it ( i.e. question answering or natural language interfacing ) .,introduction,introduction,0,17,7,7,0,introduction : introduction,0.06589147286821706,0.2258064516129032,0.2258064516129032,The use of KBs simplifies the problem by separating the issue of collecting and organizing information i e information extraction from the one of searching through it i e question answering or natural language interfacing ,36,Question answering is then defined as the task of retrieving the correct entity or set of entities from a KB given a query expressed as a question in natural language .,"However , open question answering remains challenging because of the scale of these KBs ( billions of triples , millions of entities and relationships ) and of the difficulty for machines to interpret natural language .",introduction
sentiment_analysis,13,"To make sure our questions are close to realworld questions , 2 annotators are first exposed to 400 QAs from CQA ( under the laptop category in Amazon.com or popular restaurants in Yelp.com ) to get familiar with real questions .",experiment,Experiments,0,185,9,9,0,experiment : Experiments,0.6654676258992805,0.225,0.225,To make sure our questions are close to realworld questions 2 annotators are first exposed to 400 QAs from CQA under the laptop category in Amazon com or popular restaurants in Yelp com to get familiar with real questions ,40,We keep the split of training and testing of the SemEval 2016 Task 5 datasets and annotate multiple QAs for each review following the way of constructing QAs for the SQuAD 1.1 datasets .,Then they are asked to read reviews and independently label textual spans and ask corresponding questions when they feel the textual spans contain valuable information that customers may care about .,experiment
relation_extraction,1,There are two representations for argument annotation : span - based and dependency - based .,model,Model,0,29,3,3,0,model : Model,0.3258426966292135,0.12,0.12,There are two representations for argument annotation span based and dependency based ,13,"The standard formulation of semantic role labeling decomposes into four subtasks : predicate detection , predicate sense dis ambiguation , argument identification , and argument classification .","Semantic banks such as PropBank usually represent arguments as syntactic constituents ( spans ) , whereas the CoNLL 2008 and 2009 shared tasks propose dependency - based SRL , where the goal is to identify the syntactic heads of arguments rather than the entire span .",method
question_answering,4,"second observation is that the flow of P/Q representations across the network are often well - aligned and ' synchronous ' , i.e. , P is often only matched with Q at the same hierarchical stage ( e.g. , only after they have passed through a fixed number of encoder layers ) .",introduction,introduction,0,28,17,17,0,introduction : introduction,0.10894941634241244,0.53125,0.53125,second observation is that the flow of P Q representations across the network are often well aligned and synchronous i e P is often only matched with Q at the same hierarchical stage e g only after they have passed through a fixed number of encoder layers ,48,"As such , we hypothesize that explicitly improving information flow can lead to further and considerable improvements in RC models .","To this end , we hypothesize that increasing the number of interaction interfaces , i.e. , matching in an asynchronous , cross-hierarchical fashion , can also lead to an improvement in performance .",introduction
sentiment_analysis,24,"It omits the information y ds , a ds i , and add i propagated from the documentlevel tasks in Eq. ( 4 ) .",model,Models under Comparison,0,213,24,24,0,model : Models under Comparison,0.6848874598070739,1.0,1.0,It omits the information y ds a ds i and add i propagated from the documentlevel tasks in Eq 4 ,21,The structure of IMN ?d is shown as the solid lines in .,shows the comparison results .,method
sentiment_analysis,4,Its simplicity also allows us to emphasize the contribution of the remaining components of ICON .,methodology,Fusion,0,148,43,4,0,methodology : Fusion,0.4539877300613497,0.4387755102040816,1.0,Its simplicity also allows us to emphasize the contribution of the remaining components of ICON ,16,Concatenation is one of the most common fusion methods ., ,method
natural_language_inference,6,"We choose English and Spanish for that purpose , as most of the data is aligned with these languages .",training,Training data and pre-processing,0,87,18,3,0,training : Training data and pre-processing,0.35080645161290325,0.1956521739130435,0.3,We choose English and Spanish for that purpose as most of the data is aligned with these languages ,19,"As described in Section 3.2 , training requires bitexts aligned with two target languages .","We collect training corpora for 93 input languages by combining the Europarl , United Nations , Open - Subtitles 2018 , Global Voices , Tanzil and Tatoeba corpus , which are all publicly available on the OPUS website",experiment
relation_extraction,4,The final sentence representation z is computed as :,introduction,introduction,0,64,54,54,0,introduction : introduction,0.30917874396135264,0.6585365853658537,0.6585365853658537,The final sentence representation z is computed as ,9,We regard attention weight a i as the relative contribution of the specific word to the sentence representation .,is later fed into a fully - connected layer followed by a softmax layer for relation classification .,introduction
sentiment_analysis,49,Think of an utterance,introduction,introduction,0,33,23,23,0,introduction : introduction,0.13043478260869565,0.696969696969697,0.696969696969697,Think of an utterance,4,"We argue that in multi-modal sentiment classification , not only the relation among two modalities of the same utterance is important , but also relatedness with the modalities across its context are important .","Ut that constitutes of three modalities , say At ( i.e. audio ) , Vt ( i.e. visual ) and T t ( i.e. text ) .",introduction
natural_language_inference,62,"The extraction is performed in a controllable manner , and the extracted results are provided as general knowledge to our MRC model .",method,method,0,48,3,3,0,method : method,0.21428571428571427,0.06,0.2,The extraction is performed in a controllable manner and the extracted results are provided as general knowledge to our MRC model ,22,"In this section , we elaborate a WordNet - based data enrichment method , which is aimed at extracting inter-word semantic connections from each passage - question pair in our MRC dataset .","Word Net is a lexical data base of English , where words are organized into synsets according to their senses .",method
natural_language_inference,62,Knowledge Aided Reader ( KAR ) .,method,Knowledge Aided Reader,0,89,44,3,0,method : Knowledge Aided Reader,0.39732142857142855,0.88,0.42857142857142855,Knowledge Aided Reader KAR ,5,"In this section , we elaborate our MRC model :","The key components of most existing MRC models are their attention mechanisms , which are aimed at fusing the associated representations of each given passage - question pair .",method
text-classification,5,We furthermore assess the importance of discriminative fine - tuning ( ' Discr ' ) and slanted triangular learning rates ( ' Stlr ' ) .,training,Impact of LM fine-tuning,0,219,25,7,0,training : Impact of LM fine-tuning,0.8690476190476191,0.5555555555555556,0.25925925925925924,We furthermore assess the importance of discriminative fine tuning Discr and slanted triangular learning rates Stlr ,17,"We compare training from scratch , fine - tuning the full model ( ' Full ' ) , only fine - tuning the last layer ( ' Last ' ) , ' Chain - thaw ' , and gradual unfreezing ( ' Freez ' ) .","We compare the latter to an alternative , aggressive cosine annealing schedule ( ' Cos ' ) .",experiment
natural_language_inference,56,"It encodes the inductive biases that 1 ) objects exists in the world 2 ) they can be sufficiently described by properties 3 ) properties can changeover time 4 ) objects can affect each other and 5 ) given the properties , the effects object have on each other is invariant to time .",introduction,introduction,1,27,14,14,0,introduction : introduction,0.08035714285714286,0.5,0.5,It encodes the inductive biases that 1 objects exists in the world 2 they can be sufficiently described by properties 3 properties can changeover time 4 objects can affect each other and 5 given the properties the effects object have on each other is invariant to time ,48,It serves as a modular component for many - step relational reasoning in end - to - end differentiable learning systems .,"An important insight from the work of is to decompose a function for relational reasoning into two components or "" modules "" :",introduction
text_generation,0,"But to make it more general , we focus on the situation where discriminator can only provide final reward , i.e. , the probability that a finished sequence was real .",implementation,Model Implementations,0,290,21,21,0,implementation : Model Implementations,0.8950617283950617,0.42857142857142855,0.5675675675675675,But to make it more general we focus on the situation where discriminator can only provide final reward i e the probability that a finished sequence was real ,29,"In case of some specific tasks , one may design a classifier to provide intermediate reward signal to enhance the performance of our framework .","We first represent an input sequence x 1 , . . . , x T as :",experiment
machine-translation,2,The wavelengths form a geometric progression from 2 ? to 10000 2 ?.,model,Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0,123,82,8,0,model : Layer Type Complexity per Layer Sequential Maximum Path Length Operations,0.5491071428571429,0.7522935779816514,0.7272727272727273,The wavelengths form a geometric progression from 2 to 10000 2 ,12,"That is , each dimension of the positional encoding corresponds to a sinusoid .","We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PE pos+k can be represented as a linear function of PE pos .",method
natural_language_inference,72,"First , we report machine performance for several baselines and neural readers .",introduction,introduction,0,42,34,34,0,introduction : introduction,0.1346153846153846,0.7083333333333334,0.7083333333333334,First we report machine performance for several baselines and neural readers ,12,We examine the performance on the dataset in two ways .,"To enable a more flexible answer evaluation , we expand the answers with their respective synonyms from a medical knowledge base , and additionally supplement the standard evaluation metrics with BLEU and embedding - based methods .",introduction
sentiment_analysis,7,We also use advanced hyperparameter optimization tools to achieve the best possible model configuration depending on our resource constraints .,introduction,introduction,0,23,16,16,0,introduction : introduction,0.2169811320754717,0.9411764705882352,0.9411764705882352,We also use advanced hyperparameter optimization tools to achieve the best possible model configuration depending on our resource constraints ,20,"Second , we use Motion - capture data instead of Video recording , hence we do not use 3D - Convolutions but 2D - Convolutions which are faster have less memory requirements .",Our code is open sourced for other researchers to repeat and enhance our study .,introduction
text-classification,0,The fields used are title and content . :,method,Large-scale Datasets and Results,0,157,55,15,0,method : Large-scale Datasets and Results,0.6916299559471366,0.4621848739495799,0.3333333333333333,The fields used are title and content ,8,The models for English can then be applied to this dataset without change .,Testing errors of all the models .,method
text_summarization,1,"On the other hand , summarization or question generation exhibit one - to - many relationships because * Most work done during internship at Clova AI .",introduction,introduction,0,14,4,4,0,introduction : introduction,0.05833333333333333,0.11428571428571427,0.11428571428571427,On the other hand summarization or question generation exhibit one to many relationships because Most work done during internship at Clova AI ,23,"For instance , paraphrasing or machine translation exhibit a one - to - one relationship because the source and the target should carry the same meaning .","Source Passage : in december 1878 , tesla left graz and severed all relations with his family to hide the fact that he dropped out of school .",introduction
question_answering,0,The error analysis confirms that our approach can successfully process complex semantic parses .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.030508474576271188,1.0,1.0,The error analysis confirms that our approach can successfully process complex semantic parses ,14,We show on two data sets that the graph networks outperform all baseline models that do not explicitly model the structure ., ,abstract
natural_language_inference,21,"To the best of our knowledge , DiSAN improves the last best accuracy ( given by CNN - Tensor ) by 0.52 % .",model,Model,1,235,9,9,0,model : Model,0.8103448275862069,0.15517241379310345,0.42857142857142855,To the best of our knowledge DiSAN improves the last best accuracy given by CNN Tensor by 0 52 ,20,"In , we compare previous works with DiSAN on test accuracy .","Compared to tree - based models with heavy use of the prior structure , e.g. , MV - RNN , RNTN and Tree - LSTM , DiSAN outperforms them by 7.32 % , 6.02 % and 0.72 % , respectively .",method
relation-classification,8,The results on SemEval 2018 Task 7 are shown in .,training,Results on SemEval 2018 Task 7,1,116,13,3,0,training : Results on SemEval 2018 Task 7,0.8467153284671532,0.7222222222222222,0.42857142857142855,The results on SemEval 2018 Task 7 are shown in ,11,Results on SemEval 2018,"Our Entity - Aware BERT SP gives comparable results to the top - ranked system in the shared task , with slightly lower Macro - F1 , which is the official metric of the task , and slightly higher Micro - F1 .",experiment
text-classification,6,"The transformer model space complexity also scales quadratically , O ( n 2 ) , in sentence length , while the DAN model space complexity is constant in the length of the sentence . Similar to compute usage , memory usage for the transformer model increases quickly with sentence length , while the memory usage for the DAN model remains constant .",Resource Usage,Resource Usage,0,139,11,11,0,Resource Usage : Resource Usage,0.9391891891891893,0.8461538461538461,0.8461538461538461,The transformer model space complexity also scales quadratically O n 2 in sentence length while the DAN model space complexity is constant in the length of the sentence Similar to compute usage memory usage for the transformer model increases quickly with sentence length while the memory usage for the DAN model remains constant ,54,"Since the DAN model is remarkably computational efficient , using GPUs over CPUs will often have a much larger practical impact for the transformer based encoder .","We note that , for the DAN model , memory usage is dominated by the parameters used to store the model unigram and bigram embeddings .",others
natural_language_inference,46,"First , they are largely self - contained : beyond the basic fundamental vocabulary of English , all the information about salient entities and concepts required to understand the narrative is present in the document , with the expectation that a reasonably competent language user would be able to understand it .",system description,FRANK (to the baby),0,44,28,22,0,system description : FRANK (to the baby),0.14814814814814814,0.3333333333333333,0.28205128205128205,First they are largely self contained beyond the basic fundamental vocabulary of English all the information about salient entities and concepts required to understand the narrative is present in the document with the expectation that a reasonably competent language user would be able to understand it ,47,Fictional stories have a number of advantages as a domain .,"Second , story summaries are abstractive and generally written by independent authors who know the work only as a reader .",method
natural_language_inference,97,"This weakness is not unexpected , since our architecture lacks any capacity for counting or tracking temporal order .",analysis,analysis,0,273,17,17,0,analysis : analysis,0.9349315068493148,0.6071428571428571,0.6071428571428571,This weakness is not unexpected since our architecture lacks any capacity for counting or tracking temporal order ,18,"Quantity questions makeup 9.5 % of our errors on the validation set , while order questions makeup 10.3 % .","Incorporating mechanisms for these forms of reasoning is a priority for future work ( in contrast , the Memory Network model is quite good at temporal reasoning ) .",result
natural_language_inference,46,"This requires potential solutions to these tasks to jointly model the process of searching for information ( possibly in several steps ) to serve as support for generating an answer , alongside the process of generating the answer entailed by said support .",Figure 1:,Review of Reading Comprehension Datasets and Models,0,292,19,14,0,Figure 1: : Review of Reading Comprehension Datasets and Models,0.9831649831649832,0.7916666666666666,0.7368421052631579,This requires potential solutions to these tasks to jointly model the process of searching for information possibly in several steps to serve as support for generating an answer alongside the process of generating the answer entailed by said support ,40,from the source text .,"End - to - end mechanisms for both searching for information , such as attention , do not scale beyond selecting words or n-grams in short contexts such as sentences and small documents .",others
negation_scope_resolution,0,This method showed the power of using a joint encoding for both tasks .,approach,Deep Learning Approaches,0,166,89,30,0,approach : Deep Learning Approaches,0.7217391304347827,0.978021978021978,0.9375,This method showed the power of using a joint encoding for both tasks ,14,"They used a BiLSTM to encode the sequence at the word level , and an LSTM decoder .",Chen ( 2019 ) used attention based BiLSTM networks and word embeddings to detect assertions and negations .,method
sentiment_analysis,23,Training details are further presented in Section 4.1 and 4.2 .,training,Training Objective,0,170,7,7,0,training : Training Objective,0.5821917808219178,1.0,1.0,Training details are further presented in Section 4 1 and 4 2 ,13,"To regularize our model , we apply both 2 penalty and dropout .", ,experiment
relation_extraction,0,"For these tasks , RNNs that make use of tree structures have been deemed more suitable. , for example , propose an RNN comprised of a sequencebased long short term memory ( LSTM ) for entity identification and a separate tree - based dependency LSTM layer for relation classification using shared parameters between the two components .",introduction,introduction,0,17,9,9,0,introduction : introduction,0.06640625,0.3461538461538461,0.3461538461538461,For these tasks RNNs that make use of tree structures have been deemed more suitable for example propose an RNN comprised of a sequencebased long short term memory LSTM for entity identification and a separate tree based dependency LSTM layer for relation classification using shared parameters between the two components ,51,"However , their ability to identify relations between non-adjacent tokens in a sequence , e.g. , the head nouns of two entities , is less explored .","As a result , their model depends critically on access to dependency trees , restricting it to sentencelevel extraction and to languages for which ( good ) dependency parsers exist .",introduction
text-classification,9,We use dropout on all nonlinear connections with a dropout rate of 0.5 .,experiment,Experimental Setting,1,155,17,16,0,experiment : Experimental Setting,0.6150793650793651,0.6071428571428571,0.5925925925925926,We use dropout on all nonlinear connections with a dropout rate of 0 5 ,15,"For the final sentence vector , we concatenate the feature maps to get a 300 - dimension vector .","We also use an l 2 constraint of 3 , following for accurate comparisons .",experiment
text-classification,5,"We then unfreeze the next lower frozen layer and repeat , until we finetune all layers until convergence at the last iteration .",system description,Target task classifier fine-tuning,0,135,84,17,0,system description : Target task classifier fine-tuning,0.5357142857142857,0.8842105263157894,0.6071428571428571,We then unfreeze the next lower frozen layer and repeat until we finetune all layers until convergence at the last iteration ,22,We first unfreeze the last layer and fine - tune all unfrozen layers for one epoch .,"This is similar to ' chain - thaw ' , except that we add a layer at a time to the set of ' thawed ' layers , rather than only training a single layer at a time .",method
natural_language_inference,100,jk denotes the sum of all matching signals within the k - th value range or bin .,model,Value-shared Weighting,0,167,38,27,0,model : Value-shared Weighting,0.4562841530054645,0.6785714285714286,0.9,jk denotes the sum of all matching signals within the k th value range or bin ,17,Let w denote a K + 1 dimensional model parameter from input layer to hidden layer .,"For each QA matching vector of a given query q , the combined score after the activation function of the j - th node in hidden layer is defined as",method
relation-classification,5,"In , the joint task is formulated as a sequence tagging problem , and a BiLSTM with a softmax output layer can then be used for joint prediction .",introduction,introduction,0,24,18,18,0,introduction : introduction,0.21238938053097345,0.6428571428571429,0.6428571428571429,In the joint task is formulated as a sequence tagging problem and a BiLSTM with a softmax output layer can then be used for joint prediction ,27,"Recently , formulated the joint entity and relation extraction problem as a directed graph and proposed a BiLSTM - and transition - based approach to generate the graph incrementally . [ 4 ] extended the multi-head selection - based joint model with adversarial training .","In this paper , we present a novel end - to - end neural model for joint entity and relation extraction .",introduction
sentiment_analysis,22,"No- fusion is a simplified version of HAPN , where we directly concatenate the target representation and the position - aware representation of each word as the inputs to the source2context attention .",performance,performance,0,177,24,24,0,performance : performance,0.7729257641921398,0.4528301886792453,0.4528301886792453,No fusion is a simplified version of HAPN where we directly concatenate the target representation and the position aware representation of each word as the inputs to the source2context attention ,31,"To verify the efficiency of the information fusion , we further design the following model for comparison :","In , we report the performance comparison of HAPN and No-fusion .",result
natural_language_inference,81,Four of the mummies are currently on display in the Greenland National Museum .,APPENDIX,Answer town,0,299,91,11,0,APPENDIX : Answer town,0.7102137767220903,0.4272300469483568,0.08270676691729323,Four of the mummies are currently on display in the Greenland National Museum ,14,"Formally a settlement , it is famous for the discovery of eight mummified bodies in 1972 .","Norway ( ; Norwegian : ( Bokml ) or ( Nynorsk ) ; Sami : "" Norgga "" ) , officially the Kingdom of Norway , is a sovereign and unitary monarchy whose territory comprises the western portion of the Scandinavian Peninsula plus the island Jan Mayen and the archipelago of Svalbard .",others
natural_language_inference,65,"However , most recent work on neural attention models have focused on one - way attention mechanisms based on recurrent neural networks designed . for generation tasks .",introduction,introduction,1,11,3,3,0,introduction : introduction,0.04782608695652174,0.09090909090909093,0.09090909090909093,However most recent work on neural attention models have focused on one way attention mechanisms based on recurrent neural networks designed for generation tasks ,25,"Neural networks ( NN ) with attention mechanisms have recently proven to be successful at different computer vision ( CV ) and natural language processing ( NLP ) tasks such as image captioning , machine translation and factoid question answering .","Another important family of machine learning tasks are centered around pair - wise ranking or classification , which have a broad set of applications , including but not limited to , question answering , entailment , paraphrasing and any other pair - wise matching problems .",introduction
temporal_information_extraction,0,"This alleviates the issue of high precision and low recall , typical of the rule - based sieve .",system,Temporal Reasoner,0,65,38,3,0,system : Temporal Reasoner,0.3439153439153439,0.5846153846153846,0.375,This alleviates the issue of high precision and low recall typical of the rule based sieve ,17,"Based on the output of the previous sieve , we run a transitive reasoner layer , similar to CAEVO , in order to infer new temporal links among candidate pairs .",An annotated TimeML document can be mapped into a constraint problem according to how TLINKs are mapped into Allen relations ) .,method
natural_language_inference,8,"As can be seen , the bigram model performs better than the unigram model and the addition of the IDF - weighted word count features significantly improve performance for both models by 10 % - 15 % .",experiment,experiment,1,161,13,13,0,experiment : experiment,0.7630331753554502,0.6190476190476191,0.6190476190476191,As can be seen the bigram model performs better than the unigram model and the addition of the IDF weighted word count features significantly improve performance for both models by 10 15 ,33,summarises the results of our models .,Wang et al . reported that training with the noisy dataset TRAIN - ALL negatively impacted their models .,experiment
machine-translation,7,Our approach to conditional computation is to introduce a new type of general purpose neural network component : a Sparsely - Gated Mixture - of - Experts Layer ( MoE ) .,system description,OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,1,45,2,2,0,system description : OUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER,0.12064343163538875,0.038461538461538464,0.2222222222222222,Our approach to conditional computation is to introduce a new type of general purpose neural network component a Sparsely Gated Mixture of Experts Layer MoE ,26, ,"The MoE consists of a number of experts , each a simple feed - forward neural network , and a trainable gating network which selects a sparse combination of the experts to process each input ( see ) .",method
sentiment_analysis,27,"From these examples , we can observe that our proposed model ( with - GCN model ) not only focuses the corresponding words which are useful for predicting the sentiment of each aspect , but also considers the textual information which is helpful for judging the relation between different aspects .",result,Case study,0,264,48,12,0,result : Case study,0.9565217391304348,0.96,0.8571428571428571,From these examples we can observe that our proposed model with GCN model not only focuses the corresponding words which are useful for predicting the sentiment of each aspect but also considers the textual information which is helpful for judging the relation between different aspects ,46,"In the contrary , with - GCN model enforces the model to pay attention on the word "" but "" when predicting the sentiment polarity for aspect "" fonts "" .","By using attention mechanism to focus on the textual words describing the interdependence between different aspects , the downstream GCN module can effectively further represent the sentiment dependencies between different aspects in one sentence .",result
text_generation,0,"In three realworld tasks , i.e. poem generation , speech language generation and music generation , SeqGAN significantly outperforms the compared baselines in various metrics including human expert judgement .",introduction,introduction,0,40,29,29,0,introduction : introduction,0.12345679012345676,1.0,1.0,In three realworld tasks i e poem generation speech language generation and music generation SeqGAN significantly outperforms the compared baselines in various metrics including human expert judgement ,28,"In our synthetic data environment , SeqGAN significantly outperforms the maximum likelihood methods , scheduled sampling and PG - BLEU .", ,introduction
prosody_prediction,0,"As noted in Section 3 , the difference between BERT and BiLSTM is much bigger with the The Boston University radio news corpus test set ( + 3.9 % compared to + 1.1 % with our test set ) .",REF:,REF:,0,171,9,9,0,REF: : REF:,0.890625,0.5,0.5,As noted in Section 3 the difference between BERT and BiLSTM is much bigger with the The Boston University radio news corpus test set 3 9 compared to 1 1 with our test set ,35,We expect that the difference between BERT and other models would be higher with a dataset drawn from a more contemporary source .,"This could be due to the genre , with The Boston University radio news corpus being derived from a more contemporary source .",others
question_answering,0,"In these cases , two nodes in the graph interact with each other through the single edge in both directions .",result,result,0,243,32,32,0,result : result,0.823728813559322,0.9696969696969696,0.9696969696969696,In these cases two nodes in the graph interact with each other through the single edge in both directions ,20,"Notably , GGNN also has the best performance on the most simple semantic parses that only have one edge .",The GGNN is better at capturing this interaction than other models .,result
relation-classification,3,Adversarial training ( AT ),model,Adversarial training (AT),1,67,34,1,0,model : Adversarial training (AT),0.489051094890511,0.7391304347826086,0.07692307692307693,Adversarial training AT ,4, , ,method
sentiment_analysis,15,Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition .,abstract,abstract,1,5,3,3,0,abstract : abstract,0.018518518518518517,0.3,0.3,Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition ,25,Semantic word spaces have been very useful but can not express the meaning of longer phrases in a principled way .,"To remedy this , we introduce a Sentiment Treebank .",abstract
machine-translation,0,"Instead , one should focus on the most relevant subset of the data for a given task .",baseline,Data and Baseline System,0,127,8,8,0,baseline : Data and Baseline System,0.5799086757990868,0.4705882352941176,0.4705882352941176,Instead one should focus on the most relevant subset of the data for a given task ,17,"It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance , and results in extremely large models which are difficult to handle .","We have done so by applying the data selection method proposed in , and its extension to bitexts .",result
machine-translation,2,"Similarly , self - attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .",model,Applications of Attention in our Model,0,97,56,8,0,model : Applications of Attention in our Model,0.4330357142857143,0.5137614678899083,0.7272727272727273,Similarly self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position ,28,Each position in the encoder can attend to all positions in the previous layer of the encoder .,We need to prevent leftward information flow in the decoder to preserve the auto - regressive property .,method
text_generation,5,"When we set the number of labeled samples to be 25 k , the SCNN - VAE - Semi achieves an accuracy of 70.4 , which is similar to LM - LSTM with an accuracy of 70.5 .",result,Semi-supervised VAE results,0,231,47,22,0,result : Semi-supervised VAE results,0.7830508474576271,0.5164835164835165,0.8148148148148148,When we set the number of labeled samples to be 25 k the SCNN VAE Semi achieves an accuracy of 70 4 which is similar to LM LSTM with an accuracy of 70 5 ,35,The advantage decreases as we increase the number of labeled samples .,"Also , SCNN - VAE - Semi performs better on Yahoo data set than Yelp data set .",result
sentiment_analysis,32,"Then , we can get wk ? Rd from M vd , where k is the word index in the context or target , d means the embedding dimension and v gives the vocabulary size .",system description,Interactive Attention Networks,0,61,11,11,0,system description : Interactive Attention Networks,0.26521739130434785,0.2391304347826087,0.2391304347826087,Then we can get wk Rd from M vd where k is the word index in the context or target d means the embedding dimension and v gives the vocabulary size ,32,"To represent a word , we embed each word into a lowdimensional real - value vector , called word embedding .","Then , we can get wk ? Rd from M vd , where k is the word index in the context or target , d means the embedding dimension and v gives the vocabulary size .",method
sentiment_analysis,2,"In addition to utilizing the position information , PBAN also mutually models the relationship between the sentence and different words in the aspect term by adopting a bidirectional attention mechanism .",introduction,introduction,1,40,24,24,0,introduction : introduction,0.1762114537444934,0.7272727272727273,0.7272727272727273,In addition to utilizing the position information PBAN also mutually models the relationship between the sentence and different words in the aspect term by adopting a bidirectional attention mechanism ,30,"Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) .","To be specific , our model consists of three components :",introduction
machine-translation,9,"For reference , we also report the total size ( MB ) of the embedding layer in the right table , which includes the sizes of the basis matrix and the hash table .",analysis,SENTIMENT ANALYSIS,0,198,21,21,0,analysis : SENTIMENT ANALYSIS,0.6898954703832753,0.23863636363636365,0.65625,For reference we also report the total size MB of the embedding layer in the right table which includes the sizes of the basis matrix and the hash table ,30,The average reconstruction loss on a fixed validation set is summarized in the left of .,We can see that increasing either M or K can effectively decrease the reconstruction loss .,result
question_answering,2,"is the element - wise multiplication , w p is the weight vector to learn and p is a vector of classification probability .",architecture,Focal Visual-Text Attention,0,108,29,16,0,architecture : Focal Visual-Text Attention,0.3898916967509025,0.26605504587155965,0.8,is the element wise multiplication w p is the weight vector to learn and p is a vector of classification probability ,22,where the operator [ ; ] represents the concatenation of two matrices along the last dimension .,"After obtaining the answer probability , the model can be trained end - to - end using cross - entropy loss function .",method
question_answering,2,Implementation Details In Memex,implementation,implementation,0,205,1,1,0,implementation : implementation,0.740072202166065,0.02127659574468085,0.043478260869565216,Implementation Details In Memex,4, , ,experiment
relation_extraction,9,Then we adopt a softmax function to deal with this correlation modeling matrix G to obtain an attention pooling matrix A p as,model,Convolutional Max-Pooling with Secondary Attention,0,131,84,13,0,model : Convolutional Max-Pooling with Secondary Attention,0.6267942583732058,0.9545454545454546,0.7647058823529411,Then we adopt a softmax function to deal with this correlation modeling matrix G to obtain an attention pooling matrix A p as,23,where U is a weighting matrix learnt by the network .,"where G i , j is the ( i , j ) - th entry of G and A p i , j is the ( i , j ) - th entry of A p .",method
natural_language_inference,23,"All - Level is a naive extension of FA High - Level , where all levels of information are concatenated and is fused into the context using the same attention weight .",result,EFFECTIVENESS OF HISTORY-OF-WORD,0,271,43,10,0,result : EFFECTIVENESS OF HISTORY-OF-WORD,0.5282651072124757,0.6825396825396826,0.3333333333333333,All Level is a naive extension of FA High Level where all levels of information are concatenated and is fused into the context using the same attention weight ,29,FA High - Level is the High - Level model with standard attention replaced by fully - aware attention .,"FA Multi - Level is our proposed Fully - aware Multi - level fusion , where different levels of information are attended under separate attention weight .",result
natural_language_inference,62,"However , since context embeddings contain high - level information , we believe that introducing the preextracted general knowledge into the calculation of such similarities will make the results more reasonable .",architecture,Knowledge Aided Mutual Attention,0,140,45,6,0,architecture : Knowledge Aided Mutual Attention,0.625,0.5844155844155844,0.2608695652173913,However since context embeddings contain high level information we believe that introducing the preextracted general knowledge into the calculation of such similarities will make the results more reasonable ,29,This similarity function has also been adopted by several other works .,Therefore we modify the above similarity function to the following form :,method
sentiment_analysis,27,"After embedding layer , the context embedding is denoted as a matrix E c ? Rd emb N , and the i - th aspect embedding is denoted as a matrix E a i ? Rd emb M i .",methodology,Input embedding layer,0,94,17,6,0,methodology : Input embedding layer,0.34057971014492755,0.2,0.75,After embedding layer the context embedding is denoted as a matrix E c Rd emb N and the i th aspect embedding is denoted as a matrix E a i Rd emb M i ,35,"Then each word will be represented by an embedding vector e t ? Rd emb 1 , where d emb is the dimension of word vectors .","After embedding layer , the context embedding is denoted as a matrix E c ? Rd emb N , and the i - th aspect embedding is denoted as a matrix E a i ? Rd emb M i .",method
text_generation,2,"We randomly sample 80,000 sentences for the training set , and another 5,000 for the test set .",experiment,Middle Text Generation: COCO Image Captions,0,206,29,9,0,experiment : Middle Text Generation: COCO Image Captions,0.5885714285714285,0.7837837837837838,0.8181818181818182,We randomly sample 80 000 sentences for the training set and another 5 000 for the test set ,19,"After the preprocessing , the dataset includes 4,980 words .",The results BLEU scores are provided in .,experiment
text_summarization,5,"Therefore , we argue that , the free generation based on the source sentence is not enough for a seq2seq model .",introduction,introduction,0,21,12,12,0,introduction : introduction,0.08333333333333333,0.3333333333333333,0.3333333333333333,Therefore we argue that the free generation based on the source sentence is not enough for a seq2seq model ,20,"In addition , we find seq2seq models usually focus on copying source words in order , without any actual "" summarization "" .","Template based summarization ( e.g. , ) is a traditional approach to abstractive summarization .",introduction
natural_language_inference,84,"Both words "" scientist "" and "" actress "" were not frequent enough to make it to V train , but the definitions "" actress -> a female actor "" "" scientist -> a person with advanced knowledge of one or more sciences "" apparently provided enough information about these words that the model could start matching them with named entities in the passage .",system description,QUESTION ANSWERING,0,146,79,48,0,system description : QUESTION ANSWERING,0.6347826086956522,0.7117117117117117,0.8571428571428571,Both words scientist and actress were not frequent enough to make it to V train but the definitions actress a female actor scientist a person with advanced knowledge of one or more sciences apparently provided enough information about these words that the model could start matching them with named entities in the passage ,54,"Another pattern that we saw is that the model with the dictionary was able to answer questions of the form "" which scientist "" or "" which actress "" better .","Furthermore , we compared the models G and SD in a similar way .",method
question-answering,3,"With those pre-trained embeddings , we transform Sand T into sentence matrixes S = [ s 1 , ... , s i , ... , s m ] and T = [t 1 , ... , t j , ... , tn ] , where s i and t j are d-dimension vectors of the corresponding words , and m and n are sentence length of Sand T respectively .",model,Model Overview,0,63,10,10,0,model : Model Overview,0.2480314960629921,0.1,1.0,With those pre trained embeddings we transform Sand T into sentence matrixes S s 1 s i s m and T t 1 t j tn where s i and t j are d dimension vectors of the corresponding words and m and n are sentence length of Sand T respectively ,52,"Word embedding of is an effective way to handle the lexical gap challenge in the sentence similarity task , as it represents each word with a distributed vector , and words appearing in similar contexts tend to have similar meanings .", ,method
natural_language_inference,19,"The forward masks prevent words that appear after a given word from being considered in the attention process , while backward masks prevent words that appear before from consideration by adding ?? to the logits before taking the softmax at the attention phase .",architecture,Masked Multi-Head Attention,0,105,30,14,0,architecture : Masked Multi-Head Attention,0.4117647058823529,0.3703703703703704,0.6086956521739131,The forward masks prevent words that appear after a given word from being considered in the attention process while backward masks prevent words that appear before from consideration by adding to the logits before taking the softmax at the attention phase ,42,"In the Forward Masked Multi - Head Attention phase , the forward mask is selected , and in the Backward Masked Multi - Head Attention phase , the backward mask .","The diagonal component of M dir is also set to ?? so that each token does not consider itself to attention , and the information of each token is later transmitted through the fusion gate of section 4.2.3 M dis is shown in the .",method
relation_extraction,8,"When using a neural network , the convolution approach is a natural means of merging all these features .",methodology,Convolution,0,123,39,4,0,methodology : Convolution,0.4572490706319702,0.3786407766990291,0.2222222222222222,When using a neural network the convolution approach is a natural means of merging all these features ,18,"Thus , it might be necessary to utilize all local features and perform this prediction globally .","Convolution is an operation between a vector of weights , w , and a vector of inputs that is treated as a sequence q .",method
temporal_information_extraction,1,"In parallel to the work presented here , Leeuwenberg and Moens ( 2017 ) also proposed a structured learning approach to extracting the temporal relations .",introduction,introduction,0,46,37,37,1,introduction : introduction,0.17898832684824906,0.8409090909090909,0.8409090909090909,In parallel to the work presented here Leeuwenberg and Moens 2017 also proposed a structured learning approach to extracting the temporal relations ,23,"Although L+I methods impose global constraints in the inference phase , this paper argues that global considerations are necessary in the learning phase as well ( i.e. , structured learning ) .","Their work focuses on a domain - specific dataset from Clinical TempEval , so their work does not need to address some of the difficulties of the general problem that our work addresses .",introduction
relation-classification,0,Previous joint models have employed feature - based structured learning .,introduction,introduction,0,16,6,6,0,introduction : introduction,0.07079646017699115,0.25,0.25,Previous joint models have employed feature based structured learning ,10,"Extraction of these entities is in turn encouraged by the presence of the context words transferred to , which indicate an employment relation .",An alternative approach to this end - to - end relation extraction task is to employ automatic feature learning via neural network ( NN ) based models .,introduction
semantic_role_labeling,4,Our span - based model computes and uses span representations ( Eq. 7 ) for label prediction .,performance,Function F ?,0,230,37,15,0,performance : Function F ?,0.7666666666666667,0.7551020408163265,0.5555555555555556,Our span based model computes and uses span representations Eq 7 for label prediction ,15,Qualitative Analysis on Our Model,"To investigate a relation between the span representations and predicted labels , we qualitatively analyze nearest neighbors of each span representation with its predicted label .",result
sentiment_analysis,25,"In the first category , sentiment analysis is performed toward the aspect terms thatare labeled in the given sentence .",system description,Aspect based Sentiment Analysis,0,56,12,7,0,system description : Aspect based Sentiment Analysis,0.25225225225225223,0.15,0.3888888888888889,In the first category sentiment analysis is performed toward the aspect terms thatare labeled in the given sentence ,19,Aspect - Term Sentiment Analysis .,large body of literature tries to utilize the relation or position between the target words and the surrounding context words either by using the tree structure of dependency or by simply counting the number of words between them as a relevance information .,method
natural_language_inference,51,"Compute the program interface ? p t , n ? P I ,n ( c t ) 6 :",method,On the Benefit of NSM to MANN: An Explanation from Multilevel Modeling,0,119,57,9,0,method : On the Benefit of NSM to MANN: An Explanation from Multilevel Modeling,0.4490566037735849,0.8028169014084507,0.39130434782608703,Compute the program interface p t n P I n c t 6 ,14,"for n = 1 , R do 5 :","Compute the program W c t , n ? N SM ? p t , n , M p , n 7:",method
sentiment_analysis,48,"The ATSA task aims to classify a data sample with input sentence x = {x 1 , ... , x n } and corresponding aspect 1 a = {a 1 , ... , am } , where a is a subsequence of x , into a sentiment polarity y , where y ? {P , O , N }. P , O , N denotes "" positive "" , "" neutral "" , "" negative "" .",method,Method Description,0,65,3,3,0,method : Method Description,0.2754237288135593,0.04615384615384616,0.17647058823529413,The ATSA task aims to classify a data sample with input sentence x x 1 x n and corresponding aspect 1 a a 1 am where a is a subsequence of x into a sentiment polarity y where y P O N P O N denotes positive neutral negative ,50,"In this section , the problem definition is provided and then the model framework is presented in detail .","The ATSA task aims to classify a data sample with input sentence x = {x 1 , ... , x n } and corresponding aspect 1 a = {a 1 , ... , am } , where a is a subsequence of x , into a sentiment polarity y , where y ? {P , O , N }. P , O , N denotes "" positive "" , "" neutral "" , "" negative "" .",method
natural_language_inference,55,Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine - grained detection heuristics to identify the exact answer .,introduction,introduction,0,13,7,7,0,introduction : introduction,0.08843537414965986,0.3684210526315789,0.3684210526315789,Information retrieval systems first retrieve a broad set of candidate answers by querying the search API of KBs with a transformation of the question into a valid query and then use fine grained detection heuristics to identify the exact answer ,41,"The state - of - the - art techniques in open QA can be classified into two main classes , namely , information retrieval based and semantic parsing based .","On the other hand , semantic parsing methods focus on the correct interpretation of the meaning of a question by a semantic parsing system .",introduction
relation-classification,1,"It can be seen that our method , LSTM - LSTM - Bias , outperforms all other methods in F 1 score and achieves a 3 % improvement in F 1 over the best method CoType .",experiment,Experimental Results,1,182,3,3,0,experiment : Experimental Results,0.7398373983739838,0.16666666666666666,0.16666666666666666,It can be seen that our method LSTM LSTM Bias outperforms all other methods in F 1 score and achieves a 3 improvement in F 1 over the best method CoType ,32,We report the results of different methods as shown in .,It shows the effectiveness of our proposed method .,experiment
text_generation,3,"In the testing stage , given an input utterance , the encoder E ? , the decoder D ? , and the matching module M ? work together to produce a dialogue response .",training,Training and Testing,0,69,2,2,0,training : Training and Testing,0.4893617021276596,0.15384615384615385,0.15384615384615385,In the testing stage given an input utterance the encoder E the decoder D and the matching module M work together to produce a dialogue response ,27, ,"In the testing stage , given an input utterance , the encoder E ? , the decoder D ? , and the matching module M ? work together to produce a dialogue response .",experiment
natural_language_inference,88,The model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed .,introduction,introduction,1,49,30,30,0,introduction : introduction,0.19838056680161945,0.9090909090909092,0.9090909090909092,The model processes text incrementally while learning which past tokens in the memory and to what extent they relate to the current token being processed ,26,illustrates the reading behavior of the LSTMN .,"As a result , the model induces undirected relations among tokens as an intermediate step of learning representations .",introduction
natural_language_inference,93,"Q = att ( u Q , V Q r ) is an attention - pooling vector of the question based on the parameter V Q r :",system description,Output Layer,0,100,62,9,0,system description : Output Layer,0.4830917874396135,0.984126984126984,0.9,Q att u Q V Q r is an attention pooling vector of the question based on the parameter V Q r ,23,We utilize the question vector r Q as the initial state of the answer recurrent network .,"To train the network , we minimize the sum of the negative log probabilities of the ground truth start and end position by the predicted distributions .",method
sentiment_analysis,26,It is important to note that many of the results in stem from embeddings that were created automatically using cross - lingual projection .,analysis,Results and Analysis,0,169,23,23,0,analysis : Results and Analysis,0.689795918367347,0.2948717948717949,0.3538461538461539,It is important to note that many of the results in stem from embeddings that were created automatically using cross lingual projection ,23,"In this case , fine - tuning may allow the model to adjust the embeddings to cater to domain - specific mean - ings and corpus-specific correlations , while also overcoming possible sparsity of the cross-lingual vectors resulting from alack of coverage of the translation dictionary .",Our transfer learning embeddings were induced from entirely English data .,result
natural_language_inference,90,The method was implemented in TensorFlow .,implementation,Implementation Details,1,112,2,2,0,implementation : Implementation Details,0.7466666666666667,0.1176470588235294,0.1176470588235294,The method was implemented in TensorFlow ,7, ,"Data preprocessing : Following Bowman et al. ( 2015 ) , we remove examples labeled "" - "" ( no gold label ) from the dataset , which leaves 549,367 pairs for training , 9,842 for development , and 9,824 for testing .",experiment
natural_language_inference,26,"Therefore , the whole representation for word sequence X is represented as e w = {e ( x 1 ) , . . . e (x n ) } ? R ndw where d w denotes the dimension of word embedding .",model,Integration,0,92,58,10,0,model : Integration,0.4339622641509434,0.9666666666666668,0.8333333333333334,Therefore the whole representation for word sequence X is represented as e w e x 1 e x n R ndw where d w denotes the dimension of word embedding ,31,We then apply ReLU and max pooling to the output embedding sequence for xi :,"Therefore , the whole representation for word sequence X is represented as e w = {e ( x 1 ) , . . . e (x n ) } ? R ndw where d w denotes the dimension of word embedding .",method
natural_language_inference,49,We conduct a ablation study on our base model to examine the effectiveness of each component .,ablation,ablation,0,187,2,2,0,ablation : ablation,0.7362204724409449,0.07407407407407407,0.07407407407407407,We conduct a ablation study on our base model to examine the effectiveness of each component ,17, ,We study our model on MultiNLI dataset and we use Matched validation score as the standard for model selection .,result
relation_extraction,7,Mintz proposes the humandesigned feature model .,baseline,Comparison with Baselines,1,233,3,3,0,baseline : Comparison with Baselines,0.8996138996138996,0.15,0.15,Mintz proposes the humandesigned feature model ,7,"To evaluate our approach , we select the following six methods as our baseline :",MultiR puts forward a graphical model .,result
natural_language_inference,32,We simply treat each natural language instruction I i as a question .,system,system,0,184,37,37,0,system : system,0.42105263157894735,0.7872340425531915,0.7872340425531915,We simply treat each natural language instruction I i as a question ,13,Question Q i :,Answer A i : We encode the world state change from W i?1 to W i as a sequence of tokens .,method
question-answering,9,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words .,experiment,EXPERIMENTAL SETUP,1,106,2,2,0,experiment : EXPERIMENTAL SETUP,0.5955056179775281,0.2222222222222222,0.2222222222222222,We represent each of the words in the question and document using 300 dimensional GloVe embeddings trained on a corpus of 840 bn words ,25, ,These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .,experiment
sentiment_analysis,1,"where D denotes the diagonal degree matrix of A , i.e. , prevents",system description,Simple Graph Convolution Network (SGC),0,139,67,18,0,system description : Simple Graph Convolution Network (SGC),0.351010101010101,0.32367149758454106,0.5806451612903226,where D denotes the diagonal degree matrix of A i e prevents,12,Kipf and Welling alleviated this limitation by proposing the graph convolution network ( GCN ) as follows :,"where D denotes the diagonal degree matrix of A , i.e. , prevents",method
natural_language_inference,43,"In this setup , COMPQ obtained 42.2 F 1 on the test set ( compared to 40.9 F 1 , when training on COM - PLEXQUESTIONS only , as we do ) .",experiment,Experiments,1,121,21,21,0,experiment : Experiments,0.7806451612903226,0.6176470588235294,0.6176470588235294,In this setup COMPQ obtained 42 2 F 1 on the test set compared to 40 9 F 1 when training on COM PLEXQUESTIONS only as we do ,29,"We were able to indirectly compare WEBQA - SUBSET to COMPQ : graciously provided us with the predictions of COMPQ when it was trained on COMPLEXQUESTIONS , WE - BQUESTIONS , and SIMPLEQUESTIONS .","Restricting the predictions to the subset for which candidate extraction succeeded , the F 1 of COMPQ - SUBSET is 48.5 , which is 3.4 F 1 points lower than WEBQA - SUBSET , which was trained on less data .",experiment
sentiment_analysis,3,"This work addresses the task of detecting emotions ( e.g. , happy , sad , angry , etc. ) in textual conversations , where the emotion of an utterance is detected in the conversational context .",introduction,introduction,1,14,5,5,0,introduction : introduction,0.04794520547945205,0.15151515151515152,0.15151515151515152,This work addresses the task of detecting emotions e g happy sad angry etc in textual conversations where the emotion of an utterance is detected in the conversational context ,30,"With the prevalence of social media platforms such as Facebook Messenger , as well as conversational agents such as Amazon Alexa , there is an emerging need for machines to understand human emotions in natural conversations .",Being able to effectively detect emotions in conversations leads to a wide range of applications ranging from opinion mining in social media platforms :,introduction
sentiment_analysis,27,These models are :,result,The effect of GCN module,0,236,20,3,0,result : The effect of GCN module,0.855072463768116,0.4,0.25,These models are ,4,"In this section , we design a series of models to further verify the effectiveness of GCN module .",BiAtt+ GCN is just another name of our proposed SDGCN model .,result
sentiment_analysis,4,"MFN performs multi-view learning by using Delta - memory Attention Network , a fusion mechanism to learn cross - view interactions .",baseline,Baselines,1,239,9,9,0,baseline : Baselines,0.7331288343558282,0.75,0.75,MFN performs multi view learning by using Delta memory Attention Network a fusion mechanism to learn cross view interactions ,20,"Unlike c LSTM , contextual utterances are not considered .","Similar to TFN , the modeling is performed within utterances .",result
part-of-speech_tagging,4,"In contrast , though explicitly capturing output label dependencies , CRF can be limited by its Markov assumptions , particularly when being used on top of neural encoders .",introduction,introduction,0,16,8,8,0,introduction : introduction,0.06866952789699571,0.2352941176470588,0.2352941176470588,In contrast though explicitly capturing output label dependencies CRF can be limited by its Markov assumptions particularly when being used on top of neural encoders ,26,"One possible reason is that the strong representation power of neural sentence encoders such as BiLSTMs allow models to capture implicit long - range label dependencies from input word sequences alone , thereby allowing the output layer to make local predictions .","In addition , CRF can be computationally expensive when the number of labels is large , due to the use of Viterbi decoding .",introduction
text_summarization,8,"Length penalty parameter ? and copy mask differ across models , with ? ranging from 0.6 to 1.4 , and ranging from 0.1 to 0.2 .",experiment,Data and Experiments,1,187,26,26,0,experiment : Data and Experiments,0.6538461538461539,0.7878787878787878,0.7878787878787878,Length penalty parameter and copy mask differ across models with ranging from 0 6 to 1 4 and ranging from 0 1 to 0 2 ,26,All inference parameters are tuned on a 200 example subset of the validation set .,The minimum length of the generated summary is set to 35 for CNN - DM and 6 for NYT .,experiment
relation-classification,9,Obtaining large - scale annotated data for NLP tasks in the scientific domain is challenging and expensive .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.027210884353741496,0.2857142857142857,0.2857142857142857,Obtaining large scale annotated data for NLP tasks in the scientific domain is challenging and expensive ,17, ,"We release SCIBERT , a pretrained language model based on BERT ( Devlin et al. , 2019 ) to address the lack of high - quality , large - scale labeled scientific data .",abstract
sentiment_analysis,25,"Second , the computations of our model could be easily parallelized during training , because convolutional layers do not have time dependency as in LSTM layers , and gating units also work independently .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.045045045045045036,0.8,0.8,Second the computations of our model could be easily parallelized during training because convolutional layers do not have time dependency as in LSTM layers and gating units also work independently ,31,The architecture is much simpler than attention layer used in the existing models .,The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models .,abstract
natural_language_inference,67,"For instance , "" who "" questions mostly have proper nouns / persons as answers and "" when "" questions may frequently have numbers / dates ( e.g. , a year ) as answers .",system description,Chunk Representation,0,133,93,31,0,system description : Chunk Representation,0.6519607843137255,0.9893617021276596,0.96875,For instance who questions mostly have proper nouns persons as answers and when questions may frequently have numbers dates e g a year as answers ,26,"Note that some types of questions ( e.g. , "" who "" , "" when "" questions ) have answers that have a specific POS / NE tag pattern .","Thus , we believe that the model could exploit the co-relation between question types and answer POS / NE patterns easier with POS and NE tag features .",method
relation_extraction,4,1 score keeps increasing .,analysis,analysis,1,172,9,9,0,analysis : analysis,0.8309178743961353,0.42857142857142855,0.42857142857142855,1 score keeps increasing ,5,"We find that : ( 1 ) At hop - 0 level , precision increases as we provide more negative examples , while recall stays almost unchanged .","2 ) At hop - all level , F 1 score increases by Performance by sentence length .",result
sentiment_analysis,47,"Subsequently , the context2target attention is used to capture the most important word in the target .",introduction,introduction,1,42,28,28,0,introduction : introduction,0.19534883720930232,0.7179487179487181,0.7179487179487181,Subsequently the context2target attention is used to capture the most important word in the target ,16,The target2context attention is used to capture the most indicative sentiment words in left / right contexts .,This leads to a two - side representation of the target : left - aware target and right - aware target .,introduction
relation_extraction,8,Word embeddings are distributed representations of words that map each word in a text to a ' k'dimensional real - valued vector .,methodology,Word Embeddings,0,99,15,2,0,methodology : Word Embeddings,0.3680297397769517,0.14563106796116504,0.2222222222222222,Word embeddings are distributed representations of words that map each word in a text to a k dimensional real valued vector ,22, ,"They have recently been shown to capture both semantic and syntactic information about words very well , setting performance records in several word similarity tasks .",method
sentiment_analysis,1,"Introducing excessive noise in Emotion DL causes performance drop , which is expected because excessive noise weakens the true learning signals .",analysis,Sensitivity Analysis,0,371,12,12,0,analysis : Sensitivity Analysis,0.9368686868686869,0.5,1.0,Introducing excessive noise in Emotion DL causes performance drop which is expected because excessive noise weakens the true learning signals ,21,"In particular , our model usually performs best when is set to 0.2 , demonstrating the existence of label noises and the necessity of addressing them on both datasets .", ,result
relation_extraction,11,"For alleviating noise in distant supervised datasets , attention has been utilized by .",introduction,introduction,0,24,13,13,0,introduction : introduction,0.0967741935483871,0.38235294117647056,0.38235294117647056,For alleviating noise in distant supervised datasets attention has been utilized by ,13,"Recently , neural models have demonstrated promising performance on RE . employ Convolutional Neural Networks ( CNN ) to learn representations of instances .",Syntactic information from dependency parses has been used by for capturing long - range dependencies between tokens .,introduction
semantic_role_labeling,3,combined reinforcement learning and self - attention to capture the long distance dependencies nature of abstractive summarization .,model,Labeling Confusion,0,256,63,21,0,model : Labeling Confusion,0.9696969696969696,0.9402985074626866,0.84,combined reinforcement learning and self attention to capture the long distance dependencies nature of abstractive summarization ,17,"proposed self - attentive sentence embedding and applied them to author profiling , sentiment analysis and textual entailment .",applied self - attention to neural machine translation and achieved the state - of - the - art results .,method
natural_language_inference,9,"AGRU is a variant of GRU where the update gate is replaced with soft attention , proposed by .",model,PARALLELIZATION,0,136,96,13,0,model : PARALLELIZATION,0.4121212121212121,1.0,1.0,AGRU is a variant of GRU where the update gate is replaced with soft attention proposed by ,18,": The schematics of QRN and the two state - of - the - art models , End - to - End Memory Networks and Improved Dynamic Memory Networks ( DMN + ) , simplified to emphasize the differences among the models .","story - based QA dataset bAb I story - based QA dataset is composed of 20 different tasks ( Appendix A ) , each of which has 1,000 ( 1 k ) synthetically - generated story - question pair .",method
relation_extraction,9,"For example , Cause - Effect ( e 1 , e 2 ) and Cause - Effect ( e 2 ,e 1 ) can be considered two distinct relations , so the total number | Y | of relation types is 19 .",experiment,Experimental Setup,0,144,7,6,0,experiment : Experimental Setup,0.6889952153110048,0.25925925925925924,0.24,For example Cause Effect e 1 e 2 and Cause Effect e 2 e 1 can be considered two distinct relations so the total number Y of relation types is 19 ,32,"However , for each of the aforementioned relation types , the two entities can also appear in inverse order , which implies that the sentence needs to be regarded as expressing a different relation , namely the respective inverse one .","Task 8 dataset consists of a training set of 8,000 examples , and a test set with the remaining examples .",experiment
natural_language_inference,84,"We will refer to the content of auxiliary data as "" definitions "" throughout the paper , regardless of the source .",introduction,introduction,0,24,15,15,0,introduction : introduction,0.10434782608695653,0.5357142857142857,0.5357142857142857,We will refer to the content of auxiliary data as definitions throughout the paper regardless of the source ,19,"Examples of such data could be dictionary definitions , Wikipedia infoboxes , linguistic descriptions of named entities obtained from Wikipedia articles , or something as simple as the spelling of a word .",Several sources of auxiliary data can be used simultaneously as input to a neural network that will compute a combined representation .,introduction
sentiment_analysis,13,MLM is crucial for injecting review domain knowledge and for alleviating the bias of the knowledge from Wikipedia .,system description,Post-training,0,144,68,9,0,system description : Post-training,0.5179856115107914,0.68,0.2195121951219512,MLM is crucial for injecting review domain knowledge and for alleviating the bias of the knowledge from Wikipedia ,19,"training example is formulated as ( [ CLS ] , x 1:j , [ SEP ] , x j+1:n , [ SEP ] ) , where x 1:n is a document ( with randomly masked words ) split into two sides x 1:j and x j+1:n and [ SEP ] separates those two .","For example , in the Wikipedia domain , BERT may learn to guess the [ MASK ] in "" The [ MASK ] is bright "" as "" sun "" .",method
natural_language_inference,0,Such datasets can be easily constructed automatically and the unambiguous nature of their queries provides an objective benchmark to measure a system 's performance at text comprehension .,introduction,introduction,0,15,4,4,0,introduction : introduction,0.07614213197969544,0.25,0.25,Such datasets can be easily constructed automatically and the unambiguous nature of their queries provides an objective benchmark to measure a system s performance at text comprehension ,28,"Towards this end , several large - scale datasets of cloze - style questions over a context document have been introduced recently , which allow the training of supervised machine learning systems .",Deep learning models have been shown to outperform traditional shallow approaches on text comprehension tasks .,introduction
natural_language_inference,3,"In this paper , we propose a deep architecture to model the strong interaction of sentence pair with two coupled - LSTMs .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.02884615384615385,0.5714285714285714,0.5714285714285714,In this paper we propose a deep architecture to model the strong interaction of sentence pair with two coupled LSTMs ,21,"However , most of the existing methods encode two sequences with separate encoders , in which a sentence is encoded with little or no information from the other sentence .","Specifically , we introduce two coupled ways to model the interdependences of two LSTMs , coupling the local contextualized interactions of two sentences .",abstract
natural_language_inference,31,max - over - time pooling operation is used in the pooling layer .,approach,Our Approach,0,57,21,21,0,approach : Our Approach,0.2065217391304348,0.328125,0.9545454545454546,max over time pooling operation is used in the pooling layer ,12,"Recurrent networks are slower and do not lead to further improvements , so they are not adopted here .",The details of augmented residual connections and other layers are introduced as follows .,method
text_generation,2,gives the results .,system description,Turing Test and Generated Samples,0,230,16,8,0,system description : Turing Test and Generated Samples,0.6571428571428571,0.6153846153846154,0.4444444444444444,gives the results ,4,"For the comparison fairness , the sentences used in the questionnaires are randomly sampled .",The performance on two datasets indicates that the generated sentences of Leak GAN are of higher global consistency and better readability than those of SeqGAN .,method
natural_language_inference,69,"Instead , in our formulation , this corresponds to each single sample containing the supervised learning signal from an average of 19.5 and 59.8 unique document paths .",system description,Assembly,0,114,41,32,0,system description : Assembly,0.33043478260869563,0.9318181818181818,0.9142857142857144,Instead in our formulation this corresponds to each single sample containing the supervised learning signal from an average of 19 5 and 59 8 unique document paths ,28,"For such a case , WIKIHOP and MED - HOP would have more than 1 M and 150 K paths to be classified , respectively .",are selected up until a maximum of 64 documents is reached .,method
machine-translation,5,The back - translated data were acquired from two sources :,system,Synthetic Data,0,84,61,7,0,system : Synthetic Data,0.5874125874125874,0.8243243243243243,0.6363636363636364,The back translated data were acquired from two sources ,10,"These synthetic corpora were added to the parallel corpora , thereby almost doubling the sizes of the available training data .",") the constrained system data were acquired from initial Transformer - based NMT systems that were trained on the filtered and preprocessed parallel data , which were supplemented with the unknown phenomena infused data , and 2 ) the unconstrained system data were acquired from pre-existing unconstrained MLSTM - based NMT systems - the NMT systems that were developed by Tilde for the Estonian EU Council Presidency in 2017 .",method
natural_language_inference,62,"Specifically , instead of using all the training examples , we produce several training subsets ( i.e. subsets of the training examples ) so as to study the relationship between the proportion of the available training examples and the performance .",experiment,Model Comparison in the Hunger for Data,0,205,33,3,0,experiment : Model Comparison in the Hunger for Data,0.9151785714285714,0.8684210526315791,0.375,Specifically instead of using all the training examples we produce several training subsets i e subsets of the training examples so as to study the relationship between the proportion of the available training examples and the performance ,38,We compare KAR with other MRC models in the hunger for data .,We produce each training subset by sampling a specific number of questions from all the questions relevant to each passage .,experiment
sentiment_analysis,5,"Until now , for an input EEG sample X t , the output feature S hv t is obtained .",model,The BiHDM model,0,105,41,41,0,model : The BiHDM model,0.3962264150943397,0.5774647887323944,0.6833333333333333,Until now for an input EEG sample X t the output feature S hv t is obtained ,18,"Finally , we use two learnable mapping matrices",) Discriminative prediction and domain adversarial strategy :,method
text_summarization,13,"However , it makes sense intuitively that not every word of the source will be necessary for generating a summary , and so we would like to reduce the amount of computation performed on the source .",introduction,introduction,0,15,8,8,0,introduction : introduction,0.05617977528089888,0.4705882352941176,0.4705882352941176,However it makes sense intuitively that not every word of the source will be necessary for generating a summary and so we would like to reduce the amount of computation performed on the source ,35,"For a problem such as document summarization , a source sequence of length N ( where N could potentially be very large ) requires O( N ) model computations to encode .","Therefore , in order to scale attention models for this problem , we aim to prune down the length of the source sequence in an intelligent way .",introduction
sentiment_analysis,15,"shows two examples of positive negation the RNTN correctly classified , even if negation is less obvious in the case of ' least ' .",experiment,Model Analysis: High Level Negation,0,245,49,8,0,experiment : Model Analysis: High Level Negation,0.9074074074074074,0.7101449275362319,0.3478260869565217,shows two examples of positive negation the RNTN correctly classified even if negation is less obvious in the case of least ,22,"Hence , we compute accuracy in terms of correct sentiment reversal from positive to negative .",Table 2 ( left ) gives the accuracies over 21 positive sentences and their negation for all models .,experiment
relation-classification,1,"Based on this tagging scheme , the joint extraction of entities and relations can be transformed into a tagging problem .",introduction,introduction,0,31,23,23,0,introduction : introduction,0.12601626016260162,0.5609756097560976,0.5609756097560976,Based on this tagging scheme the joint extraction of entities and relations can be transformed into a tagging problem ,20,We design a kind of novel tags which contain the information of entities and the relationships they hold .,"In this way , we can also easily use neural networks to model the task without complicated feature engineering .",introduction
text-classification,7,Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue .,introduction,introduction,0,30,20,20,0,introduction : introduction,0.12345679012345676,0.7142857142857143,0.7142857142857143,Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue ,21,"However , they are unavoidably more restricted to encode rich structures presented in a sequence .",recent method called capsule network introduced by possesses this attractive potential to address the aforementioned issue .,introduction
natural_language_inference,33,These representations are typically used as general purpose features for words across a range of NLP problems .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.019157088122605363,0.3,0.3,These representations are typically used as general purpose features for words across a range of NLP problems ,18,lot of the recent success in natural language processing ( NLP ) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner .,"However , extending this success to learning representations of sequences of words , such as sentences , remains an open problem .",abstract
machine-translation,2,"We apply dropout to the output of each sub - layer , before it is added to the sub - layer input and normalized .",training,Regularization,1,173,23,4,0,training : Regularization,0.7723214285714286,0.8214285714285714,0.4444444444444444,We apply dropout to the output of each sub layer before it is added to the sub layer input and normalized ,22,We employ three types of regularization during training :,"In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .",experiment
text_summarization,13,"We attribute this to the fact that our models usually produce a single sentence as the summary , while the ILP system can produce multiple .",result,result,0,205,3,3,0,result : result,0.7677902621722846,0.1,0.1,We attribute this to the fact that our models usually produce a single sentence as the summary while the ILP system can produce multiple ,25,The ILP model ROUGE scores are surprisingly low .,ILP therefore has comparatively high ROUGE recall while suffering in precision .,result
text_summarization,3,Bold scores are the best between the two optimization strategies .,analysis,Quantitative Analysis,0,196,10,8,0,analysis : Quantitative Analysis,0.8099173553719008,0.9090909090909092,0.8888888888888888,Bold scores are the best between the two optimization strategies ,11,Underlined scores are the best without additional optimization .,mark indicates the improvements from the baselines to the concept pointer are statistically significant using a two - tailed t- test ( p < 0.01 ) .,result
relation_extraction,8,"In relation extraction , we focus on assigning labels to entity pairs .",methodology,Position Embeddings,0,108,24,2,0,methodology : Position Embeddings,0.4014869888475837,0.2330097087378641,0.3333333333333333,In relation extraction we focus on assigning labels to entity pairs ,12, ,"Similar to , we use PFs to specify entity pairs .",method
natural_language_inference,74,These datasets are either not large enough to support complex deep neural network models or too easy to challenge natural language .,system description,Natural Language Inference,0,211,13,3,0,system description : Natural Language Inference,0.9419642857142856,0.65,0.3,These datasets are either not large enough to support complex deep neural network models or too easy to challenge natural language ,22,"Earlier research on the natural language inference was based on small - scale datasets , which relied on traditional methods such as shallow methods , natural logic methods ( MacCartney and , etc .",Large and complicated networks have been successful in many natural language processing tasks .,method
sarcasm_detection,0,"This subsample contains 17 thousand sequences , with the average proportion of sarcastic responses being 23.2 % .",evaluation,Politics,0,140,19,5,0,evaluation : Politics,0.7954545454545454,1.0,1.0,This subsample contains 17 thousand sequences with the average proportion of sarcastic responses being 23 2 ,17,"Thus we also test human and machine performance on comments drawn solely from the politics subreddit , a topic for which all evaluators had sufficient background information .", ,result
part-of-speech_tagging,5,We tested the system on the 2017 CoNLL shared task data sets and gain improvements compared to the top performing systems for the majority of languages for part - of - speech and morphological tagging .,introduction,introduction,0,28,18,18,0,introduction : introduction,0.13861386138613865,0.9,0.9,We tested the system on the 2017 CoNLL shared task data sets and gain improvements compared to the top performing systems for the majority of languages for part of speech and morphological tagging ,34,"Empirically , we found this optimal as it allowed control over the fact that each representation has a different learning capacity .","As we will see , a pattern emerged where gains were largest for morphologically rich languages , especially those in the Slavic family group .",introduction
text_summarization,11,"To tackle this problem , we propose a model of global encoding for abstractive summarization .",introduction,introduction,1,23,16,16,0,introduction : introduction,0.1597222222222222,0.7272727272727273,0.7272727272727273,To tackle this problem we propose a model of global encoding for abstractive summarization ,15,"Attention - based seq2seq model for abstractive summarization can suffer from repetition and semantic irrelevance , causing grammatical errors and insufficient reflection of the main idea of the source text .",We set a convolutional gated unit to perform global encoding on the source context .,introduction
natural_language_inference,91,We also find that SPINN yields speed increases of up to 25 over a standard TreeRNN implementation .,introduction,introduction,0,29,21,21,0,introduction : introduction,0.12446351931330472,1.0,1.0,We also find that SPINN yields speed increases of up to 25 over a standard TreeRNN implementation ,18,"We evaluate SPINN on the Stanford Natural Language Inference entailment task , and find that it significantly outperforms other sentence - encoding - based models , even with a relatively simple and underpowered implementation of the built - in parser .", ,introduction
text_summarization,5,ROUGE mainly evaluates informativeness .,evaluation,Evaluation Metrics,0,121,8,8,0,evaluation : Evaluation Metrics,0.4801587301587302,0.3333333333333333,0.3333333333333333,ROUGE mainly evaluates informativeness ,5,"where "" RG "" stands for ROUGE for short .",We also introduce a series of metrics to measure the summary quality from the following aspects :,result
text_summarization,12,"We further test the SEASS model with different sentence lengths on English Gigaword test sets , which are merged from the test set and our internal test set .",Effectiveness of Selective Encoding,Effectiveness of Selective Encoding,0,208,2,2,0,Effectiveness of Selective Encoding : Effectiveness of Selective Encoding,0.912280701754386,0.1176470588235294,0.1176470588235294,We further test the SEASS model with different sentence lengths on English Gigaword test sets which are merged from the test set and our internal test set ,28, ,The length of sentences in the test sets ranges from 10 to 80 .,others
natural_language_inference,92,We include full hyperparameters and detailed ablations in Appendix B.,experiment,Multi-mention Reading Comprehension,0,173,22,19,0,experiment : Multi-mention Reading Comprehension,0.6006944444444444,0.7857142857142857,0.76,We include full hyperparameters and detailed ablations in Appendix B ,11,We observe that the performance is improved by annealing while not being overly sensitive to the hyperparameter ? .,"compares the results of baselines , our method and the state - of - the - art on four datasets .",experiment
natural_language_inference,28,We use a batch size 128 .,experiment,ASSOCIATIVE RECALL TASK,1,176,37,12,0,experiment : ASSOCIATIVE RECALL TASK,0.6494464944649446,0.7254901960784313,0.8,We use a batch size 128 ,7,All the models have the same hidden state N h = 50 for different lengths T .,The optimizer is RMSProp with a learning rate 0.001 .,experiment
sentiment_analysis,35,"For example , in the aspect term "" techs at HP "" , the sentiment is usually expressed over the headword "" techs "" but seldom over modifiers like the brand name "" HP "" .",system description,Context2Aspect (C2A) Attention,0,95,36,3,0,system description : Context2Aspect (C2A) Attention,0.38306451612903225,0.3673469387755102,0.04615384615384616,For example in the aspect term techs at HP the sentiment is usually expressed over the headword techs but seldom over modifiers like the brand name HP ,28,Not all aspect words contribute equally to the semantic of the aspect .,"Thus , "" techs "" is more important than "" at "" and "" HP "" .",method
semantic_role_labeling,3,Dropout layers are added before residual connections with a keep probability of 0.8 .,model,Settings and Regularization,1,168,14,7,0,model : Settings and Regularization,0.6363636363636364,0.5833333333333334,0.4117647058823529,Dropout layers are added before residual connections with a keep probability of 0 8 ,15,We apply dropout to prevent the networks from over-fitting .,"Dropout is also applied before the attention softmax layer and the feed - froward ReLU hidden layer , and the keep probabilities are set to 0.9 .",method
natural_language_inference,99,"In order to comprehensively model the mutual information between the question and passage , we adopt a heuristic combining strategy to yield the extension as follows :",model,Contextual Encoding,0,127,50,41,0,model : Contextual Encoding,0.503968253968254,0.5208333333333334,0.7592592592592593,In order to comprehensively model the mutual information between the question and passage we adopt a heuristic combining strategy to yield the extension as follows ,26,We further utilize Q to form the question - aware passage representation .,"where Pt i ? R 2 d denotes the ith question - aware passage word under the tth hop , the ? function is a concatenation function that fuses four input vectors .",method
sentiment_analysis,22,TD - LSTM adopts two LSTMs to model the left context with target and the right context with target respectively ; It takes the hidden states of LSTM at last time - step to represent the sentence for prediction .,method,Hierarchical Attention Based Position-aware Network for Aspect-level Sentiment Analysis,1,141,7,2,0,method : Hierarchical Attention Based Position-aware Network for Aspect-level Sentiment Analysis,0.6157205240174672,0.3684210526315789,1.0,TD LSTM adopts two LSTMs to model the left context with target and the right context with target respectively It takes the hidden states of LSTM at last time step to represent the sentence for prediction ,37, , ,method
text_summarization,6,"The final objective function , which needs to be minimized , is formulated as follows :",system description,Learning,0,165,9,9,0,system description : Learning,0.6297709923664122,1.0,1.0,The final objective function which needs to be minimized is formulated as follows ,14,"Since the variational lower bound L ( ? , ? ; Y ) also contains a likelihood term , we can merge it with the likelihood term of summaries .", ,method
machine-translation,7,APPENDICES A LOAD - BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,APPENDICES A LOAD-BALANCING LOSS,0,223,1,1,0,APPENDICES A LOAD-BALANCING LOSS : APPENDICES A LOAD-BALANCING LOSS,0.5978552278820375,0.006622516556291391,0.05555555555555555,APPENDICES A LOAD BALANCING LOSS,5, , ,others
named-entity-recognition,8,"BERT and OpenAI GPT are singlemodel , single task .",experiment,GLUE,0,167,15,13,0,experiment : GLUE,0.4315245478036176,0.2112676056338028,0.18840579710144928,BERT and OpenAI GPT are singlemodel single task ,9,"The "" Average "" column is slightly different than the official GLUE score , since we exclude the problematic WNLI set .","F1 scores are reported for QQP and MRPC , Spearman correlations are reported for STS - B , and accuracy scores are reported for the other tasks .",experiment
sentence_classification,0,"In addition , to address the limited scope and size of this dataset , we introduce SciCite , a new dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL - ARC .",method,Method,0,108,9,9,0,method : Method,0.4044943820224719,0.5625,0.9,In addition to address the limited scope and size of this dataset we introduce SciCite a new dataset of citation intents that addresses multiple scientific domains and is more than five times larger than ACL ARC ,37,We use the most recent and comprehensive ( ACL - ARC citations dataset ) by as a benchmark dataset to compare the performance of our model to previous work .,Below is a description of both datasets .,method
natural_language_inference,56,See the supplementary material for details .,experiment,bAbI question-answering tasks,0,103,24,14,0,experiment : bAbI question-answering tasks,0.3065476190476191,0.22429906542056074,0.7368421052631579,See the supplementary material for details ,7,We find that using dropout and appending the question encoding to the fact encodings is important for the performance .,"Surprisingly , we find that we only need a single step of relational reasoning to solve all the bAbI tasks .",experiment
negation_scope_resolution,0,"We can observe that ' not ' is the negation word ( known as the negation cue ) and the words whose meaning is altered by ' not ' are ' a ' and ' negation ' , which belong to what is known as the cue 's scope .",introduction,introduction,0,21,10,10,0,introduction : introduction,0.09130434782608696,0.3333333333333333,0.3333333333333333,We can observe that not is the negation word known as the negation cue and the words whose meaning is altered by not are a and negation which belong to what is known as the cue s scope ,39,This is not [ a negation ] .,"Negation detection involves finding these negation cues , and scope resolution for each cue necessitates finding the words affected negatively by that cue ( finding its scope ) .",introduction
machine-translation,3,that the model ensemble can improve the performance further to 38.9 .,method,Post processing,0,261,10,7,0,method : Post processing,0.8338658146964856,0.5,0.4117647058823529,that the model ensemble can improve the performance further to 38 9 ,13,It is shown in Tab .,"In and there are eight models for the best scores , but we only use three models and we do not obtain further gain from more models . : BLEU scores of different models .",method
relation-classification,2,"Our model outperforms the previous neural models that use automatically extracted features , while it performs within a reasonable margin of feature - based neural models , or even beats them .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.03389830508474576,1.0,1.0,Our model outperforms the previous neural models that use automatically extracted features while it performs within a reasonable margin of feature based neural models or even beats them ,29,"We present an extensive experimental setup , to demonstrate the effectiveness of our method using datasets from various contexts ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .", ,abstract
topic_models,0,We present the comparison of 20 Newsgroups test data perplexities obtained using Bayesian SMM and NVDM in .,result,result,0,329,20,20,0,result : result,0.7985436893203883,0.4166666666666667,0.4166666666666667,We present the comparison of 20 Newsgroups test data perplexities obtained using Bayesian SMM and NVDM in ,18,"In , the authors used 20 samples .",It shows the perplexities of 20 Newsgroups corpus under full and a limited vocabulary of 2000 words .,result
relation-classification,2,We use dropout to regularize our network .,implementation,Hyperparameters and implementation details,1,211,5,5,0,implementation : Hyperparameters and implementation details,0.7152542372881356,0.35714285714285715,0.35714285714285715,We use dropout to regularize our network ,8,We fix the size of the LSTM to d = 64 and the layer width of the neural network to l = 64 ( both for the entity and the relation scoring layers ) .,Dropout is applied in the input embeddings and in between the hidden layers for both tasks .,experiment
natural_language_inference,11,"ConceptNet 3 ( Speer and Havasi , 2012 ) , a freelyavailable , multi-lingual semantic network that originated from the Open Mind Common Sense project and incorporates selected knowledge from various other knowledge sources , such as Wiktionary , Open Multilingual WordNet , OpenCyc and DBpedia .",model,Supplementary Knowledge Sources We use,0,111,17,3,1,model : Supplementary Knowledge Sources We use,0.4021739130434783,0.4473684210526316,0.125,ConceptNet 3 Speer and Havasi 2012 a freelyavailable multi lingual semantic network that originated from the Open Mind Common Sense project and incorporates selected knowledge from various other knowledge sources such as Wiktionary Open Multilingual WordNet OpenCyc and DBpedia ,40,"The results demonstrate that the introduction of the refinement module helps consistently , and further improvements come from using commonsense knowledge from Concept - 6 Statistics were extracted from the DBpedia Anchor Text dataset ( http://downloads.dbpedia.org/ 2016-10/core-i18n/en/anchor_text_en.ttl. bz2 ) .",It presents information in the form of relational triples .,method
natural_language_inference,23,"They suggested a range of new organizational options , from tightening the selection of lead authors and contributors , to dumping it in favor of a small permanent body , or even turning the whole climate science assessment process into a moderated "" living "" Wikipedia - IPCC .",model,FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0,476,83,60,0,model : FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0.9278752436647172,0.6916666666666667,0.7058823529411765,They suggested a range of new organizational options from tightening the selection of lead authors and contributors to dumping it in favor of a small permanent body or even turning the whole climate science assessment process into a moderated living Wikipedia IPCC ,43,"Context : In February 2010 , in response to controversies regarding claims in the Fourth Assessment Report , five climate scientists all contributing or lead IPCC report authors wrote in the journal Nature calling for changes to the IPCC .",Other recommendations included that the panel employ a full - time staff and remove government oversight from its processes to avoid political interference .,method
question_answering,2,Accuracy 0.219 - MemN2N 0.342 - DEMN - 0.300 Soft Attention 0.321 - MCB 0.362 - TGIF Temporal 0.371 - RWMN 0.387 0.363 FVTA 0.410 0.373 clips from the same movie and the corresponding subtitles .,ablation,Ablations,0,254,3,2,0,ablation : Ablations,0.9169675090252708,0.75,0.6666666666666666,Accuracy 0 219 MemN2N 0 342 DEMN 0 300 Soft Attention 0 321 MCB 0 362 TGIF Temporal 0 371 RWMN 0 387 0 363 FVTA 0 410 0 373 clips from the same movie and the corresponding subtitles ,40, ,More details of the dataset can be viewed in .,result
natural_language_inference,79,"The first approach , weighted averaging of word vectors , loses the word order in the same way as the standard bag - of - words models do .",introduction,introduction,0,39,27,27,0,introduction : introduction,0.1455223880597015,0.7941176470588235,0.7941176470588235,The first approach weighted averaging of word vectors loses the word order in the same way as the standard bag of words models do ,25,Both approaches have weaknesses .,"The second approach , using a parse tree to combine word vectors , has been shown to work for only sentences because it relies on parsing .",introduction
text-classification,1,"We used four datasets : IMDB , Elec , RCV1 ( second - level topics ) , and 20 - newsgroup ( 20 NG ) 3 , to facilitate direct comparison with JZ15 and DL15 .",system description,Experiments (supervised),0,125,79,2,0,system description : Experiments (supervised),0.48828125,0.395,0.03333333333333333,We used four datasets IMDB Elec RCV1 second level topics and 20 newsgroup 20 NG 3 to facilitate direct comparison with JZ15 and DL15 ,25, ,The first three were used in JZ15 .,method
text_summarization,5,Code and results can be found at http://www4.comp.polyu.edu.hk/cszqcao/,introduction,introduction,1,43,34,34,0,introduction : introduction,0.17063492063492064,0.9444444444444444,0.9444444444444444,Code and results can be found at http www4 comp polyu edu hk cszqcao ,15,We propose to introduce soft templates as additional input to improve the readability and stability of seq2seq summarization systems .,We extend the seq2seq framework to conduct template reranking and template - aware summary generation simultaneously .,introduction
natural_language_inference,44,Existing QA models focus on learning the context over different parts in the full document .,system description,Task analyses,0,30,17,17,0,system description : Task analyses,0.1048951048951049,0.4047619047619048,0.8947368421052632,Existing QA models focus on learning the context over different parts in the full document ,16,Illuminated manuscripts in the library dating from author are collected in the library ? Task analyses,"Although effective , learning the context within the full document is challenging and inefficient .",method
named-entity-recognition,3,The language model component ( in orange ) is used to augment the input token representation in a traditional sequence tagging models ( in grey ) .,model,Baseline sequence tagging model,0,42,16,16,0,model : Baseline sequence tagging model,0.22702702702702704,0.372093023255814,0.372093023255814,The language model component in orange is used to augment the input token representation in a traditional sequence tagging models in grey ,23,"The main components in TagLM , our language - model - augmented sequence tagging system .","The second RNN layer is similar and uses h k , 1 to output h k ,2 .",method
query_wellformedness,0,"Here , we introduce a new task of identifying a well - formed natural language question .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.07291666666666667,0.7142857142857143,0.7142857142857143,Here we introduce a new task of identifying a well formed natural language question ,15,"Hence , identifying whether or not a query is well formed can enhance query understanding .","We construct and release a dataset of 25,100 publicly available questions classified into well - formed and non-wellformed categories and report an accuracy of 70.7 % on the test set .",abstract
natural_language_inference,68,"Because the length of an answer is not fixed , in order to stop generating answer tokens at certain point , we allow each a k to take up an integer value between 1 and P + 1 , where P + 1 is a special value indicating the end of the answer .",method,Answer Pointer Layer,0,144,94,42,0,method : Answer Pointer Layer,0.5783132530120482,0.8245614035087719,0.6774193548387096,Because the length of an answer is not fixed in order to stop generating answer tokens at certain point we allow each a k to take up an integer value between 1 and P 1 where P 1 is a special value indicating the end of the answer ,49,The Ans - Ptr layer models the generation of these integers in a sequential manner .,"Once a k is set to be P + 1 , the generation of the answer stops .",method
text_summarization,0,We set the k = 5 in the Equation 17 and ? = 0.5 in Equation 23 and 24 .,implementation,implementation,0,233,4,4,0,implementation : implementation,0.7740863787375415,0.6666666666666666,0.6666666666666666,We set the k 5 in the Equation 17 and 0 5 in Equation 23 and 24 ,18,The word embedding dimension is set to 256 and the number of hidden units is 512 .,We use Adagrad optimizer as our optimizing algorithm .,experiment
text_summarization,4,"Moreover , they hold commonsense information once they are linked to a knowledge base .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.02325581395348837,0.4444444444444444,0.4444444444444444,Moreover they hold commonsense information once they are linked to a knowledge base ,14,These entities buildup the topic of the summary .,"Based on these observations , this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries .",abstract
natural_language_inference,97,"Partly because of its limited size , prior work on MCTest has focused mainly on engineering better features .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.023972602739726026,0.5555555555555556,0.5555555555555556,Partly because of its limited size prior work on MCTest has focused mainly on engineering better features ,18,"In this work , we investigate machine comprehension on the challenging MCTest benchmark .","We tackle the dataset with a neural approach , harnessing simple neural networks arranged in a parallel hierarchy .",abstract
sentiment_analysis,34,"Recently , the world witnessed a strong revolution in deep learning which was the driving force for many improvements in many fields .",introduction,introduction,0,21,13,13,0,introduction : introduction,0.14189189189189189,0.65,0.65,Recently the world witnessed a strong revolution in deep learning which was the driving force for many improvements in many fields ,22,"However , most of the work is focused on English whereas Arabic did not receive much attention until recently , but it still lacks behind due to the many challenges of the Arabic language ; including the large variety in dialects and the complex morphology of the language .","The work on English NLP started utilising deep learning models from an early stage , then followed by Arabic NLP .",introduction
relation-classification,1,"Hence , LSTM decoding manner is a little better than CRF .",experiment,Experimental Results,0,194,15,15,0,experiment : Experimental Results,0.7886178861788617,0.8333333333333334,0.8333333333333334,Hence LSTM decoding manner is a little better than CRF ,11,The related tags may have along distance from each other .,"Hence , LSTM decoding manner is a little better than CRF .",experiment
sarcasm_detection,1,We can infer from this observation that the user embeddings belonging to this non-overlapping red-region provide discriminative information regarding the sarcastic tendencies of their users .,analysis,User Embedding Analysis,0,308,11,11,0,analysis : User Embedding Analysis,0.9221556886227544,0.34375,1.0,We can infer from this observation that the user embeddings belonging to this non overlapping red region provide discriminative information regarding the sarcastic tendencies of their users ,28,This is also evident from the variances of the distributions where the sarcastic distribution comprises of 10.92 variance as opposed to 5.20 variance of the non-sarcastic distribution ., ,result
natural_language_inference,69,"shows an example from WIKIPEDIA , where the goal is to identify the country property of the Hanging Gardens of Mumbai .",introduction,introduction,0,23,11,11,0,introduction : introduction,0.06666666666666668,0.3928571428571429,0.3928571428571429,shows an example from WIKIPEDIA where the goal is to identify the country property of the Hanging Gardens of Mumbai ,21,They would also benefit search and Question Answering ( QA ) applications where the required information can not be found in a single location .,"This can not be inferred solely from the article about them without additional background knowledge , as the answer is not stated explicitly .",introduction
sentiment_analysis,34,Our online tool provides four modes of operation as follows :,performance,Mazajak Online API,0,126,5,4,0,performance : Mazajak Online API,0.8513513513513513,0.2777777777777778,0.2352941176470588,Our online tool provides four modes of operation as follows ,11,The final model hosted online is trained on the SemEval and ASTD dataset combined 4 .,"Text Input : where the user can input any piece of text into a text - box , and the system will display the polarity of the sentiment in the text .",result
natural_language_inference,74,"They also apply their pre-trained sentence encoder to a series of natural language understanding tasks such as sentiment analysis , question - type , entailment , and relatedness .",system description,Discourse Marker Applications,0,202,4,4,0,system description : Discourse Marker Applications,0.9017857142857144,0.2,0.4,They also apply their pre trained sentence encoder to a series of natural language understanding tasks such as sentiment analysis question type entailment and relatedness ,26,They follow to collect a large sentence pairs corpus from Book - Corpus and propose a sentence representation based on that .,"However , all those datasets are provided by for evaluating sentence embeddings and are almost all small - scale and are notable to support more complex neural network .",method
sentiment_analysis,1,"Similar to , presents the subject - independent classification results .",system description,Subject-Independent Classification,0,302,5,2,0,system description : Subject-Independent Classification,0.7626262626262627,0.38461538461538464,0.2,Similar to presents the subject independent classification results ,9, ,"When using features from all frequency bands , our model performs marginally worse than BiHDM on SEED but much better than BiHDM on SEED - IV ( nearly 5 % improvement ) .",method
machine-translation,1,The paper is organized as follows .,introduction,introduction,0,46,34,34,0,introduction : introduction,0.22885572139303484,0.8947368421052632,0.8947368421052632,The paper is organized as follows ,7,"These results show that deep CNNs are simple , scalable and effective architectures for challenging linguistic processing tasks .",Section 2 lays out the background and some desiderata for neural architectures underlying translation models .,introduction
natural_language_inference,34,the answer type .,system description,Prediction,0,192,89,4,0,system description : Prediction,0.6508474576271186,0.8811881188118812,0.25,the answer type ,4,"The framework has four output dimensions , including 1 . supporting sentences , 2 . the start position of the answer , 3 . the end position of the answer , and 4 .","We use a cascade structure to solve the output dependency , where four isomorphic LSTMs F i are stacked layer by layer .",method
text_generation,3,Recent studies have applied the attention mechanism to dialogue generation to improve the dialogue coherence .,introduction,introduction,0,19,9,9,0,introduction : introduction,0.1347517730496454,0.4090909090909091,0.4090909090909091,Recent studies have applied the attention mechanism to dialogue generation to improve the dialogue coherence ,16,"Recently , the neural attention mechanism has been proved successful in many tasks including neural machine translation and abstractive summarization , for its ability of capturing word - level dependency by associating a generated word with relevant words in the source - side context .","However , conversation generation is a much more complex and flexible task as there are less "" word - to - words "" relations between inputs and responses .",introduction
natural_language_inference,5,"Wiki QA : For the WikiQA dataset , the pointwise learning approach shows a better performance than the listwise learning approach .",method,method,1,127,2,2,0,method : method,0.8639455782312925,0.1111111111111111,0.1111111111111111,Wiki QA For the WikiQA dataset the pointwise learning approach shows a better performance than the listwise learning approach ,20, ,We combine LM with the base model ( Comp - Clip + LM ) and observe a significant improvement in performance in terms of MAP ( 0.714 to 0.746 absolute ) .,method
sentiment_analysis,33,We report accuracy results for both binary classification and fine - grained classification settings .,experiment,Experimental settings,0,71,7,4,0,experiment : Experimental settings,0.568,0.7777777777777778,0.6666666666666666,We report accuracy results for both binary classification and fine grained classification settings ,14,We use the Stanford Sentiment Treebank in our sentence classification experiments .,"Two answer sentence selection datasets , QASent and Wik - iQA , are adopted in our sentence matching experiments .",experiment
sentiment_analysis,15,"The error as a function of the RNTN parameters ? = ( V , W , W s , L ) for a sentence is :",model,Tensor Backprop through Structure,0,174,85,11,0,model : Tensor Backprop through Structure,0.6444444444444445,0.794392523364486,0.3333333333333333,The error as a function of the RNTN parameters V W W s L for a sentence is ,19,This is equivalent ( up to a constant ) to minimizing the KL - divergence between the two distributions .,The derivative for the weights of the softmax classifier are standard and simply sum up from each node 's error .,method
natural_language_inference,11,"Nevertheless , both ESIM and our BiL - STM models when trained with knowledge from ConceptNet are sensitive to the semantics of the provided assertions as demonstrated in our analysis in 5.3 .",model,Model,1,157,22,22,0,model : Model,0.5688405797101449,0.8461538461538461,0.8461538461538461,Nevertheless both ESIM and our BiL STM models when trained with knowledge from ConceptNet are sensitive to the semantics of the provided assertions as demonstrated in our analysis in 5 3 ,32,"These results suggest that ESIM is able to learn important background information from the large - scale datasets and from pretrained embeddings , but this can be supplemented when necessary .",We argue that this is a desirable side effect because it makes the predictions of our model more interpretable than those not trained with knowledge .,method
text-classification,8,Friendly staff and nice selection of vegetarian options .,model,model,0,179,49,49,0,model : model,0.6654275092936803,0.6805555555555556,0.6805555555555556,Friendly staff and nice selection of vegetarian options ,9,"However , according to the case study above , the most important features for sentiment prediction maybe some key n-gram phrase / words from Negative :","Food is just okay , not great .",method
machine-translation,1,HM- LSTM 1.40 Layer,model,model,0,148,4,4,0,model : model,0.7363184079601991,0.08163265306122447,0.08163265306122447,HM LSTM 1 40 Layer,5,GF - LSTM 1.58 Grid- LSTM 1.47 Layer - normalized LSTM 1.46 MI- LSTM 1.44 Recurrent Memory Array Structures,Norm HyperLSTM 1.38 Large Layer,method
natural_language_inference,76,The attention weight for the ith word in the premise is the result of a weighted combination ( parameterized by w ) of values in mi .,method,ATTENTION,0,69,41,16,0,method : ATTENTION,0.4726027397260274,0.7192982456140351,0.8421052631578947,The attention weight for the ith word in the premise is the result of a weighted combination parameterized by w of values in mi ,25,"Hence , the intermediate attention representation mi ( ith column vector in M ) of the ith word in the premise is obtained from a non-linear combination of the premise 's output vector hi ( ith column vector in Y ) and the transformed h N .",The final sentence - pair representation is obtained from a non-linear combination of the attentionweighted representation r of the premise and the last output vector h N using,method
natural_language_inference,66,"We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ? 1 set to 0.9 and ? 2 set to 0.999 for optimization .",experiment,Experiment Settings,1,144,12,11,1,experiment : Experiment Settings,0.5161290322580645,0.4137931034482759,0.5789473684210527,We use the Adam method Kingma and Ba 2014 with hyperparameters 1 set to 0 9 and 2 set to 0 999 for optimization ,25,"We use the Adam method ( Kingma and Ba , 2014 ) with hyperparameters ? 1 set to 0.9 and ? 2 set to 0.999 for optimization .",The initial learning rate is set to be 0.001 with a decay ratio of 0.95 for each iteration .,experiment
natural_language_inference,60,"We train embeddings on three kinds of data : Wikipedia , the Toronto Books Corpus and the English OpenSubtitles 4 .",analysis,Multi-domain Embeddings,0,156,20,4,0,analysis : Multi-domain Embeddings,0.8082901554404145,0.9523809523809524,0.8,We train embeddings on three kinds of data Wikipedia the Toronto Books Corpus and the English OpenSubtitles 4 ,19,This allows us to inspect the applicability of source domain data for a specific genre .,We examine the atten -,result
machine-translation,2,"For the base model , we use a rate of P drop = 0.1 .",training,Regularization,0,175,25,6,0,training : Regularization,0.78125,0.8928571428571429,0.6666666666666666,For the base model we use a rate of P drop 0 1 ,14,"In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .","During training , we employed label smoothing of value ls = 0.1 .",experiment
text_summarization,10,"To this end , we allow different components of model parameters of related tasks to be shared vs. unshared , as described next .",model,Layer-Specific Sharing Mechanism,0,98,53,4,0,model : Layer-Specific Sharing Mechanism,0.3726235741444867,0.5698924731182796,0.3333333333333333,To this end we allow different components of model parameters of related tasks to be shared vs unshared as described next ,22,"Therefore , related tasks should share some common representations ( e.g. , high - level information ) , as well as need their own individual task - specific representations ( esp. low - level information ) .","Encoder Layer Sharing : observed that lower layers ( i.e. , the layers closer to the input words ) of RNN cells in a seq2seq machine translation model learn to represent word structure , while higher layers ( farther from input ) are more focused on high - level semantic meanings ( similar to findings in the computer vision community for image features ( Zeiler and Fergus , 2014 ) ) .",method
natural_language_inference,15,"More specifically , if the output of the final RNN layer is a 100d vector for a sentence with 30 words , a 30 100 matrix is obtained which is max - pooled column - wise such that the size of the resultant vector p or q is 100 .",method,Interaction and Prediction Layer,0,101,43,3,0,method : Interaction and Prediction Layer,0.4469026548672566,0.86,0.3,More specifically if the output of the final RNN layer is a 100d vector for a sentence with 30 words a 30 100 matrix is obtained which is max pooled column wise such that the size of the resultant vector p or q is 100 ,46,"To extract a proper representation for each sentence , we apply the step - wise max - pooling operation over densely connected recurrent and co-attentive features ( pooling in ) .","Then , we aggregate these representations p and q for the two sentences P and Q in various ways in the interaction layer and the final feature vector v for semantic sentence matching is obtained as follows :",method
question_answering,0,"To produce a graph - level output vector , we take the hidden vector for the q -node at the last time step t = T and transform it with a fully - connected layer and the ReLU non-linearity : v g = ReLU Wh",system description,Graph-level Output Vector,0,139,90,19,0,system description : Graph-level Output Vector,0.4711864406779661,1.0,1.0,To produce a graph level output vector we take the hidden vector for the q node at the last time step t T and transform it with a fully connected layer and the ReLU non linearity v g ReLU Wh,40,We unroll the recurrence for T = 5 steps in the experiments ., ,method
sentiment_analysis,13,Line 6 accumulates the gradients produced by backpropagation from the partial joint loss .,system description,Post-training,0,171,95,36,0,system description : Post-training,0.6151079136690647,0.95,0.8780487804878049,Line 6 accumulates the gradients produced by backpropagation from the partial joint loss ,14,"Note that the summation of two sub - batches ' losses is divided by u , which compensate the scale change introduced by gradient accumulation inline","To this end , accumulating the gradients u times is equivalent to computing the gradients on the whole batch once .",method
natural_language_inference,8,"While this could be approached as a multi - labelling task , we simply treat each data point as a triple ( q i , a ij , y ij ) .",model,model,0,94,5,5,0,model : model,0.4454976303317536,0.2272727272727273,0.2272727272727273,While this could be approached as a multi labelling task we simply treat each data point as a triple q i a ij y ij ,26,"Assume a set of questions Q , where each question q i ? Q is associated with a list of answer sentences {a i 1 , a i 2 , , aim } , together with their judgements {y i 1 , y i 2 , , y im } , where y ij = 1 if the answer is correct and y ij = 0 otherwise .","Thus , our task is to learn a classifier over these triples so that it can predict the judgements of any additional QA pairs .",method
text_summarization,9,: sars toll on toronto economy estimated at c$ # billion A + : think tank under control in canada 's largest city R : think tank says economic growth in toronto will suffer this year I ( 3 ) : colin l. powell said nothing - a silence that spoke volumes to many in the white house on thursday morning .,result,Results,0,140,35,35,0,result : Results,0.915032679738562,0.813953488372093,0.813953488372093, sars toll on toronto economy estimated at c billion A think tank under control in canada s largest city R think tank says economic growth in toronto will suffer this year I 3 colin l powell said nothing a silence that spoke volumes to many in the white house on thursday morning ,54,": football : pepe out for season A + : ferreira out for rest of season with knee injury R : brazilian defender pepe out for rest of season with knee injury I ( 2 ) : economic growth in toronto will suffer this year because of sars , a think tank said friday as health authorities insisted the illness was under control in canada 's largest city .",": in meeting with former officials bush defends iraq policy A + : colin powell speaks volumes about silence in white house R : powell speaks volumes on the white house I ( 4 ) : an international terror suspect who had been under a controversial loose form of house arrest is on the run , british home secretary john reid said tuesday .",result
natural_language_inference,4,proposed stacked self - attention modules to facilitate signal traversal .,system description,DEEP RESIDUAL COATTENTION ENCODER,0,43,18,3,0,system description : DEEP RESIDUAL COATTENTION ENCODER,0.2171717171717172,0.1855670103092784,0.16666666666666666,proposed stacked self attention modules to facilitate signal traversal ,10,"Because it only has a single - layer coattention encoder , the DCN is limited in its ability to compose complex input representations .",They also showed that the network 's ability to model long - range dependencies can be improved by reducing the length of signal paths .,method
sentiment_analysis,6,"LSTM ) is a kind of RNN , an extension of conventional feedforward neural network .",method,Long Short-Term Memory,0,117,59,12,0,method : Long Short-Term Memory,0.4048442906574394,0.8939393939393939,0.6666666666666666,LSTM is a kind of RNN an extension of conventional feedforward neural network ,14,Long Short - Term Memory,"Specifically , LSTM cells are capable of modeling long - range dependencies , which other traditional RNNs fail to do given the vanishing gradient issue .",method
relation_extraction,11,The over all architecture of RE - SIDE is shown in .,system description,RESIDE Overview,0,102,35,7,0,system description : RESIDE Overview,0.4112903225806452,0.2734375,0.35,The over all architecture of RE SIDE is shown in ,11,Each component will be described in detail in the subsequent sections .,Syntactic Sentence Encoding : RESIDE uses a Bi - GRU over the concatenated positional and word embedding for encoding the local context of each token .,method
machine-translation,8,"Furthermore , qualitative analysis reveals that the ( soft - ) alignments found by the model agree well with our intuition .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.027190332326283987,1.0,1.0,Furthermore qualitative analysis reveals that the soft alignments found by the model agree well with our intuition ,18,"With this new approach , we achieve a translation performance comparable to the existing state - of - the - art phrase - based system on the task of English - to - French translation .", ,abstract
natural_language_inference,56,This paper chiefly develops the relational reasoning side of that interface .,introduction,introduction,0,32,19,19,0,introduction : introduction,0.09523809523809523,0.6785714285714286,0.6785714285714286,This paper chiefly develops the relational reasoning side of that interface ,12,"In computer science parlance , the relational reasoning module implements an interface : it operates on a graph of nodes and directed edges , where the nodes are represented by real valued vectors , and is differentiable .",Some of the tasks we evaluate on can be efficiently and perfectly solved by hand - crafted algorithms that operate on the symbolic level .,introduction
natural_language_inference,62,"Machine Reading Comprehension ( MRC ) , as the name suggests , requires a machine to read a passage and answer its relevant questions .",introduction,introduction,0,10,2,2,0,introduction : introduction,0.04464285714285714,0.1111111111111111,0.1111111111111111,Machine Reading Comprehension MRC as the name suggests requires a machine to read a passage and answer its relevant questions ,21, ,"Since the answer to each question is supposed to stem from the corresponding passage , a common MRC solution is to develop a neural - network - based MRC model that predicts an answer span ( i.e. the answer start position and the answer end position ) from the passage of each given passage - question pair .",introduction
natural_language_inference,60,The sentence is from the SNLI validation set .,result,Discussion & Analysis,0,134,21,6,0,result : Discussion & Analysis,0.6943005181347149,0.9130434782608696,0.75,The sentence is from the SNLI validation set ,9,"Visualizing Attention shows the attention weights for a CDME model trained on SNLI , using the aforementioned six embedding sets .",We observe that different embeddings are preferred for different words .,result
natural_language_inference,63,"Similar to the read heads in read operation , a write head determines whereto write by using content - based weighting .",model,Operation Name Vector,0,106,66,11,0,model : Operation Name Vector,0.5888888888888889,0.8684210526315791,0.6875,Similar to the read heads in read operation a write head determines whereto write by using content based weighting ,20,These read weights are applied to memory locations to get the final read vectors as follow :,We adopted dynamic memory allocation as described in to maintain a free list and a usage vector to track the memory freeness .,method
named-entity-recognition,8,"As shown in , in the question answering task , we represent the input question and passage as a single packed sequence , with the question using the A embedding and the passage using the B embedding .",experiment,GLUE,0,186,34,32,0,experiment : GLUE,0.4806201550387597,0.4788732394366197,0.463768115942029,As shown in in the question answering task we represent the input question and passage as a single packed sequence with the question using the A embedding and the passage using the B embedding ,35,"Wikipedia containing the answer , the task is to predict the answer text span in the passage .",We only introduce a start vector S ? R H and an end vector E ? R H during fine - tuning .,experiment
relation-classification,2,"Comparing to , we train our model by modeling all the entities and the relations of the sentence at once .",introduction,introduction,0,37,27,27,0,introduction : introduction,0.12542372881355932,0.84375,0.84375,Comparing to we train our model by modeling all the entities and the relations of the sentence at once ,20,This shows that the model of strongly relies on the features extracted by the dependency parsers and can not generalize well into different contexts where dependency parser features are weak .,This type of inference is beneficial in obtaining information about neighboring entities and relations instead of just examining a pair of entities each time .,introduction
text_generation,5,LCNN - VAE improves over LSTM - LM from 362.7 to 359.1 in NLL and from 42.6 to 41.1 in PPL .,result,Language modeling results,0,201,17,17,0,result : Language modeling results,0.6813559322033899,0.1868131868131868,1.0,LCNN VAE improves over LSTM LM from 362 7 to 359 1 in NLL and from 42 6 to 41 1 in PPL ,24,Similar results for the sentiment data set are shown in ., ,result
natural_language_inference,48,"Finally , the prediction phase uses the information gathered from the repeated attentions through the query and the document to maximize the probability of the correct answer .",system description,Alternating Iterative Attention,0,50,21,7,0,system description : Alternating Iterative Attention,0.2262443438914027,0.3043478260869565,0.875,Finally the prediction phase uses the information gathered from the repeated attentions through the query and the document to maximize the probability of the correct answer ,27,"To accomplish this , we use an iterative process that , at each iteration , alternates attentive memory accesses to the query and the document .",We describe each of the phases in the following sections .,method
natural_language_inference,27,We consider the Stanford Question Answering Dataset ( SQuAD ) for machine reading comprehension .,dataset,DATASET AND EXPERIMENTAL SETTINGS,0,185,3,3,0,dataset : DATASET AND EXPERIMENTAL SETTINGS,0.5473372781065089,0.10344827586206896,0.10344827586206896,We consider the Stanford Question Answering Dataset SQuAD for machine reading comprehension ,13,DATASET AND EXPERIMENTAL SETTINGS,"SQuAD contains 107.7K query - answer pairs , with 87.5 K for training , 10.1 K for validation , and another 10.1 K for testing .",experiment
natural_language_inference,66,From the three plots we can see that the alignment weights generally make sense .,result,Word Alignment,0,213,52,11,0,result : Word Alignment,0.7634408602150538,0.5591397849462365,0.5,From the three plots we can see that the alignment weights generally make sense ,15,This means the weights in the same row in these plots add up to 1 .,"For example , in Example 1 , "" animal "" is strongly aligned with "" dog "" and "" toy "" aligned with "" Frisbee . """,result
natural_language_inference,12,We assume w i ? Rd is a word embedding vector which are initialized using some pre-trained vector embeddings ( and is then fine - tuned end - to - end via the NLI supervision ) .,model,Sentence Encoder,0,38,16,9,0,model : Sentence Encoder,0.4318181818181818,0.4571428571428571,0.5625,We assume w i Rd is a word embedding vector which are initialized using some pre trained vector embeddings and is then fine tuned end to end via the NLI supervision ,32,"Let W = ( w 1 , w 2 , ... , w n ) represent words in the source sentence .",We assume w i ? Rd is a word embedding vector which are initialized using some pre-trained vector embeddings ( and is then fine - tuned end - to - end via the NLI supervision ) .,method
semantic_role_labeling,4,"Words at the beginning and inside of argument spans have the "" B "" and "" I "" tags , and words outside argument spans have the tag "" O . """,introduction,introduction,0,17,9,9,0,introduction : introduction,0.05666666666666665,0.3333333333333333,0.3333333333333333,Words at the beginning and inside of argument spans have the B and I tags and words outside argument spans have the tag O ,25,"Using features induced by neural networks , they predict a BIO tag for each word .","While yielding high accuracies , this approach reconstructs argument spans from the predicted BIO tags instead of directly predicting the spans .",introduction
natural_language_inference,40,Output for the forward subgraph is given by,method,MAGE-GRUs,0,120,43,19,0,method : MAGE-GRUs,0.4332129963898917,0.6615384615384615,0.6551724137931034,Output for the forward subgraph is given by,8,"The output representation at time step t is given by , where ht ? Re de , and denotes concatenation .","Similarly , we can obtain the output of the backward subgraph H b , and concatenate with Hf such that elements of the original sequence lineup .",method
natural_language_inference,48,"To train our model , we used stochastic gradient descent with the ADAM optimizer ( Kingma and Ba , 2014 ) , with an initial learning rate of 0.001 .",training,Training Details,1,118,2,2,1,training : Training Details,0.5339366515837104,0.15384615384615385,0.15384615384615385,To train our model we used stochastic gradient descent with the ADAM optimizer Kingma and Ba 2014 with an initial learning rate of 0 001 ,26, ,"We set the batch size to 32 and we decay the learning rate by 0.8 if the accuracy on the validation set does not increase after a half - epoch , i.e. 2000 batches ( for CBT ) and 5000 batches for ( CNN ) .",experiment
sentence_compression,3,This property makes it quite suitable for the sentence compression task aiming to shorten sentences by removing unnecessary words .,analysis,Evaluator Analysis,0,120,6,6,0,analysis : Evaluator Analysis,0.967741935483871,1.0,1.0,This property makes it quite suitable for the sentence compression task aiming to shorten sentences by removing unnecessary words ,20,"Interestingly , when deleting words such as new or / and huge , the score becomes lower , suggesting that the model may prefer short sentences , with unnecessary parts such as amod being removed .", ,result
sentiment_analysis,47,We first adopt con-text2target attention and then adopt target2 context attention .,performance,The Effect of Rotatory Attention,0,193,41,7,0,performance : The Effect of Rotatory Attention,0.8976744186046511,0.7068965517241379,0.2916666666666667,We first adopt con text2target attention and then adopt target2 context attention ,13,"We continue to remove the target2 context attention mechanism in No - Target - Attention and use the average of hidden states to represent left and right contexts ; 2 . Attention - Reverse is based on LCR - Rot , where we reverse the order of attention .",We have known that context2target attention is rewarding in our model according to previous subsection .,result
text_generation,1,"Accordingly , u is a reference sentence sampled from set U .",architecture,Rank score,0,109,43,20,0,architecture : Rank score,0.3978102189781022,0.9347826086956522,0.8695652173913043,Accordingly u is a reference sentence sampled from set U ,11,It is either human - written or produced by G ? .,"Given the reference set and the comparison set , we are able to compute the rank scores indicating the relative ranks for the complete sentences .",method
sentiment_analysis,36,Abstract features usually refer to the features ultimately useful for the task .,introduction,introduction,0,43,32,32,0,introduction : introduction,0.172,0.8888888888888888,0.8888888888888888,Abstract features usually refer to the features ultimately useful for the task ,13,"One method could be concatenating the target representation with each word representation , but the effect as shown in is limited .","In summary , our contributions are as follows :",introduction
sentence_compression,3,"The syntax - based evaluator should assess the degree to which the compressed sentence is grammatical , through being used as a reward function during the reinforcement learning phase .",methodology,Syntax-based Evaluator,0,45,20,2,0,methodology : Syntax-based Evaluator,0.3629032258064516,0.5882352941176471,0.125,The syntax based evaluator should assess the degree to which the compressed sentence is grammatical through being used as a reward function during the reinforcement learning phase ,28, ,"It needs to satisfy three conditions : ( i ) grammatical compressions should obtain a higher score than ungrammatical compressions , ( ii ) for two ungrammatical compressions , it should be able to discriminate them through the score despite the ungrammaticality , ( iii ) lack of important parts ( such as the primary subject or verb ) in the original sentence should receive a greater penalty .",method
sentiment_analysis,12,"To enhance the persuasiveness of our experimental results , we also displayed the previously reported scores of MN and TNet on the same data set .",result,result,0,172,3,3,0,result : result,0.7678571428571429,0.2,0.2,To enhance the persuasiveness of our experimental results we also displayed the previously reported scores of MN and TNet on the same data set ,25,Table 4 provides all the experimental results .,"According to the experimental results , we can come to the following conclusions :",result
sentiment_analysis,8,) Audio Features : a) Pitch : Pitch is important because waveforms produced by our vocal cords change depending on our emotion .,methodology,Feature Extraction,0,63,17,3,0,methodology : Feature Extraction,0.2692307692307692,0.19101123595505606,0.15, Audio Features a Pitch Pitch is important because waveforms produced by our vocal cords change depending on our emotion ,21,"We now describe the handcrafted features used to train both , the ML - and the DL - based models .",Many algorithms for estimating the pitch signal exist .,method
sentiment_analysis,33,"where ht is a hidden state vector that encoded information about previously processed words , and the function RNN is a recurrent unit such as Long Short - Term Memory ( LSTM ) unit ( Hochreiter and Schmidhuber , 1997 ) or Gated Recurrent Unit ( GRU ) .",approach,Recurrent neural filters,0,45,20,15,1,approach : Recurrent neural filters,0.36,0.8333333333333334,0.7894736842105263,where ht is a hidden state vector that encoded information about previously processed words and the function RNN is a recurrent unit such as Long Short Term Memory LSTM unit Hochreiter and Schmidhuber 1997 or Gated Recurrent Unit GRU ,40,RNFs compose the words of the m-gram from left to right using the same recurrent unit :,We use the last hidden state h i +m?1 as the RNF output feature vector c i .,method
machine-translation,6,Our method outperforms the baseline method for 1.26%/0.66%/0.44 % on three different datasets .,model,Text Classification,1,229,16,3,0,model : Text Classification,0.7869415807560137,0.8888888888888888,0.6,Our method outperforms the baseline method for 1 26 0 66 0 44 on three different datasets ,18,The results are listed in .,"As a summary , our experiments on four different tasks with 10 datasets verify the effectiveness of our method .",method
sentiment_analysis,18,"Furthermore , the proposed target representation also outputs aspect embeddings after the training process .",analysis,Impact of Target Representation,0,212,25,9,0,analysis : Impact of Target Representation,0.8870292887029289,0.5434782608695652,0.6,Furthermore the proposed target representation also outputs aspect embeddings after the training process ,14,"Our proposed target representation is able to capture the correct aspect semantics for both targets and as a result , the attention mechanism can capture the correct opinion context .",Each aspect can be interpreted by its nearby words in vector space .,result
sentiment_analysis,22,AOA - LSTM introduces an attention - over- attention ( AOA ) based network to model aspects and sentences in a joint way and explicitly capture the interaction between aspects and context sentences .,method,Hierarchical Attention Based Fusion Layer,1,152,18,2,0,method : Hierarchical Attention Based Fusion Layer,0.6637554585152838,0.9473684210526316,0.6666666666666666,AOA LSTM introduces an attention over attention AOA based network to model aspects and sentences in a joint way and explicitly capture the interaction between aspects and context sentences ,30, ,without manual feature engineering and improve the performance in this task .,method
prosody_prediction,0,"We take the last hidden layer of BERT and train a single fully - connected classifier layer on top of it , mapping the representation of each word to the labels .",experiment,Experimental Setup,1,89,9,7,0,experiment : Experimental Setup,0.4635416666666667,0.375,0.3181818181818182,We take the last hidden layer of BERT and train a single fully connected classifier layer on top of it mapping the representation of each word to the labels ,30,"We use the Huggingface PyTorch implementation of BERT available in the pytorch transformers library , 3 which we further fine - tune during training .",For our experiments we use the smaller BERT - base model using the uncased alternative .,experiment
sentiment_analysis,42,"In this line of research , memory is encoded as a continuous representation and operations on memory ( e.g. reading and writing ) are typically implemented with neural networks .",system description,Attention and Memory Networks,0,243,9,3,0,system description : Attention and Memory Networks,0.9642857142857144,0.8181818181818182,0.6,In this line of research memory is encoded as a continuous representation and operations on memory e g reading and writing are typically implemented with neural networks ,28,"Recently , there is a resurgence in computational models with attention mechanism and explicit mem-ory to learn representations of texts .","Attention mechanism could be viewed as a compositional function , where lower level representations are regarded as the memory , and the function is to choose "" where to look "" by assigning a weight / importance to each lower position when computing an upper level representation .",method
semantic_role_labeling,2,shared dropout mask z l is applied to the hidden state :,model,model,0,52,16,16,0,model : model,0.23214285714285715,0.372093023255814,0.9411764705882352,shared dropout mask z l is applied to the hidden state ,12,"To reduce over - fitting , we use dropout as described in .",l is shared across timesteps at layer l to avoid amplifying the dropout noise along the sequence .,method
sentiment_analysis,31,Parameterized filters and gates have the same size and number as normal filters .,training,Hyperparameters and Training,1,113,3,3,0,training : Hyperparameters and Training,0.710691823899371,0.25,0.25,Parameterized filters and gates have the same size and number as normal filters ,14,"We use rectifier as non-linear function f in the CNN g , CNN t and sigmoid in the CNN s , filter window sizes of 1 , 2 , 3 , 4 with 100 feature maps each , l 2 regularization term of 0.001 and minibatch size of 25 .","They are generated uniformly by CNN with window sizes of 1 , 2 , 3 , 4 , eg. among 100 parameterized filters with size 3 , 25 of them are generated by aspect CNN with filter size 1 , 2 , 3 , 4 respectively .",experiment
relation-classification,1,"In this section , we firstly introduce how to change the extraction problem to a tagging problem based on our tagging method .",method,Method,0,68,3,3,0,method : Method,0.2764227642276423,0.042857142857142864,0.2,In this section we firstly introduce how to change the extraction problem to a tagging problem based on our tagging method ,22,We propose a novel tagging scheme and an end -toend model with biased objective function to jointly extract entities and their relations .,Then we detail the model we used to extract results .,method
part-of-speech_tagging,6,"In addition , others propose novel neural architectures for parsing to handle feature - engineering .",introduction,introduction,0,14,7,7,0,introduction : introduction,0.09395973154362416,0.5,0.5,In addition others propose novel neural architectures for parsing to handle feature engineering ,14,Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural network - based classifiers .,Part - of - speech ( POS ) tags are essential features used in most dependency parsers .,introduction
sentiment_analysis,15,"However , distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts .",system description,Semantic Vector Spaces.,0,41,5,5,0,system description : Semantic Vector Spaces.,0.15185185185185185,0.09433962264150944,0.625,However distributional vectors often do not properly capture the differences in antonyms since those often have similar contexts ,19,Variants of this idea use more complex frequencies such as how often a word appears in a certain syntactic context .,One possibility to remedy this is to use neural word vectors .,method
sentiment_analysis,6,We also investigate an architecture where the dense layer after the LSTM cell is omitted .,architecture,Different Network Architectures,0,151,7,7,0,architecture : Different Network Architectures,0.5224913494809689,0.12280701754385966,0.3888888888888889,We also investigate an architecture where the dense layer after the LSTM cell is omitted ,16,"As this is the simple variant of the contextual LSTM , we termed it as simple contextual LSTM ( sc - LSTM 1 ) .","Thus , the output of the LSTM cell h i ,t provides our context - dependent features and the softmax layer provides the classification .",method
text-classification,7,"To analyze the effect of varying different components of our capsule architecture for text classification , we also report the ablation test of the capsule - B model in terms of using different setups of the capsule network .",ablation,Ablation Study,0,152,2,2,0,ablation : Ablation Study,0.6255144032921811,0.03571428571428571,0.4,To analyze the effect of varying different components of our capsule architecture for text classification we also report the ablation test of the capsule B model in terms of using different setups of the capsule network ,37, ,The experimental results are summarized in .,result
phrase_grounding,0,This procedure can be seen as finding projection of the textual embeddings on hyperplanes spanned by visual features .,method,Feature Level Selection,0,128,55,4,0,method : Feature Level Selection,0.5688888888888889,0.7333333333333333,0.3076923076923077,This procedure can be seen as finding projection of the textual embeddings on hyperplanes spanned by visual features ,19,"Intuitively , each visual feature map level could carry different semantic information , thus for each word we propose to apply a hard level - attention to get the score from the level contributing the most as","For each word feature st , we compute an attention map H l, t and an attended visual feature a l,t at each level l .",method
question_answering,3,leaderboard for this dataset is maintained at http://www.qizhexie.com/ data/RACE_leaderboard .,method,RACE,0,183,8,6,0,method : RACE,0.6535714285714286,0.2352941176470588,0.3333333333333333,leaderboard for this dataset is maintained at http www qizhexie com data RACE_leaderboard ,14,"It uses ( 1 ) BiMPM 's matching functions for extensive matching between Q , P and A , ( 2 ) multi-hop reasoning powered by ReasoNet and employs reinforcement learning techniques for dynamic strategy selection .","Note that the current state - of - the - art 5 , is a generative pre-training model trained on a large external corpus .",method
sentiment_analysis,4,Self - Emotional Influence b) Inter-speaker,ablation,Dependency on distant history,0,294,28,13,0,ablation : Dependency on distant history,0.9018404907975459,0.5384615384615384,0.8666666666666667,Self Emotional Influence b Inter speaker,6,This result indicates the presence of long - term emotional dependencies and the need to consider histories faraway from the current test utterance . u11 u 12 a ),Influence : Case studies for emotional influence .,result
natural_language_inference,6,"Some tasks ( BUCC , MLDoc ) tend to perform better when the encoder is trained on long and formal sentences , whereas other tasks ( XNLI , Tatoeba ) benefit from training on shorter and more informal sentences .",training,Training data,0,209,21,21,0,training : Training data,0.842741935483871,0.35,0.65625,Some tasks BUCC MLDoc tend to perform better when the encoder is trained on long and formal sentences whereas other tasks XNLI Tatoeba benefit from training on shorter and more informal sentences ,33,"In our preliminary experiments , we observed that the domain of the training data played a key role in the performance of our sentence embeddings .","So as to obtain a good balance , we used at most two million sentences from Open - Subtitles , although more data is available for some languages .",experiment
natural_language_inference,24,"During training , we apply stochastic dropout to before the above averaging operation .",system description,Experiment Setup,0,111,84,21,0,system description : Experiment Setup,0.4743589743589744,0.9230769230769232,0.75,During training we apply stochastic dropout to before the above averaging operation ,13,"is a multinomial distribution over { 1 , . . . , n} , so the average distribution is straightforward to compute .","For example , as illustrated in , we randomly delete several steps ' predictions in Equations 7 and 8 so that P begin might be avg ( [ P begin 1 , P begin 3 ] ) and P end might be avg ( [ P end 0 , P end 3 , P end 4 ] ) .",method
text_summarization,13,", + PRETRAIN for starting with a model pretrained with soft attention for 1 epoch , and + ALTERNATE for sampling between hard and soft attention with probability 0.5 .",model,model,0,82,13,13,0,model : model,0.30711610486891383,0.13131313131313133,0.13131313131313133, PRETRAIN for starting with a model pretrained with soft attention for 1 epoch and ALTERNATE for sampling between hard and soft attention with probability 0 5 ,28,"For C2 F , we include options + MULTI fork mul >","In , the function a ( ) is implemented with an attention network .",method
natural_language_inference,30,"So for some types of triple it has much better coverage , despite the larger size of Freebase ; for example Freebase does not cover verbs like afraid - of or suffer - from .",training,Training Data,0,85,11,11,0,training : Training Data,0.32945736434108525,0.24444444444444444,1.0,So for some types of triple it has much better coverage despite the larger size of Freebase for example Freebase does not cover verbs like afraid of or suffer from ,31,"In contrast to highly curated data bases such Freebase , ReVerb has more noise but also many more relation types ( Freebase has around 20 k ) .", ,experiment
sentiment_analysis,17,"Note that in the dependency parse of the second query sentence , the word "" ocean "" is the second - furthest word from the root ( "" waving "" ) , with a depth of 4 .",model,Modeling Semantic Relatedness,0,195,5,5,0,model : Modeling Semantic Relatedness,0.8666666666666667,0.3333333333333333,0.625,Note that in the dependency parse of the second query sentence the word ocean is the second furthest word from the root waving with a depth of 4 ,29,The Dependency Tree - LSTM model exhibits several desirable properties .,"Regardless , the retrieved sentences are all semantically related to the word "" ocean "" , which indicates that the Tree - LSTM is able to both preserve and emphasize information from relatively distant nodes .",method
machine-translation,9,We firstly train a baseline NMT model to obtain the task - specific embeddings for all in - vocabulary words in both languages .,analysis,MACHINE TRANSLATION,0,238,61,29,0,analysis : MACHINE TRANSLATION,0.8292682926829268,0.6931818181818182,0.90625,We firstly train a baseline NMT model to obtain the task specific embeddings for all in vocabulary words in both languages ,22,"Similar to the code learning , the training is distributed to 4 GPUs , each GPU computes a mini-batch of 16 samples .","Then based on these baseline embeddings , we obtain the hash codes and basis vectors by training the code learning model .",result
sentiment_analysis,36,It is a CNN - based model implemented by us which directly concatenates target representation to each word embedding ; TD - LSTM :,experiment,Experimental Setup,1,156,15,14,0,experiment : Experimental Setup,0.624,0.42857142857142855,0.4117647058823529,It is a CNN based model implemented by us which directly concatenates target representation to each word embedding TD LSTM ,21,It is a CNN - based model implemented by us which directly concatenates target representation to each word embedding ; TD - LSTM :,"It employs two LSTMs to model the left and right contexts of the target separately , then performs predictions based on concatenated context representations ; MemNet : It applies attention mechanism over the word embeddings multiple times and predicts sentiments based on the top - most sentence representations ; BILSTM - ATT -G : It models left and right contexts using two attention - based LSTMs and introduces gates to measure the importance of left context , right context , and the entire sentence for the prediction ; RAM : RAM is a multilayer architecture where each layer consists of attention - based aggregation of word features and a GRU cell to learn the sentence representation .",experiment
semantic_role_labeling,3,"Like , our system take the very original utterances and predicate masks as the inputs without context windows .",model,Labeling Confusion,0,249,56,14,0,model : Labeling Confusion,0.9431818181818182,0.8358208955223879,0.56,Like our system take the very original utterances and predicate masks as the inputs without context windows ,18,We choose self - attention as the key component in our architecture instead of LSTMs .,"At the inference stage , we apply argmax decoding approach on top of a simple logistic regression while chose a CRF approach and chose constrained decoding .",method
sentiment_analysis,35,"To resolve the challenges , we propose a novel framework named Multi - Granularity Alignment Network ( MGAN ) to simultaneously align aspect granularity and aspect- specific feature representations across domains .",introduction,introduction,1,38,24,24,0,introduction : introduction,0.1532258064516129,0.6857142857142857,0.6857142857142857,To resolve the challenges we propose a novel framework named Multi Granularity Alignment Network MGAN to simultaneously align aspect granularity and aspect specific feature representations across domains ,28,"For example , in the source Restaurant domain , tasty and delicious are used to express positive sentiment towards the aspect category "" food "" , while lightweight and responsive often indicate positive sentiment towards the aspect term "" mouse "" in the target Laptop domain .","Specifically , the MGAN consists of two networks for learning aspect - specific representations for the two domains , respectively .",introduction
sentiment_analysis,31,"If we ignore the aspect information , it is hard to determine the sentiment for a target aspect , which accounts for a large portion of sentiment classification error .",introduction,introduction,0,14,8,8,0,introduction : introduction,0.0880503144654088,0.42105263157894735,0.42105263157894735,If we ignore the aspect information it is hard to determine the sentiment for a target aspect which accounts for a large portion of sentiment classification error ,28,"For example , given a sentence "" great food but the service was dreadful "" , the sentiment polarity about aspect "" food "" is positive while the sentiment polarity about "" service "" is negative .","Recently , machine learning based approaches are becoming popular for this task .",introduction
sentiment_analysis,11,"In the end , emotional influence can be seen when B , despite being sad , reacts angrily to A 's angry statement .",introduction,introduction,0,56,46,46,0,introduction : introduction,0.16279069767441862,0.9787234042553192,0.9787234042553192,In the end emotional influence can be seen when B despite being sad reacts angrily to A s angry statement ,21,Initially both A and B are emotionally driven by their own emotional inertia .,"perimental results are covered in Section 5 ; finally , Section 6 provides concluding remarks .",introduction
natural_language_inference,31,"On WikiQA dataset , our method does not seem to be robust to structural hyperparameter changes .",analysis,Analysis,0,209,26,26,0,analysis : Analysis,0.7572463768115942,0.4262295081967213,0.4262295081967213,On WikiQA dataset our method does not seem to be robust to structural hyperparameter changes ,16,We can see in the table that fewer blocks or layers may not be sufficient but adding more blocks or layers than necessary hardly harms the performance .,mentions that on Wiki,result
machine-translation,8,See for the graphical illustration of the proposed model .,system description,The backward RNN,0,102,72,9,0,system description : The backward RNN,0.3081570996978852,1.0,1.0,See for the graphical illustration of the proposed model ,10,This sequence of annotations is used by the decoder and the alignment model later to compute the context vector ( Eqs. ( 5 ) - ) ., ,method
natural_language_inference,2,"Most of the previously released datasets are closed - world , i.e. , the questions and answers are formulated given the text passages .",introduction,introduction,0,19,8,8,0,introduction : introduction,0.0708955223880597,0.2857142857142857,0.2857142857142857,Most of the previously released datasets are closed world i e the questions and answers are formulated given the text passages ,22,released the SQuAD dataset where the answers are freeform unlike in the previous MC datasets .,"As such , the answer spans can often be extracted by simple word and context matching .",introduction
natural_language_inference,27,Model Encoder Layer .,model,Model Encoder Layer. Similar to,0,110,66,1,0,model : Model Encoder Layer. Similar to,0.3254437869822485,0.4962406015037594,0.05555555555555555,Model Encoder Layer ,4, ,This layer is task - specific .,method
natural_language_inference,38,Our model consists of three parts :,introduction,introduction,1,28,18,18,0,introduction : introduction,0.14814814814814814,0.5806451612903226,0.5806451612903226,Our model consists of three parts ,7,"In this paper , we introduce the Multi - layer Embedding with Memory Networks ( MEMEN ) , an end - to - end neural network for machine comprehension task .",") the encoding of context and query , in which we add useful syntactic and semantic information in the embedding of every word , 2 ) the high - efficiency multilayer memory network of full - orientation matching to match the question and context , 3 ) the pointer - network based answer boundary prediction layer to get the location of the answer in the passage .",introduction
question-answering,7,"Given a shared memory Mn ? R kn that has been encoded by processing a relevant sequence with length n , MMA - NSE with the access to one relevant memory is defined as",training,Shared and Multiple Memory Accesses,0,116,43,7,0,training : Shared and Multiple Memory Accesses,0.4218181818181818,0.8775510204081632,0.5384615384615384,Given a shared memory Mn R kn that has been encoded by processing a relevant sequence with length n MMA NSE with the access to one relevant memory is defined as,31,The first memory ( in green ) is the shared memory accessed by more than one NSEs .,"Given a shared memory Mn ? R kn that has been encoded by processing a relevant sequence with length n , MMA - NSE with the access to one relevant memory is defined as",experiment
text_generation,2,"Given the goal embedding vector wt , the WORKER module takes the current word x t as input and outputs a matrix O t , which is further combined with wt by matrix product to determine the final action space distribution undercurrent state",methodology,Hierarchical Structure of G,0,106,36,14,0,methodology : Hierarchical Structure of G,0.3028571428571429,0.5901639344262295,0.35897435897435903,Given the goal embedding vector wt the WORKER module takes the current word x t as input and outputs a matrix O t which is further combined with wt by matrix product to determine the final action space distribution undercurrent state,41,"To incorporate goals produced by MANAGER , a linear transformation ? with weight matrix W ? is performed on a summation over recent c goals to produce a k-dimensional goal embedding vector wt as","where W ( ; ? w ) denotes the WORKER module , i.e. an LSTM with h W t as its recurrent hidden vector , O t is a | V |k matrix that represents the current vector for all words , thus O t wt yields the calculated logits for all words , and ? is the temperature parameter to control the generation entropy .",method
text_summarization,4,"Furthermore , we provide analysis on how our model effectively uses the extracted linked entities to produce concise and better summaries .",introduction,introduction,0,39,28,28,0,introduction : introduction,0.1511627906976744,1.0,1.0,Furthermore we provide analysis on how our model effectively uses the extracted linked entities to produce concise and better summaries ,21,"Moreover , when compared with the state - of - the - art models for each dataset , the model obtains a comparable performance on the Gigaword dataset where the texts are short , and outperforms all competing models on the CNN dataset where the texts are longer .", ,introduction
sentiment_analysis,16,"We can drop all ? terms here as they all equal to 1 , i.e. , they are the only context word in the snippets to attend to ( the target words are not contexts ) .",system description,The Proposed Approaches,0,101,49,4,0,system description : The Proposed Approaches,0.3176100628930817,0.7,0.16,We can drop all terms here as they all equal to 1 i e they are the only context word in the snippets to attend to the target words are not contexts ,33,Based on the above example snippets or phrases we have four corresponding inequalities :,"We can drop all ? terms here as they all equal to 1 , i.e. , they are the only context word in the snippets to attend to ( the target words are not contexts ) .",method
sentiment_analysis,32,"As Jiang et al. point out that 40 % of sentiment classification errors are caused by not considering targets in sentiment classification , recent work tends to especially strengthen the effect of targets when modeling the contexts .",introduction,introduction,0,18,8,8,1,introduction : introduction,0.0782608695652174,0.2222222222222222,0.2222222222222222,As Jiang et al point out that 40 of sentiment classification errors are caused by not considering targets in sentiment classification recent work tends to especially strengthen the effect of targets when modeling the contexts ,36,"With the development of deep learning techniques , some researchers have designed effective neural networks to automatically generate useful lowdimensional representations from targets and their contexts and obtain a promising result on the aspect - level sentiment classification task .","As Jiang et al. point out that 40 % of sentiment classification errors are caused by not considering targets in sentiment classification , recent work tends to especially strengthen the effect of targets when modeling the contexts .",introduction
natural_language_inference,92,"It outperforms recent state - of - the - art reward - based semantic parsing algorithms by 13 % absolute percentage on WIKISQL , strongly suggesting that having a small precomputed set of possible solutions is a key ingredient .",introduction,introduction,0,33,23,23,0,introduction : introduction,0.11458333333333333,0.9583333333333334,0.9583333333333334,It outperforms recent state of the art reward based semantic parsing algorithms by 13 absolute percentage on WIKISQL strongly suggesting that having a small precomputed set of possible solutions is a key ingredient ,34,"Our learning approach significantly outperforms previous methods which use heuristic supervision and MML updates , including absolute gains of 2 - 10 % , and achives the state - of - the - art on five datasets .","Finally , we present a detailed analysis showing that , in practice , the introduction of hard updates encourages models to assign much higher probability to the correct solution .",introduction
named-entity-recognition,8,ablation studies can be found in Appendix C.,ablation,ablation,0,230,7,7,0,ablation : ablation,0.5943152454780362,0.2692307692307692,0.2692307692307692,ablation studies can be found in Appendix C ,9,"+ BiLSTM "" adds a randomly initialized BiLSTM on top of the "" LTR + No NSP "" model during fine - tuning .",Effect of Pre-training Tasks,result
relation_extraction,0,We verified the same and we hypothesize that for this reason the performance on the entities depends largely on the pretrained word embeddings being used .,analysis,Entities,0,219,11,6,0,analysis : Entities,0.85546875,0.3235294117647059,0.4615384615384616,We verified the same and we hypothesize that for this reason the performance on the entities depends largely on the pretrained word embeddings being used ,26,"mentioned in their analysis of the dataset that there were many "" UNK "" tokens in the test set which were never seen during training .","We found considerable improvements on entity recall when using pretrained word embeddings , if available , for these "" UNK "" tokens .",result
question_generation,1,is the training dataset that contains all set of possible triplets .,method,Mixture Module,0,127,47,10,0,method : Mixture Module,0.3239795918367347,0.5164835164835165,0.625,is the training dataset that contains all set of possible triplets ,12,"where D ( t ( s i ) , t ( s j ) ) = ||t ( s i ) ? t ( s j ) || 2 2 is the euclidean distance between two embeddings t( s i ) and t(s j ) .","( s i , s + i , s ? i ) is the triplet loss function .",method
machine-translation,7,Our work builds on this use of MoEs as a general purpose neural network component .,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,65,22,13,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.1742627345844504,0.4230769230769231,0.8666666666666667,Our work builds on this use of MoEs as a general purpose neural network component ,16,"They also allude in their conclusion to the potential to introduce sparsity , turning MoEs into a vehicle for computational computation .","While uses two stacked MoEs allowing for two sets of gating decisions , our convolutional application of the MoE allows for different gating decisions at each position in the text .",method
sentiment_analysis,12,Neg / Neg 3.07 -: The example of mining influential context words from the first training sentence in .,approach,Details of Our Approach,0,107,42,28,0,approach : Details of Our Approach,0.4776785714285714,0.5833333333333334,0.5957446808510638,Neg Neg 3 07 The example of mining influential context words from the first training sentence in ,18,"In Step 2 , on the basis of v ( t ) and h( x ) , we The [ place ] is mask and mask but the service is mask .","( ? ( x ) ) denotes the entropy of the attention weight distribution ?( x ) , ? is entropy threshold set as 3.0 , and x m indicates the context word with the maximum attention weight .",method
natural_language_inference,39,Wgt Word Cnt 0.5099 0.5132 PV 0.5110 0.5160 PV - Cnt 0.5976 0.6058 LCLR 0.5993 0.6086 CNN 0.6190 0.6281 CNN - Cnt 0.6520 0.6652 0.6930 0.7090,result,result,0,185,11,11,0,result : result,0.8980582524271845,0.3928571428571429,0.3928571428571429,Wgt Word Cnt 0 5099 0 5132 PV 0 5110 0 5160 PV Cnt 0 5976 0 6058 LCLR 0 5993 0 6086 CNN 0 6190 0 6281 CNN Cnt 0 6520 0 6652 0 6930 0 7090,38,Word Cnt 0.4891 0.4924,This work 0.7090 0.7234 0.4271 0.5259 0.6029 0.6852 0.6091 0.6917 0.5951 0.6951 0.6307 0.7477 Severyn and Moschitti ( 2013 ) 0.6781 0.7358 0.7092 0.7700 0.7134 0.7913 10 Analysis,result
text-classification,5,We otherwise use the same practices used in .,hyperparameters,Hyperparameters,0,167,9,9,0,hyperparameters : Hyperparameters,0.6626984126984127,1.0,1.0,We otherwise use the same practices used in ,9,"We use a batch size of 64 , a base learning rate of 0.004 and 0.01 for finetuning the LM and the classifier respectively , and tune the number of epochs on the validation set of each task 7 .", ,experiment
machine-translation,1,The convolutions in the decoder CNN are masked to prevent the network from seeing future tokens in the target sequence .,introduction,introduction,1,33,21,21,0,introduction : introduction,0.16417910447761194,0.5526315789473685,0.5526315789473685,The convolutions in the decoder CNN are masked to prevent the network from seeing future tokens in the target sequence ,21,The two CNNs use increasing factors of dilation to rapidly grow the receptive fields ; a similar technique is also used in .,The network has beneficial computational and learning properties .,introduction
named-entity-recognition,8,Left - to - right language model - BERT BERT E E 1 E ...,system description,Unsupervised Fine-tuning Approaches,0,58,19,6,0,system description : Unsupervised Fine-tuning Approaches,0.14987080103359174,0.4871794871794872,0.42857142857142855,Left to right language model BERT BERT E E 1 E ,12,"At least partly due to this advantage , OpenAI achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark .",CT 1 T ... E 1 E ...,method
sentiment_analysis,15,"Hence , even a 5 - class classification into these categories captures the main variability of the labels .",system description,Stanford Sentiment Treebank,0,88,52,26,0,system description : Stanford Sentiment Treebank,0.32592592592592595,0.981132075471698,0.9629629629629628,Hence even a 5 class classification into these categories captures the main variability of the labels ,17,The extreme values were rarely used and the slider was not often left in between the ticks .,We will name this fine - grained sentiment classification and our main experiment will be to recover these five labels for phrases of all lengths .,method
sentiment_analysis,39,In this section we explain the data collection and annotation process and summarise properties of the dataset .,system description,SentiHood,0,62,7,5,0,system description : SentiHood,0.2530612244897959,0.09859154929577464,1.0,In this section we explain the data collection and annotation process and summarise properties of the dataset ,18,Answers that is filtered for questions relating to neighbourhoods of the city of London ., ,method
natural_language_inference,79,"In this paper , we propose Paragraph Vector , an unsupervised algorithm that learns fixed - length feature representations from variable - length pieces of texts , such as sentences , paragraphs , and documents .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.02985074626865672,0.6,0.6,In this paper we propose Paragraph Vector an unsupervised algorithm that learns fixed length feature representations from variable length pieces of texts such as sentences paragraphs and documents ,29,"For example , "" powerful , "" "" strong "" and "" Paris "" are equally distant .",Our algorithm represents each document by a dense vector which is trained to predict words in the document .,abstract
natural_language_inference,53,"We also include the FastSent variant "" FastSent + AE "" and the SkipThought - LN version that uses layer normalization .",model,Task transfer,0,168,27,7,0,model : Task transfer,0.8076923076923077,0.4426229508196721,0.17073170731707318,We also include the FastSent variant FastSent AE and the SkipThought LN version that uses layer normalization ,18,The second group consists of models trained with unsupervised ordered sentences such as FastSent and SkipThought ( also trained on the Toronto book corpus ) .,"We report results from models trained on supervised data in the third group , and also report some results of supervised methods trained directly on each task for comparison with transfer learning approaches .",method
text-to-speech_synthesis,1,"In the inference process , the output mel-spectrograms of our FastSpeech model are transformed into audio samples using the pretrained WaveGlow [ 20 ] 5 .",training,Training and Inference,1,139,11,11,0,training : Training and Inference,0.634703196347032,1.0,1.0,In the inference process the output mel spectrograms of our FastSpeech model are transformed into audio samples using the pretrained WaveGlow 20 5 ,24,The FastSpeech model training takes about 80 k stepson 4 NVIDIA V100 GPUs ., ,experiment
sentiment_analysis,42,It is helpful to restate that the difference between four location - based attention models lies in the usage of location vectors for context words .,method,Effects of Location Attention,0,193,9,4,0,method : Effects of Location Attention,0.7658730158730159,0.4090909090909091,0.2352941176470588,It is helpful to restate that the difference between four location based attention models lies in the usage of location vectors for context words ,25,We incorporate each of them separately into the basic content - based attention model .,"In Model 1 and Model 2 , the values of location vectors are fixed and calculated in a heuristic way .",method
natural_language_inference,16,"Finally , based on the matching vector , a decision is made through a fully connected layer .",abstract,abstract,0,11,9,9,0,abstract : abstract,0.051643192488262914,0.8181818181818182,0.8181818181818182,Finally based on the matching vector a decision is made through a fully connected layer ,16,"Then , another BiLSTM layer is utilized to aggregate the matching results into a fixed - length matching vector .","We evaluate our model on three tasks : paraphrase identification , natural language inference and answer sentence selection .",abstract
named-entity-recognition,1,"The representation of a word using this model is obtained by concatenating its left and right context representations , These representations effectively include a representation of a word in context , which is useful for numerous tagging applications .",model,LSTM,0,47,21,18,0,model : LSTM,0.2270531400966184,0.9130434782608696,0.9473684210526316,The representation of a word using this model is obtained by concatenating its left and right context representations These representations effectively include a representation of a word in context which is useful for numerous tagging applications ,37,This forward and backward LSTM pair is referred to as a bidirectional LSTM .,"The representation of a word using this model is obtained by concatenating its left and right context representations , These representations effectively include a representation of a word in context , which is useful for numerous tagging applications .",method
text_summarization,5,More details will be described in the rest of this section,method,Method,0,52,7,7,0,method : Method,0.20634920634920634,0.5,1.0,More details will be described in the rest of this section,11,"In the Rewrite module , a RNN decoder combines the hidden states of x and r to generate a summary y.", ,method
sentiment_analysis,24,"dd contains review documents from at least two domains with y ds i denoting the domain label , where one of the domains is similar to the domains of D a and D ds .",method,Learning,0,146,91,18,0,method : Learning,0.4694533762057878,0.6791044776119403,0.29508196721311475,dd contains review documents from at least two domains with y ds i denoting the domain label where one of the domains is similar to the domains of D a and D ds ,34,ds and D a are from similar domains .,"In this way , linguistic knowledge can be transferred from DS and DD to AE and AS , as Datasets .",method
natural_language_inference,62,"The passages in the adversarial sets contain misleading sentences , which are aimed at distracting MRC models .",experiment,Experimental Settings,0,178,6,5,0,experiment : Experimental Settings,0.7946428571428571,0.15789473684210525,0.35714285714285715,The passages in the adversarial sets contain misleading sentences which are aimed at distracting MRC models ,17,"Besides , we also use two of its adversarial sets , namely AddSent and Add OneSent , to evaluate the robustness to noise of MRC models .","Specifically , each passage in AddSent contains several sentences thatare similar to the question but not contradictory to the answer , while each passage in AddOneSent contains a human - approved random sentence that maybe unrelated to the passage .",experiment
natural_language_inference,0,"We compare the GA Reader as described hereto a model which is exactly the same in all aspects , except that it passes document embeddings D ( k ) in each layer directly to the inputs of the next layer without using the GA module .",analysis,GA Reader Analysis,0,154,3,3,0,analysis : GA Reader Analysis,0.7817258883248731,0.125,0.125,We compare the GA Reader as described hereto a model which is exactly the same in all aspects except that it passes document embeddings D k in each layer directly to the inputs of the next layer without using the GA module ,43,In this section we do an ablation study to see the effect of Gated Attention .,In other words X ( k ) = D ( k ) for all k >,result
text_summarization,12,sri lanka closes schools as war escalates :,introduction,introduction,0,23,13,13,0,introduction : introduction,0.10087719298245612,0.3939393939393939,0.3939393939393939,sri lanka closes schools as war escalates ,8,the sri lankan government on wednesday announced the closure of government schools with immediate effect as a military campaign against tamil separatists escalated in the north of the country .,An abstractive sentence summarization system may produce the output summary by distilling the salient information from the highlight to generate a fluent sentence .,introduction
natural_language_inference,50,"Additionally , we constrain the question and answer embeddings to the unit ball before passing to the next layer , i.e. , ?y * ? ? 1 . This is easily done via y * = y * ?y * ? when ?y * ? >",approach,Hyperbolic Representations of QA Pairs,0,117,25,8,0,approach : Hyperbolic Representations of QA Pairs,0.3690851735015773,0.3333333333333333,0.3076923076923077,Additionally we constrain the question and answer embeddings to the unit ball before passing to the next layer i e y 1 This is easily done via y y y when y ,33,"Unlike popular neural encoders such as LSTM or CNN , the NBOW representation does not add any parameters and is much more efficient .","Additionally , we constrain the question and answer embeddings to the unit ball before passing to the next layer , i.e. , ?y * ? ? 1 . This is easily done via y * = y * ?y * ? when ?y * ? >",method
sentiment_analysis,28,"Attention mechanism , which has been successfully used in machine translation , is incorporated to enforce the model to pay more attention to context words with closer semantic relations with the target .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.09444444444444444,0.2413793103448276,0.2413793103448276,Attention mechanism which has been successfully used in machine translation is incorporated to enforce the model to pay more attention to context words with closer semantic relations with the target ,31,"However , these neural network models are still in infancy to deal with the fine - grained targeted sentiment classification task .",There are already some studies use attention to generate targetspecific sentence representations or to transform sentence representations according to target words .,introduction
relation_extraction,2,We then locate the positions of the two target entities in the output embedding from BERT model .,introduction,introduction,1,27,17,17,0,introduction : introduction,0.2,0.7727272727272727,0.7727272727272727,We then locate the positions of the two target entities in the output embedding from BERT model ,18,"We insert special tokens before and after the target entities before feeding the text to BERT for fine - tuning , in order to identify the locations of the two target entities and transfer the information into the BERT model .",We use their embeddings as well as the sentence encoding ( embedding of the special first token in the setting of BERT ) as the input to a multi - layer neural network for classification .,introduction
natural_language_inference,36,"Actually , the easiest way to deal with segmentation or sequence labeling problems is to transform them into raw labeling problems .",model,Downstream Model,0,114,14,14,0,model : Downstream Model,0.5428571428571428,0.4516129032258064,0.4516129032258064,Actually the easiest way to deal with segmentation or sequence labeling problems is to transform them into raw labeling problems ,21,The details are 1 https://spacy.io/,"standard way to do this is the BIO encoding , representing a token at the beginning , interior , or outside of any span , respectively .",method
named-entity-recognition,1,"Although we observe faster convergence using these methods , none of them perform as well as SGD with gradient clipping .",training,Training,0,159,4,4,0,training : Training,0.7681159420289855,0.3076923076923077,0.3076923076923077,Although we observe faster convergence using these methods none of them perform as well as SGD with gradient clipping ,20,"Several methods have been proposed to enhance the performance of SGD , such as Adadelta or Adam ( Kingma and Ba , 2014 ) .",Our LSTM - CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100 .,experiment
text_generation,0,Then 70 experts on Chinese poems are invited to judge whether each of the 60 poem is created by human or machines .,Real-world Scenarios,Text Generation,0,238,16,14,0,Real-world Scenarios : Text Generation,0.7345679012345679,0.5714285714285714,0.7777777777777778,Then 70 experts on Chinese poems are invited to judge whether each of the 60 poem is created by human or machines ,23,"Specifically , we mix the 20 real poems and 20 each generated from SeqGAN and MLE .","Once regarded to be real , it gets + 1 score , otherwise 0 .",others
natural_language_inference,65,"In , the authors present an LTSM architecture for answer selection .",experiment,InsuranceQA,0,206,28,27,0,experiment : InsuranceQA,0.8956521739130435,0.6086956521739131,0.75,In the authors present an LTSM architecture for answer selection ,11,On the top part of we present the results of three recent work that use TREC - QA as a benchmark .,Their best result consists of a combination of LSTM and the BM25 algorithm .,experiment
part-of-speech_tagging,0,"We train the model parameters and word / character embeddings by the mini-batch stochastic gradient descent ( SGD ) with batch size 10 , momentum 0.9 , initial learning rate 0.01 and decay rate 0.05 .",model,Model,1,132,5,5,0,model : Model,0.5365853658536586,0.5,0.5,We train the model parameters and word character embeddings by the mini batch stochastic gradient descent SGD with batch size 10 momentum 0 9 initial learning rate 0 01 and decay rate 0 05 ,35,"We apply dropout to input embeddings and BiLSTM outputs for both baseline and adversarial training , with dropout rate 0.5 .",We also use a gradient clipping of 5.0 .,method
sentiment_analysis,16,We thus will not repeat similar discussions .,approach,approach,0,166,44,44,0,approach : approach,0.5220125786163522,0.5365853658536586,0.5365853658536586,We thus will not repeat similar discussions ,8,The proposed techniques introduced below also follow this core idea but with different implementations or properties .,Coupled Interaction ( CI ) :,method
natural_language_inference,28,1 receives the input from the mini-batch and feeds it s state into S ; S feeds it s state into F 2 ; the output of F k is the probability distribution of the predicted character .,model,PENN TREEBANK CORPUS DATA SET,0,205,15,10,0,model : PENN TREEBANK CORPUS DATA SET,0.7564575645756457,0.5555555555555556,0.4545454545454545,1 receives the input from the mini batch and feeds it s state into S S feeds it s state into F 2 the output of F k is the probability distribution of the predicted character ,37,The organization is roughly as follows :,"outlines the performance of some FS - RNN models along with other results in the PTB data set , in which we present the improved test BPC .",method
machine-translation,3,Out - of - vocabulary words are replaced with the unknown symbol unk .,data set,Data sets,0,199,12,12,0,data set : Data sets,0.6357827476038339,0.9230769230769232,0.9230769230769232,Out of vocabulary words are replaced with the unknown symbol unk ,12,"The full vocabulary of the German corpus is larger , so we select more German words to build the target vocabulary .","For complete comparison to previous work on the Englishto - French task , we also show the results with a smaller vocabulary of 30K input words and 30 K output words on the sub train set with selected 12M parallel sequences .",experiment
text_summarization,8,We also observe a decrease in average sentence length of summaries from 13 to 12 words when adding content selection compared to the Pointer - Generator while holding all other inference parameters constant .,result,Results,0,214,20,20,0,result : Results,0.7482517482517482,0.6060606060606061,0.6060606060606061,We also observe a decrease in average sentence length of summaries from 13 to 12 words when adding content selection compared to the Pointer Generator while holding all other inference parameters constant ,33,An independent - samples t- test shows that this improvement is statistically significant with t= 14.7 ( p < 10 ?5 ) .,"While end - to - end training has become common , there are benefits to a twostep method .",result
natural_language_inference,41,"Kipchoge ran in Vienna , Austria .",system description,BART Summary,0,243,19,19,0,system description : BART Summary,0.9455252918287936,0.5757575757575758,0.5757575757575758,Kipchoge ran in Vienna Austria ,6,His time was 1 hour 59 minutes 40.2 seconds .,It was an event specifically designed to help Kipchoge break the two hour barrier .,method
sentiment_analysis,26,Multiple Layers in Memory Module .,approach,Dual-Module Memory based CNNs,0,79,58,20,0,approach : Dual-Module Memory based CNNs,0.3224489795918367,0.6904761904761905,0.43478260869565216,Multiple Layers in Memory Module ,6,"Next , the c i in feature maps c ? R s+h?1 are computed as :","The memory module obtains as input a sequence of sentiment embedding vectors for the input , and attempts to draw conclusions about the over all sentiment polarity of the entire input sequence .",method
sentiment_analysis,12,"In order to know how our method improves neural ASC models , we deeply analyze attention results of TNet - ATT and TNet - ATT ( + AS ) .",model,model,0,190,6,6,0,model : model,0.8482142857142857,0.375,0.375,In order to know how our method improves neural ASC models we deeply analyze attention results of TNet ATT and TNet ATT AS ,24,These results strongly demonstrate the effectiveness and generality of our approach .,It has been found that our proposed approach can solve the above - mentioned two issues well .,method
sentiment_analysis,34,"This , in turn , would help to continuously collect more training data .",performance,Mazajak Online API,0,130,9,8,0,performance : Mazajak Online API,0.8783783783783784,0.5,0.4705882352941176,This in turn would help to continuously collect more training data ,12,feedback on the output sentiment using the form shown in .,The collected data is used periodically to update our model to improve the system performance .,result
sentiment_analysis,30,"We take the results reported in and , resp ; Bold = best performance ; "" - "" = not reported ; = average performance over 5 runs .",experiment,Experimental Setup,0,85,6,5,0,experiment : Experimental Setup,0.6640625,0.1935483870967742,0.16666666666666666,We take the results reported in and resp Bold best performance not reported average performance over 5 runs ,19,"Each sentence is annotated with a list of tuples { ( t , a , y ) } with each identifying the sentiment polarity y towards a specific aspect a of : Performance on Sentihood .",given target tin s .,experiment
sentiment_analysis,48,"The distribution q ? ( y|x , a ) will be tuned during maximizing the objective in Eq .",method,Classifier,0,99,37,4,0,method : Classifier,0.4194915254237288,0.5692307692307692,0.8,The distribution q y x a will be tuned during maximizing the objective in Eq ,16,"For the unlabeled data , the classifier is used to predict the distribution of label y for the decoder , i.e. , y ? q ? ( y|x , a ) .",". In this work , five classifiers are implemented in ASVAET and they are also used as the supervised baselines for the comparison .",method
sentiment_analysis,49,In this section we describe the datasets used for our experiments and report the results along with the necessary analysis .,methodology,"Datasets, Experiments and Analysis",0,135,74,2,0,"methodology : Datasets, Experiments and Analysis",0.5335968379446641,1.0,1.0,In this section we describe the datasets used for our experiments and report the results along with the necessary analysis ,21, , ,method
question_answering,0,The propagation model for GGNN is defined as follows :,system description,Graph-level Output Vector,0,127,78,7,0,system description : Graph-level Output Vector,0.4305084745762712,0.8666666666666667,0.3684210526315789,The propagation model for GGNN is defined as follows ,10,The recurrence is unrolled for a fixed number of steps T and the gating mechanism works akin to Gated Recurrent Units .,where ? is the logistic sigmoid function and is the element - wise multiplication .,method
natural_language_inference,72,The learning points are typically paraphrased portions of passage text and do not match passage sentences exactly .,introduction,introduction,0,36,28,28,0,introduction : introduction,0.1153846153846154,0.5833333333333334,0.5833333333333334,The learning points are typically paraphrased portions of passage text and do not match passage sentences exactly ,18,"Each report contains a Learning points section , summarizing the key pieces of information from that report .",We use these learning points to create queries by blanking out a medical entity .,introduction
sentence_classification,2,"However , other vectors might also contain errors which may reflect to the fixing of the source sentence vector .",model,Self Usability Module,0,94,9,3,0,model : Self Usability Module,0.37301587301587297,0.16981132075471694,0.25,However other vectors might also contain errors which may reflect to the fixing of the source sentence vector ,19,"To fix a source sentence vector 3 , we use the other sentence vectors as guide to know which dimensions to fix and to what extent do we need to fix them .","In order to cope with this , we introduce self usability modules .",method
natural_language_inference,73,We test ReSAN on natural language inference and semantic relatedness tasks .,introduction,introduction,0,41,29,29,0,introduction : introduction,0.15648854961832062,0.8529411764705882,0.8529411764705882,We test ReSAN on natural language inference and semantic relatedness tasks ,12,"Finally , we build an sentence - encoding model , "" reinforced self - attention network ( ReSAN ) "" , based on ReSA without any CNN / RNN structure .","The results show that ReSAN achieves the best test accuracy among all sentence - encoding models on the official leaderboard of the Stanford Natural Language Inference ( SNLI ) dataset , and state - of - the - art performance on the Sentences Involving Compositional Knowledge ( SICK ) dataset .",introduction
natural_language_inference,17,6 - 7 ) shows the effectiveness of heuristics used in the fusion function .,ablation,ablation,0,211,7,7,0,ablation : ablation,0.8115384615384615,0.28,0.28,6 7 shows the effectiveness of heuristics used in the fusion function ,13,"Next , we relace the default attention function with the dot product : f ( u , v ) = u v ( 5 ) , and both metrics suffer from degradations .","Removing any of the two heuristics leads to some performance declines , and heuristic subtraction is more effective than multiplication .",result
semantic_parsing,1,"Each field in a constructor is also strongly typed , which specifies the type of value the field can hold .",methodology,Modeling ASTs using ASDL Grammar,0,48,23,12,0,methodology : Modeling ASTs using ASDL Grammar,0.3356643356643357,0.5609756097560976,0.4,Each field in a constructor is also strongly typed which specifies the type of value the field can hold ,20,"For instance , the Call constructor under the composite type expr denotes function call expressions , and has three fields : func , args and keywords .",field with a composite type can be instantiated by constructors of the same type .,method
natural_language_inference,97,"Nevertheless , the model achieves stateof - the - art results on the multi questions , which ( putatively ) require some limited reasoning .",analysis,analysis,0,278,22,22,0,analysis : analysis,0.952054794520548,0.7857142857142857,0.7857142857142857,Nevertheless the model achieves stateof the art results on the multi questions which putatively require some limited reasoning ,19,It s simplicity is a response to the limited data of MCTest .,Our model is able to handle them reasonably well just by stringing important sentences together .,result
natural_language_inference,78,Standard feed - forward neural networks are commonly used to model similarity between aligned ( decomposed ) sub-phrases and then aggregated into the final prediction layers .,introduction,introduction,0,21,10,10,0,introduction : introduction,0.07608695652173914,0.2564102564102564,0.2564102564102564,Standard feed forward neural networks are commonly used to model similarity between aligned decomposed sub phrases and then aggregated into the final prediction layers ,25,The key idea is to learn an alignment of sub-phrases in both sentences and learn to compare the relationship between them .,Alignment between sentences has become a staple technique in NLI research and many recent state - of - the - art models such as the Enhanced Sequential Inference Model ( ESIM ) also incorporate the alignment strategy .,introduction
natural_language_inference,11,"This is reminiscent of the ( soft ) attention mechanism used in reading comprehension models ( e.g. , ; ) .",system description,Unrefined Word Embeddings (E 0 ),0,89,55,25,0,system description : Unrefined Word Embeddings (E 0 ),0.322463768115942,0.9821428571428572,0.9615384615384616,This is reminiscent of the soft attention mechanism used in reading comprehension models e g ,16,"Pooling over lemma-occurrences effectively connects different text passages ( even across texts ) thatare otherwise disconnected , mitigating the problems arising from long - distance dependencies .","However , our setup is more general as it allows for the connection of multiple passages ( via pooling ) at once and is able to deal with multiple inputs which is necessary to make use of additional input texts such as relevant background knowledge .",method
natural_language_inference,28,"From here , the orthogonal R t acts on the state h to produce an evolved hidden stateh .",architecture,THE RUM ARCHITECTURE,0,115,17,17,0,architecture : THE RUM ARCHITECTURE,0.42435424354243545,0.4146341463414634,0.4146341463414634,From here the orthogonal R t acts on the state h to produce an evolved hidden stateh ,18,Here ? is a non-negative integer that is a hyper - parameter of the model .,"Finally RUM obtains the new hidden state via u , just as in GRU .",method
machine-translation,3,"However , the best score was obtained using models with three layers .",system description,Neural Machine Translation,0,69,27,27,0,system description : Neural Machine Translation,0.22044728434504798,0.225,0.84375,However the best score was obtained using models with three layers ,12,They validated the modification of these shortcuts on an MT task and a language modeling task .,"Similarly , proposed a two dimensional structure for the LSTM .",method
relation_extraction,12,The attention guided layer is included starting from the second block .,system description,Attention Guided Layer,0,113,58,33,0,system description : Attention Guided Layer,0.3434650455927052,0.5858585858585859,1.0,The attention guided layer is included starting from the second block ,12,"In practice , we treat the original adjacency matrix as an initialization so that the dependency information can be captured in the node representations for later attention calculation .", ,method
natural_language_inference,88,The computation of intra-attention follows Equations ( 4 ) - ( 9 ) .,system description,Deep Attention Fusion Deep fusion combines,0,129,57,6,0,system description : Deep Attention Fusion Deep fusion combines,0.5222672064777328,0.8636363636363636,0.4,The computation of intra attention follows Equations 4 9 ,10,"Following Section 3.2 , C and H denote the target memory tape and hidden tape , which store representations of the target symbols that have been processed so far .","Additionally , we use A = [ ? 1 , , ? m ] and Y = [ ? 1 , , ? m ] to represent the source memory tape and hidden tape , with m being the length of the source sequence conditioned upon .",method
natural_language_inference,84,"This forces model designers to rely on overly large vocabularies , as observed by , which are parametrically expensive , or to employ vocabulary selection strategies .",introduction,introduction,0,15,6,6,0,introduction : introduction,0.06521739130434782,0.21428571428571427,0.21428571428571427,This forces model designers to rely on overly large vocabularies as observed by which are parametrically expensive or to employ vocabulary selection strategies ,24,"This essentially heuristic solution is inelegant , as words from technical domains , names of people , places , institutions , and soon will lack a specific representation unless sufficient data are available to justify their inclusion in the vocabulary .","In both cases , we face the issue that words in the tail of the Zipfian distribution will typically still be too rare to learn good representations for through standard embedding methods .",introduction
named-entity-recognition,9,"To fine - tune BioBERT for QA , we used the same BERT architecture used for SQuAD .",method,Fine-tuning BioBERT,0,90,44,16,0,method : Fine-tuning BioBERT,0.4522613065326633,0.88,0.7272727272727273,To fine tune BioBERT for QA we used the same BERT architecture used for SQuAD ,16,Question answering is a task of answering questions posed in natural language given related passages .,We used the BioASQ factoid datasets because their format is similar to that of SQuAD .,method
natural_language_inference,51,"We can avoid this phenomenon by minimizing a regularization loss defined as the following , Neural Universal Turing Machine",method,Neural Stored-program Memory,0,81,19,18,0,method : Neural Stored-program Memory,0.3056603773584905,0.2676056338028169,0.6923076923076923,We can avoid this phenomenon by minimizing a regularization loss defined as the following Neural Universal Turing Machine,18,"When this happens , pt is a balanced mixture of all programs regardless of the query key and thus having multiple programs is useless .","We can avoid this phenomenon by minimizing a regularization loss defined as the following , Neural Universal Turing Machine",method
sentiment_analysis,29,"In the Bi - LSTM network , the final output hidden states h s ? R n 2d hare generated by concatenating ? ? h sand ? ? h s .",method,Problem Definition,0,86,25,24,0,method : Problem Definition,0.4942528735632184,0.4716981132075472,0.6153846153846154,In the Bi LSTM network the final output hidden states h s R n 2d hare generated by concatenating h sand h s ,24,We generate another state sequence ? ? h s by feeding s into another backward LSTM .,"In the Bi - LSTM network , the final output hidden states h s ? R n 2d hare generated by concatenating ? ? h sand ? ? h s .",method
text-to-speech_synthesis,1,We propose a focus rate F to measure how an attention head is close to diagonal :,system description,Duration Predictor,0,101,40,12,0,system description : Duration Predictor,0.4611872146118721,0.8888888888888888,0.7058823529411765,We propose a focus rate F to measure how an attention head is close to diagonal ,17,"There are multiple attention alignments due to the multihead self - attention , and not all attention heads demonstrate the diagonal property ( the phoneme and mel-spectrogram sequence are monotonously aligned ) .","= 1 SS s=1 max 1?t ? T a s ,t , where Sand T are the lengths of the ground - truth spectrograms and phonemes , a s ,t donates the element in the s - th row and t- th column of the attention matrix .",method
text-classification,4,Note that the corresponding model can be readily adapted to other sentence matching problems as well ( see Section 5.2 ) .,model,Extension to text sequence matching,0,95,48,20,0,model : Extension to text sequence matching,0.4298642533936652,0.5333333333333333,0.3508771929824561,Note that the corresponding model can be readily adapted to other sentence matching problems as well see Section 5 2 ,21,"In this section , we will describe the proposed Adaptive Question Answering ( AdaQA ) model in the context of answer sentence selection task .","Given a factual question q ( associated with a list of candidate answers {a 1 , a 2 , . . . , am } and their corresponding labels y = {y 1 , y 2 , . . . , y m } ) , the goal of the model is to identify the correct answers from the set of candidates .",method
natural_language_inference,14,The Situations With Adversarial Generations ( SWAG ) dataset introduced a large - scale benchmark for commonsense question answering in the form of multiple choice sentence completion questions describing situations as observed in video .,introduction,introduction,0,17,4,4,0,introduction : introduction,0.09941520467836257,0.21052631578947367,0.21052631578947367,The Situations With Adversarial Generations SWAG dataset introduced a large scale benchmark for commonsense question answering in the form of multiple choice sentence completion questions describing situations as observed in video ,32,The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text .,"However , while SWAG was constructed to be resistant to certain baseline algorithms , powerful subsequent methods were able to perform very well on the dataset .",introduction
natural_language_inference,63,"We train our model for 12 epochs , and batch size is set to 30 .",implementation,Implementation Details,1,141,6,6,0,implementation : Implementation Details,0.7833333333333333,0.8571428571428571,0.8571428571428571,We train our model for 12 epochs and batch size is set to 30 ,15,"The optimizer is AdaDelta ( Zeiler , 2012 ) with an initial learning rate of 0.5 .","During the training , we keep the exponential moving average of weights with 0.001 decay and use these averages at test time .",experiment
named-entity-recognition,2,Multi - Scale Context Aggregation,model,Multi-Scale Context Aggregation,0,82,37,1,0,model : Multi-Scale Context Aggregation,0.38497652582159625,0.6271186440677966,0.1,Multi Scale Context Aggregation,4, , ,method
relation_extraction,7,"As for the second challenge , a robust model could extract precise relation features even from low - quality sentences containing noisy words .",introduction,introduction,0,30,19,19,0,introduction : introduction,0.11583011583011585,0.5588235294117647,0.5588235294117647,As for the second challenge a robust model could extract precise relation features even from low quality sentences containing noisy words ,22,"Moreover , word - level attention has been leveraged to alleviate the impact of noisy words , but it weakens the importance of entity features for relation extraction .","However , previous neural methods are always lacking in robustness because parameters are initialized randomly and hard to tune with noisy training data , resulting in the poor performance of extractors .",introduction
negation_scope_resolution,0,They used this system on the BioScope Corpus and outperformed all existing systems on the BioScope Abstracts .,approach,Deep Learning Approaches,0,141,64,5,0,approach : Deep Learning Approaches,0.6130434782608696,0.7032967032967034,0.15625,They used this system on the BioScope Corpus and outperformed all existing systems on the BioScope Abstracts ,18,"They used Convolutional Neural Networks to path features to generate embeddings , which they concatenated with position features and fed to a softmax layer to compute confidence scores of its location labels .",looked at neural networks for scope detection .,method
natural_language_inference,31,"In the final block , the alignment results take consideration of the semantics and structures of the whole sentences .",analysis,Analysis,0,239,56,56,0,analysis : Analysis,0.8659420289855072,0.918032786885246,0.918032786885246,In the final block the alignment results take consideration of the semantics and structures of the whole sentences ,19,"parked next to "" is associated mostly with "" bike "" and "" door "" since there is a weaker direct connection between "" parked "" and "" chained "" .","The word "" parked "" is strongly associated with "" chained "" and "" next to "" is aligned with "" to the "" following "" chained "" .",result
natural_language_inference,68,We then define ? ? z i in a similar way and finally define ? ? hr i to be the hidden representation at position i produced by the match - LSTM in the reverse direction .,method,Answer Pointer Layer,0,129,79,27,0,method : Answer Pointer Layer,0.5180722891566265,0.6929824561403509,0.4354838709677419,We then define z i in a similar way and finally define hr i to be the hidden representation at position i produced by the match LSTM in the reverse direction ,32,"Note that the parameters here ( W q , W p , W r , b p , wand b ) are the same as used in Eqn .",We then define ? ? z i in a similar way and finally define ? ? hr i to be the hidden representation at position i produced by the match - LSTM in the reverse direction .,method
question_generation,1,"where M is the total number of samples , ? is a constant , which controls both the loss .",method,Cost function,0,151,71,4,0,method : Cost function,0.3852040816326531,0.7802197802197802,0.4444444444444444,where M is the total number of samples is a constant which controls both the loss ,17,The total loss is :,"where M is the total number of samples , ? is a constant , which controls both the loss .",method
sentiment_analysis,25,GCAE improves the performance by 1.1 % to 2.5 % compared with ATAE - LSTM .,analysis,ACSA,1,177,11,10,0,analysis : ACSA,0.7972972972972973,0.3055555555555556,0.4166666666666667,GCAE improves the performance by 1 1 to 2 5 compared with ATAE LSTM ,15,"Moreover , the attention scores generated by the similarity scoring function are for the entire context vector .","First , our model incorporates GTRU to control the sentiment information flow according to the given aspect information at each dimension of the context vectors .",result
named-entity-recognition,5,- LSTM also gives highly competitive results when compared with existing methods in the literature .,result,Final Results for Classification,0,172,8,8,0,result : Final Results for Classification,0.8229665071770335,0.27586206896551724,0.6153846153846154, LSTM also gives highly competitive results when compared with existing methods in the literature ,16,Observations on CNN and hierarchical attention are consistent with the development results .,"As shown in , among the 16 datasets of , S - LSTM gives the best results on 12 , compared with BiLSTM and 2 layered BiL - STM models .",result
natural_language_inference,62,"For both the passage and the question , we pass the concatenation of the word embeddings and the character embeddings through a shared dense layer with ReLU activation , whose output dimensionality is d .",architecture,Overall Architecture,0,101,6,6,0,architecture : Overall Architecture,0.4508928571428572,0.07792207792207792,0.15384615384615385,For both the passage and the question we pass the concatenation of the word embeddings and the character embeddings through a shared dense layer with ReLU activation whose output dimensionality is d ,33,"For each word , we use the pre-trained Glo Ve word vector as its word embedding , and obtain its character embedding with a Convolutional Neural Network ( CNN ) .",Therefore we obtain the passage lexicon embeddings LP ? R dn and the question lexicon embeddings L Q ? R dm .,method
question-answering,7,"For this task , a linear mapping layer transforms the 300 - D word embeddings to the 512- D LSTM inputs .",experiment,Answer Sentence Selection,1,176,54,15,0,experiment : Answer Sentence Selection,0.64,0.7012987012987013,0.625,For this task a linear mapping layer transforms the 300 D word embeddings to the 512 D LSTM inputs ,20,The word embeddings are pre-trained 300 - D Glove 840B vectors .,presents the results of our model and the previous models for the task .,experiment
natural_language_inference,83,"Accordingly , our model also primarily used the aspect analyzing events in this story , which is indicated by the long light grey block in its weight bar .",ablation,Ablation Study,0,207,13,13,0,ablation : Ablation Study,0.6764705882352942,0.6190476190476191,0.6190476190476191,Accordingly our model also primarily used the aspect analyzing events in this story which is indicated by the long light grey block in its weight bar ,27,human reader can guess from commonsense knowledge that people usually recover ( correct ending ) after being hurt and do not repeat their mistake ( incorrect ending ) .,"Also , we can see that the topic of both the options is consistent with the story , and the model gave a very small weight to the Topical Consistency aspect indicated by the almost indiscernible black block in its weight bar .",result
natural_language_inference,23,Question : Who was one prominent Huguenot - descended arms manufacturer ? Answer : E.I. du Pont Fusion,model,FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0,495,102,79,0,model : FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0.9649122807017544,0.85,0.9294117647058824,Question Who was one prominent Huguenot descended arms manufacturer Answer E I du Pont Fusion,15,Westinghouse was one prominent Neptune arms manufacturer .,Question : Who was one prominent Huguenot - descended arms manufacturer ? Answer : E.I. du Pont Fusion,method
text-classification,5,"No matter how diverse the general - domain data used for pretraining is , the data of the target task will likely come from a different distribution .",system description,Target task LM fine-tuning,0,83,32,2,0,system description : Target task LM fine-tuning,0.32936507936507936,0.3368421052631579,0.054054054054054064,No matter how diverse the general domain data used for pretraining is the data of the target task will likely come from a different distribution ,26, ,We thus fine - tune the LM on data of the target task .,method
natural_language_inference,54,"Experimental results on SQuAD dataset illustrates that the syntactic information helps the model to choose the answers thatare both succinct and grammatically coherent , which boosted the performance on both qualitative studies and numerical results .",introduction,introduction,0,27,20,20,0,introduction : introduction,0.11790393013100435,0.9090909090909092,0.9090909090909092,Experimental results on SQuAD dataset illustrates that the syntactic information helps the model to choose the answers thatare both succinct and grammatically coherent which boosted the performance on both qualitative studies and numerical results ,35,"In this paper , we propose Structural Embedding of Syntactic Trees ( SEST ) that encode syntactic information structured by constituency tree and dependency tree into neural attention models for the question answering task .","Our focus is to show adding structural embedding can improve baseline models , rather than directly compare to published SQuAD results .",introduction
natural_language_inference,92,"In practice , Z is defined in a task - specific manner , as we will see in Section 4 .",method,Setup,0,70,17,15,0,method : Setup,0.24305555555555555,0.17346938775510204,0.75,In practice Z is defined in a task specific manner as we will see in Section 4 ,18,"We assume it contains one solution that we want to learn to produce , and potentially many other spurious ones .","At inference time , the model produces a solutionz ? Z tot from an input x with respect to ? and predicts the final answer as f ( z ) .",method
text_summarization,9,Other works which have recently been proposed for the problem of sentence summarization include .,system description,93,0,35,7,7,0,system description : 93,0.2287581699346405,0.7777777777777778,0.7777777777777778,Other works which have recently been proposed for the problem of sentence summarization include ,15,"Later , along the lines of , MOSES was used directly as a method for text simplification by .",Very recently proposed a neural attention model for this problem using a new data set for training and showing state - of - the - art performance on the DUC tasks .,method
text-to-speech_synthesis,0,"Our internal dataset contains 184243 training words , 10837 validation words , 21678 test words , which includes uppercase and lowercase letters and stress markings .",dataset,Datasets,0,103,6,6,0,dataset : Datasets,0.6358024691358025,0.5454545454545454,0.5454545454545454,Our internal dataset contains 184243 training words 10837 validation words 21678 test words which includes uppercase and lowercase letters and stress markings ,23,"To be consistent with the previous works , stress markings are removed and the multiple pronunciations are retained .",We keep the stress markings in training and ignore the stress during test .,experiment
natural_language_inference,28,"By treating the RNN weights as memory determined by the current input data , a larger memory size is provided and less trainable parameters are required .",approach,ASSOCIATIVE MEMORY APPROACH,0,74,34,12,0,approach : ASSOCIATIVE MEMORY APPROACH,0.2730627306273063,0.9444444444444444,0.8571428571428571,By treating the RNN weights as memory determined by the current input data a larger memory size is provided and less trainable parameters are required ,26,"In particular , At is determined by A t?1 , h t?1 and x t which can be apart of a multi - layer or a Hopfiled net .",This significantly increases the memorization ability of RNN .,method
sentiment_analysis,9,". The LCF - ATEPC model proposed in this paper integrates the pre-trained BERT model , significantly improves both the performance of ATE task and APC subtask , and achieves new state - of - the - art performance especially the F 1 score of ATE task .",introduction,introduction,0,44,30,30,0,introduction : introduction,0.1588447653429603,0.7894736842105263,0.7894736842105263, The LCF ATEPC model proposed in this paper integrates the pre trained BERT model significantly improves both the performance of ATE task and APC subtask and achieves new state of the art performance especially the F 1 score of ATE task ,43,". This paper firstly applies self - attention and local context focus techniques to aspect word extraction task , and fully explore their potential in aspect term extraction task .","Besides , we adopted the domain - adapted BERT model trained on the domain - related",introduction
sentiment_analysis,24,We use the same structure as encoders in IMN .,model,Models under Comparison,0,197,8,8,0,model : Models under Comparison,0.6334405144694534,0.3333333333333333,0.3333333333333333,We use the same structure as encoders in IMN ,10,It utilizes a multi - layer CNN structure with both general - purpose and domainspecific embeddings .,"For AS , we use ATAE - LSTM ( denoted as ALSTM for short ) and the model from which we denote as d Trans .",method
natural_language_inference,32,They consist of a sequence of question / answer pairs where questions can only be understood along with the conversation history .,system description,BACKGROUND: MACHINE COMPREHENSION,0,30,21,21,0,system description : BACKGROUND: MACHINE COMPREHENSION,0.06864988558352403,0.5675675675675675,0.7,They consist of a sequence of question answer pairs where questions can only be understood along with the conversation history ,21,Recently proposed conversational machine comprehension ( MC ) datasets aim to enable models to assist in such information seeking dialogs .,illustrates this new challenge .,method
sentiment_analysis,15,"The recursive models work very well on shorter phrases , where negation and composition are important , while bag of features baselines perform well only with longer sentences .",experiment,Fine-grained Sentiment For All Phrases,1,220,24,6,0,experiment : Fine-grained Sentiment For All Phrases,0.8148148148148148,0.3478260869565217,0.75,The recursive models work very well on shorter phrases where negation and composition are important while bag of features baselines perform well only with longer sentences ,27,"The RNTN gets the highest performance , followed by the MV - RNN and RNN .",The RNTN accuracy upper bounds other models at most n-gram lengths .,experiment
relation-classification,0,This bidirectional structure propagates to each node not only the information from the leaves but also information from the root .,model,Dependency Layer,0,88,40,6,0,model : Dependency Layer,0.3893805309734513,0.5194805194805194,0.3,This bidirectional structure propagates to each node not only the information from the leaves but also information from the root ,21,"We employ bidirectional tree - structured LSTM - RNNs ( i.e. , bottom - up and top - down ) to represent a relation candidate by capturing the dependency structure around the target word pair .","This is especially important for relation classification , which makes use of argument nodes near the bottom of the tree , and our top - down LSTM - RNN sends information from the top of the tree to such near - leaf nodes ( unlike in standard bottom - up LSTM - RNNs ) .",method
natural_language_inference,42,Our interpretation of that behavior is that the position encoding is somehow dissolved in the matrix reduction process .,system description,Reduction Layer,0,185,145,15,0,system description : Reduction Layer,0.6401384083044983,0.8285714285714286,0.5769230769230769,Our interpretation of that behavior is that the position encoding is somehow dissolved in the matrix reduction process ,19,"To incorporate that information before discarding it , we could add a large processing layer followed by a matrix reduction , but our experiments have shown that this approach does not yield positive results in FABIR .",possible solution is to process embeddings of size d input and encodings of size d model independently and thus limit the reduction operation to the embeddings .,method
relation_extraction,5,"where = A + I with I being then n identity matrix , and d i = n j=1 ij is the degree of token i in the resulting graph . :",model,Graph Convolutional Networks over Dependency Trees,0,50,15,13,0,model : Graph Convolutional Networks over Dependency Trees,0.19011406844106465,0.22388059701492533,0.5652173913043478,where A I with I being then n identity matrix and d i n j 1 ij is the degree of token i in the resulting graph ,28,"We resolve these issues by normalizing the activations in the graph convolution before feeding it through the nonlinearity , and adding self - loops to each node in the graph :",Relation extraction with a graph convolutional network .,method
named-entity-recognition,0,"There are theoretical and empirical evidences to verify that compared with 2 or 1 norms , the p - norm is more able to prevent outlier elements from dominating the objective , enhancing the robustness ) .",system description,Accelerated Robust Subset Selection (ARSS),0,107,85,9,0,system description : Accelerated Robust Subset Selection (ARSS),0.3948339483394834,0.4775280898876405,0.08823529411764706,There are theoretical and empirical evidences to verify that compared with 2 or 1 norms the p norm is more able to prevent outlier elements from dominating the objective enhancing the robustness ,33,"To boost the robustness against outliers in both samples and features , we formulate the discrepancy between X and XA via the p ( 0 < p < 1 ) - norm .","Thus , we have the following objective min",method
natural_language_inference,41,"In both of these tasks , information is copied from the input but manipulated , which is closely related to the denoising pre-training objective .",architecture,Sequence Generation Tasks,0,76,38,4,0,architecture : Sequence Generation Tasks,0.29571984435797666,0.42696629213483145,0.8,In both of these tasks information is copied from the input but manipulated which is closely related to the denoising pre training objective ,24,"Because BART has an autoregressive decoder , it can be directly fine tuned for sequence generation tasks such as abstractive question answering and summarization .","Here , the encoder input is the input sequence , and the decoder generates outputs autoregressively .",method
sentiment_analysis,23,Therefore structural feature learning becomes effective .,system description,Tree-based Convolution,0,92,56,18,0,system description : Tree-based Convolution,0.3150684931506849,0.5333333333333333,0.6666666666666666,Therefore structural feature learning becomes effective ,7,"From the designed architecture ( ) , we see that our TBCNN models allow short propagation paths between the output layer and any position in the tree .",Several main technical points in tree - based convolution include :,method
question_generation,1,"We conduct our experiments on Visual Question Generation ( VQG ) dataset , which contains human annotated questions based on images of MS - COCO dataset .",dataset,Dataset,0,173,2,2,0,dataset : Dataset,0.4413265306122449,0.16666666666666666,0.16666666666666666,We conduct our experiments on Visual Question Generation VQG dataset which contains human annotated questions based on images of MS COCO dataset ,23, ,This dataset was developed for generating natural and engaging questions based on commonsense reasoning .,experiment
natural_language_inference,89,"As for answer verifiers , we use the original configuration from for Model - I. For Model - II , the Adam optimizer ( Kingma and Ba 2014 ) with a learning rate of 0.0008 is used , the hidden size is set as 300 , and a dropout ) of 0.3 is applied for preventing overfitting .",implementation,Implementation,1,179,7,7,1,implementation : Implementation,0.6884615384615385,0.6363636363636364,0.6363636363636364,As for answer verifiers we use the original configuration from for Model I For Model II the Adam optimizer Kingma and Ba 2014 with a learning rate of 0 0008 is used the hidden size is set as 300 and a dropout of 0 3 is applied for preventing overfitting ,51,"Based on the performance on development set , we set ? as 0.3 and ? to be 1 .","The batch size is 48 for the reader , 64 for Model - II , and 32 for Model - I as well as Model - III .",experiment
relation-classification,0,Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM - RNNs on bidirectional sequential LSTM - RNNs .,abstract,abstract,0,5,3,3,0,abstract : abstract,0.02212389380530973,0.375,0.375,Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM RNNs on bidirectional sequential LSTM RNNs ,27,We present a novel end - to - end neural model to extract entities and relations between them .,This allows our model to jointly represent both entities and relations with shared parameters in a single model .,abstract
phrase_grounding,0,"Then , in Section 3.3 we describe how we choose the most representative visual feature level for the given text .",method,Method,0,78,5,5,0,method : Method,0.3466666666666667,0.06666666666666668,0.8333333333333334,Then in Section 3 3 we describe how we choose the most representative visual feature level for the given text ,21,In Section 3.2 we describe how we calculate multi-level multimodal attention map and attended visual feature for each word / sentence .,"Finally , in Section 3.4 we define a multimodal loss to train the whole model with weak supervision .",method
natural_language_inference,97,"Each window is treated like a sentence in the previous subsection , but we include a location - based weight ?( k ) .",model,Sequential Sliding Window,0,141,59,6,0,model : Sequential Sliding Window,0.4828767123287671,0.5267857142857143,0.5454545454545454,Each window is treated like a sentence in the previous subsection but we include a location based weight k ,20,"The sliding window scans over the words of the text as one continuous sequence , without sentence breaks .","This weight is based on a word 's position in the window , which , given a window , depends on its global position k.",method
relation-classification,0,"Many traditional , feature - based relation classification models extract features from both sequences and parse trees .",introduction,introduction,0,25,15,15,0,introduction : introduction,0.11061946902654868,0.625,0.625,Many traditional feature based relation classification models extract features from both sequences and parse trees ,16,"For instance , dependencies between words are not enough to predict that source and U.S. have an ORG - AFF relation in the sentence "" This is ... "" , one U.S. source said , and the context word said is required for this prediction .","However , previous RNNbased models focus on only one of these linguistic structures .",introduction
relation_extraction,9,We then rank all such trigrams in the sentences in the test set according to their total contribution and list the top - ranked trigrams . *,architecture,Our Architectures,0,186,15,15,0,architecture : Our Architectures,0.8899521531100478,0.4411764705882353,0.4411764705882353,We then rank all such trigrams in the sentences in the test set according to their total contribution and list the top ranked trigrams ,25,"In the network , we trace back the trigram that contributed most to the correct classification in terms of ? ? ( S i , y ) for each sentence Si .","In , we see that these are indeed very informative for deducing the relation .",method
semantic_role_labeling,2,"We present detailed error analyses to better understand the performance gains , including ( 1 ) design choices on architecture , initialization , and regularization that have a surprisingly large impact on model performance ; ( 2 ) different types of prediction errors showing , e.g. , that deep models excel at predicting long - distance dependencies but still struggle with known challenges such as PPattachment errors and adjunct - argument distinctions ; ( 3 ) the role of syntax , showing that there is significant room for improvement given oracle syntax but errors from existing automatic parsers prevent effective use in SRL .",introduction,introduction,0,18,11,11,0,introduction : introduction,0.08035714285714286,0.6875,0.6875,We present detailed error analyses to better understand the performance gains including 1 design choices on architecture initialization and regularization that have a surprisingly large impact on model performance 2 different types of prediction errors showing e g that deep models excel at predicting long distance dependencies but still struggle with known challenges such as PPattachment errors and adjunct argument distinctions 3 the role of syntax showing that there is significant room for improvement given oracle syntax but errors from existing automatic parsers prevent effective use in SRL ,89,We also report performance with predicted predicates to encourage future exploration of end - to - end SRL systems .,"In summary , our main contributions incluede :",introduction
question_answering,2,"In this paper , we call the format visual - text sequence data .",introduction,introduction,0,24,14,14,0,introduction : introduction,0.08664259927797834,0.3684210526315789,0.3684210526315789,In this paper we call the format visual text sequence data ,12,"At each time there are visual data , text annotations and other metadata .","Note that not all the photos and videos are annotated , which requires a robust method to leverage inconsistently available multimodal data .",introduction
sentence_compression,3,h( w i ) = 5 if w i is in the headline ; h( w i ) = 1 otherwise .,method,Comparison Methods,0,75,5,5,0,method : Comparison Methods,0.6048387096774194,0.2777777777777778,0.2777777777777778,h w i 5 if w i is in the headline h w i 1 otherwise ,17,"2 ) whether a word w i is retained should partly depend on the word importance score that is the product of the TF - IDF score and headline score h( w i ) , tf - idf ( w i ) h ( w i ) where h( w i ) represents that whether a word ( limited to nouns and verbs ) is also in the headline .","3 ) the dependency relations , ROOT , dobj , nsubj , pobj , should be retained as they are the skeletons of a sentence .",method
sentiment_analysis,39,However we can find many discussions and threads on several blogs and question answering platforms that discuss aspects of are as in many cities around the world .,introduction,introduction,0,33,19,19,0,introduction : introduction,0.1346938775510204,0.4634146341463415,0.4634146341463415,However we can find many discussions and threads on several blogs and question answering platforms that discuss aspects of are as in many cities around the world ,28,Currently there are no dedicated platforms for reviewing and rating aspects of neighbourhoods of a city .,"In general , these conversations are very comprehensible : they often contain specific information about several aspects of several neighbourhoods .",introduction
sentiment_analysis,11,"els. CMN succeeds over both neural and methods by 3.3 % and 8.12 % , respectively .",baseline,bc-LSTM:,0,283,29,24,0,baseline : bc-LSTM:,0.8226744186046512,0.90625,0.8888888888888888,els CMN succeeds over both neural and methods by 3 3 and 8 12 respectively ,16,We assert significance when p < 0.05 under McNemar 's test .,Improvement in performance is seen for all emotions over the ensemble - SVM based method .,result
sentiment_analysis,12,Details of Our Approach,approach,Details of Our Approach,0,80,15,1,0,approach : Details of Our Approach,0.35714285714285715,0.2083333333333333,0.02127659574468085,Details of Our Approach,4, , ,method
sentiment_analysis,22,Keras is used for implementing our neural network model .,experiment,experiment,1,132,9,9,0,experiment : experiment,0.5764192139737991,0.8181818181818182,0.8181818181818182,Keras is used for implementing our neural network model ,10,The dimension of position embeddings is set to 50 .,"In model training , we set the learning rate to 0.001 , the batch size to 64 , and dropout rate to 0.5 .",experiment
natural_language_inference,0,detailed analysis of these differences is studied in the next section .,performance,Performance Comparison,0,134,5,5,0,performance : Performance Comparison,0.6802030456852792,0.2272727272727273,0.2272727272727273,detailed analysis of these differences is studied in the next section ,12,"GA Reader -- refers to an earlier version of the model , unpublished but described in a preprint , with the following differences - ( 1 ) it does not utilize token - specific attentions within the GA module , as described in equation , ( 2 ) it does not use a character composition model , ( 3 ) it is initialized with word embeddings pretrained on the corpus itself rather than Glo Ve .","Here we present 4 variants of the latest GA Reader , using combinations of whether the qe-comm feature is used ( + feature ) or not , and whether the word lookup table L ( w ) is updated during training or fixed to its initial value .",result
named-entity-recognition,3,Semi-supervised sequence tagging with bidirectional language models,title,title,1,2,1,1,0,title : title,0.010810810810810813,1.0,1.0,Semi supervised sequence tagging with bidirectional language models,8, , ,title
relation-classification,8,Extracting Multiple - Relations in One - Pass with Pre-Trained Transformers,title,title,1,2,1,1,0,title : title,0.0145985401459854,1.0,1.0,Extracting Multiple Relations in One Pass with Pre Trained Transformers,10, , ,title
text_generation,3,"As our model is constructed for dialogue generation , we design the mapping module to ensure that the generated response is semantically consistent with the source .",approach,Mapping Module,0,62,30,2,0,approach : Mapping Module,0.4397163120567376,0.8571428571428571,0.2857142857142857,As our model is constructed for dialogue generation we design the mapping module to ensure that the generated response is semantically consistent with the source ,26, ,There are many matching models that can be used to learn such dependency relations .,method
sentiment_analysis,1,"To leverage the differential asymmetry information , we initialize the global inter-channel relations in A to [ ?1 , 0 ] as follows :",system description,Adjacency Matrix in RGNN,0,198,126,21,0,system description : Adjacency Matrix in RGNN,0.5,0.6086956521739131,0.75,To leverage the differential asymmetry information we initialize the global inter channel relations in A to 1 0 as follows ,21,"The selection of global channels is supported by prior studies showing that the asymmetry in neuronal activities between the left and right hemispheres is informative in valence and arousal predictions , , .","where ( i , j ) denotes the indices of empirically selected symmetric channel pairs that balance wiring cost and global efficiency : ( FP1 , FP2 ) , ( AF3 , AF4 ) , ( F5 , F6 ) , ( FC5 , FC6 ) , ( C5 , C6 ) , ( CP5 , CP6 ) , ( P5 , P6 ) , ( PO5 , PO6 ) , and ( O1 , O2 ) .",method
text_summarization,10,Empirical results in 8 prove that soft - sharing method is statistically significantly better than hard - sharing with p < 0.001 in all metrics .,ablation,Ablation and Analysis Studies,1,194,4,4,0,ablation : Ablation and Analysis Studies,0.7376425855513308,0.13793103448275862,0.3333333333333333,Empirical results in 8 prove that soft sharing method is statistically significantly better than hard sharing with p 0 001 in all metrics ,24,"As described in Sec. 4.2 , we choose soft - sharing over hard - sharing because of the more expressive parameter sharing it provides to the model .",Comparison of Different Layer - Sharing Methods,result
semantic_role_labeling,1,"Denoting the attention weight from token t to a candidate head q as A parse [ t , q ] , we model the probability of token t having parent q as :",model,Syntactically-informed self-attention,0,92,59,12,0,model : Syntactically-informed self-attention,0.4220183486238532,0.6704545454545454,0.5714285714285714,Denoting the attention weight from token t to a candidate head q as A parse t q we model the probability of token t having parent q as ,29,"We apply auxiliary supervision at this attention head to encourage it to attend to each token 's parent in a syntactic dependency tree , and to encode information about the token 's dependency label .",using the attention weights A parse [ t ] as the distribution over possible heads for token t.,method
relation_extraction,5,"In our study , we observed a high correlation between the entity mentions in a sentence and its relation label in the SemEval dataset .",model,Entity Bias in the SemEval Dataset,0,208,10,2,0,model : Entity Bias in the SemEval Dataset,0.7908745247148289,0.4545454545454545,0.14285714285714285,In our study we observed a high correlation between the entity mentions in a sentence and its relation label in the SemEval dataset ,24, ,"We experimented with PA - LSTM models to analyze this dependency tree corresponding to K = 1 in path-centric pruning is shown , and the shortest dependency path is thickened .",method
sentiment_analysis,24,"This is done by extracting explicit aspect mentions , referred to as aspect term extraction ( AE ) , and detecting the sentiment orientation towards each extracted aspect term , referred to as aspect - level sentiment classification ( AS ) .",introduction,introduction,1,12,3,3,0,introduction : introduction,0.03858520900321544,0.15,0.15,This is done by extracting explicit aspect mentions referred to as aspect term extraction AE and detecting the sentiment orientation towards each extracted aspect term referred to as aspect level sentiment classification AS ,34,Aspect - based sentiment analysis ( ABSA ) aims to determine people 's attitude towards specific aspects in a review .,"For example , in the sentence "" Great food but the service is dreadful "" , the aspect terms are "" food "" and "" service "" , and the sentiment orientations towards them are positive and negative respectively .",introduction
natural_language_inference,72,"Thereafter , tapering of corticosteroids was initiated with no clinical relapse .",introduction,introduction,0,19,11,11,0,introduction : introduction,0.0608974358974359,0.22916666666666666,0.22916666666666666,Thereafter tapering of corticosteroids was initiated with no clinical relapse ,11,The patient was then subjected to a thoracic CT scan that also showed significant radiological improvement .,The patient was discharged after being treated for a total of 30 days and continued receiving antituberculous therapy with no reported problems for a total of 6 months under the supervision of his hometown physicians .,introduction
sentiment_analysis,30,"Essentially , gate ? ? g j i determines how much the j - th memory should be updated , factoring in three elements : ( 1 ) how similar the current input w i is to the entity being tracked ( k j ) ; ( 2 ) how related the current input w i is to the state of the j - th entity ( ? ? h j i?1 ) ; and how past activation should influence the current one .",methodology,Methodology,0,52,25,25,0,methodology : Methodology,0.40625,0.4807692307692308,0.4807692307692308,Essentially gate g j i determines how much the j th memory should be updated factoring in three elements 1 how similar the current input w i is to the entity being tracked k j 2 how related the current input w i is to the state of the j th entity h j i 1 and how past activation should influence the current one ,66,"where is the Hadamard product , and ? ? h j i is the unnormalised memory representation for the j - th entity .","Essentially , gate ? ? g j i determines how much the j - th memory should be updated , factoring in three elements : ( 1 ) how similar the current input w i is to the entity being tracked ( k j ) ; ( 2 ) how related the current input w i is to the state of the j - th entity ( ? ? h j i?1 ) ; and how past activation should influence the current one .",method
natural_language_inference,100,"This work was supported in part by the Center for Intelligent Information Retrieval , in part by NSF IIS - 1160894 , and in part by NSF grant # IIS - 1419693 .",ACKNOWLEDGMENTS,ACKNOWLEDGMENTS,0,365,2,2,0,ACKNOWLEDGMENTS : ACKNOWLEDGMENTS,0.9972677595628416,0.6666666666666666,0.6666666666666666,This work was supported in part by the Center for Intelligent Information Retrieval in part by NSF IIS 1160894 and in part by NSF grant IIS 1419693 ,28, ,"Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor .",others
sarcasm_detection,1,"To further test the effectiveness of our user embeddings , we perform experiments on a subset of Main , comprising of forums associated with the topic of politics .",dataset,Dataset,0,238,17,17,0,dataset : Dataset,0.7125748502994012,0.7083333333333334,0.7083333333333334,To further test the effectiveness of our user embeddings we perform experiments on a subset of Main comprising of forums associated with the topic of politics ,27,"Specifically , we maintain a 20 ? 80 ratio ( approx . ) between the sarcastic and non-sarcastic comments in both training / testing sets .",The choice of using SARC for our experiments comes with multiple reasons .,experiment
text-classification,7,"Then , we use routingby - agreement to produce parent capsules feature maps totally ( L?K 1 ? K 2 + 2 ) D d-dimensional capsules in this layer .",model,Convolutional Capsule Layer,0,115,77,9,0,model : Convolutional Capsule Layer,0.4732510288065844,0.8191489361702128,0.9,Then we use routingby agreement to produce parent capsules feature maps totally L K 1 K 2 2 D d dimensional capsules in this layer ,26,"When the transformation matrices are shared across the child capsules , each potential parent - capsule j|i is produced b? where b j|i is the capsule bias term , u i is a child capsule in a local region K 2 C and W c 1 j is the j th matrix in tensor W c 1 .","When using the non-shared weights across the child capsules , we replace the transformation matrix W c 1 j in Eq. ( 10 ) with W c 2 j .",method
sentiment_analysis,24,We also present the comparison results in a setting without using opinion term labels in Table 7 12 .,model,Model Comparison Details,0,302,22,22,0,model : Model Comparison Details,0.9710610932475884,0.7096774193548387,0.7096774193548387,We also present the comparison results in a setting without using opinion term labels in Table 7 12 ,19,It is computed by summing up the predicted probabilities on the opinion - related labels BP and IP for the ith token .,"In this setting , we modify the proposed IMN and IMN ?d to recognize aspect terms only",method
text_summarization,0,The rating score ranges from 1 to 3 and 3 is the best .,evaluation,evaluation,0,277,4,4,0,evaluation : evaluation,0.920265780730897,0.4,0.4,The rating score ranges from 1 to 3 and 3 is the best ,14,These annotators are all native speakers .,"We take the average score of all summaries as the final score of each model , as shown in .",result
sentiment_analysis,20,However our model differs in the way it incorporates topic information and in the attention mechanism .,model,TSA Model (topic-based),0,106,30,4,0,model : TSA Model (topic-based),0.5698924731182796,0.43478260869565216,0.16,However our model differs in the way it incorporates topic information and in the attention mechanism ,17,Our model is comparable to the work of .,"The network has two inputs , the sequence of words in the tweet X tw = ( x tw 1 , x tw 2 , ... , x tw Ttw ) , where T tw the number of words in the tweet , and the sequence of words in the topic X to = ( x to 1 , x to 2 , ... , x to Tto ) , where T to the number of words in the topic .",method
question_answering,3,MCQ datasets are evaluated using the standard accuracy metric .,dataset,Datasets,0,171,13,13,0,dataset : Datasets,0.6107142857142858,0.7647058823529411,0.7647058823529411,MCQ datasets are evaluated using the standard accuracy metric ,10,We focus on the summaries setting instead of reading full stories since our model is targetted at standard RC tasks .,"For RACE , we train models on the entire dataset , i.e. , both RACE - M and RACE - H , and evaluate them separately .",experiment
relation-classification,2,Entity and relation extraction includes the task of ( i ) identifying the entities ( described in Section 2.1 ) and ( ii ) extracting the relations among them ( described in Section 2.2 ) .,system description,Joint entity and relation extraction,0,90,43,2,0,system description : Joint entity and relation extraction,0.3050847457627119,0.7413793103448276,0.1176470588235294,Entity and relation extraction includes the task of i identifying the entities described in Section 2 1 and ii extracting the relations among them described in Section 2 2 ,30, ,Feature - based joint models have been proposed to simultaneously solve the entity recognition and relation extraction ( RE ) subtasks .,method
natural_language_inference,56,"For Sudoku , the graph has i ? { 1 , 2 , ... , 81 } nodes , one for each cell in the Sudoku .",system description,Recurrent Relational Networks,0,51,10,10,0,system description : Recurrent Relational Networks,0.15178571428571427,0.2631578947368421,0.2631578947368421,For Sudoku the graph has i 1 2 81 nodes one for each cell in the Sudoku ,18,The recurrent relational network will learn to pass messages on a graph .,"Each node has an input feature vector x i , and edges to and from all nodes thatare in the same row , column and box in the Sudoku .",method
natural_language_inference,77,"In general , these results are interesting in many ways .",model,model,0,164,24,24,0,model : model,0.6007326007326007,0.4615384615384616,0.7272727272727273,In general these results are interesting in many ways ,10,Beam - search slightly improves results which shows that the most probable start is not necessarily the start of the best answer span .,"For instance , it is surprising that a simple binary feature like wiq b can have such a dramatic effect on the over all performance .",method
natural_language_inference,33,STN + Fr + De + NLI :,evaluation,MULTI-TASK MODEL DETAILS,0,200,76,6,0,evaluation : MULTI-TASK MODEL DETAILS,0.7662835249042146,0.5547445255474452,0.3333333333333333,STN Fr De NLI ,5,"STN + Fr + De : The sentence representation h x is the concatenation of the final hidden vectors from a forward GRU with 1500 - dimensional hidden vectors and a bidirectional GRU , also with 1500 - dimensional hidden vectors .",The sentence representation h x is the concatenation of the final hidden vectors from a bidirectional GRU with 1500 - dimensional hidden vectors and another bidirectional GRU with 1500 - dimensional hidden vectors trained without NLI .,result
question-answering,1,"However , unlike recursive models , the convolutional architecture has a fixed depth , which bounds the level of composition it could do .",model,model,0,66,7,7,0,model : model,0.34196891191709844,0.15217391304347827,0.875,However unlike recursive models the convolutional architecture has a fixed depth which bounds the level of composition it could do ,21,"Second , our convolutional model can take supervised training and tune the parameters for a specific task , a property vital to our supervised learning - to - match framework .","For tasks like matching , this limitation can be largely compensated with a network afterwards that can take a "" global "" synthesis on the learned sentence representation .",method
prosody_prediction,0,The Minitagger SVM model 's test accuracies are slightly lower than the CRF 's with 80.8 % and 65.4 % test accuracies .,result,Results,1,113,9,9,0,result : Results,0.5885416666666666,0.2727272727272727,0.3,The Minitagger SVM model s test accuracies are slightly lower than the CRF s with 80 8 and 65 4 test accuracies ,23,"The traditional feature - based classifiers perform slightly below the neural network models , with the CRF obtaining 81.8 % and 66.4 % for the two classification tasks , respectively .",Finally taking a simple majority class per word gives 80.2 % for the 2 - way classification task and 62.4 % for the 3 - way classification task .,result
sentiment_analysis,16,"The main difference from MN is in its attention alignment function , which concatenates the distributed representations of the context and aspect , and uses an additional weight matrix for attention calculation , following the method introduced in .",experiment,Experiments,0,230,7,7,0,experiment : Experiments,0.7232704402515723,0.4375,0.5833333333333334,The main difference from MN is in its attention alignment function which concatenates the distributed representations of the context and aspect and uses an additional weight matrix for attention calculation following the method introduced in ,36,AMN : A state - of - the - art memory network used for ASC .,"BL - MN : Our basic memory network presented in Section 2 , which does not use the proposed techniques for capturing target - sensitive sentiments .",experiment
passage_re-ranking,0,An overview of the proposed method is shown in .,introduction,introduction,0,26,16,16,0,introduction : introduction,0.21311475409836064,0.6666666666666666,0.6666666666666666,An overview of the proposed method is shown in ,10,"Focusing on question answering , we train a sequence - to - sequence model , that given a document , generates possible questions that the document might answer .",We view this work as having several contributions :,introduction
natural_language_inference,77,Most such systems describe several innovations for the different layers of the architecture with a special focus on developing powerful interaction layer that aims at modeling word - by - word interaction between question and context .,introduction,introduction,0,17,8,8,0,introduction : introduction,0.06227106227106227,0.3478260869565217,0.3478260869565217,Most such systems describe several innovations for the different layers of the architecture with a special focus on developing powerful interaction layer that aims at modeling word by word interaction between question and context ,35,"typical neural architecture consists of an embedding - , encoding - , interaction - and answer layer .","Although a variety of extractive QA systems have been proposed , there is no competitive neural baseline .",introduction
text_summarization,4,Linked entity : https://en.wikipedia.org/wiki/Gold,result,Results,0,247,53,53,0,result : Results,0.9573643410852714,0.913793103448276,0.913793103448276,Linked entity https en wikipedia org wiki Gold,8,"E1.1 : andy roddick got the better of dmitry tursunov in straight sets on friday , assuring the @united states a # - # lead over defending champions russia in the #### davis cup final . 0.719 E1.2 : sir alex ferguson revealed friday that david beckham 's move to the @united states had not surprised him because he knew the midfielder would not return to england if he could not comeback to manchester united .","E2.1 : following is the medal standing at the ##th olympic winter games - lrb - tabulated under team , @gold , silver and bronze",result
sentiment_analysis,1,"Thus , we assume that participants are likely to generate emotions that have similar ratings in either valence or arousal dimensions , e.g. , both angry and happy have high arousal , but unlikely to generate emotions thatare faraway in both dimensions , e.g. , sad and happy are different in both valence and arousal .",system description,Emotion-aware Distribution Learning,0,259,187,20,0,system description : Emotion-aware Distribution Learning,0.6540404040404041,0.9033816425120772,0.8695652173913043,Thus we assume that participants are likely to generate emotions that have similar ratings in either valence or arousal dimensions e g both angry and happy have high arousal but unlikely to generate emotions thatare faraway in both dimensions e g sad and happy are different in both valence and arousal ,52,"Specifically , in the self - reported ratings , neutral , sad , fear , and happy movie ratings cluster in the zero valence zero arousal , negative valence negative arousal , negative valence positive arousal , and positive valence positive arousal regions , respectively .","After obtaining the converted class distributions ? , our model can be optimized by minimizing the following Kullback - Leibler ( KL ) divergence instead of ( 11 ) :",method
question_answering,0,"Hence , we choose two data sets that can be processed with Wikidata to compare the GGNNs to other models :",experiment,Data,0,143,4,3,0,experiment : Data,0.4847457627118644,0.17391304347826084,0.2,Hence we choose two data sets that can be processed with Wikidata to compare the GGNNs to other models ,20,We use Wikidata to show experimentally that GGNNs are better at learning representations of semantic graphs than previous approaches .,WebQSP - WD and QALD - 7 .,experiment
natural_language_inference,41,"For example , a simple language model achieves the best ELI5 performance , but the worst SQUAD results .",result,Results,0,133,6,6,0,result : Results,0.5175097276264592,0.24,0.6,For example a simple language model achieves the best ELI5 performance but the worst SQUAD results ,17,The effectiveness of pre-training methods is highly dependent on the task .,Token masking is crucial,result
temporal_information_extraction,1,"i:y i =r ? i ? i:? i =r ? i ) , ?r ? Y 10 return {w r } r?Y ( e.g. , perceptron , SVM , or even structured perceptron ; here we used the averaged perceptron ( Freund and Schapire , 1998 ) ) and subscript "" r "" means selecting the learned weight vector for relation r ? Y .",training,Semi-supervised Structured Learning,0,146,71,5,1,training : Semi-supervised Structured Learning,0.5680933852140078,0.6698113207547169,0.5,i y i r i i i r i r Y 10 return w r r Y e g perceptron SVM or even structured perceptron here we used the averaged perceptron Freund and Schapire 1998 and subscript r means selecting the learned weight vector for relation r Y ,49,"Given the inherent global constraints in temporal graphs , we propose to perform semi-supervised structured learning using the constraint - driven learning ( CoDL ) algorithm , as shown in Algorithm 2 , where the function "" Learn "" in Lines 2 and 9 represents any standard learning algorithm w r = w r + ?(","i:y i =r ? i ? i:? i =r ? i ) , ?r ? Y 10 return {w r } r?Y ( e.g. , perceptron , SVM , or even structured perceptron ; here we used the averaged perceptron ( Freund and Schapire , 1998 ) ) and subscript "" r "" means selecting the learned weight vector for relation r ? Y .",experiment
sentiment_analysis,24,"This operation is performed iteratively , allowing the information to be modified and propagated across multiple links as the number of iterations increases .",introduction,introduction,0,25,16,16,0,introduction : introduction,0.08038585209003216,0.8,0.8,This operation is performed iteratively allowing the information to be modified and propagated across multiple links as the number of iterations increases ,23,The information is then combined with the shared latent representation and made available to all tasks for further processing .,"In contrast to most multi-task learning schemes which share information through learning a common feature representation , IMN not only allows shared features , but also explicitly models the interactions between tasks through the message passing mechanism , allowing different tasks to better influence each other .",introduction
relation_extraction,9,"On the one hand , this leads to a seesaw phenomenon in the result curve , but on the other hand it enables us to obtain better - suited models with slightly higher F 1 scores .",architecture,Our Architectures,0,205,34,34,0,architecture : Our Architectures,0.9808612440191388,1.0,1.0,On the one hand this leads to a seesaw phenomenon in the result curve but on the other hand it enables us to obtain better suited models with slightly higher F 1 scores ,34,"It can be seen that Att-Input - CNN quite smoothly converges to its final F 1 score , while for the Att-Pooling - CNN model , which includes an additional attention layer , the joint effect of these two attention layer induces stronger back - propagation effects .", ,method
sentiment_analysis,13,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,method,method,0,230,14,14,0,method : method,0.8273381294964028,0.4516129032258064,0.4516129032258064,BERT leverages the vanilla BERT pre trained 12 https github com facebookresearch DrQA,13,"Lastly , to answer RQ1 , RQ2 , and RQ3 , we have the following BERT variants .",weights and fine - tunes on all 3 end tasks .,method
natural_language_inference,100,Copyrights for components of this work owned by others than ACM must be honored .,introduction,introduction,0,16,6,6,0,introduction : introduction,0.04371584699453552,0.14285714285714285,0.14285714285714285,Copyrights for components of this work owned by others than ACM must be honored ,15,"For instance , Surdeanu et al . Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page .",Abstracting with credit is permitted .,introduction
sentence_compression,3,"When further applying Evaluator - SLM , only a tiny improvement is observed ( &3 vs & 4 ) , not comparable to the improvement between # 3 and # 5 .",result,Result and Discussion,1,110,11,11,0,result : Result and Discussion,0.8870967741935484,0.7333333333333333,0.7333333333333333,When further applying Evaluator SLM only a tiny improvement is observed 3 vs 4 not comparable to the improvement between 3 and 5 ,24,"3 ) As for Google news dataset , LSTMs ( LSTM + pos+dep ) ( & 3 ) is a relatively strong baseline , suggesting that incorporating dependency relations and part - of - speech tags may help model learn the syntactic relations and thus make a better prediction .",This maybe due to the difference in perplexity of the our Evaluator - SLM .,result
part-of-speech_tagging,3,"For a training set { ( z i , y i ) } , the logarithm of the likelihood ( a.k.a. the log-likelihood ) is given by :",architecture,CRF,0,71,40,12,0,architecture : CRF,0.3497536945812808,0.8163265306122449,0.8571428571428571,For a training set z i y i the logarithm of the likelihood a k a the log likelihood is given by ,23,"For CRF training , we use the maximum conditional likelihood estimation .",Decoding is to search for the label sequence y * with the highest conditional probability :,method
smile_recognition,0,"Inspired by biological processes and exploiting the fact that nearby pixels are strongly correlated , CNNs are relatively insensitive to small translations or rotations of the image input .",system description,Deep neural networks,0,31,11,11,0,system description : Deep neural networks,0.34444444444444444,0.3333333333333333,0.3333333333333333,Inspired by biological processes and exploiting the fact that nearby pixels are strongly correlated CNNs are relatively insensitive to small translations or rotations of the image input ,28,"CNN consists of two layers : a convolutional layer , followed by a subsampling layer .",Training deep neural networks is slow due to the number of parameters in the model .,method
text_summarization,0,"To begin with , for a document",system description,Problem Formulation,0,90,3,3,0,system description : Problem Formulation,0.29900332225913623,0.06,0.375,To begin with for a document,6,"Before presenting our approach for the reader - aware summarization , we first introduce our notations and key concepts .","denotes the i - th word in document X d , and x c i , j denotes the j - th word in i -th comment sentence c i .",method
natural_language_inference,41,BART also opens up new ways of thinking about fine tuning .,introduction,introduction,0,27,16,16,0,introduction : introduction,0.10505836575875488,0.6956521739130435,0.6956521739130435,BART also opens up new ways of thinking about fine tuning ,12,"For example , it improves performance by 6 ROUGE over previous work on XSum .",We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers .,introduction
natural_language_inference,92,Kansas city would take the lead prior to halftime with croyle completing a 9 yard td pass to FB Kris Wilson .,method,Reading Comprehension with Discrete Reasoning (DROPnum),0,126,73,7,0,method : Reading Comprehension with Discrete Reasoning (DROPnum),0.4375,0.7448979591836735,0.3684210526315789,Kansas city would take the lead prior to halftime with croyle completing a 9 yard td pass to FB Kris Wilson ,22,Afterwards the Titans responded with Kicker Rob Bironas managing to get a 37 yard field goal .,In the third quarter Tennessee would draw close as Bironas kicked a 37 yard field goal .,method
natural_language_inference,27,"In this work , we consider attention - based neural machine translation ( NMT ) models ; , which have demonstrated excellent translation quality , as the core models of our data augmentation pipeline .",model,DATA AUGMENTATION BY BACKTRANSLATION,0,135,91,8,0,model : DATA AUGMENTATION BY BACKTRANSLATION,0.3994082840236686,0.6842105263157895,0.5714285714285714,In this work we consider attention based neural machine translation NMT models which have demonstrated excellent translation quality as the core models of our data augmentation pipeline ,28,The augmentation process is illustrated in with French as a pivotal language .,"Specifically , we utilize the publicly available codebase 3 provided by , which replicates the Google 's NMT ( GNMT ) systems .",method
sentiment_analysis,17,"At each time step , the hidden state of the Bidirectional LSTM is the concatenation of the forward and backward hidden states .",system description,Variants,0,66,27,5,0,system description : Variants,0.2933333333333333,0.3461538461538461,0.5,At each time step the hidden state of the Bidirectional LSTM is the concatenation of the forward and backward hidden states ,22,Bidirectional LSTM consists of two LSTMs thatare run in parallel : one on the input sequence and the other on the reverse of the input sequence .,This setup allows the hidden state to capture both past and future information .,method
natural_language_inference,76,"where W p , W x ? R kk are trained projection matrices .",method,ATTENTION,0,71,43,18,0,method : ATTENTION,0.4863013698630137,0.7543859649122807,0.9473684210526316,where W p W x R kk are trained projection matrices ,12,The final sentence - pair representation is obtained from a non-linear combination of the attentionweighted representation r of the premise and the last output vector h N using,"where W p , W x ? R kk are trained projection matrices .",method
question_answering,1,Each example has a news article and an incomplete sentence extracted from the human - written summary of the article .,experiment,CLOZE TEST EXPERIMENTS,0,235,72,6,0,experiment : CLOZE TEST EXPERIMENTS,0.7413249211356467,0.8,0.25,Each example has a news article and an incomplete sentence extracted from the human written summary of the article ,20,"Hermann et al. have recently compiled a massive Cloze - style comprehension dataset , consisting of 300 k / 4 k / 3 k and 879k / 65 k / 53 k ( train / dev / test ) examples from CNN and DailyMail news articles , respectively .","To distinguish this task from language modeling and force one to refer to the article to predict the correct missing word , the missing word is always a named entity , anonymized with a random ID .",experiment
text-classification,7,"Here , they form a recursive process to articulate what to be modeled .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.09465020576131687,0.4642857142857143,0.4642857142857143,Here they form a recursive process to articulate what to be modeled ,13,"In neural network approaches , spatial patterns aggregated at lower levels contribute to representing higher level concepts .","For example , CNN builds convolutional feature detectors to extract local patterns from a window of vector sequences and uses max - pooling to select the most prominent ones .",introduction
relation-classification,3,We employ early stopping in all of the experiments .,experiment,Experimental setup,1,89,10,10,0,experiment : Experimental setup,0.6496350364963503,0.2127659574468085,0.2127659574468085,We employ early stopping in all of the experiments ,10,"To obtain comparable results thatare not affected by the input embeddings , we use the embeddings of the previous works .","We use the Adam optimizer and we fix the hyperparameters ( i.e. , ? , dropout values , best epoch , learning rate ) on the validation sets .",experiment
paraphrase_generation,0,Learning Semantic Sentence Embeddings using Pair- wise Discriminator,title,title,1,2,1,1,0,title : title,0.008438818565400843,1.0,1.0,Learning Semantic Sentence Embeddings using Pair wise Discriminator,8, , ,title
text_summarization,0,Then we use the attention difference ? t to sum up the document hidden states h d :,model,Supervisor,0,180,43,28,0,model : Supervisor,0.5980066445182725,0.7962962962962963,0.9032258064516128,Then we use the attention difference t to sum up the document hidden states h d ,17,"In order to model the gap between reader focused aspect and decoder focused aspect , we subtract the reader attention ? R T d by ? t ? R T d resulting in attention difference ? t ? R T d , shown in Equation 25 .",Then we use the attention difference ? t to sum up the document hidden states h d :,method
text_summarization,14,We implement Reinforcement Learning ( RL ) models ( policy gradient ) with reward metrics of Entailment and ROUGE - 2 .,method,Comparative Methods,1,165,17,17,0,method : Comparative Methods,0.7236842105263158,0.85,0.85,We implement Reinforcement Learning RL models policy gradient with reward metrics of Entailment and ROUGE 2 ,17,We apply ROUGE - 2 RAML training for seq2seq model .,employ a selective encoding model to control the information flow from encoder to decoder .,method
natural_language_inference,64,This finetuning is rather critical as the initial task learned during the pre-training stage is very different from AS2 .,system description,Transformers for AS2,0,96,25,18,0,system description : Transformers for AS2,0.3824701195219123,0.38461538461538464,0.6428571428571429,This finetuning is rather critical as the initial task learned during the pre training stage is very different from AS2 ,21,"For AS2 , the training data comprises of question and sentence pairs with positive or negative labels according to the test : the sentence correctly answers the question or not .","When only small target data is available , the transfer process from the language model to AS2 task is unstable .",method
sentiment_analysis,20,"We use an Embedding layer to project the words X = ( x 1 , x 2 , ... , x T ) to a low - dimensional vector space R E , where E the size of the Embedding layer and T the number of words in a tweet .",model,MSA Model (message-level),0,83,7,5,0,model : MSA Model (message-level),0.44623655913978494,0.10144927536231883,0.2083333333333333,We use an Embedding layer to project the words X x 1 x 2 x T to a low dimensional vector space R E where E the size of the Embedding layer and T the number of words in a tweet ,42,"The input to the network is a Twitter message , treated as a sequence of words .",We initialize the weights of the embedding layer with our pre-trained word embeddings .,method
natural_language_inference,11,"The non-contextual word representation e 0 w for a single word w is computed by using a gated combination of fixed , pre-trained word vectors e p w ? Rn with learned character - based embeddings e char w ? Rn .",system description,Unrefined Word Embeddings (E 0 ),0,67,33,3,0,system description : Unrefined Word Embeddings (E 0 ),0.2427536231884058,0.5892857142857143,0.1153846153846154,The non contextual word representation e 0 w for a single word w is computed by using a gated combination of fixed pre trained word vectors e p w Rn with learned character based embeddings e char w Rn ,40,"The first representation level consists of noncontextual word representations , that is , word representations that do not depend on any input ; these can be conceived of as an embedding matrix E 0 whose columns are indexed by words in ? * .","The non-contextual word representation e 0 w for a single word w is computed by using a gated combination of fixed , pre-trained word vectors e p w ? Rn with learned character - based embeddings e char w ? Rn .",method
sentiment_analysis,4,This suggests that multi-hop is more crucial than the latter .,ablation,Self vs Dual History:,0,280,14,9,0,ablation : Self vs Dual History:,0.8588957055214724,0.2692307692307692,0.9,This suggests that multi hop is more crucial than the latter ,12,"Also , removal of multi-hop leads to worse performance than the removal of DGIM .","However , best performance is achieved by variant 6 which contains all the proposed modules in its pipeline .",result
question_answering,1,"where G :t is the t - th column vector ( corresponding to t- th context word ) , ? is a trainable vector function that fuses its ( three ) input vectors , and d G is the output dimension of the ? function .",model,Attention Flow,0,75,43,34,0,model : Attention Flow,0.2365930599369085,0.4257425742574257,0.4927536231884058,where G t is the t th column vector corresponding to t th context word is a trainable vector function that fuses its three input vectors and d G is the output dimension of the function ,37,We define G by,"where G :t is the t - th column vector ( corresponding to t- th context word ) , ? is a trainable vector function that fuses its ( three ) input vectors , and d G is the output dimension of the ? function .",method
natural_language_inference,9,"Our proposed model , Query - Reduction Network 1 ( QRN ) , is a single recurrent unit that addresses the long - term dependency problem of most RNN - based models by simplifying the recurrent update , while taking the advantage of RNN 's capability to model sequential data ) .",introduction,introduction,1,24,16,16,0,introduction : introduction,0.07272727272727272,0.5,0.5,Our proposed model Query Reduction Network 1 QRN is a single recurrent unit that addresses the long term dependency problem of most RNN based models by simplifying the recurrent update while taking the advantage of RNN s capability to model sequential data ,43,"However , one major drawback of these standard attention mechanisms is that they are insensitive to the time step ( memory address ) of the sentences when accessing them .","QRN considers the context sentences as a sequence of state - changing triggers , and transforms ( reduces ) the original query to a more informed query as it observes each trigger through time .",introduction
natural_language_inference,68,3 ) An Answer Pointer ( Ans - Ptr ) layer that uses Ptr-Net to select a set of tokens from the passage as the answer .,method,OUR METHOD,0,92,42,21,0,method : OUR METHOD,0.3694779116465863,0.3684210526315789,0.9545454545454546,3 An Answer Pointer Ans Ptr layer that uses Ptr Net to select a set of tokens from the passage as the answer ,24,2 ) A match - LSTM layer that tries to match the passage against the question .,The difference between the two models only lies in the third layer .,method
natural_language_inference,77,"For example , if a person is mentioned in the question the context encoder only needs to remember that the "" question - person "" was mentioned but not the concrete name of the person .",model,model,0,169,29,29,0,model : model,0.6190476190476191,0.5576923076923077,0.8787878787878788,For example if a person is mentioned in the question the context encoder only needs to remember that the question person was mentioned but not the concrete name of the person ,32,It can further abstract over concrete entities to their respective types because it is rarely the case that many entities of the same type occur in the question .,Another interesting finding is the fact that additional character based embeddings have a notable effect on the over all performance which was already observed by ; .,method
paraphrase_generation,0,"Towards this problem , we propose a supervised method that uses a sequential encoder - decoder framework for paraphrase generation .",introduction,introduction,0,18,4,4,0,introduction : introduction,0.0759493670886076,0.13793103448275862,0.13793103448275862,Towards this problem we propose a supervised method that uses a sequential encoder decoder framework for paraphrase generation ,19,This would be relevant for a wide variety of machine reading comprehension and related tasks such as sentiment analysis .,The task of generating paraphrases is closely related to the task of obtaining semantic sentence embeddings .,introduction
natural_language_inference,98,"As for hidden states of hypothesis h h , we can obtain v h through similar calculation procedure .",method,Composition Layer,0,65,38,13,0,method : Composition Layer,0.5508474576271186,0.8260869565217391,0.9285714285714286,As for hidden states of hypothesis h h we can obtain v h through similar calculation procedure ,18,"Then , the final fixed - length vector representation of premise is v p = [ v pg ; v pa ; v pm ] .","Consequently , both the premise and hypothesis are fed into the composition layer to obtain fixed - length vector representations respectively ( v p , v h ) .",method
part-of-speech_tagging,4,shows a comparison of training and decoding speeds .,experiment,Development Experiments,0,171,33,27,0,experiment : Development Experiments,0.7339055793991416,0.8461538461538461,0.8181818181818182,shows a comparison of training and decoding speeds ,9,Speed vs CRF .,"BiLSTM - LAN processes 805 and 714 sentences per second on the WSJ and CCGBank development data , respectively , outperforming BiLSTM - CRF by 3 % and 19 % , respectively .",experiment
natural_language_inference,9,"Note that we only show some of recent sentences here , even the dialog has more sentences .",model,VISUALIZATIONS,0,325,38,31,0,model : VISUALIZATIONS,0.9848484848484848,0.8837209302325582,0.8611111111111112,Note that we only show some of recent sentences here even the dialog has more sentences ,17,We do not put reset gate in the last layer .,Visualization of Dialog .,method
question_answering,4,This dataset comprises 43 k factoid - based QA pairs and is constructed using ClueWeb09 as its backbone corpus .,dataset,Datasets and Competitor Baselines,0,158,9,9,0,dataset : Datasets and Competitor Baselines,0.6147859922178989,0.3333333333333333,0.3333333333333333,This dataset comprises 43 k factoid based QA pairs and is constructed using ClueWeb09 as its backbone corpus ,19,"On this dataset , the key competitors are BiDAF , Match - LSTM , FastQA / Fast QA - Ext , R2-BiLSTM , AMANDA .",The key competitors on this dataset are BiDAF and the Reinforced Ranker - Reader ( R 3 ) .,experiment
sentiment_analysis,16,The model design follows previous studies except that a different attention alignment function is used ( shown in Eq. 1 ) .,system description,Memory Network for ASC,0,56,4,4,0,system description : Memory Network for ASC,0.1761006289308176,0.057142857142857134,0.15384615384615385,The model design follows previous studies except that a different attention alignment function is used shown in Eq 1 ,20,"It does not include the proposed target - sensitive sentiment solutions , which are introduced in Section 4 .",Their original models will be compared in our experiments as well .,method
sentiment_analysis,32,"Finally , the target representation tr and context representation c rare concatenated as a vector d for a classifier .",system description,Interactive Attention Networks,0,91,41,41,0,system description : Interactive Attention Networks,0.39565217391304347,0.8913043478260869,0.8913043478260869,Finally the target representation tr and context representation c rare concatenated as a vector d for a classifier ,19,"After computing the word attention weights , we can get context and target representations c rand tr based on the attention vectors ? i and ? i by :","Here , we use a non-linear layer to project d into the space of the targeted C classes .",method
natural_language_inference,3,"For this task , we use the contrastive max - margin criterion The ranking - based loss is defined as",training,Training,0,142,6,6,0,training : Training,0.6826923076923077,0.5454545454545454,0.8571428571428571,For this task we use the contrastive max margin criterion The ranking based loss is defined as,17,"The matching score s ( X , Y ) should be larger than s ( X , ? ) .","where s ( X , Y ) is predicted matching score for ( X , Y ) .",experiment
sentence_compression,0,"In the in - domain setting , even compared with the We use an open source implementation : https:// github.com/cnap/sentence-compression .",evaluation,Automatic Evaluation,0,200,16,16,0,evaluation : Automatic Evaluation,0.7168458781362007,0.3636363636363637,0.4848484848484849,In the in domain setting even compared with the We use an open source implementation https github com cnap sentence compression ,22,This shows that our method is comparable to the LSTM + method in the in - domain setting .,"performance of LSTM + trained on 2 million sentence pairs , our method trained on 8,000 sentence pairs does not perform substantially worse .",result
natural_language_inference,24,This observation is consistent with previous works using dropout .,result,SAN:,0,191,59,49,0,result : SAN:,0.8162393162393162,0.7375,0.7,This observation is consistent with previous works using dropout ,10,"However , SAN continues to improve afterwards as other models start to saturate .","We believe that while training time per epoch is similar between SAN and other models , it is recommended to train SAN for more epochs in order to achieve gains in EM / F1 .",result
part-of-speech_tagging,2,Our approach combines the objectives of the two tasks and uses gradient - based methods for efficient training .,introduction,introduction,1,26,15,15,0,introduction : introduction,0.14606741573033707,0.7142857142857143,0.7142857142857143,Our approach combines the objectives of the two tasks and uses gradient based methods for efficient training ,18,"We present a transfer learning approach based on a deep hierarchical recurrent neural network , which shares the hidden feature repre-sentation and part of the model parameters between the source task and the target task .","We study cross - domain , cross - application , and cross-lingual transfer , and present a parameter - sharing architecture for each case .",introduction
sentence_compression,0,"Because we are mostly interested in a crossdomain setting where the model is trained on one domain and test on a different domain , we need data from different domains for our evaluation .",dataset,Datasets and Experiment Settings,0,147,2,2,0,dataset : Datasets and Experiment Settings,0.5268817204301075,0.05128205128205128,0.05128205128205128,Because we are mostly interested in a crossdomain setting where the model is trained on one domain and test on a different domain we need data from different domains for our evaluation ,33, ,Here we use three datasets .,experiment
natural_language_inference,15,And the max pooling operation selects the softpositions that may extract the clues on inference correctly .,ablation,ablation,0,192,41,41,0,ablation : ablation,0.8495575221238938,1.0,1.0,And the max pooling operation selects the softpositions that may extract the clues on inference correctly ,17,The densely connected recurrent and co-attentive features are well - semanticized over multiple layers as collective knowledge ., ,result
natural_language_inference,14,"In our initial experiments , we found that a lower learning rate and more training epochs produced higher accuracy on CODAH , so we replaced the 5e - 5 learning rate in the original grid search with 1 e - 5 , and we added a 6 - epoch setting .",evaluation,evaluation,1,115,25,25,0,evaluation : evaluation,0.6725146198830411,0.8620689655172413,0.8620689655172413,In our initial experiments we found that a lower learning rate and more training epochs produced higher accuracy on CODAH so we replaced the 5e 5 learning rate in the original grid search with 1 e 5 and we added a 6 epoch setting ,45,"Also , when training the initial SWAG model we use the hyperparameters recommended in the BERT paper , namely a batch size of 16 , learning rate of 2 e - 5 , and 3 epochs .",The final hyperparameter grid is as follows :,result
natural_language_inference,11,All our models were trained with 3 different random seeds and the top performance is reported 10 .,implementation,Implementation Details,0,199,2,2,0,implementation : Implementation Details,0.7210144927536232,0.02531645569620253,0.15384615384615385,All our models were trained with 3 different random seeds and the top performance is reported 10 ,18, ,An overview of hyper - parameters used in our experiments can be found in .,experiment
relation_extraction,0,We also add an additional layer to our network to encode the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,introduction,introduction,1,29,21,21,0,introduction : introduction,0.11328125,0.8076923076923077,0.8076923076923077,We also add an additional layer to our network to encode the output sequence from right to left and find significant improvement on the performance of relation identification using bi directional encoding ,33,"At each time step , we use an attention - like model on the previously decoded time steps , to identify the tokens in a specified relation with the current token .","Our model significantly outperforms the feature - based structured perceptron model of , showing improvements on both entity and relation extraction on the ACE05 dataset .",introduction
natural_language_inference,78,The size of the hidden layers of the highway network layers are set to 300 .,implementation,Implementation Details,1,198,8,8,0,implementation : Implementation Details,0.7173913043478259,0.6666666666666666,0.6666666666666666,The size of the hidden layers of the highway network layers are set to 300 ,16,"The number of latent factors k for the factorization layer is tuned amongst { 5 , 10 , 50 , 100 , 150 } .",All parameters are initialized with xavier initialization .,experiment
sarcasm_detection,1,"The task involves detection of sarcasm for comments made in online discussion forums , i.e. , Reddit .",method,Task Definition,0,72,3,2,0,method : Task Definition,0.2155688622754491,0.019867549668874173,0.5,The task involves detection of sarcasm for comments made in online discussion forums i e Reddit ,17, ,"Let us denote the set U = {u 1 , ... , u Nu } for N u -users , where each user participates across a subset of N t - discussion forums ( subreddits ) .",method
text-classification,3,"These techniques require a different kind of preprocessing and , while they have been shown effective in various settings , in this work we only focus on the mainstream word - based models .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.13008130081300814,0.2727272727272727,0.2727272727272727,These techniques require a different kind of preprocessing and while they have been shown effective in various settings in this work we only focus on the mainstream word based models ,31,"These include lemmatization , lowercasing and 1 Note that although word - based models are mainstream in NLP in general and text classification in particular , recent work has also considered other linguistic units , such as characters or word senses .","multiword grouping , among others .",introduction
paraphrase_generation,1,"shows a macro view ( without the LSTM ) of our proposed model architecture , which is essentially a VAE based generative model for each paraphrase 's vector representation x ( p ) , which in turn is generated by a latent code z and the original sentence x o .",model,Model Architecture,0,74,6,6,0,model : Model Architecture,0.334841628959276,0.2307692307692308,0.2307692307692308,shows a macro view without the LSTM of our proposed model architecture which is essentially a VAE based generative model for each paraphrase s vector representation x p which in turn is generated by a latent code z and the original sentence x o ,45,"These representations will be learned using LSTM networks , whose parameters will be learned in an end - to - end fashion , with the rest of the model .","In addition , unlike the standard VAE , note that our VAE decoder model p ? ( x ( p ) | z , x ( o ) ) is also conditioned on the vector representation x ( o ) of the original sentence .",method
text_summarization,7,Parameter estimation ( Training ),system description,Parameter estimation (Training),0,93,72,1,0,system description : Parameter estimation (Training),0.6158940397350994,0.8780487804878049,0.09090909090909093,Parameter estimation Training ,4, , ,method
named-entity-recognition,1,"Additionally , we observed that adding a hidden layer between c i and the CRF layer marginally improved our results .",training,Parameterization and Training,0,79,11,11,0,training : Parameterization and Training,0.38164251207729466,0.12941176470588234,0.8461538461538461,Additionally we observed that adding a hidden layer between c i and the CRF layer marginally improved our results ,20,"Instead of using the softmax output from this layer , we use a CRF as previously described to take into account neighboring tags , yielding the final predictions for every wordy i .",All results reported with this model incorporate this extra-layer .,experiment
text_summarization,5,"Re 3 Sum , the standard deviation of LEN DF is 0.7 times larger in Open NMT , indicating that Open - NMT works quite unstably .",evaluation,Linguistic Quality Evaluation,0,194,33,13,0,evaluation : Linguistic Quality Evaluation,0.7698412698412699,0.5238095238095238,0.6190476190476191,Re 3 Sum the standard deviation of LEN DF is 0 7 times larger in Open NMT indicating that Open NMT works quite unstably ,25,We use Bold font to indicate the crucial rewriting behavior from the templates to generated summaries .,"Moreover , Open NMT generates 53 extreme short summaries , which seriously reduces readability .",result
sentiment_analysis,49,Dataset statistics are presented in .,dataset,Dataset Statistics,0,251,2,2,0,dataset : Dataset Statistics,0.9920948616600792,1.0,1.0,Dataset statistics are presented in ,6, , ,experiment
relation_extraction,9,An extra padding token is repeated multiple times for well - definedness at the beginning and end of the input .,model,Input Attention Mechanism,0,90,43,3,0,model : Input Attention Mechanism,0.43062200956937796,0.4886363636363637,0.375,An extra padding token is repeated multiple times for well definedness at the beginning and end of the input ,20,"Using a sliding window of size k centered around the i - th word , we encode k successive","While position - based encodings are useful , we conjecture that they do not suffice to fully capture the relationships of specific words with the target entities and the influence that they may bear on the target relations of interest .",method
machine-translation,7,Taking Advantage of Convolutionality :,performance,THE SHRINKING BATCH PROBLEM,0,126,25,23,0,performance : THE SHRINKING BATCH PROBLEM,0.3378016085790885,0.2155172413793104,0.71875,Taking Advantage of Convolutionality ,5,"We have not scaled our systems this far as of the writing of this paper , but it should be possible by adding more hardware .","In our language models , we apply the same MoE to each time step of the previous layer .",result
natural_language_inference,2,One difference with TriviaQA is that the evidence passages in Search QA are Google snippets instead of Wikipedia or Web search documents .,dataset,Datasets,0,169,12,12,0,dataset : Datasets,0.6305970149253731,0.8,0.8,One difference with TriviaQA is that the evidence passages in Search QA are Google snippets instead of Wikipedia or Web search documents ,23,They first collected existing question - answer pairs from a Jeopardy archive and augmented them with text snippets retrieved by Google .,This makes reasoning more challenging as the snippets are often very noisy .,experiment
natural_language_inference,70,The first four features indicate whether c appears in the proximity of a comment by u q .,system description,Thread-based features,0,92,63,7,0,system description : Thread-based features,0.5257142857142857,0.6774193548387096,0.3181818181818182,The first four features indicate whether c appears in the proximity of a comment by u q ,18,"The following notation will be adopted : q is the question posted by user u q , c is a comment from user u c , in the comment thread .",The assumption is that an acknowledgment or further questions by u q in the thread could signal a good answer .,method
natural_language_inference,81,"However , honey sometimes contains dormant endospores of the bacterium "" Clostridium botulinum "" , which can be dangerous to babies , as it may result in botulism .",APPENDIX,Answer town,0,362,154,74,0,APPENDIX : Answer town,0.8598574821852731,0.7230046948356808,0.556390977443609,However honey sometimes contains dormant endospores of the bacterium Clostridium botulinum which can be dangerous to babies as it may result in botulism ,24,"Most microorganisms do not grow in honey , so sealed honey does not spoil , even after thousands of years .",People who have a weakened immune system should not eat honey because of the risk of bacterial or fungal infection .,others
text_generation,3,"For simplicity , we only use a simple feedforward network for implementation .",approach,Mapping Module,0,64,32,4,0,approach : Mapping Module,0.4539007092198582,0.9142857142857144,0.5714285714285714,For simplicity we only use a simple feedforward network for implementation ,12,There are many matching models that can be used to learn such dependency relations .,The mapping module M ? transforms the source semantic representation h to a new representation t.,method
question_answering,2,"For ablating intra-sequence dependency , we use the representations from the last timestep of each context document .",implementation,Comparison to the state-of-the-art,1,244,40,17,0,implementation : Comparison to the state-of-the-art,0.8808664259927798,0.851063829787234,0.7083333333333334,For ablating intra sequence dependency we use the representations from the last timestep of each context document ,18,Results show that standard cosine similarity is inferior to our similarity function .,"For ablating cross sequence interaction , we average all attended context representation from different modalities to get the final context vector .",experiment
relation_extraction,1,"We see that the BERT - LSTM - large model ( using the predicate sense dis ambiguation results from above ) yields large F 1 score improvements over the existing state of the art , and beats existing ensemble models as well .",experiment,Dependency-Based SRL Results,0,74,12,8,0,experiment : Dependency-Based SRL Results,0.8314606741573034,0.8571428571428571,0.8888888888888888,We see that the BERT LSTM large model using the predicate sense dis ambiguation results from above yields large F 1 score improvements over the existing state of the art and beats existing ensemble models as well ,38,Our end - to - end results are shown in .,This is achieved without using any linguistic features and declarative decoding constraints .,experiment
sentiment_analysis,34,"Our Arabic SA model is deployed as an online system , Mazajak 3 , and can be accessed online at "" Mazajak.inf.ed.ac.uk:8000 "" .",performance,Mazajak Online API,0,124,3,2,0,performance : Mazajak Online API,0.8378378378378378,0.16666666666666666,0.1176470588235294,Our Arabic SA model is deployed as an online system Mazajak 3 and can be accessed online at Mazajak inf ed ac uk 8000 ,25, ,The final model hosted online is trained on the SemEval and ASTD dataset combined 4 .,result
sentiment_analysis,15,"Now , we can compute the error message for the two children of p 2 :",model,Tensor Backprop through Structure,0,190,101,27,0,model : Tensor Backprop through Structure,0.7037037037037037,0.9439252336448598,0.8181818181818182,Now we can compute the error message for the two children of p 2 ,15,"where ? p 2 , com k is just the k'th element of this vector .","The children of p 2 , will then each take half of this vector and add their own softmax error message for the complete ?.",method
relation_extraction,10,The task - specific token embeddings are used to generate span embeddings for each possible span .,model,Span Representation Generation,0,59,5,3,0,model : Span Representation Generation,0.3072916666666667,0.06097560975609756,1.0,The task specific token embeddings are used to generate span embeddings for each possible span ,16,Use task - agnostic raw token embeddings to create task - specific token embeddings for each token ., ,method
text_summarization,4,"When softmax is applied , this gives very small , negligible , and closeto - zero values to non -top k entities .",model,Pooling submodule,0,142,72,16,0,model : Pooling submodule,0.5503875968992248,0.7826086956521741,0.8421052631578947,When softmax is applied this gives very small negligible and closeto zero values to non top k entities ,19,"In this new vector , important scores of non -top k entities are ??.",The value k depends on the lengths of the input text and summary .,method
question_generation,0,We use the same vocabulary for both encoder and decoder .,implementation,Model Parameters,0,144,3,2,0,implementation : Model Parameters,0.7912087912087912,0.15789473684210525,0.2857142857142857,We use the same vocabulary for both encoder and decoder ,11, ,"The vocabulary is collected from the training data and we keep the top 20,000 frequent words .",experiment
semantic_role_labeling,2,"The absolute gap between our 2 layer and 8 layer models is 3 - 4 F1 for arguments that are within 2 words to the predicate , and 5 - 6 F1 for arguments that are farther away from the predicate .",analysis,Long-range Dependencies,0,151,34,5,0,analysis : Long-range Dependencies,0.6741071428571429,0.3953488372093023,0.8333333333333334,The absolute gap between our 2 layer and 8 layer models is 3 4 F1 for arguments that are within 2 words to the predicate and 5 6 F1 for arguments that are farther away from the predicate ,39,"Interestingly , the gap between shallow and deep models becomes much larger for the long - distance predicate - argument structures .","Surpris - ingly , the neural model performance deteriorates less severely on long - range dependencies than traditional syntax - based models .",result
natural_language_inference,28,"We call this technique time normalization as we usually feed mini-batches to the RNN during learning that have the shape ( N b , NT ) , where Nb is the size of the batch and NT is the length of the sequence that we feed in .",architecture,THE RUM ARCHITECTURE,0,130,32,32,0,architecture : THE RUM ARCHITECTURE,0.4797047970479705,0.7804878048780488,0.7804878048780488,We call this technique time normalization as we usually feed mini batches to the RNN during learning that have the shape N b NT where Nb is the size of the batch and NT is the length of the sequence that we feed in ,45,"Therefore , we suggest normalizing the hidden state ht to a have norm ?.",Time normalization happens along the sequence dimension as opposed to the batch dimension in batch normalization .,method
natural_language_inference,38,Then we obtain the relationship between query and context through a novel fullorientation matching and apply memory networks in order to deeply understand .,model,Model Structure,0,45,4,4,0,model : Model Structure,0.2380952380952381,0.060606060606060615,0.8,Then we obtain the relationship between query and context through a novel fullorientation matching and apply memory networks in order to deeply understand ,24,"First , we concatenate several layers of embedding of questions and contexts and pass them into a bidirectional RNN .","In the end , the output layer helps locate the answer in the passage .",method
natural_language_inference,79,better representation is one that achieves a small distance for pairs of paragraphs of the same query and a larg distance for pairs of paragraphs of different queries .,model,Information Retrieval with Paragraph Vectors,0,214,29,9,0,model : Information Retrieval with Paragraph Vectors,0.7985074626865671,0.4603174603174603,0.32142857142857145,better representation is one that achieves a small distance for pairs of paragraphs of the same query and a larg distance for pairs of paragraphs of different queries ,29,"To achieve this , we will use paragraph vectors and compute the distances the paragraphs .","Here is a sample of three paragraphs , where the first paragraph should be closer to the second paragraph than the third paragraph :",method
question_similarity,0,"The data augmentation steps have an effect on two factors , the training time and the accuracy measurement ( F1 - score ) .",experiment,Effect of Data Augmentation,0,103,23,3,0,experiment : Effect of Data Augmentation,0.7463768115942029,0.6052631578947368,0.2727272727272727,The data augmentation steps have an effect on two factors the training time and the accuracy measurement F1 score ,20,"In this experiments set , we use the RNN cell type that gives the best results in Section 3.2 ( ON - LSTM with chunk size 8 ) and the same model structure described in Section 2.3 to explore the effect of data augmentation steps mentioned in Section 2.2 .",shows the av - 7 https://colab.research.google.com erage training time over five runs for each data augmentation step .,experiment
temporal_information_extraction,1,"The structured approach also gives rise to a semisupervised method , making it possible to take advantage of the readily available unlabeled data .",introduction,introduction,1,18,9,9,0,introduction : introduction,0.07003891050583658,0.20454545454545456,0.20454545454545456,The structured approach also gives rise to a semisupervised method making it possible to take advantage of the readily available unlabeled data ,23,"In this paper , we propose a structured learning approach to temporal relation extraction , where local models are updated based on feedback from global inferences .","As a byproduct , this approach further provides a new , effective perspective on handling those missing relations .",introduction
natural_language_inference,8,"This formulation can intuitively be understood as an expression of the generative approach to open domain question answering : given a candidate answer sentence , we ' generate ' a question through the transformation q ? = M a , and then measure the similarity of the generated question q ? and the given question q by their dot product .",model,model,0,101,12,12,0,model : model,0.4786729857819905,0.5454545454545454,0.5454545454545454,This formulation can intuitively be understood as an expression of the generative approach to open domain question answering given a candidate answer sentence we generate a question through the transformation q M a and then measure the similarity of the generated question q and the given question q by their dot product ,53,where the bias term band the transformation matrix M ? R dd are model parameters .,"This formulation can intuitively be understood as an expression of the generative approach to open domain question answering : given a candidate answer sentence , we ' generate ' a question through the transformation q ? = M a , and then measure the similarity of the generated question q ? and the given question q by their dot product .",method
sentiment_analysis,6,It s ability to achieve stateof - the - art results motivated us to adopt it in our framework .,method,3D-CNN: Visual Feature Extraction,0,96,38,5,0,method : 3D-CNN: Visual Feature Extraction,0.3321799307958477,0.5757575757575758,0.5,It s ability to achieve stateof the art results motivated us to adopt it in our framework ,18,"In the past , 3D - CNN has been successfully applied to object classification on tridimensional data .","Let vid ? R cf hw be a video , where c = number of channels in an image ( in our case c = 3 , since we consider only RGB images ) , f = number of frames , h = height of the frames , and w = width of the frames .",method
sentiment_analysis,29,"Among these machine learning - based approaches , there are mainly two different types .",introduction,introduction,0,20,12,12,0,introduction : introduction,0.11494252873563215,0.22641509433962265,0.22641509433962265,Among these machine learning based approaches there are mainly two different types ,13,The typical way is to build a machine learning classifier by supervised training .,One is to build a classifier based on manually created features .,introduction
sentiment_analysis,50,"score is a content - based function that captures the semantic association between a word and the target , for which we adopt the formulation used in with parameter matrix W a ? R dd .",model,Attention-based LSTM,0,51,16,15,0,model : Attention-based LSTM,0.3148148148148148,0.7619047619047619,0.75,score is a content based function that captures the semantic association between a word and the target for which we adopt the formulation used in with parameter matrix W a R dd ,33,where t is the target representation computed as the averaged word embedding of the target .,"score is a content - based function that captures the semantic association between a word and the target , for which we adopt the formulation used in with parameter matrix W a ? R dd .",method
named-entity-recognition,6,Training requires about 2.5 hours for CONLL and 8 hours for ONTONOTES .,training,Training and Implementation,0,149,12,12,0,training : Training and Implementation,0.7028301886792453,0.3636363636363637,0.3870967741935484,Training requires about 2 5 hours for CONLL and 8 hours for ONTONOTES ,14,"We implemented our system using the Tensorflow library , and ran our models on a GeForce GTX TITAN Xp GPU .",shows the development set performance of our final models on each dataset compared to the work of .,experiment
paraphrase_generation,1,"Note that one can use the metric between the variant and the input question to provide filtering in the case of multiple variants , or even to decide if a variant needs to be reported or not .",result,Results,0,209,43,43,0,result : Results,0.9457013574660632,0.8775510204081632,0.8775510204081632,Note that one can use the metric between the variant and the input question to provide filtering in the case of multiple variants or even to decide if a variant needs to be reported or not ,37,"Note that Relevance of the MSCOCO dataset is 3.38 which is far from a perfect score of 5 because unlike Quora , MSCOCO dataset is an image caption dataset , and therefore allows for a larger variation in the human annotations .","So in order to make the system more practical ( a high precision system ) , we choose to report the variant only when the confidence in the variant is more than a threshold .",result
sentiment_analysis,11,Results are an average of 10 runs with varied weight initializations .,baseline,bc-LSTM:,0,281,27,22,0,baseline : bc-LSTM:,0.8168604651162791,0.84375,0.8148148148148148,Results are an average of 10 runs with varied weight initializations ,12,UAR is a popular metric that is used when dealing with imbalanced classes .,We assert significance when p < 0.05 under McNemar 's test .,result
sentiment_analysis,2,where W sand b s are the weight matrix and bias respectively for softmax layer .,model,Word Representation,0,108,59,43,0,model : Word Representation,0.4757709251101322,1.0,1.0,where W sand b s are the weight matrix and bias respectively for softmax layer ,16,"Finally , we add a softmax layer to compute the probability distribution for judging the sentiment polarities as positive , negative or neutral :", ,method
phrase_grounding,0,Visual Feature Extraction :,method,Feature Extraction and Common Space,0,81,8,2,0,method : Feature Extraction and Common Space,0.36,0.10666666666666667,0.1,Visual Feature Extraction ,4, ,"In contrast to many vision tasks where the last layer of a pre-trained CNN is being used as visual representation of an image , we use feature maps from different layers and map them separately to a common space to obtain a multi - level set of feature maps to be compared with text .",method
natural_language_inference,89,"We explore three different architectures , as shown in :",approach,Answer Verifier,0,107,41,4,0,approach : Answer Verifier,0.4115384615384615,0.4659090909090909,0.0784313725490196,We explore three different architectures as shown in ,9,"Here , we define the answer sentence as the context sentence that contains either gold answers or plausible answers .","1 ) a sequential model that takes the inputs as along sequence , ( 2 ) an interactive model that encodes two sentences interdependently , and ( 3 ) a hybrid model that takes both of the two approaches into account .",method
natural_language_inference,97,"However , we handle the linearization in data preprocessing so that the model sees only reordered word - vector inputs .",model,Dependency Sliding Window,0,153,71,7,0,model : Dependency Sliding Window,0.523972602739726,0.6339285714285714,0.28,However we handle the linearization in data preprocessing so that the model sees only reordered word vector inputs ,19,"Moreover , linearization of the dependency graph , because it relies on an eigendecomposition , is not differentiable .","Specifically , we run the Stanford Dependency Parser on each text sentence to build a dependency graph .",method
natural_language_inference,31,The result is shown in .,analysis,Analysis,0,197,14,14,0,analysis : Analysis,0.7137681159420289,0.2295081967213115,0.2295081967213115,The result is shown in ,6,The reported results are the average of 10 runs and the standard deviations are omitted for clarity .,"The first ablation baseline shows that without richer features as the alignment input , the performance on all datasets degrades significantly .",result
semantic_role_labeling,4,The first layer of the stacked BiLSTMs receives word embeddings x word ? Rd word and predicate mark embeddings x mark ? Rd mark .,architecture,architecture,0,104,16,16,0,architecture : architecture,0.3466666666666667,0.27586206896551724,0.27586206896551724,The first layer of the stacked BiLSTMs receives word embeddings x word Rd word and predicate mark embeddings x mark Rd mark ,23,The stacked BiLSTMs process an input sequence in a left - to - right manner at odd - numbered layers and in a right - to - left manner at even- numbered layers .,The first layer of the stacked BiLSTMs receives word embeddings x word ? Rd word and predicate mark embeddings x mark ? Rd mark .,method
semantic_parsing,0,"To address the need for a large and high - quality dataset for a new complex and cross-domain semantic parsing task , we introduce Spider , which consists of 200 databases with multiple tables , 10,181 questions , and 5,693 corresponding complex SQL queries , all written by 11 college students spending a total of 1,000 man-hours .",introduction,introduction,1,34,23,23,0,introduction : introduction,0.1223021582733813,0.71875,0.71875,To address the need for a large and high quality dataset for a new complex and cross domain semantic parsing task we introduce Spider which consists of 200 databases with multiple tables 10 181 questions and 5 693 corresponding complex SQL queries all written by 11 college students spending a total of 1 000 man hours ,57,All of these processes require very specific knowledge in data bases .,"As illustrates , given a database with multiple tables including foreign keys , our corpus creates and annotates complex questions and SQL queries including different SQL clauses such as joining and nested query .",introduction
natural_language_inference,83,"However , it makes the final prediction by training another logistic regression over their predictions .",baseline,Baselines,0,191,21,21,0,baseline : Baselines,0.6241830065359477,0.9130434782608696,0.9130434782608696,However it makes the final prediction by training another logistic regression over their predictions ,15,"Like the voting methods , this baseline also trains K different aspectspecific classifiers .","Lastly , we can see that the proposed Hidden Coherence model , with an accuracy of 77. 60 % , outperforms all other models .",result
text_generation,2,"Then we randomly sample 200,000 sentences as the training set and another 10,000 sentences as the test set .",experiment,Long Text Generation: EMNLP2017 WMT News,0,193,16,9,0,experiment : Long Text Generation: EMNLP2017 WMT News,0.5514285714285714,0.4324324324324325,0.6923076923076923,Then we randomly sample 200 000 sentences as the training set and another 10 000 sentences as the test set ,21,"After the preprocessing , the news dataset has 5,742 words and 397,726 sentences .",We use the BLEU - ( 2 to 5 ) scores as the evaluation metrics .,experiment
semantic_role_labeling,2,"The gap is biggest between 2 - layer and 4 - layer models , and diminishes after that .",analysis,BIO Violations,0,162,45,7,0,analysis : BIO Violations,0.7232142857142857,0.5232558139534884,0.14583333333333334,The gap is biggest between 2 layer and 4 layer models and diminishes after that ,16,The number of BIO violations decreases when we use a deeper model .,It is surprising that although the deeper models generate impressively accurate token - level predic - :,result
sentiment_analysis,39,"Aspect general refers to a generic opinion about a location , e.g. "" I love Camden Town "" .",system description,Aspects,0,81,26,6,0,system description : Aspects,0.3306122448979592,0.3661971830985916,1.0,Aspect general refers to a generic opinion about a location e g I love Camden Town ,17,"However in the initial round of annotations , we realised that it had a negative effect on the decisiveness of annotators and it led to a lower over all agreement .", ,method
machine-translation,5,The models were trained using parallel data and back - translated data in a 1 - to - 1 proportion .,system,System Overview,1,28,5,5,0,system : System Overview,0.1958041958041958,0.06756756756756757,0.5,The models were trained using parallel data and back translated data in a 1 to 1 proportion ,18,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c-nmt ) that were deployed as ensembles of averaged factored data ( see Section 3 ) Transformer models .,Unconstrained English - Estonian and Estonian - English NMT systems ( tilde - nc - nmt ) that were deployed as averaged Transformer models .,method
natural_language_inference,15,It means that only the word embedding features are densely connected to the uppermost layer while recurrent and attentive features are connected to the upper layer using the residual connection .,ablation,ablation,0,165,14,14,0,ablation : ablation,0.7300884955752213,0.3414634146341464,0.3414634146341464,It means that only the word embedding features are densely connected to the uppermost layer while recurrent and attentive features are connected to the upper layer using the residual connection ,31,"In ( 8 ) , we replace dense connection with residual connection only over recurrent and co-attentive features .","In ( 9 ) , we removed additional dense connection over word embedding features from ( 8 ) .",result
natural_language_inference,81,"In addition to the mainland , the country is made up of more than 790 islands , including the Northern Isles and the Hebrides .",APPENDIX,ATTENTION MAPS,0,237,29,14,0,APPENDIX : ATTENTION MAPS,0.5629453681710214,0.13615023474178406,0.2333333333333333,In addition to the mainland the country is made up of more than 790 islands including the Northern Isles and the Hebrides ,23,"It shares a border with England to the south , and is otherwise surrounded by the Atlantic Ocean , with the North Sea to the east and the North Channel and Irish Sea to the south - west .","Avon Water , also known locally as the River Avon , is a river in Scotland , and a tributary of the River Clyde .",others
sentiment_analysis,13,We do not use SemEval 2014 Task 4 or SemEval 2015 Task 12 because these datasets do not come with the review ( document ) level XML tags to recover whole reviews from review sentences .,experiment,Experiments,0,183,7,7,0,experiment : Experiments,0.6582733812949639,0.175,0.175,We do not use SemEval 2014 Task 4 or SemEval 2015 Task 12 because these datasets do not come with the review document level XML tags to recover whole reviews from review sentences ,34,"As there are no existing datasets for RRC and to be consistent with existing research on sentiment analysis , we adopt the laptop and restaurant reviews of SemEval 2016 Task 5 as the source to create datasets for RRC .",We keep the split of training and testing of the SemEval 2016 Task 5 datasets and annotate multiple QAs for each review following the way of constructing QAs for the SQuAD 1.1 datasets .,experiment
relation-classification,2,"The outputs for each token ( e.g. , Smith ) are twofold : ( i ) an entity recognition label ( e.g. , I - PER , denoting the token is inside a named entity of type PER ) and ( ii ) a set of tuples comprising the head tokens of the entity and the types of relations between them ( e.g. , {( Center , Works for ) , ( Atlanta , Lives in ) } ) .",model,Joint model,1,114,9,9,0,model : Joint model,0.3864406779661017,0.13846153846153847,0.6923076923076923,The outputs for each token e g Smith are twofold i an entity recognition label e g I PER denoting the token is inside a named entity of type PER and ii a set of tuples comprising the head tokens of the entity and the types of relations between them e g Center Works for Atlanta Lives in ,59,Then the CRF and the sigmoid layers are able to produce the outputs for the two tasks .,"Since we assume token - based encoding , we consider only the last token of the entity as head of another token , eliminating redundant relations .",method
natural_language_inference,66,"The same explanation applies to the match between "" washes "" and "" jumping . """,result,Word Alignment,0,220,59,18,0,result : Word Alignment,0.7885304659498208,0.6344086021505376,0.8181818181818182,The same explanation applies to the match between washes and jumping ,12,"However , "" dog "" is likely the best match for "" cat "" among all the words in the premise , and as we will show later , this match between "" cat "" and "" dog "" is actually a strong indication of a contradiction between the two sentences .",We also observe that some words are aligned with the NULL token we inserted .,result
natural_language_inference,2,supports this fact as well .,analysis,Quantitative Error Analysis,0,238,3,3,0,analysis : Quantitative Error Analysis,0.8880597014925373,0.2,0.5,supports this fact as well ,6,We analyzed the performance of AMANDA across different question types and different predicted answer lengths . ( a ) shows that it performs poorly on why and other questions whose answers are usually longer .,"When the predicted answer length increases , both F1 and EM start to degrade .",result
passage_re-ranking,0,Expanding with copied words gives an MRR@10 of 19.7 .,result,result,1,104,22,22,0,result : result,0.8524590163934426,0.6875,0.6875,Expanding with copied words gives an MRR 10 of 19 7 ,12,"If we expand MS MARCO documents using only new words and retrieve the development set queries with BM25 , we obtain an MRR@10 of 18.8 ( as opposed to 18.4 when indexing with original documents ) .","We achieve a higher MRR@10 of 21.5 when documents are expanded with both types of words , showing that they are complementary .",result
natural_language_inference,31,Research on general purpose text matching algorithm is beneficial to a large number of relevant applications .,introduction,introduction,0,12,5,5,0,introduction : introduction,0.043478260869565216,0.1724137931034483,0.1724137931034483,Research on general purpose text matching algorithm is beneficial to a large number of relevant applications ,17,"wide range of tasks , including natural language inference ( also known as recognizing textual entailment ) , paraphrase identification , answer selection , and soon , can be seen as specific forms of text matching problems .",Deep neural networks are the most popular choices for text matching nowadays .,introduction
natural_language_inference,18,"According to the joint representation ? ? , we then generate the parameters ? and ? ? , which parameterise the variational distribution over the question semantics h.",model,Neural Answer Selection Model,0,120,47,25,0,model : Neural Answer Selection Model,0.4395604395604396,0.8867924528301887,0.8064516129032258,According to the joint representation we then generate the parameters and which parameterise the variational distribution over the question semantics h ,22,"where q and a are also modelled by LSTMs 1 , and the relatedness label y is modelled by a simple linear transformation into the vector s y .","To emphasise , though both p ? ( h|q ) and q ? ( h|q , a , y ) are modelled as parameterised Gaussian distributions , q ? ( h|q , a , y ) as an approximation only functions during inference by producing samples to compute the stochastic gradients , while p ? ( h|q ) is the generative distribution that generates the samples for predicting the question - answer relatedness y.",method
natural_language_inference,14,"We experimentally demonstrate that CODAH 's generation procedure produces a dataset with a large gap between system performance and human expert accuracy , even when using state - of the - art pre-trained language models with and without fine - tuning on the large SWAG dataset .",introduction,introduction,0,27,14,14,0,introduction : introduction,0.15789473684210525,0.7368421052631579,0.7368421052631579,We experimentally demonstrate that CODAH s generation procedure produces a dataset with a large gap between system performance and human expert accuracy even when using state of the art pre trained language models with and without fine tuning on the large SWAG dataset ,44,"Annotators are rewarded for submissions in which the model fails to identify the correct sentence completion both before and after fine - tuning on a sample of the submitted questions , encouraging the creation of questions that are not easily learnable .","Using a model initially fine - tuned on SWAG , we find that the OpenAI GPT - 1 and BERT neural question answering models yield 65.5 % and 69.5 % accuracy , respectively , on the CODAH dataset in cross-validation .",introduction
text-classification,0,They have dropout probability of 0.5 .,model,model,0,75,5,5,0,model : model,0.3303964757709251,0.15625,0.3333333333333333,They have dropout probability of 0 5 ,8,We also insert 2 dropout modules in between the 3 fully - connected layers to regularize .,"lists the configurations for convolutional layers , and table 2 lists the configurations for fully - connected ( linear ) layers .",method
sentiment_analysis,25,"Convolutional neural networks CNN and GCN are not designed for aspect based sentiment analysis , but their performance exceeds that of ATAE - LSTM .",analysis,ACSA,0,182,16,15,0,analysis : ACSA,0.8198198198198198,0.4444444444444444,0.625,Convolutional neural networks CNN and GCN are not designed for aspect based sentiment analysis but their performance exceeds that of ATAE LSTM ,23,"By comparing the performance on the hard test datasets against CNN , it is easy to see the convolutional layer of GCAE is able to differentiate the sentiments of multiple entities .",The performance of SVM depends on the availability of the features it can use .,result
natural_language_inference,21,We give an illustration of traditional attention and multidimensional attention in .,system description,Directional Self-Attention,0,106,59,4,0,system description : Directional Self-Attention,0.3655172413793104,0.4645669291338583,0.16666666666666666,We give an illustration of traditional attention and multidimensional attention in ,12,The output scan be written as,"In the rest of this paper , we will ignore the subscript k which indexes feature dimension for simplification if no confusion is possible .",method
sentiment_analysis,13,"This task exploits the rich relationships between two sides in the input , such as whether two sides of texts have the same rating or not ( when two reviews with different ratings are combined as a positive example ) , or whether two sides are targeting the same product or not ( when two reviews from different products are merged as a positive example ) .",system description,Post-training,0,150,74,15,0,system description : Post-training,0.5395683453237411,0.74,0.36585365853658536,This task exploits the rich relationships between two sides in the input such as whether two sides of texts have the same rating or not when two reviews with different ratings are combined as a positive example or whether two sides are targeting the same product or not when two reviews from different products are merged as a positive example ,61,"In the context of reviews , NSP formulates a task of "" artificial review prediction "" , where a negative example is an original review but a positive example is a synthesized fake review by combining two different reviews .","In summary , these two objectives encourage to learn a myriad of fine - grained features for potential end tasks .",method
sentiment_analysis,2,The process can be formulated as follows :,model,Word Representation,0,99,50,34,0,model : Word Representation,0.4361233480176212,0.8474576271186439,0.7906976744186046,The process can be formulated as follows ,8,"Since we obtain the hidden contextual representation of the inputs by the right Bi - GRU , we utilize both the position and semantic information for calculating the attention weights of different words in aspect term .","where ? i stands for the attention weights from inputs to the words in aspect term , denoting which word in aspect term should be more focused .",method
natural_language_inference,25,Our formulation lends itself to an inspection of the different dynamic weightings computed by the model for interpolating between contextual and noncontextual terms .,analysis,Importance of context,0,70,10,8,0,analysis : Importance of context,0.6730769230769231,0.5555555555555556,0.5,Our formulation lends itself to an inspection of the different dynamic weightings computed by the model for interpolating between contextual and noncontextual terms ,24,Context complements rare words,"In we plot the average gate value gt for each word - type , where the average is taken across entries of the gate vector and across all occurrences of the word in both passages :",result
text-classification,2,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .",analysis,Tag prediction,0,81,29,13,0,analysis : Tag prediction,0.8709677419354839,0.8285714285714286,0.6842105263157895,While the Tagspace model is described using convolutions we consider the linear version which achieves comparable performance but is much faster ,22,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",Results and training time . and 200 .,result
natural_language_inference,32,shows a simplified CoQA example where such conversation flow is crucial .,model,Context:,0,69,23,10,0,model : Context:,0.15789473684210525,0.4509803921568628,0.2631578947368421,shows a simplified CoQA example where such conversation flow is crucial ,12,"This includes knowing the main topic currently being discussed , as well as the relevant events and facts .","As the conversation progresses , the topic being discussed changes overtime .",method
sentiment_analysis,6,Voice normalization is performed and voice intensity is thresholded to identify samples with and without voice .,method,openSMILE: Audio Feature Extraction,0,87,29,4,0,method : openSMILE: Audio Feature Extraction,0.30103806228373703,0.4393939393939394,0.5,Voice normalization is performed and voice intensity is thresholded to identify samples with and without voice ,17,"To compute the features , we use open SMILE , an open - source software that automatically extracts audio features such as pitch and voice intensity .",Z-standardization is used to perform voice normalization .,method
text_generation,4,Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation .,introduction,introduction,1,11,2,2,0,introduction : introduction,0.10091743119266056,0.25,0.25,Numerous efforts have been made in the field of natural language text generation for tasks such as sentiment analysis and machine translation ,23, ,"Early techniques for generating text conditioned on some input information were template or rule - based engines , or probabilistic models such as n-gram .",introduction
sentiment_analysis,0,The MDRE model ) compensates for the weaknesses of the previous two models ( ARE and TRE ) and benefits from their strengths to a surprising degree .,analysis,Error analysis,1,168,10,10,0,analysis : Error analysis,0.9438202247191012,0.8333333333333334,0.8333333333333334,The MDRE model compensates for the weaknesses of the previous two models ARE and TRE and benefits from their strengths to a surprising degree ,25,"On the other hand , it is striking that the TRE model incorrectly predicts instances of the sad class as the happy class 16 . 20 % of the time , even though these emotional states are opposites of one another .",The values arranged along the diagonal axis show that all of the accuracies of the correctly predicted class have increased .,result
semantic_role_labeling,2,"However , we differ by ( 1 ) simplifying the input and output layers , ( 2 ) introducing highway connections , ( 3 ) using recurrent dropout , ( 4 ) decoding with BIOconstraints , and ( 5 ) ensembling with a product of experts .",introduction,introduction,1,15,8,8,0,introduction : introduction,0.06696428571428571,0.5,0.5,However we differ by 1 simplifying the input and output layers 2 introducing highway connections 3 using recurrent dropout 4 decoding with BIOconstraints and 5 ensembling with a product of experts ,32,"Fol - lowing , we treat SRL as a BIO tagging problem and use deep bidirectional LSTMs .",Our model gives a 10 % relative error reduction over previous state of the art on the test sets of CoNLL 2005 and 2012 .,introduction
relation_extraction,13,"Formally , let x = [ x 0 . . . x n ] be a sequence of tokens , where x 0 = [ CLS ] and x n = [ SEP ] are special start and end markers .",system description,Overview,0,34,4,4,0,system description : Overview,0.1596244131455399,0.11428571428571427,0.2857142857142857,Formally let x x 0 x n be a sequence of tokens where x 0 CLS and x n SEP are special start and end markers ,27,"In this paper , we focus on learning mappings from relation statements to relation representations .","Let s 1 = ( i , j ) and s 2 = ( k , l ) be pairs of integers such that 0 < i < j ? 1 , j < k , k ? l ? 1 , and l ? n.",method
natural_language_inference,93,propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions .,introduction,introduction,0,20,10,10,0,introduction : introduction,0.0966183574879227,0.35714285714285715,0.35714285714285715,propose dynamic co attention networks which attend the question and passage simultaneously and iteratively refine answer predictions ,18,introduce bi-directional attention flow networks to model question - passage pairs at multiple levels of granularity .,and predict answers by ranking continuous text spans within passages .,introduction
natural_language_inference,23,"Augmenting with fully - aware attention yields the biggest improvement , which demonstrates the usefulness of this simple enhancement .",DETAILED CONFIGURATIONS IN THE ABLATION STUDY,APPLICATION TO NATURAL LANGUAGE INFERENCE,0,390,91,42,0,DETAILED CONFIGURATIONS IN THE ABLATION STUDY : APPLICATION TO NATURAL LANGUAGE INFERENCE,0.7602339181286549,0.9680851063829788,0.9333333333333332,Augmenting with fully aware attention yields the biggest improvement which demonstrates the usefulness of this simple enhancement ,18,The results of ESIM under different attention mechanism is shown in .,Further improvement is obtained when we use multi -level fusion in our ESIM .,result
natural_language_inference,71,The RNN is fed by one character each step from are al context and supposed to output the prediction for the next character .,model,Language Modeling: Character-level Prediction,0,184,25,3,0,model : Language Modeling: Character-level Prediction,0.8558139534883721,0.5102040816326531,0.16666666666666666,The RNN is fed by one character each step from are al context and supposed to output the prediction for the next character ,24,We test each RNN on character - level language modeling .,We used the Penn Treebank corpus .,method
relation_extraction,4,"We observe that all neural models achieve higher F 1 scores than the logistic regression and patterns systems , which demonstrates the effectiveness of neural models for relation extraction .",evaluation,evaluation,1,141,7,7,0,evaluation : evaluation,0.6811594202898551,0.2413793103448276,0.2413793103448276,We observe that all neural models achieve higher F 1 scores than the logistic regression and patterns systems which demonstrates the effectiveness of neural models for relation extraction ,29,summarizes our results .,"Although positional embeddings help increase the F 1 by around 2 % over the plain CNN model , a simple ( 2 - layer ) LSTM model performs surprisingly better than CNN and dependency - based models .",result
natural_language_inference,37,Paragraph separator tokens with learned embeddings are added between merged paragraphs to preserve formatting information .,system description,Preprocessing,0,150,8,8,0,system description : Preprocessing,0.5836575875486382,0.4,1.0,Paragraph separator tokens with learned embeddings are added between merged paragraphs to preserve formatting information ,16,We use a maximum size of 400 unless stated otherwise ., ,method
sentence_classification,0,"Through examination of citation intents , we found out many of the categories defined in previous work such as motivation , extension or future work , can be considered as background information providing more context for the current research topic .",dataset,SciCite dataset,0,122,7,7,0,dataset : SciCite dataset,0.4569288389513109,0.6363636363636364,0.6363636363636364,Through examination of citation intents we found out many of the categories defined in previous work such as motivation extension or future work can be considered as background information providing more context for the current research topic ,38,"To address these limitations , we introduce Sci - Cite , a new dataset of citation intents that is significantly larger , more coarse - grained and generaldomain compared with existing datasets .",More interesting intent categories are a direct use of a method or comparison of results .,experiment
natural_language_inference,59,"shows some example predictions from the DE - CATT glove , DECATT char and the pt - DECATT char models .",result,Results,0,125,13,13,0,result : Results,0.925925925925926,0.8125,0.8125,shows some example predictions from the DE CATT glove DECATT char and the pt DECATT char models ,18,It also gives an important insight into trade off between having more but costly human annotated data versus cheap but noisy pretraining .,The GloVe - trained model often makes mistakes related to spelling and tokenization artifacts .,result
natural_language_inference,100,Input Layer to Hidden Layer .,model,Value-shared Weighting,0,165,36,25,0,model : Value-shared Weighting,0.4508196721311475,0.6428571428571429,0.8333333333333334,Input Layer to Hidden Layer ,6,"Specifically , value - shared weights are adopted in the forward propagation prediction process from the input layer to the hidden layer over each question term in a NMM - 1 as follows :",Let w denote a K + 1 dimensional model parameter from input layer to hidden layer .,method
natural_language_inference,12,"Let bilstm i represent the ith biLSTM layer , which is defined as :",model,Sentence Encoder,0,32,10,3,0,model : Sentence Encoder,0.3636363636363637,0.2857142857142857,0.1875,Let bilstm i represent the ith biLSTM layer which is defined as ,13,Our sentence encoder is simply composed of multiple stacked bidirectional LSTM ( biLSTM ) layers with shortcut connections followed by a max pooling layer .,"where hi t is the output of the ith biLSTM at time t over input sequence ( x i 1 , x i 2 , ... , xi n ) .",method
natural_language_inference,10,The detailed experimental settings are described in the supplementary material .,experiment,Experiments,0,138,4,4,0,experiment : Experiments,0.5822784810126582,0.12903225806451613,1.0,The detailed experimental settings are described in the supplementary material ,11,The implementation is made publicly available ., ,experiment
paraphrase_generation,0,"Typically , the discriminator is a binary classifier loss , but here we use a global loss , similar to which acts on the last hidden state of the recurrent neural network ( LSTM ) .",method,Discriminative-LSTM,0,109,49,5,0,method : Discriminative-LSTM,0.4599156118143461,0.7903225806451613,0.35714285714285715,Typically the discriminator is a binary classifier loss but here we use a global loss similar to which acts on the last hidden state of the recurrent neural network LSTM ,31,The discriminator module estimates a loss function between the generated and ground truth paraphrases .,The main objective of this loss is to bring the generated paraphrase embeddings closer to its ground truth paraphrase embeddings and farther from the other ground truth paraphrase embeddings ( other sentences in the batch ) .,method
text_summarization,11,"To reach this goal , we implement a gated unit on top of the encoder outputs at each time step , which is a CNN that convolves all the encoder outputs .",system description,Convolutional Gated Unit,0,48,19,10,0,system description : Convolutional Gated Unit,0.3333333333333333,0.4634146341463415,0.3125,To reach this goal we implement a gated unit on top of the encoder outputs at each time step which is a CNN that convolves all the encoder outputs ,30,Abstractive summarization requires the core information at each encoding time step .,"The parameter sharing of the convolutional kernels enables the model to extract certain types of features , specifically n-gram features .",method
natural_language_inference,58,Recent approaches on cloze - style datasets can be separated into two categories : single - turn and multi-turn reasoning .,introduction,introduction,0,15,6,6,0,introduction : introduction,0.04477611940298507,0.15384615384615385,0.15384615384615385,Recent approaches on cloze style datasets can be separated into two categories single turn and multi turn reasoning ,19,Some large - scale cloze - style datasets have gained signi cant attention along with powerful deep learning models .,Single turn reasoning models utilize attention mechanisms to emphasize speci c parts of the document which are relevant to the query .,introduction
sentiment_analysis,1,"In particular , our model usually performs best when is set to 0.2 , demonstrating the existence of label noises and the necessity of addressing them on both datasets .",analysis,Sensitivity Analysis,0,370,11,11,0,analysis : Sensitivity Analysis,0.9343434343434344,0.4583333333333333,0.9166666666666666,In particular our model usually performs best when is set to 0 2 demonstrating the existence of label noises and the necessity of addressing them on both datasets ,29,"Specifically , by increasing , the performance of our model first increases and then decreases .","Introducing excessive noise in Emotion DL causes performance drop , which is expected because excessive noise weakens the true learning signals .",result
part-of-speech_tagging,7,We also report accuracies on WSJ ( 45 POS ) using the standard splits .,dataset,Datasets,0,69,6,6,0,dataset : Datasets,0.552,0.8571428571428571,0.8571428571428571,We also report accuracies on WSJ 45 POS using the standard splits ,13,"We consider all languages that have at least 60 k tokens and are distributed with word forms , resulting in 22 languages .",The overview of languages is provided in .,experiment
sentiment_analysis,47,"In the early studies , methods for the aspect - based sentiment classification task were similar as that used in standard sentiment classification task .",introduction,introduction,1,17,3,3,0,introduction : introduction,0.07906976744186046,0.07692307692307693,0.07692307692307693,In the early studies methods for the aspect based sentiment classification task were similar as that used in standard sentiment classification task ,23,"Aspect - based sentiment analysis is a fine - grained classification task in sentiment analysis , identifying sentiment polarity of a sentence expressed toward a target .","Researchers normally designed a set of features ( such as bag - of - words , sentiment lexicons , and linguistic features ) to train a statistical learning algorithm for sentiment classification ; * The corresponding author of this paper ..",introduction
sentiment_analysis,1,"For both subject - dependent and subject - independent settings on SEED , our model can recognize better for positive and neutral emotions than negative emotion .",evaluation,Confusion Matrix,0,338,22,3,0,evaluation : Confusion Matrix,0.8535353535353535,0.8148148148148148,0.375,For both subject dependent and subject independent settings on SEED our model can recognize better for positive and neutral emotions than negative emotion ,24,We present the confusion matrix of our model in .,"By combining training data from other subjects ( see ( a ) and ( b ) ) , our model is getting much worse at detecting negative emotion , indicating that participants are likely to generate distinct EEG patterns when experiencing negative emotion .",result
sentiment_analysis,38,"As a demonstration of the capability of unsupervised representation learning to simplify data collection and remove preprocessing steps , our reported results ignore these dense labels and computed parse trees , using only the raw text and sentence level labels .",analysis,Review Sentiment Analysis,0,104,6,6,0,analysis : Review Sentiment Analysis,0.6190476190476191,0.11320754716981132,0.2727272727272727,As a demonstration of the capability of unsupervised representation learning to simplify data collection and remove preprocessing steps our reported results ignore these dense labels and computed parse trees using only the raw text and sentence level labels ,39,"For the binary subtask , this amounts to 76961 total labels compared to the 6920 sentence level labels .",The representation learned by our model achieves 91.8 % significantly outperforming the state of the art of 90.2 % by a 30 model ensemble .,result
natural_language_inference,99,"when the gate has high value , more information flows from the word - level representation ; otherwise , char - level will take the dominating place .",system description,Input Embedding Layer,0,72,30,21,0,system description : Input Embedding Layer,0.2857142857142857,0.8571428571428571,0.8076923076923077,when the gate has high value more information flows from the word level representation otherwise char level will take the dominating place ,23,where is the element - wise multiplication operator .,It is practical in real scenarios .,method
text_summarization,9,We implemented our models in the Torch library ( http://torch.ch/),system description,Architectural Choices,1,96,2,2,0,system description : Architectural Choices,0.6274509803921569,0.18181818181818185,0.18181818181818185,We implemented our models in the Torch library http torch ch ,12, ,. To optimize our loss ( Equation 5 ) we used stochastic gradient descent with mini-batches of size 32 .,method
sentiment_analysis,26,"The Stanford Sentiment Treebank ( SST ) dataset consists of movie reviews taken from the Rotten Tomatoes website , including binary labels .",experiment,Experimental Setup,0,113,8,6,0,experiment : Experimental Setup,0.4612244897959184,0.1951219512195122,0.15384615384615385,The Stanford Sentiment Treebank SST dataset consists of movie reviews taken from the Rotten Tomatoes website including binary labels ,20,"In our experimental setup , these are all cast as binary polarity classification tasks , for which we use accuracy as our evaluation metric .",We only used sentence - level data in our experiment . The SemEval- 2016 Task 5 ( SE16 - T5 ) dataset provides Spanish reviews of restaurants .,experiment
natural_language_inference,7,RASOR : RECURRENT SPAN REPRESENTATION,model,RASOR: RECURRENT SPAN REPRESENTATION,0,71,20,1,0,model : RASOR: RECURRENT SPAN REPRESENTATION,0.398876404494382,0.37735849056603776,0.1111111111111111,RASOR RECURRENT SPAN REPRESENTATION,4, , ,method
natural_language_inference,94,"eventually he does offer the letter to anvoy , but anvoy declines to read it .",training,training,0,333,68,68,0,training : training,0.8694516971279374,0.576271186440678,0.576271186440678,eventually he does offer the letter to anvoy but anvoy declines to read it ,15,"he is willing to do so if it will save his friend gravener s engagement with anvoy , but gravener is unable to assure him of this .","she awards the coxon fund to saltram , who lives off it exactly as he lived off his friends , producing nothing of intellectual value .",experiment
relation-classification,0,We also try two other dependency structures : SubTree and Full - Tree .,model,Dependency Layer,0,96,48,14,0,model : Dependency Layer,0.4247787610619469,0.6233766233766234,0.7,We also try two other dependency structures SubTree and Full Tree ,12,"We primarily employ the shortest path structure ( SP - Tree ) , which captures the core dependency path between a target word pair and is widely used in relation classification models , e.g. , .",SubTree is the subtree under the lowest common ancestor of the target word pair .,method
text-classification,1,Chopping for speeding up training,system description,More simplifications,0,102,56,12,0,system description : More simplifications,0.3984375,0.28,0.3636363636363637,Chopping for speeding up training,5,"Another related work is , which combined pooling with non -LSTM recurrent networks and a word embedding .","In addition to simplifying the sub-problem , pooling has the merit of enabling faster training via chopping .",method
sentence_classification,2,b ) SUBJ : Subjectivity data where the task is to classify whether the sentence is subjective or objective .,experiment,Experimental Setting,0,143,5,4,0,experiment : Experimental Setting,0.5674603174603174,0.17857142857142858,0.14814814814814814,b SUBJ Subjectivity data where the task is to classify whether the sentence is subjective or objective ,18,a ) MR 4 : Movie reviews data where the task is to classify whether the review sentence has positive or negative sentiment .,c ) CR 5 : Customer reviews where,experiment
topic_models,0,"We will introduce the document suffix d , to make the notation explicit :",model,INTRODUCTION,0,106,51,5,0,model : INTRODUCTION,0.25728155339805825,0.9807692307692308,0.8333333333333334,We will introduce the document suffix d to make the notation explicit ,13,"Combining , and , we get the approximation to L ( q ) .","For the entire training data X , the complete ELBO will be simply the summation over all the documents , i.e. , d L ( q d ) .",method
natural_language_inference,21,The attention is proposed to compute an alignment score between elements from two sources .,system description,Attention,0,59,12,2,0,system description : Attention,0.20344827586206896,0.09448818897637797,0.2,The attention is proposed to compute an alignment score between elements from two sources ,15, ,"In particular , given the token embeddings of a source sequence x = [ x 1 , x 2 , . . . , x n ] and the vector representation of a query q , attention computes the alignment score between x i and q by a compatibility function f ( x i , q ) , which measures the dependency between x i and q , or the attention of q to xi .",method
sentiment_analysis,24,"As observed in example 2 , both PIPELINE and INABSA extract "" Pizza "" .",analysis,Results and Analysis,0,263,50,50,0,analysis : Results and Analysis,0.8456591639871383,0.8620689655172413,0.8620689655172413,As observed in example 2 both PIPELINE and INABSA extract Pizza ,12,"Due to the same reason , the message passing mechanism also helps to avoid extracting terms on which no opinion is expressed .","However , since no opinion is expressed in the given sentence , "" Pizza "" should not be considered as an aspect term .",result
sentiment_analysis,6,Extracting Context - Independent Unimodal Features,method,Extracting Context-Independent Unimodal Features,0,69,11,1,0,method : Extracting Context-Independent Unimodal Features,0.2387543252595156,0.16666666666666666,0.3333333333333333,Extracting Context Independent Unimodal Features,5, , ,method
natural_language_inference,45,"After investigating the effectiveness of the word - character fine - grained gating mechanism on the Twitter dataset , we now move onto a more challenging task , reading comprehension .",performance,PERFORMANCE ON READING COMPREHENSION,0,154,2,2,0,performance : PERFORMANCE ON READING COMPREHENSION,0.7738693467336684,0.06451612903225806,0.6666666666666666,After investigating the effectiveness of the word character fine grained gating mechanism on the Twitter dataset we now move onto a more challenging task reading comprehension ,27, ,"In this section , we experiment with two datasets , the Children 's Book Test dataset and the SQuAD dataset .",result
text_summarization,4,"E2.1 : following is the medal standing at the ##th olympic winter games - lrb - tabulated under team , @gold , silver and bronze",result,Results,0,248,54,54,0,result : Results,0.9612403100775194,0.9310344827586208,0.9310344827586208,E2 1 following is the medal standing at the th olympic winter games lrb tabulated under team gold silver and bronze,21,Linked entity : https://en.wikipedia.org/wiki/Gold,"rrb- : UNK 0.862 E2.2 : @gold opened lower hereon monday at # ## . ## -### .## us dollars an ounce , against friday 's closing rate of # # # . ## - ### . ## . 0.130 : Examples with highest / lowest dis ambiguation gate d values of two example entities ( United States and gold ) .",result
text_generation,4,"Skip - Thought is an encoder - decoder framework with an unsupervised approach to train a generic , distributed sentence encoder .",system description,Skip-Thought Vectors,0,39,12,2,0,system description : Skip-Thought Vectors,0.3577981651376147,0.3,0.4,Skip Thought is an encoder decoder framework with an unsupervised approach to train a generic distributed sentence encoder ,19, ,The encoder maps sentences sharing semantic and syntactic properties to similar vector representations and the decoder reconstructs the surrounding sentences of an encoded passage .,method
question-answering,7,functions are neural networks and are the training parameters in our NSE .,training,"Read, Compose and Write",0,107,34,26,0,"training : Read, Compose and Write",0.3890909090909091,0.6938775510204082,0.9285714285714286,functions are neural networks and are the training parameters in our NSE ,13,"As NSE reads more input content in time , the baby memory evolves and refines the encoded mental image .","As the name suggests , we use LSTM and multi -layer perceptron ( MLP ) in this paper .",experiment
text_generation,2,We use the BLEU - ( 2 to 5 ) scores as the evaluation metrics .,experiment,Long Text Generation: EMNLP2017 WMT News,0,194,17,10,0,experiment : Long Text Generation: EMNLP2017 WMT News,0.5542857142857143,0.4594594594594595,0.7692307692307693,We use the BLEU 2 to 5 scores as the evaluation metrics ,13,"Then we randomly sample 200,000 sentences as the training set and another 10,000 sentences as the test set .",The results are provided in .,experiment
named-entity-recognition,3,"We evaluate our approach on two well benchmarked sequence tagging tasks , the CoNLL 2003 NER task and the CoNLL 2000 Chunking task .",experiment,experiment,0,71,2,2,0,experiment : experiment,0.3837837837837838,0.04651162790697674,0.04651162790697674,We evaluate our approach on two well benchmarked sequence tagging tasks the CoNLL 2003 NER task and the CoNLL 2000 Chunking task ,23, ,We report the official evaluation metric ( micro - averaged F 1 ) .,experiment
relation-classification,1,"The same applies to { Apple Inc , Company - Founder , Steven Paul Jobs } .",method,From Tag Sequence To Extracted Results,0,92,27,6,0,method : From Tag Sequence To Extracted Results,0.3739837398373984,0.3857142857142857,0.6,The same applies to Apple Inc Company Founder Steven Paul Jobs ,12,"Because , the relation role of "" Trump "" is "" 2 "" and "" United States "" is "" 1 "" , the final result is { United States , Country - President , Trump } .","Besides , if a sentence contains two or more triplets with the same relation type , we combine every two entities into a triplet based on the nearest principle .",method
text_generation,5,The results for language modeling are shown in .,result,Language modeling results,1,186,2,2,0,result : Language modeling results,0.6305084745762712,0.02197802197802198,0.1176470588235294,The results for language modeling are shown in ,9, ,We report the negative log likelihood ( NLL ) and perplexity ( PPL ) of the test set .,result
natural_language_inference,79,"The dataset consists of three sets : 8544 sentences for training , 2210 sentences for test and 1101 sentences for validation ( or development ) .",experiment,Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0,147,12,5,0,experiment : Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0.5485074626865671,0.24,0.2631578947368421,The dataset consists of three sets 8544 sentences for training 2210 sentences for test and 1101 sentences for validation or development ,22,It has 11855 sentences taken from the movie review site Rotten Tomatoes .,Every sentence in the dataset has a label which goes from very negative to very positive in the scale from 0.0 to 1.0 .,experiment
natural_language_inference,52,"As we are interested in not only correctly answering questions , but also selecting valid justification for those answers , we keep track of the scores of all justifications and use this information to return the top k justifications for each answer choice .",model,Neural Network,0,137,55,9,0,model : Neural Network,0.5249042145593871,0.9821428571428572,0.9,As we are interested in not only correctly answering questions but also selecting valid justification for those answers we keep track of the scores of all justifications and use this information to return the top k justifications for each answer choice ,42,"At testing time , we use the trained model to score each answer choice ( again using the maximum justification score ) and select the highest - scoring .",These are evaluated along with the answer selection performance in Section 6 .,method
named-entity-recognition,8,"We find that BERT LARGE significantly outperforms BERT BASE across all tasks , especially those with very little training data .",experiment,GLUE,1,179,27,25,0,experiment : GLUE,0.4625322997416021,0.38028169014084506,0.3623188405797101,We find that BERT LARGE significantly outperforms BERT BASE across all tasks especially those with very little training data ,20,"On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5 , compared to OpenAI GPT , which obtains 72.8 as of the date of writing .",The effect of model size is explored more thoroughly in Section 5.2 .,experiment
named-entity-recognition,6,"Following , we use a forward and a backward LSTM to derive a representation of each word from its characters ( right part of .",model,Character Embeddings,0,117,13,2,0,model : Character Embeddings,0.5518867924528302,0.65,0.6666666666666666,Following we use a forward and a backward LSTM to derive a representation of each word from its characters right part of ,23, ,"character lookup table is randomly initialized , then trained at the same time as the Bi - LSTM model sketched in Section 4.1 .",method
topic_models,0,Cross - entropy gives a notion of how confident the classifier is about its prediction .,result,result,0,360,3,3,0,result : result,0.8737864077669902,0.1111111111111111,0.1111111111111111,Cross entropy gives a notion of how confident the classifier is about its prediction ,15,This section presents the topic ID results in terms of classification accuracy ( in % ) and cross-entropy ( CE ) on the test sets .,well calibrated classifier tends to have lower cross - entropy .,result
sentiment_analysis,29,"To analyze which word contributes the most to the aspect sentiment polarity , we visualize the final sentence attention vectors ? in .",method,Case Study,0,156,12,3,0,method : Case Study,0.8965517241379308,0.6,0.2727272727272727,To analyze which word contributes the most to the aspect sentiment polarity we visualize the final sentence attention vectors in ,21,"In , We list five examples from the test set .","The color depth indicates the importance of a word in a sentence , the darker the more important .",method
sentiment_analysis,1,The noise transition matrix specifies the probabilities of transition from each ground true label to each noisy label and is often applied to modify the crossentropy loss .,system description,Learning with Noisy Labels,0,115,43,3,0,system description : Learning with Noisy Labels,0.2904040404040404,0.20772946859903385,0.42857142857142855,The noise transition matrix specifies the probabilities of transition from each ground true label to each noisy label and is often applied to modify the crossentropy loss ,28,Commonly adopted approaches to learning with noisy labels are based on the noise transition matrix and robust loss functions .,The matrix can be pre-computed as a prior or estimated from noisy data .,method
part-of-speech_tagging,7,"For further details , see .",introduction,introduction,0,14,5,5,0,introduction : introduction,0.11199999999999999,0.2631578947368421,0.2631578947368421,For further details see ,5,Bidirectional LSTMs make a backward and forward pass through the sequence before passing onto the next layer .,We consider using bi - LSTMs for POS tagging .,introduction
natural_language_inference,99,We then introduce our model from these three perspectives .,system description,Smarnet Structure,0,51,9,4,0,system description : Smarnet Structure,0.2023809523809524,0.2571428571428571,1.0,We then introduce our model from these three perspectives ,10,"Input embedding layer , Interaction modeling layer , answer refining layer .", ,method
natural_language_inference,62,"Besides those mentioned above , other interesting attention mechanisms include performing multi-round alignment to avoid the problems of attention redundancy and attention deficiency , and using mutual attention as a skip - connector to densely connect pairwise layers .",architecture,Knowledge Aided Self Attention,0,172,77,15,0,architecture : Knowledge Aided Self Attention,0.7678571428571429,1.0,1.0,Besides those mentioned above other interesting attention mechanisms include performing multi round alignment to avoid the problems of attention redundancy and attention deficiency and using mutual attention as a skip connector to densely connect pairwise layers ,37,"Therefore we obtain the fusion resulth pi ? Rd , and further the outputs H = {h p 1 , . . . , h pn } ? R dn .","The MRC dataset used in this paper is SQuAD 1.1 , which contains over 100 , 000 passage - question pairs and has been randomly partitioned into three parts : a training set ( 80 % ) , a development set ( 10 % ) , and a test set ( 10 % ) .",method
natural_language_inference,28,"Finally , even though RUM uses an update gate , it is not a standard gated mechanism , as it does not have a reset gate .",architecture,THE RUM ARCHITECTURE,0,135,37,37,0,architecture : THE RUM ARCHITECTURE,0.4981549815498155,0.902439024390244,0.902439024390244,Finally even though RUM uses an update gate it is not a standard gated mechanism as it does not have a reset gate ,24,"This in turn means that the smaller ? is , the more we reduce the effect of exploding gradients .",Instead we suggest utilizing additional memory via the target vector ? .,method
sentiment_analysis,13,weights and fine - tunes on all 3 end tasks .,method,method,0,231,15,15,0,method : method,0.8309352517985612,0.4838709677419355,0.4838709677419355,weights and fine tunes on all 3 end tasks ,10,BERT leverages the vanilla BERT pre-trained 12 https://github.com/facebookresearch/DrQA,We use this baseline to answer RQ2 and show that BERT 's pre-trained weights alone have limited performance gains on review - based tasks .,method
natural_language_inference,5,Transfer Learning ( TL ) :,approach,Pretrained Language Model (LM):,0,94,22,19,0,approach : Pretrained Language Model (LM):,0.6394557823129252,0.6470588235294118,0.6129032258064516,Transfer Learning TL ,4,Note that we average word - embedding to obtain sentence representation in the previous equation .,"To observe the efficacy in a large dataset , we apply transfer learning using the question - answering NLI ( QNLI ) corpus .",method
natural_language_inference,40,"The intuition is that when processing referring expressions , the model should also receive information about previous mentions of the referent entity .",experiment,Text Comprehension with Coreference,0,153,11,10,0,experiment : Text Comprehension with Coreference,0.5523465703971119,0.4074074074074074,0.38461538461538464,The intuition is that when processing referring expressions the model should also receive information about previous mentions of the referent entity ,22,"separate line of work , however , has led to the development of sophisticated coreference annotators 1 , and here we use these annotations over document and question pairs as an explicit memory signal for an RNN reader .",We study the effect of adding an explicit coreference signal to RNN based models for three well studied text comprehension benchmarks .,experiment
semantic_role_labeling,4,"toy makers to move [ across the border ] . """,performance,Function F ?,0,224,31,9,0,performance : Function F ?,0.7466666666666667,0.6326530612244898,0.3333333333333333,toy makers to move across the border ,8,"To remedy these two problematic issues , it can be a promising approach to incorporate frame knowledge into SRL models by using verb frame dictionaries .",GOLD : A2 PRED : DIR,result
machine-translation,7,suggest an ensemble model in the format of mixture of experts for machine translation .,system description,RELATED WORK ON MIXTURES OF EXPERTS,0,57,14,5,0,system description : RELATED WORK ON MIXTURES OF EXPERTS,0.15281501340482573,0.2692307692307692,0.3333333333333333,suggest an ensemble model in the format of mixture of experts for machine translation ,15,"Other work has focused on different expert configurations such as a hierarchical structure , infinite numbers of experts , and adding experts sequentially .",The gating network is trained on a pre-trained ensemble NMT model .,method
sentiment_analysis,42,Compositionality in Vector Space,system description,Compositionality in Vector Space,0,235,1,1,0,system description : Compositionality in Vector Space,0.9325396825396826,0.09090909090909093,0.16666666666666666,Compositionality in Vector Space,4, , ,method
paraphrase_generation,0,"Here , we are usingq 0 andq T + 1 as the special START and STOP token respectively .",method,Decoder-LSTM,0,99,39,11,0,method : Decoder-LSTM,0.4177215189873418,0.6290322580645161,0.6875,Here we are usingq 0 andq T 1 as the special START and STOP token respectively ,17,"At t = ? 1 , we are feeding the embedding of input sentence obtained by the encoder module .? i = {q 0 , q 1 , ... , q T +1 } are the predicted question tokens for the input X i .",The predicted question token ( q i ) is obtained by applying Softmax on the probability distributionp i .,method
named-entity-recognition,1,Ratinov and Roth and showed that using a more expressive tagging scheme like IOBES improves model performance marginally .,training,Tagging Schemes,0,88,20,7,0,training : Tagging Schemes,0.4251207729468599,0.2352941176470588,0.875,Ratinov and Roth and showed that using a more expressive tagging scheme like IOBES improves model performance marginally ,19,"Using this scheme , tagging a word as I-label with high - confidence narrows down the choices for the subsequent word to I-label or E-label , however , the IOB scheme is only capable of determining that the subsequent word can not be the interior of another label .","However , we did not observe a significant improvement over the IOB tagging scheme .",experiment
natural_language_inference,46,"Answers in the dataset are human written , short , averaging 4.73 tokens , but not restricted to spans from the documents .",method,method,0,134,34,34,0,method : method,0.4511784511784512,0.68,0.68,Answers in the dataset are human written short averaging 4 73 tokens but not restricted to spans from the documents ,21,"An interesting category are questions which ask for something related to or occurring together / before / after with an event , of which there are about 15 % .","There are 44.05 % and 29.57 % answers that appear as spans of the summaries and the stories , respectively ; as expected , lower proportion of answers are spans on stories compared to summaries on which they were constructed .",method
natural_language_inference,21,The attention uses a hidden layer to compute a categorical distribution over elements from the input sequence to reflect their importance weights .,abstract,abstract,0,13,11,11,0,abstract : abstract,0.04482758620689655,0.25,0.25,The attention uses a hidden layer to compute a categorical distribution over elements from the input sequence to reflect their importance weights ,23,"performance on a large number of NLP tasks , including neural machine translation , natural language inference , conversation generation , question answering , machine reading comprehension , and sentiment analysis .","It allows RNN / CNN to maintain a variable - length memory , so that elements from the input sequence can be selected by their importance / relevance and merged into the output .",abstract
relation_extraction,9,"To this end , we propose a novel multi - level attention - based convolution neural network model .",model,The Proposed Model,0,51,4,4,0,model : The Proposed Model,0.2440191387559809,0.045454545454545456,0.3333333333333333,To this end we propose a novel multi level attention based convolution neural network model ,16,"Since the only input is a raw sentence with two marked mentions , it is non-trivial to obtain all the lexical , semantic and syntactic cues necessary to make an accurate prediction .",schematic overview of our architecture is given in .,method
text-classification,5,Classifier fine - tuning behavior,training,Impact of LM fine-tuning,0,231,37,19,0,training : Impact of LM fine-tuning,0.9166666666666666,0.8222222222222222,0.7037037037037037,Classifier fine tuning behavior,4,"Importantly , ULMFiT is the only method that shows excellent performance across the board - and is therefore the only universal method .","While our results demonstrate that how we fine - tune the classifier makes a significant difference , fine - tuning for inductive transfer is currently under-explored in NLP as it mostly has been thought to be unhelpful .",experiment
named-entity-recognition,6,use three layers of stacked residual RNN ( Bi - LSTM ) with bias decoding .,training,Training and Implementation,0,158,21,21,0,training : Training and Implementation,0.7452830188679245,0.6363636363636364,0.6774193548387096,use three layers of stacked residual RNN Bi LSTM with bias decoding ,13,"Third , our system matches state - of - the - art performances of models that use either more complex architectures or more elaborate features .",Our model is much simpler and faster .,experiment
question_answering,5,"First , we write the answer candidate a , question q and the union passagep of a as matrices A , Q , P , with each column being the embedding of a word in the sequence .",method,EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0,101,50,21,0,method : EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0.3659420289855073,0.5434782608695652,0.3333333333333333,First we write the answer candidate a question q and the union passagep of a as matrices A Q P with each column being the embedding of a word in the sequence ,33,We achieve this with word - by - word attention followed by a comparison module .,We then feed them to the bidirectional LSTM as follows :,method
prosody_prediction,0,The aligned sentences were then prosodically annotated with word - level acoustic prominence labels .,dataset,Dataset,0,67,10,10,0,dataset : Dataset,0.3489583333333333,0.43478260869565216,0.43478260869565216,The aligned sentences were then prosodically annotated with word level acoustic prominence labels ,14,"with the Montreal forced aligner , using a pronunciation lexicon and acoustic models trained on the LibriSpeech dataset .","For the annotation , we used the Wavelet Prosody Analyzer toolkit 2 , which implements the method described in .",experiment
natural_language_inference,46,"However , upon examining answers in the Jeopardy data used to construct this dataset , one finds that 80 % of answers are bigrams or unigrams , and 99 % are 5 tokens or fewer .",system description,FRANK (to the baby),0,81,65,59,0,system description : FRANK (to the baby),0.2727272727272727,0.7738095238095238,0.7564102564102564,However upon examining answers in the Jeopardy data used to construct this dataset one finds that 80 of answers are bigrams or unigrams and 99 are 5 tokens or fewer ,31,"As such , it is not open to same annotation bias as the datasets discussed above .","Of a sample of 100 answers , 72 % are named entities , all are short noun - phrases .",method
machine-translation,0,"On the other hand , the update gate controls how much information from the previous hidden state will carryover to the current hidden state .",system description,Hidden Unit that Adaptively Remembers and Forgets,0,64,29,17,0,system description : Hidden Unit that Adaptively Remembers and Forgets,0.2922374429223744,0.35365853658536583,0.7391304347826086,On the other hand the update gate controls how much information from the previous hidden state will carryover to the current hidden state ,24,"This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future , thus , allowing a more compact representation .",This acts similarly to the memory cell in the LSTM network and helps the RNN to remember longterm information .,method
sentiment_analysis,11,"Specifically , the memory cells of CMN are continuous vectors that store the context information found in the utterance histories .",introduction,introduction,1,30,20,20,0,introduction : introduction,0.0872093023255814,0.425531914893617,0.425531914893617,Specifically the memory cells of CMN are continuous vectors that store the context information found in the utterance histories ,20,It improves speakerbased emotion modeling by using memory networks which are efficient in capturing long - term dependencies and summarizing task - specific details using attention models .,CMN also models interplay of these memories to capture interspeaker dependencies .,introduction
sentiment_analysis,14,Such distant supervision method using hashtags has already been proved to provide reasonably relevant emotion labels by previous works .,system description,Larger dataset,0,71,3,3,0,system description : Larger dataset,0.4671052631578948,0.3333333333333333,0.3333333333333333,Such distant supervision method using hashtags has already been proved to provide reasonably relevant emotion labels by previous works ,20,We collect a larger dataset from Twitter with hashtags as distant supervision .,"We construct our hashtag corpus from , and 2 .",method
text-classification,0,The output dimension is 512 .,method,Deep Learning Methods,0,135,33,15,0,method : Deep Learning Methods,0.5947136563876652,0.2773109243697479,0.8823529411764706,The output dimension is 512 ,6,"The model is formed by taking mean of the outputs of all LSTM cells to form a feature vector , and then using multinomial logistic regression on this feature vector .","The variant of LSTM we used is the common "" vanilla "" architecture [ 8 ] .",method
named-entity-recognition,0,"Then improves by employing a robust loss via the 2 , 1 - norm ; the 1 - norm is applied to samples , and the 2 - norm is used for features .",system description,Classifiers,0,39,17,17,0,system description : Classifiers,0.14391143911439114,0.09550561797752807,0.4358974358974359,Then improves by employing a robust loss via the 2 1 norm the 1 norm is applied to samples and the 2 norm is used for features ,28,"Besides , the representation loss is measured by the least square measure , which is sensitive to outliers in data .","In this way , the side effect of outlier samples is relieved .",method
semantic_parsing,2,Algorithm 1 Sketch for GEO and ATIS Input : t : Tree-structure ?- calculus expression t.pred :,training,Natural Language to Logical Form,0,125,16,7,0,training : Natural Language to Logical Form,0.42955326460481097,0.13559322033898305,0.2058823529411765,Algorithm 1 Sketch for GEO and ATIS Input t Tree structure calculus expression t pred ,16,We use brackets to linearize the hierarchical structure .,"Predicate name , or operator name Output : a : Meaning sketch ( count $ 0 ( < ( fare $ 0 ) 50:do ) ) ? ( count# 1 ( < fare@1 ? ) ) function SKETCH ( t )",experiment
natural_language_inference,44,"Examples on SQuAD , which MINIMAL predicts the wrong answer .",training,SQuAD-Adversarial,0,272,57,19,0,training : SQuAD-Adversarial,0.951048951048951,0.8142857142857143,0.59375,Examples on SQuAD which MINIMAL predicts the wrong answer ,10,Who was the Jin dynasty defector who betrayed the location of the Jin army ? :,"Grountruth span is in underlined text , the prediction from MINIMAL is in red text .",experiment
machine-translation,6,"First , such embeddings will affect the semantic understanding of words .",introduction,introduction,0,29,20,20,0,introduction : introduction,0.09965635738831616,0.5714285714285714,0.5714285714285714,First such embeddings will affect the semantic understanding of words ,11,We argue that the different behaviors of the embeddings of popular words and rare words are problematic .,We observe more than half of the rare words are nouns or variants of popular words .,introduction
sentiment_analysis,35,"In MGAN , a novel Coarse 2 Fine attention guided by an auxiliary task can help the AC task modeling at the same finegrained level with the AT task .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.03629032258064517,0.5833333333333334,0.5833333333333334,In MGAN a novel Coarse 2 Fine attention guided by an auxiliary task can help the AC task modeling at the same finegrained level with the AT task ,29,"To resolve both the aspect granularity inconsistency and feature mismatch between domains , we propose a Multi - Granularity Alignment Network ( MGAN ) .","To alleviate the feature false alignment , a contrastive feature alignment method is adopted to align aspect - specific feature representations semantically .",abstract
relation_extraction,11,"For cases when an entity has multiple types in different contexts , for instance , Paris may have types government and location , we take the average over the embeddings of each type .",system description,Entity Type Side Information,0,178,111,8,0,system description : Entity Type Side Information,0.717741935483871,0.8671875,0.8,For cases when an entity has multiple types in different contexts for instance Paris may have types government and location we take the average over the embeddings of each type ,31,"For each type , we define a kt - dimensional embedding which we call as entity type embedding ( h type ) .",We concatenate the entity type embedding of target entities to the final bag representation before using it for relation classification .,method
natural_language_inference,9,The recursive definition of Equation 3 can be explicitly written as,model,PARALLELIZATION,0,131,91,8,0,model : PARALLELIZATION,0.39696969696969703,0.9479166666666666,0.6153846153846154,The recursive definition of Equation 3 can be explicitly written as,11,The proof for QRN with vector gates is shown in Appendix B .,Let bi = log ( 1 ? z i ) for brevity .,method
named-entity-recognition,3,"To test the sensitivity to the LM training domain , we also applied Tag LM with a LM trained on news articles to the SemEval 2017 Shared Task 10 , Science IE .",analysis,analysis,0,148,22,22,0,analysis : analysis,0.8,0.88,0.88,To test the sensitivity to the LM training domain we also applied Tag LM with a LM trained on news articles to the SemEval 2017 Shared Task 10 Science IE ,31,Does the LM transfer across domains ? One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles .,"Scien - ce IE requires end - to - end joint entity and relationship extraction from scientific publications across three diverse fields ( computer science , material sciences , and physics ) and defines three broad entity types ( Task , Material and Process ) .",result
sentiment_analysis,5,"When the improvement of BiHDM over the method is statistically significant , the results will be underlined in the table .",experiment,The EEG emotion recognition experiments,0,182,47,17,0,experiment : The EEG emotion recognition experiments,0.6867924528301886,0.5402298850574713,0.4857142857142857,When the improvement of BiHDM over the method is statistically significant the results will be underlined in the table ,20,"To test if the proposed BiHDM is statistically significantly better than the baseline method , paired t- test statistical analysis is conducted at the significant level of 0.05 .","shows the t- test statistical analysis results , from which we can see BiHDM is significantly better than the baseline method .",experiment
natural_language_inference,23,We argue that this hypothesis also holds in language understanding and MRC .,introduction,introduction,1,35,19,19,0,introduction : introduction,0.0682261208576998,0.5588235294117647,0.5588235294117647,We argue that this hypothesis also holds in language understanding and MRC ,13,"Taking image recognition as an example , information in various levels of representations can capture different aspects of details in an image : pixel , stroke and shape .","In other words , an approach that utilizes all the information from the word embedding level up to the highest level representation would be substantially beneficial for understanding both the question and the context , hence yielding more accurate answers .",introduction
named-entity-recognition,9,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma . . .,BioBERT,BioBERT,0,183,2,2,0,BioBERT : BioBERT,0.9195979899497488,0.1111111111111111,1.0,Corynebacterium minutissimum is the bacteria that leads to cutaneous eruptions of erythrasma ,13, ,"Like the DMA , but unlike all other mammalian class II A genes , the zebrafish gene codes for two cysteine residues . . .",others
natural_language_inference,47,"We applied L2 regularization to all models , manually tuning the strength coefficient ? for each , and additionally applied dropout to the inputs and outputs of the sen - tence embedding models ( though not to its internal connections ) with a fixed dropout rate .",model,Sentence embeddings and NLI,0,159,45,14,0,model : Sentence embeddings and NLI,0.7395348837209302,0.8333333333333334,0.6086956521739131,We applied L2 regularization to all models manually tuning the strength coefficient for each and additionally applied dropout to the inputs and outputs of the sen tence embedding models though not to its internal connections with a fixed dropout rate ,41,"All of the models are randomly initialized using standard techniques and trained using AdaDelta ( Zeiler , 2012 ) minibatch SGD until performance on the development set stops improving .","We applied L2 regularization to all models , manually tuning the strength coefficient ? for each , and additionally applied dropout to the inputs and outputs of the sen - tence embedding models ( though not to its internal connections ) with a fixed dropout rate .",method
natural_language_inference,19,"In max pooling , the max value is selected for each column of U ? R n2 de .",result,SNLI Mix,0,241,69,43,0,result : SNLI Mix,0.9450980392156862,0.8313253012048193,0.7543859649122807,In max pooling the max value is selected for each column of U R n2 de ,17,"For the multi-dimensional attention corresponding to ( a ) , we visualized the attention weights averaged for each word , where attention weights correspond to softmax ( L ) ? R n2 de in equation 15 .","In max pooling , the max value is selected for each column of U ? R n2 de .",result
phrase_grounding,0,Results are sorted by performance to show the most successful combinations .,ablation,Ablation Study,0,199,4,4,0,ablation : Ablation Study,0.8844444444444445,0.2352941176470588,0.2352941176470588,Results are sorted by performance to show the most successful combinations ,12,We report evaluation results on Flickr30 k in .,"We first evaluated the efficacy of using multi-level feature maps ( ML ) with level selection compared to a fixed choice of visual layer ( M : middle layer , L : last layer ) for comparison to word and sentence embeddings ( WL and SL ) .",result
text-classification,1,6.08 9.24 5 oh -CNN 1200 - dim CNN tv-embed .,system description,Comparison with the previous best results on 20NG,0,193,147,10,0,system description : Comparison with the previous best results on 20NG,0.75390625,0.735,0.15873015873015872,6 08 9 24 5 oh CNN 1200 dim CNN tv embed ,13,Note that being able to naturally combine several tv-embeddings is a strength of 2100 - dim LSTM tv-embed .,"6.57 7.97 our framework , which uses unlabeled data to produce additional input to LSTM instead of pre-training .",method
sentiment_analysis,31,These units both are generated from aspect - specific features and are further applied on the sentence .,introduction,introduction,1,24,18,18,0,introduction : introduction,0.1509433962264151,0.9473684210526316,0.9473684210526316,These units both are generated from aspect specific features and are further applied on the sentence ,17,"One is parameterized filter , the other is parameterized gate .",Our experiments show that our two model variants work surprisingly well on this type of task .,introduction
sentiment_analysis,21,"Another contribution of this paper is demonstrating the effectiveness of concatenating naive bayes weighted bag of n-grams with DV - ngram , L2 R dot product , or document vectors trained with cosine similarity , the last achieving state of the art accuracy on the IMDB dataset .",Feature Combination,Feature Combination,0,127,2,2,0,Feature Combination : Feature Combination,0.9071428571428573,0.5,0.5,Another contribution of this paper is demonstrating the effectiveness of concatenating naive bayes weighted bag of n grams with DV ngram L2 R dot product or document vectors trained with cosine similarity the last achieving state of the art accuracy on the IMDB dataset ,45, ,We note that all models utilize unigrams to trigrams and additional unlabeled data if possible .,others
sentiment_analysis,11,We perform feature level fusion to map the individual modalities to a joint space .,approach,Fusion:,0,139,44,2,0,approach : Fusion:,0.4040697674418605,0.8979591836734694,0.2857142857142857,We perform feature level fusion to map the individual modalities to a joint space ,15, ,This is done through a simple feature concatenation .,method
natural_language_inference,24,"At the beginning , the initial state s 0 is the summary of the Q:",system description,Experiment Setup,0,104,77,14,0,system description : Experiment Setup,0.4444444444444444,0.8461538461538461,0.5,At the beginning the initial state s 0 is the summary of the Q ,15,"This module is a memory network and has some similarities to other multi-step reasoning networks : namely , it maintains a state vector , one state per step .","Here , x t is computed from the previous state s t?1 and memory M : x t = j ? j M j and ? j = sof tmax ( s t?1 W 5 M ) .",method
natural_language_inference,38,We first evaluate our model on a large scale reading comprehension dataset Trivia QA version 1.0 .,result,result,0,120,2,2,0,result : result,0.6349206349206349,0.16666666666666666,1.0,We first evaluate our model on a large scale reading comprehension dataset Trivia QA version 1 0 ,18, , ,result
machine-translation,6,We introduce a discriminator to categorize word embeddings into two classes : popular ones or rare ones .,method,Our Method,0,112,4,4,0,method : Our Method,0.3848797250859107,0.08333333333333333,0.08333333333333333,We introduce a discriminator to categorize word embeddings into two classes popular ones or rare ones ,17,"As we have a strong prior that many rare words should share the same region in the embedding space as popular words , the basic idea of our algorithm is to train the word embeddings in an adversarial framework :",We hope the learned word embeddings not only minimize the task - specific training loss but also fool the discriminator .,method
natural_language_inference,96,"Further , if a Turker ranks a generated ending as best , and the found ending as second best , then we have reason to believe that the generation is good .",system description,Human verification,0,174,116,8,0,system description : Human verification,0.4461538461538462,0.9666666666666668,0.6666666666666666,Further if a Turker ranks a generated ending as best and the found ending as second best then we have reason to believe that the generation is good ,29,"If a Turker ranks the found ending as either best or second best ( 73.7 % of the time ) , we add the found ending as a gold example , with negatives from the generations not labelled best or gibberish .","This lets us add an additional training example , consisting of the generated best ending as the gold , and remaining generations as negatives .",method
paraphrase_generation,0,Also the colored lines between the two models represents that these models are not significantly different from each other .,model,Model,0,176,8,8,0,model : Model,0.7426160337552743,0.8,0.8888888888888888,Also the colored lines between the two models represents that these models are not significantly different from each other ,20,Here EDD - LG - S refers to our EDD - LG shared model and others are the different variations of our model described in section 4.1.3 and the models on the right are the different variations proposed in .,"CD=5.199,p=0.0069",method
natural_language_inference,28,The optimizer is RMSProp with a learning rate 0.001 .,experiment,ASSOCIATIVE RECALL TASK,1,177,38,13,0,experiment : ASSOCIATIVE RECALL TASK,0.6531365313653137,0.7450980392156863,0.8666666666666667,The optimizer is RMSProp with a learning rate 0 001 ,11,We use a batch size 128 .,"We find that LSTM fails to learn the task , because of its lack of sufficient memory capacity .",experiment
sentiment_analysis,10,"This enables the model to track the parties ' emotion dynamics through the conversations , which is related to the emotion behind the utterances .",model,Our Model,0,67,7,7,0,model : Our Model,0.2607003891050584,0.11864406779661014,0.125,This enables the model to track the parties emotion dynamics through the conversations which is related to the emotion behind the utterances ,23,"Our model DialogueRNN , 1 shown in , models these three factors as follows : each party is modeled using a party state which changes as and when that party utters an utterance .","Furthermore , the context of an utterance is modeled using a global state ( called global , because of being shared among the parties ) , where the preceding utterances and the party states are jointly encoded for context representation , necessary for accurate party state representation .",method
natural_language_inference,10,"where W s clf ? R 1 Dc , b s clf ? R 1 , and ?",analysis,Sentiment Analysis,0,173,8,8,0,analysis : Sentiment Analysis,0.729957805907173,0.1509433962264151,0.3809523809523809,where W s clf R 1 Dc b s clf R 1 and ,14,"Specifically , for a sentence embedding h , the probability for the sentence to be predicted as label s ? { 0 , 1 } ( in the binary setting , SST - 2 ) or s ? { 1 , 2 , 3 , 4 , 5 } ( in the fine - grained setting , SST - 5 ) is computed as follows :","where W s clf ? R 1 Dc , b s clf ? R 1 , and ?",result
text_generation,5,We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders .,system description,Model configurations and Training details,1,154,2,2,0,system description : Model configurations and Training details,0.5220338983050847,0.0625,0.0625,We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders ,17, ,"For CNNs , we explore several different configurations .",method
natural_language_inference,15,"Motivated by Densenet ) , we employ direct connections using the concatenation operation from any layer to all the subsequent layers so that the features of previous layers are not to be modified but to be retained as they are as depicted in .",method,Densely connected Recurrent Networks,0,91,33,12,0,method : Densely connected Recurrent Networks,0.4026548672566372,0.66,0.9230769230769232,Motivated by Densenet we employ direct connections using the concatenation operation from any layer to all the subsequent layers so that the features of previous layers are not to be modified but to be retained as they are as depicted in ,42,"However , the summation operation in the residual connection may impede the information flow in the network .",The densely connected recurrent neural networks can be described ash l,method
natural_language_inference,58,Internal State Controller :,system description,Small Graph Large Graph,0,235,2,2,0,system description : Small Graph Large Graph,0.7014925373134329,0.020202020202020204,0.08333333333333333,Internal State Controller ,4, ,We use a GRU model with 256 dimensional hidden units as the internal state controller .,method
natural_language_inference,67,"1 ) We pro - We also propose several simple but effective features to strengthen the attention mechanism , which fundamentally improves candidate ranking , with the by -product of higher exact boundary match accuracy .",introduction,introduction,0,29,22,22,0,introduction : introduction,0.14215686274509806,0.6666666666666666,0.6666666666666666,1 We pro We also propose several simple but effective features to strengthen the attention mechanism which fundamentally improves candidate ranking with the by product of higher exact boundary match accuracy ,32,The contributions of this paper are three - fold .,"The experiments on the Stanford Question Answering Dataset ( SQuAD ) , which contains a variety of human - generated factoid and non-factoid questions , have shown the effectiveness of above three contributions .",introduction
natural_language_inference,99,We further utilize Q to form the question - aware passage representation .,model,Contextual Encoding,0,126,49,40,0,model : Contextual Encoding,0.5,0.5104166666666666,0.7407407407407407,We further utilize Q to form the question aware passage representation ,12,"Hence the attend question vector for all passage words Q ? R 2d is obtained by Q = j a j Q :j , where Q :j ? R 2 dn .","In order to comprehensively model the mutual information between the question and passage , we adopt a heuristic combining strategy to yield the extension as follows :",method
natural_language_inference,60,"The words , however , where characterized by their polysemy , in particular by having both noun and verb senses .",model,Examining Contextualization,0,182,25,6,0,model : Examining Contextualization,0.9430051813471504,0.6944444444444444,0.35294117647058826,The words however where characterized by their polysemy in particular by having both noun and verb senses ,18,"In part , this maybe explained by the small size of the dev set , but for the Glove + FastText model we inspected there were only around twenty words with any variance at all , which suggests that the field needs to work on more difficult semantic benchmark tasks .","The following words were all in the top 20 most contextdependent words : mob , boards , winds , trains , pitches , camp .",method
natural_language_inference,14,Enabling commonsense reasoning in machines is a longstanding challenge in AI .,introduction,introduction,0,15,2,2,0,introduction : introduction,0.08771929824561403,0.10526315789473684,0.10526315789473684,Enabling commonsense reasoning in machines is a longstanding challenge in AI ,12, ,The rise of datadriven methods has led to interest in developing large datasets for commonsense reasoning over text .,introduction
sentiment_analysis,32,"With this design , the IAN model can well represent a target and its collocative context , which is helpful to sentiment classification .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.0391304347826087,0.875,0.875,With this design the IAN model can well represent a target and its collocative context which is helpful to sentiment classification ,22,"Then , we propose the interactive attention networks ( IAN ) to interactively learn attentions in the contexts and targets , and generate the representations for targets and contexts separately .",Experimental results on Se -m Eval 2014 Datasets demonstrate the effectiveness of our model .,abstract
text_summarization,3,The first was the English Gigaword Fifth Edition corpus .,system description,Distant Supervision (DS) for Model Adaption,0,148,93,20,0,system description : Distant Supervision (DS) for Model Adaption,0.6115702479338843,0.7099236641221374,0.3448275862068966,The first was the English Gigaword Fifth Edition corpus ,10,"To evaluate the effectiveness of our proposed model , we conducted training and testing on two popular datasets .",We replicated the pre-processing steps in to obtain the same training / testing data .,method
paraphrase_generation,1,"detailed zoomed - in view of our model architecture is shown in , where we show all the components , including the LSTM encoders and decoders .",model,Model Architecture,0,77,9,9,0,model : Model Architecture,0.34841628959276016,0.3461538461538461,0.3461538461538461,detailed zoomed in view of our model architecture is shown in where we show all the components including the LSTM encoders and decoders ,24,"In particular , as shows , the VAE encoder as well as decoder are conditioned on the original sentence .","In particular , our model consists of three LSTM encoders and one LSTM decoder ( thus a total of four LSTMs ) , which are employed by our VAE based architecture as follows :",method
sentiment_analysis,2,are weight matrix and bias respectively .,model,Word Representation,0,105,56,40,0,model : Word Representation,0.4625550660792952,0.9491525423728814,0.9302325581395348,are weight matrix and bias respectively ,7,where W Rand b,"We feed x into a linear layer , the length of whose output equals to the number of class labels S .",method
sentiment_analysis,40,"Specifically , it 's a comparative sentence in which the reviewer has a positive sentiment on the first commented person , but a negative sentiment on the second person , and our model predicts both of them correctly .",result,Case Study,0,210,47,13,0,result : Case Study,0.9417040358744396,0.903846153846154,0.7222222222222222,Specifically it s a comparative sentence in which the reviewer has a positive sentiment on the first commented person but a negative sentiment on the second person and our model predicts both of them correctly ,36,"present a case that there are more than one opinion targets in a comment , which can not be analyzed with sentence - level sentiment analysis methods properly .","Although all useful information ( e.g. "" than "" and "" stronger "" ) is attended in both cases , the attention procedures of them show some interesting differences .",result
machine-translation,1,Our comparison criteria reflect the desiderata set out in Sect. 2.1 .,model,model,0,126,15,15,0,model : model,0.6268656716417911,0.4545454545454545,0.4545454545454545,Our comparison criteria reflect the desiderata set out in Sect 2 1 ,13,In our comparison we consider the following neural translation models : the Recurrent Continuous Translation Model ( RCTM ) 1 and 2 ; the RNN Enc - Dec ; the RNN Enc - Dec Att with the attentional pooling mechanism of which there are a few variations ; the Grid LSTM translation model ) that uses a multi-dimensional architecture ; the Extended Neural GPU model ) that has a convolutional RNN architecture ; the ByteNet and the two Recurrent ByteNet variants .,We separate the first ( computation time ) desider - atum into three columns .,method
natural_language_inference,30,"ReVerb is an open - source data base composed of more than 14M triples , made of more than 2 M entities and 600 k relationships , which have been automatically extracted from the ClueWeb09 corpus .",training,Training Data,0,79,5,5,0,training : Training Data,0.3062015503875969,0.1111111111111111,0.4545454545454545,ReVerb is an open source data base composed of more than 14M triples made of more than 2 M entities and 600 k relationships which have been automatically extracted from the ClueWeb09 corpus ,34,The set of potential answers K is given by the KB ReVerb .,"In the following , entities are denoted with a .e suffix and relationships with a .r suffix .",experiment
question_answering,1,"This is in contrast with our approach , where the attention is pre-computed before flowing to the modeling layer .",experiment,QUESTION ANSWERING EXPERIMENTS,0,198,35,35,0,experiment : QUESTION ANSWERING EXPERIMENTS,0.6246056782334385,0.3888888888888889,0.5303030303030303,This is in contrast with our approach where the attention is pre computed before flowing to the modeling layer ,20,"To evaluate the attention flow , we study a dynamic attention model , where the attention is dynamically computed within the modeling layer 's LSTM , following previous work .","Despite being a simpler attention mechanism , our proposed static attention outperforms the dynamically computed attention by more than 3 points .",experiment
natural_language_inference,38,"Second , we introduce a memory networks of fullorientation matching .",introduction,introduction,0,34,24,24,0,introduction : introduction,0.17989417989417988,0.7741935483870968,0.7741935483870968,Second we introduce a memory networks of fullorientation matching ,10,The analogy inference provided by skip - gram model can make the similar attributes close in their embedding space such that more adept at helping find the answer .,"To combines the advantages of one dimensional attention and two dimensional attention , our novel hierarchical attention vectors contain both of them .",introduction
natural_language_inference,96,"More formally , let our input space be X and the label space be Y .",system description,Formal definition,0,108,50,4,0,system description : Formal definition,0.2769230769230769,0.4166666666666667,0.3636363636363637,More formally let our input space be X and the label space be Y ,15,"Intuitively , we say that an adversarial dataset for a model f is one on which f will not generalize , even if evaluated on test data from the same distribution .","Our trainable classifier f , taking parameters ? is defined as f ? :",method
relation_extraction,10,We consider every ordered pair of selected spans ( from Step 2 ) such that both spans are from the same sentence .,model,Step 3: Relation Extraction (RE),0,120,66,3,0,model : Step 3: Relation Extraction (RE),0.625,0.8048780487804879,0.2727272727272727,We consider every ordered pair of selected spans from Step 2 such that both spans are from the same sentence ,21,"In this paper , we only consider ordered binary relations , the most common setting of RE i.e. only relations between exactly two arguments and where the two pairs ( span i , span j ) and ( span j , span i ) are considered different .","For each such pair ( span i , span j ) , we first compute an ordered pair embedding r ( i , j ) :",method
natural_language_inference,62,"In the future , we plan to use some larger knowledge bases , such as Con-ceptNet and Freebase , to improve the quality and scope of the general knowledge .",analysis,Analysis,0,224,14,14,0,analysis : Analysis,1.0,1.0,1.0,In the future we plan to use some larger knowledge bases such as Con ceptNet and Freebase to improve the quality and scope of the general knowledge ,28,"Experimental results show that KAR is not only comparable in performance with the state - of - the - art MRC models , but also superior to them in terms of both the hunger for data and the robustness to noise .", ,result
natural_language_inference,78,"Moreover , our lightweight adaptation achieves 87.7 % with only 750K parameters , which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model ( 86.8 % ) .",experiment,Experimental Results,1,218,16,16,0,experiment : Experimental Results,0.7898550724637681,0.41025641025641024,0.41025641025641024,Moreover our lightweight adaptation achieves 87 7 with only 750K parameters which makes it extremely performant amongst models having the same amount of parameters such as the decomposable attention model 86 8 ,33,"At 88.1 % , our model has about three times less parameters than ESIM / DIIN ( i.e. , 1.4 M versus 4.3M / 4.4M ) .","Finally , an ensemble of 5 CAFE models achieves 89.3 % test accuracy , the best test scores on the SNLI benchmark to date 3 .",experiment
natural_language_inference,65,"Sections 5 and 6 detail our experimental setup and results , respectively .",introduction,introduction,0,40,32,32,0,introduction : introduction,0.17391304347826084,0.9696969696969696,0.9696969696969696,Sections 5 and 6 detail our experimental setup and results respectively ,12,"In Section 4 , we discuss some related work .",In Section 7 we present our final remarks .,introduction
sentiment_analysis,40,"The backward LSTM does the same thing , except that its input sequence is reversed .",model,BLSTM for Memory Building,0,91,26,14,0,model : BLSTM for Memory Building,0.4080717488789238,0.41935483870967744,0.875,The backward LSTM does the same thing except that its input sequence is reversed ,15,"The gates i , f , o ? R ? ? d l simulate binary switches that control whether to update the information from the current input , whether to forget the information in the memory cells , and whether to reveal the information in memory cells to the output , respectively .","If there are L layers stacked in the BLSTM , the final memory generated in this",method
sentiment_analysis,6,"Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data , e.g. , positive sentiment can be caused by joy or anticipation , while negative sentiment can be caused by fear or disgust .",introduction,introduction,1,11,4,4,0,introduction : introduction,0.03806228373702422,0.1111111111111111,0.1111111111111111,Emotion recognition further breaks down the inferred polarity into a set of emotions conveyed by the subjective data e g positive sentiment can be caused by joy or anticipation while negative sentiment can be caused by fear or disgust ,40,"Sentiment analysis can be performed at different granularity levels , e.g. , subjectivity detection simply classifies data as either subjective ( opinionated ) or objective ( neutral ) , while polarity detection focuses on determining whether subjective data indicate positive or negative sentiment .","Even though the primary focus of this paper is to classify sentiment in videos , we also show the performance of the proposed method for the finergrained task of emotion recognition .",introduction
sentiment_analysis,5,Here d l = d r .,model,The BiHDM model,0,83,19,19,0,model : The BiHDM model,0.3132075471698113,0.2676056338028169,0.31666666666666665,Here d l d r ,6,"where s l i , s r i and d l , d rare the hidden units and the dimensions of RNN modules on the left and right hemispheres , respectively ; ? ( ) denotes the nonlinear operation such as Sigmoid function ; } are the learnable transformation matrices of the two hemispheric RNN modules ; and N ( x i ) denotes the set of predecessors of the node x i .","As the RNN modules traverse all the nodes in N land N r , the obtained hidden states s l i and s r i can be used as the deep features to represent the i - th electrode 's data on two hemispheres .",method
relation-classification,9,"For sequence labeling , we use the same BiLSTM layers and use a conditional random field to guarantee well - formed predictions .",system description,Frozen BERT Embeddings,0,96,39,5,0,system description : Frozen BERT Embeddings,0.6530612244897959,0.8297872340425532,0.38461538461538464,For sequence labeling we use the same BiLSTM layers and use a conditional random field to guarantee well formed predictions ,21,"For text classification , we feed each sentence of BERT vectors into a 2 - layer BiLSTM of size 200 and apply a multilayer perceptron ( with hidden size 200 ) on the concatenated first and last BiLSTM vectors .","For DEP , we use the full model from with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks .",method
natural_language_inference,61,"and a hypothesis in a large training set , then predicts the relation between a new pair of premise and hypothesis .",introduction,introduction,0,21,14,14,0,introduction : introduction,0.1346153846153846,0.27450980392156865,0.27450980392156865,and a hypothesis in a large training set then predicts the relation between a new pair of premise and hypothesis ,21,Publication rights licensed to ACM .,existing methods of NLI can be roughly partitioned into two categories : feature - based models and neural network - based models .,introduction
natural_language_inference,64,We conjecture that a large number of examples are required to fine - tune the large number of Transformer parameters on the new task .,system description,Transformers for AS2,0,98,27,20,0,system description : Transformers for AS2,0.3904382470119522,0.4153846153846154,0.7142857142857143,We conjecture that a large number of examples are required to fine tune the large number of Transformer parameters on the new task ,24,"When only small target data is available , the transfer process from the language model to AS2 task is unstable .","An evidence of this is the on - off effect , that is , the fine - tuned model always predicts a single label for all examples .",method
passage_re-ranking,0,"Although retrieved output can be further re-ranked using a neural model to greatly enhance effectiveness , the output can also be returned as - is .",introduction,introduction,0,33,23,23,0,introduction : introduction,0.2704918032786885,0.9583333333333334,0.9583333333333334,Although retrieved output can be further re ranked using a neural model to greatly enhance effectiveness the output can also be returned as is ,25,"Document expansion also presents another major advantage , since the enrichment is performed prior to indexing :","These results already yield a noticeable improvement in effectiveness over a "" bag of words "" baseline without the need to apply expensive and slow neural network inference at retrieval time .",introduction
natural_language_inference,56,For each graph we compute the shortest path from the anchorperson to the person in question .,experiment,Age arithmetic task details,0,307,30,9,0,experiment : Age arithmetic task details,0.9136904761904762,0.5084745762711864,0.5294117647058824,For each graph we compute the shortest path from the anchorperson to the person in question ,17,The question is the age of one of the persons at random ( 0 - 7 ) .,This is the minimum number of arithmetic computations that must be performed to infer the persons age from the given facts .,experiment
relation-classification,2,"To demonstrate the effectiveness of the proposed method , we conduct the largest experimental evaluation to date ( to the best of our knowledge ) in jointly performing both entity recognition and relation extraction ( see Section 4 and Section 5 ) , using different datasets from various domains ( i.e. , news , biomedical , real estate ) and languages ( i.e. , English , Dutch ) .",introduction,introduction,0,40,30,30,0,introduction : introduction,0.13559322033898305,0.9375,0.9375,To demonstrate the effectiveness of the proposed method we conduct the largest experimental evaluation to date to the best of our knowledge in jointly performing both entity recognition and relation extraction see Section 4 and Section 5 using different datasets from various domains i e news biomedical real estate and languages i e English Dutch ,56,"Finally , we solve the underlying problem of the models proposed by and , who essentially assume classes ( i.e. , relations ) to be mutually exclusive : we solve this by phrasing the relation extraction component as a multi-label prediction problem .","Specifically , we apply our method to four datasets , namely ACE04 ( news ) , Adverse Drug Events ( ADE ) , Dutch Real Estate Classifieds ( DREC ) and CoNLL'04 ( news ) .",introduction
natural_language_inference,58,"Interestingly , the model starts from the end node , traverses backward till nding the starting node ( 10 ) in step 9 , and makes arm termination prediction .",system description,Internal State Controller:,0,316,83,25,0,system description : Internal State Controller:,0.9432835820895522,0.8383838383838383,0.6097560975609756,Interestingly the model starts from the end node traverses backward till nding the starting node 10 in step 9 and makes arm termination prediction ,25,. The highest attention word at each step shows the reasoning process of the model .,"On the other hand , in , the model learns to stop in step",method
natural_language_inference,40,We denote the set of all forward edge types by E f and all backward edge types by E b .,method,From Sequences to DAGs,0,98,21,20,0,method : From Sequences to DAGs,0.35379061371841153,0.3230769230769231,0.8695652173913043,We denote the set of all forward edge types by E f and all backward edge types by E b ,21,"By construction , G f and G bare DAGs , i.e. , they do not contain cycles .",For every DAG there exists a topological ordering of its nodes in a sequence such that all edges in the graph are directed from preceding nodes to succeeding nodes in the sequence .,method
semantic_role_labeling,0,"The final SRL output would be all the non-empty relations { ( p , a , l ) ? Y | l = }.",model,Model,0,28,6,6,0,model : Model,0.26666666666666666,0.2727272727272727,0.2727272727272727,The final SRL output would be all the non empty relations p a l Y l ,17,"contains all the spans ( arguments ) , and L is the space of semantic role labels , including a null label indicating no relation .","We then define a set of random variables , where each random variable y p , a corresponds to a predicate p ? P and an argument a ? A , taking value from the discrete label space L.",method
natural_language_inference,11,"Indeed , there is a strong performance drop of about 10 % on MultiNLI examples for both the BiLSTM and the ESIM model for which either a synonym or an antonym-assertion is present .",analysis,Qualitative Analysis,0,167,6,6,0,analysis : Qualitative Analysis,0.605072463768116,0.2857142857142857,0.2857142857142857,Indeed there is a strong performance drop of about 10 on MultiNLI examples for both the BiLSTM and the ESIM model for which either a synonym or an antonym assertion is present ,33,We hypothesize that in many cases these two predicates are very important for predicting either contradic - tion or entailment .,This very large drop clearly shows that our models are sensitive to the semantics of the provided knowledge .,result
sentiment_analysis,41,Attention - based LSTM with Aspect Embedding ( ATAE - LSTM ),system description,Attention-based LSTM with Aspect Embedding (ATAE-LSTM),0,110,72,1,0,system description : Attention-based LSTM with Aspect Embedding (ATAE-LSTM),0.4932735426008968,0.935064935064935,0.16666666666666666,Attention based LSTM with Aspect Embedding ATAE LSTM ,9, , ,method
text-to-speech_synthesis,1,"Our frame size and hop size are set to 1024 and 256 , respectively .",dataset,Datasets,0,113,6,6,0,dataset : Datasets,0.5159817351598174,0.8571428571428571,0.8571428571428571,Our frame size and hop size are set to 1024 and 256 respectively ,14,"For the speech data , we convert the raw waveform into mel-spectrograms following .","In order to evaluate the robustness of our proposed FastSpeech , we also choose 50 sentences which are particularly hard for TTS system , following the practice in .",experiment
machine-translation,4,Experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self - attention layers of NMT .,introduction,introduction,0,44,36,36,0,introduction : introduction,0.18410041841004185,1.0,1.0,Experimental results reveal that it deserves more efforts for researchers to investigate the temporal order information within self attention layers of NMT ,23,"Last but not least , we introduce the directional self - attention to model temporal order information for the proposed model .", ,introduction
natural_language_inference,23,"For the ensemble model , we apply the standard voting scheme : each model generates an answer span , and the answer with the highest votes is selected .",model,MODEL DETAILS,0,403,10,10,0,model : MODEL DETAILS,0.7855750487329435,0.08333333333333333,0.43478260869565216,For the ensemble model we apply the standard voting scheme each model generates an answer span and the answer with the highest votes is selected ,26,All models are implemented in PyTorch ( http://pytorch.org / ) .,We break ties randomly .,method
text-classification,0,Bag - of - means is a misuse of word2vec .,method,Figure 3: Relative errors with comparison models,0,215,113,28,0,method : Figure 3: Relative errors with comparison models,0.947136563876652,0.9495798319327732,0.8235294117647058,Bag of means is a misuse of word2vec ,9,This dichotomy in task semantics does not seem to play a role in deciding which method is better .,One of the most obvious facts one could observe from table 4 and figure 3 a is that the bag - of - means model performs worse in every case .,method
text_summarization,0,RQ1 : Does RASG outperform other baselines ? RQ2 : What is the effect of each module in RASG ? RQ3 : Does RASG capture useful information from noisy comments ? RQ4 : Can goal tracker give a helpful guidance to decoder ? Dataset,experiment,Research questions,0,202,4,3,0,experiment : Research questions,0.6710963455149501,0.21052631578947367,0.16666666666666666,RQ1 Does RASG outperform other baselines RQ2 What is the effect of each module in RASG RQ3 Does RASG capture useful information from noisy comments RQ4 Can goal tracker give a helpful guidance to decoder Dataset,36,We list four research questions that guide the experiments :,RQ1 : Does RASG outperform other baselines ? RQ2 : What is the effect of each module in RASG ? RQ3 : Does RASG capture useful information from noisy comments ? RQ4 : Can goal tracker give a helpful guidance to decoder ? Dataset,experiment
text-classification,7,"The stronger the connection strength , the bigger the font size .",ablation,Connection Strength Visualization,0,187,37,8,0,ablation : Connection Strength Visualization,0.7695473251028807,0.6607142857142857,0.7272727272727273,The stronger the connection strength the bigger the font size ,11,We use the tag cloud to visualize the 3 - gram phrases for Interest Rates and Money / Foreign Exchange categories .,"From the results , we observe that capsule networks can correctly recognize and cluster the important phrases with respect to the text categories .",result
relation-classification,1,We run 10 times for each experiment then report the average results and their standard deviation as shows .,evaluation,evaluation,0,151,8,8,0,evaluation : evaluation,0.6138211382113821,1.0,1.0,We run 10 times for each experiment then report the average results and their standard deviation as shows ,19,We create a validation set by randomly sampling 10 % data from test set and use the remaining data as evaluation based on ) 's suggestion ., ,result
natural_language_inference,10,is a single - hidden layer MLP with the ReLU activation function .,analysis,Sentiment Analysis,1,175,10,10,0,analysis : Sentiment Analysis,0.7383966244725738,0.18867924528301888,0.4761904761904762,is a single hidden layer MLP with the ReLU activation function ,12,"where W s clf ? R 1 Dc , b s clf ? R 1 , and ?",Note that subtrees labeled as neutral are ignored in the binary setting in both training and evaluation .,result
sentence_classification,0,"Therefore , we opted for a standard setup of stratified train / validation / test data splits with 85 % data used for training and the rest equally split between validation and test .",baseline,Baselines,0,178,8,8,0,baseline : Baselines,0.6666666666666666,1.0,1.0,Therefore we opted for a standard setup of stratified train validation test data splits with 85 data used for training and the rest equally split between validation and test ,30,leave - one - out cross validation in our experiments since it is impractical to re-train each variant of our deep learning models thousands of times ., ,result
natural_language_inference,47,Write one alternate caption that is definitely a true description of the photo .,system description,Data collection,0,68,33,11,0,system description : Data collection,0.31627906976744186,0.4177215189873418,0.3666666666666665,Write one alternate caption that is definitely a true description of the photo ,14,Using only the caption and what you know about the world :,"For the caption "" Two dogs are running through a field . "" you could write "" There are animals outdoors . """,method
sentiment_analysis,1,The EEG data was collected when 1 .,dataset,Datasets,0,283,4,4,0,dataset : Datasets,0.7146464646464646,0.2222222222222222,0.2222222222222222,The EEG data was collected when 1 ,8,The SEED dataset comprises EEG data of 15 subjects ( 7 males ) recorded in 62 channels using the ESI NeuroScan System 1 .,"https://compumedicsneuroscan.com/ participants watch emotion - eliciting movies in three types of emotions , namely negative , neutral and positive .",experiment
natural_language_inference,52,"This domain is challenging as : ( a ) approximately 70 % of science exam ques- tion shave been shown to require complex forms of inference to solve , and ( b ) there are few structured knowledge bases to support this inference .",introduction,introduction,0,31,22,22,0,introduction : introduction,0.11877394636015325,0.6111111111111112,0.6111111111111112,This domain is challenging as a approximately 70 of science exam ques tion shave been shown to require complex forms of inference to solve and b there are few structured knowledge bases to support this inference ,37,"In this work , we focus on answering multiplechoice science exam questions ; see example in ) .","Within this domain , we propose an approach that learns to both select and explain answers , when the only supervision available is for which answer is correct ( but not how to explain it ) .",introduction
relation_extraction,7,Neural Relation Extraction via Inner - Sentence Noise Reduction and Transfer Learning,title,title,1,2,1,1,0,title : title,0.007722007722007722,1.0,1.0,Neural Relation Extraction via Inner Sentence Noise Reduction and Transfer Learning,11, , ,title
natural_language_inference,27,The expensive training not only leads to high turnaround time for experimentation and limits researchers from rapid iteration but also prevents the models from being used for larger dataset .,introduction,introduction,0,19,7,7,0,introduction : introduction,0.056213017751479286,0.21875,0.21875,The expensive training not only leads to high turnaround time for experimentation and limits researchers from rapid iteration but also prevents the models from being used for larger dataset ,30,"weakness of these models is that they are often slow for both training and inference due to their recurrent nature , especially for long texts .",Meanwhile the slow inference prevents the machine comprehension systems from being deployed in real - time applications .,introduction
relation-classification,3,"This seemingly small performance increase is mainly due to the limited performance benefit for the NER component , which is in accordance with the recent advances in NER using neural networks that report similarly small gains ( e.g. , the performance improvement in and on the CoNLL - 2003 test set is 0.01 % and 0.17 % F 1 percentage points , while in the work of , a 0.07 % F 1 improvement on CoNLL - 2000 using AT for NER is reported ) .",result,Results,0,130,4,4,1,result : Results,0.9489051094890508,0.5714285714285714,0.5714285714285714,This seemingly small performance increase is mainly due to the limited performance benefit for the NER component which is in accordance with the recent advances in NER using neural networks that report similarly small gains e g the performance improvement in and on the CoNLL 2003 test set is 0 01 and 0 17 F 1 percentage points while in the work of a 0 07 F 1 improvement on CoNLL 2000 using AT for NER is reported ,79,The im - provement of AT over our baseline ( depending on the dataset ) ranges from ? 0.4 % to ? 0.9 % in terms of over all F 1 score .,"However , the relation extraction performance increases by ? 1 % F 1 scoring points , except for the ACE04 dataset .",result
question-answering,7,This dataset comes with standard train / dev / test sets and two subtasks : binary sentence classification or fine - grained classification of five classes .,experiment,Sentence Classification,0,188,66,3,0,experiment : Sentence Classification,0.6836363636363636,0.8571428571428571,0.21428571428571427,This dataset comes with standard train dev test sets and two subtasks binary sentence classification or fine grained classification of five classes ,23,We evaluated NSE on the Stanford Sentiment Treebank ( SST ) .,We trained our model on the text spans corresponding to labeled phrases in the training set and evaluated the model on the full sentences .,experiment
machine-translation,2,"These are concatenated and once again projected , resulting in the final values , as depicted in .",model,Multi-Head Attention,0,83,42,4,0,model : Multi-Head Attention,0.3705357142857143,0.3853211009174312,0.4,These are concatenated and once again projected resulting in the final values as depicted in ,16,"On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding d v - dimensional output values .",Multi - head attention allows the model to jointly attend to information from different representation subspaces at different positions .,method
natural_language_inference,62,"As for the comparative objects , we only consider the single MRC models that rank in the top 20 on the SQuAD 1.1 leader board and have reported their performance on the adversarial sets .",experiment,Model Comparison in both Performance and the Robustness to Noise,0,191,19,4,0,experiment : Model Comparison in both Performance and the Robustness to Noise,0.8526785714285714,0.5,0.26666666666666666,As for the comparative objects we only consider the single MRC models that rank in the top 20 on the SQuAD 1 1 leader board and have reported their performance on the adversarial sets ,35,"Specifically , we not only evaluate the performance of KAR on the development set and the test set , but also do this on the adversarial sets .","There are totally five such comparative objects , which can be considered as representatives of the state - of - the - art MRC models .",experiment
relation_extraction,5,"In the entire dev set of 22,631 examples , 1,450 had at least 3 more GCN models predicting the label correctly compared to the PA - LSTM , and 1,550 saw an improvement from using the PA - LSTM .",experiment,Comparing GCN models and PA-LSTM on TACRED,0,260,36,5,0,experiment : Comparing GCN models and PA-LSTM on TACRED,0.988593155893536,0.9230769230769232,0.625,In the entire dev set of 22 631 examples 1 450 had at least 3 more GCN models predicting the label correctly compared to the PA LSTM and 1 550 saw an improvement from using the PA LSTM ,39,"As is shown in the figure , both GCN models capture very different examples from the PA - LSTM model .","The C - GCN , on the other hand , outperformed the PA - LSTM by at least 3 models on a total of 847 examples , and lost by a margin of at least 3 on another 629 examples , as reported in the main text .",experiment
natural_language_inference,52,Complex inference required Question : Mr. Harris mows his lawn twice each month .,analysis,Type:,0,245,10,2,0,analysis : Type:,0.9386973180076628,0.4545454545454545,0.14285714285714285,Complex inference required Question Mr Harris mows his lawn twice each month ,13, ,He claims that it is better to leave the clippings on the ground .,result
sentiment_analysis,26,This suggests that a simple concatenation may harm the model 's ability to harness the semantic information carried by regular word vectors .,analysis,Results and Analysis,0,181,35,35,0,analysis : Results and Analysis,0.7387755102040816,0.4487179487179487,0.5384615384615384,This suggests that a simple concatenation may harm the model s ability to harness the semantic information carried by regular word vectors ,23,"Even on English , adding the 250 dimensional SocialSent embedding seems to degrade the effectiveness of the CNN , although all input information that was previously there continues to be provided to it .",This risk seems more pronounced for larger - dimensional sentiment embeddings .,result
text-classification,5,"For each task , we compare against the current state - of - theart .",model,model,0,169,2,2,0,model : model,0.6706349206349206,0.5,0.5,For each task we compare against the current state of theart ,12, ,"For the IMDb and TREC - 6 datasets , we compare against CoVe , a stateof - the - art transfer learning method for NLP .",method
semantic_parsing,0,"Also , we extend their modules to ORDER BY and GROUP BY components as well .",method,SELECT COUNT(*) FROM cars_data WHERE cylinders > 4,0,235,24,18,0,method : SELECT COUNT(*) FROM cars_data WHERE cylinders > 4,0.8453237410071942,0.96,0.9473684210526316,Also we extend their modules to ORDER BY and GROUP BY components as well ,15,"In our experiment , we use the question type info extracted from data base content .",It is the only model that uses data base content .,method
sentiment_analysis,41,Aspect - level sentiment classification is a finegrained task in sentiment analysis .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.017937219730941704,0.2222222222222222,0.2222222222222222,Aspect level sentiment classification is a finegrained task in sentiment analysis ,12, ,"Since it provides more complete and in - depth results , aspect - level sentiment analysis has received much attention these years .",abstract
relation_extraction,13,"We show that through training by matching the blanks , we can outperform 's top performance on FewRel , without having seen any of the FewRel training data .",introduction,introduction,0,28,19,19,0,introduction : introduction,0.13145539906103287,0.9047619047619048,0.9047619047619048,We show that through training by matching the blanks we can outperform s top performance on FewRel without having seen any of the FewRel training data ,27,"presented FewRel as a supervised dataset , intended to evaluate models ' ability to adapt to relations from new domains at test time .",We also show that a model pre-trained by matching the blanks and tuned on FewRel outperforms humans on the FewRel evaluation .,introduction
machine-translation,9,"Results : For different settings of the number of components M and the number of codewords K , we train the code learning network .",analysis,SENTIMENT ANALYSIS,0,196,19,19,0,analysis : SENTIMENT ANALYSIS,0.6829268292682927,0.2159090909090909,0.59375,Results For different settings of the number of components M and the number of codewords K we train the code learning network ,23,The parameters with lowest validation loss are saved .,The average reconstruction loss on a fixed validation set is summarized in the left of .,result
semantic_parsing,2,"Omit variable information defined by t.pred t.pred ? "" %s# %d "" % ( t.pred , len ( variable ) ) for c ? argument in t.args do if c is nonterminal then c ? SKETCH ( c ) else c ? "" ? """,training,Natural Language to Logical Form,0,130,21,12,0,training : Natural Language to Logical Form,0.44673539518900346,0.17796610169491525,0.35294117647058826,Omit variable information defined by t pred t pred s d t pred len variable for c argument in t args do if c is nonterminal then c SKETCH c else c ,33,"No nonterminal in arguments return "" %s@%d "" % ( t.pred , len ( t.args ) ) if t.pred is ? operator , or quantifier then e.g. , count","Omit variable information defined by t.pred t.pred ? "" %s# %d "" % ( t.pred , len ( variable ) ) for c ? argument in t.args do if c is nonterminal then c ? SKETCH ( c ) else c ? "" ? """,experiment
machine-translation,7,Results : and 4 in Section 5.3 show comparisons of our results to other published methods .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,347,125,28,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.9302949061662198,0.8278145695364238,0.875,Results and 4 in Section 5 3 show comparisons of our results to other published methods ,17,"We reported tokenized BLEU score as computed by the multi -bleu.pl script , downloaded from the public implementation of Moses ( on Github ) , which was also used in .",shows test perplexity as a function of number of words in the ( training data 's ) source sentences processed for models with different numbers of experts .,others
part-of-speech_tagging,6,"For parsing from raw text to universal dependencies , we utilize CoNLL - U test files preprocessed by the baseline UDPipe 1.1 .",result,MQuni at the CoNLL 2017 shared task,0,106,18,8,0,result : MQuni at the CoNLL 2017 shared task,0.7114093959731543,0.4,0.2285714285714285,For parsing from raw text to universal dependencies we utilize CoNLL U test files preprocessed by the baseline UDPipe 1 1 ,22,"Note that for each "" surprise "" language where there are only few sample sentences with gold - standard annotation or a "" small "" treebank whose development set is not available , we simply split it s sample or training set into two parts with a ratio 4:1 , and then use the larger part for training and the smaller part for development .","These pre-processed CoNLL - U test files are available to all participants who do not want to train their own models for any steps preceding the dependency analysis , including : tokenization , word segmentation , sentence segmentation , POS tagging and morphological analysis .",result
sentiment_analysis,22,"In this paper , we apply a Bi - GRU to learn a more abstract representation of the sentence .",model,model,0,74,15,15,0,model : model,0.3231441048034934,0.4838709677419355,0.4838709677419355,In this paper we apply a Bi GRU to learn a more abstract representation of the sentence ,18,The position embedding lookup table is initialized randomly and tuned in the training phase .,"In the following , we describe our encoding layer in detail .",method
natural_language_inference,26,Formal semantics generally presents the semantic relationship as predicate - argument structure .,model,Explicit Contextual Semantics,0,50,16,4,0,model : Explicit Contextual Semantics,0.2358490566037736,0.26666666666666666,0.21052631578947367,Formal semantics generally presents the semantic relationship as predicate argument structure ,12,"There are a few formal semantic frames , including FrameNet and PropBank , in which the latter is more popularly implemented in computational linguistics .","For example , given the following sentence with target verb ( predicate ) sold , all the arguments are labeled as follows , where ARG0 represents the seller ( agent ) , ARG1 represents the thing sold ( theme ) , ARG2 represents the buyer ( recipient ) , AM ? TM",method
sentiment_analysis,51,"To learn the relationship between sentences , BERT takes two sentences A and B as inputs and learns to classify whether B actually follows A or is it just a random sentence .",methodology,BERT,0,86,12,7,0,methodology : BERT,0.5733333333333334,0.3870967741935484,0.5833333333333334,To learn the relationship between sentences BERT takes two sentences A and B as inputs and learns to classify whether B actually follows A or is it just a random sentence ,32,Next sentence prediction :,"Unlike traditional sequential or recurrent models , the attention architecture processes the whole input sequence at once , enabling all input tokens to be processed in parallel .",method
sentiment_analysis,30,"We see consistent performance gains for our model in both aspect detection and sentiment classification , compared to EntNet , esp. for aspect detection , underlining the benefit of delayed update gate activation .",result,Results,1,117,7,7,0,result : Results,0.9140625,1.0,1.0,We see consistent performance gains for our model in both aspect detection and sentiment classification compared to EntNet esp for aspect detection underlining the benefit of delayed update gate activation ,31,Ent Net vs. our model ., ,result
text-classification,7,Suppose W c 1 ? R Ddd and W c 2 ? R K,model,Convolutional Capsule Layer,0,110,72,4,0,model : Convolutional Capsule Layer,0.4526748971193416,0.7659574468085106,0.4,Suppose W c 1 R Ddd and W c 2 R K,12,Those capsules in the region multiply transformation matrices to learn child - parent relationships followed by routing by agreement to produce parent capsules in the layer above .,Suppose W c 1 ? R Ddd and W c 2 ? R K,method
sentiment_analysis,13,The annotated data is in the format of SQuAD 1.1 to ensure compatibility with existing implementations of MRC models .,experiment,Experiments,0,192,16,16,0,experiment : Experiments,0.6906474820143885,0.4,0.4,The annotated data is in the format of SQuAD 1 1 to ensure compatibility with existing implementations of MRC models ,21,training review is encouraged to have 2 questions ( training examples ) on average to have good coverage of reviews .,The statistics of the RRC dataset ( ReviewRC ) are shown in .,experiment
natural_language_inference,9,"For vector gates , we modify the row dimension of weights and biases in Equation 1 and 5 from 1 to d .",model,EXTENSIONS,0,121,81,14,0,model : EXTENSIONS,0.3666666666666665,0.84375,0.875,For vector gates we modify the row dimension of weights and biases in Equation 1 and 5 from 1 to d ,22,"As in LSTM and GRU , update and reset gates can be vectors instead of scalar values for fine - controlled gating .","Then we obtain z t , rt ? Rd ( instead of z t , rt ? R ) , and these can be element - wise multiplied ( ) instead of being broadcasted in Equation 3 and 6 .",method
natural_language_inference,45,Experiments show that these changes are key to the performance improvements for reading comprehension tasks .,system description,WORD-CHARACTER FINE-GRAINED GATING,0,112,44,22,0,system description : WORD-CHARACTER FINE-GRAINED GATING,0.5628140703517588,0.6875,0.9565217391304348,Experiments show that these changes are key to the performance improvements for reading comprehension tasks ,16,"For example , for noun phrases and entities , we would expect the gate to bias towards character - level representations because noun phrases and entities are usually less common and display richer morphological structure .","Our approach can be further generalized to a setting of multi - level networks so that we can combine multiple levels of representations using fine - grained gating mechanisms , which we leave for future work .",method
natural_language_inference,41,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.0311284046692607,0.6666666666666666,0.6666666666666666,BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks ,18,"We evaluate a number of noising approaches , finding the best performance by both randomly shuffling the order of the original sentences and using a novel in - filling scheme , where spans of text are replaced with a single mask token .","It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD , achieves new stateof - the - art results on a range of abstractive dialogue , question answering , and summarization tasks , with gains of up to 6 ROUGE .",abstract
natural_language_inference,95,"Given a question Q and a set of passages { P i } retrieved by search engines , our task is to find the best concise answer to the question .",model,Question and Passage Modeling,0,69,2,2,0,model : Question and Passage Modeling,0.2948717948717949,0.04,0.125,Given a question Q and a set of passages P i retrieved by search engines our task is to find the best concise answer to the question ,28, ,"First , we formally present the details of modeling the question and passages .",method
natural_language_inference,13,"Unless stated otherwise , the encoder in the pointer layer for span prediction models also uses MRU .",method,Our Methods,0,171,21,8,0,method : Our Methods,0.7467248908296943,0.875,0.7272727272727273,Unless stated otherwise the encoder in the pointer layer for span prediction models also uses MRU ,17,The encompassing framework for MRU is the Bi- Attentive models described for MCQ - based problems and Span prediction problems .,"However , for the Hybrid MRU - LSTM models , answer pointer layers use BiLSTMs .",method
sentiment_analysis,15,"p 2 , P 2 ) ( a , A ) ( p 1 , P 1 ) ( b , B ) ( c , C ) the MV - RNN computes the first parent vector and its matrix via two equations :",model,MV-RNN: Matrix-Vector RNN,0,134,45,10,0,model : MV-RNN: Matrix-Vector RNN,0.4962962962962963,0.4205607476635514,0.6666666666666666,p 2 P 2 a A p 1 P 1 b B c C the MV RNN computes the first parent vector and its matrix via two equations ,29,"For the tree with ( vector , matrix ) nodes :",where W M ? R d2 d and the result is again ad d matrix .,method
text_summarization,13,"Using the top - level representations h s i and the word representations h i , j , we compute coarse attention weights ? s 1 , . . . , ? s m for the toplevel chunks in the same way as STANDARD , and similarly compute fine attention weights ? w i , 1 , . . . , ? w i , n i for each i .",model,model,0,99,30,30,0,model : model,0.3707865168539326,0.30303030303030304,0.30303030303030304,Using the top level representations h s i and the word representations h i j we compute coarse attention weights s 1 s m for the toplevel chunks in the same way as STANDARD and similarly compute fine attention weights w i 1 w i n i for each i ,51,"For the word representations , we run an LSTM encoder separately on the words of each chunk ; specifically , we apply an RNN on s i to get hidden states h i , j for i = 1 , . . . , m and","Using the top - level representations h s i and the word representations h i , j , we compute coarse attention weights ? s 1 , . . . , ? s m for the toplevel chunks in the same way as STANDARD , and similarly compute fine attention weights ? w i , 1 , . . . , ? w i , n i for each i .",method
question_answering,1,These two layers are responsible for aligning the embeddings between the query and context words which are the inputs to the subsequent attention layer .,experiment,QUESTION ANSWERING EXPERIMENTS,0,205,42,42,0,experiment : QUESTION ANSWERING EXPERIMENTS,0.6466876971608833,0.4666666666666667,0.6363636363636364,These two layers are responsible for aligning the embeddings between the query and context words which are the inputs to the subsequent attention layer ,25,"First , we visualize the feature spaces after the word and contextual embedding layers .","To visualize the embeddings , we choose a few frequent query words in the dev data and look at the context words that have the highest cosine similarity to the query words .",experiment
sentiment_analysis,12,"In doing so , we make our model adaptive to discover more influential context words .",approach,Details of Our Approach,0,120,55,41,0,approach : Details of Our Approach,0.5357142857142857,0.7638888888888888,0.8723404255319149,In doing so we make our model adaptive to discover more influential context words ,15,"Then , we leverage D ( k ) to continue updating model parameters for the next iteration ( Line 24 ) .","Through K iterations of the above steps , we manage to extract influential context words of all training instances .",method
natural_language_inference,21,They are the main components of DiSAN and maybe of independent interest to other neural nets for other NLP problems in which an attention is needed .,system description,Two Proposed Attention Mechanisms,0,89,42,3,0,system description : Two Proposed Attention Mechanisms,0.30689655172413793,0.3307086614173229,1.0,They are the main components of DiSAN and maybe of independent interest to other neural nets for other NLP problems in which an attention is needed ,27,"In this section , we introduce two novel attention mechanisms , multi-dimensional attention in Section 3.1 ( with two extensions to self - attention in Section 3.2 ) and directional self - attention in Section 3.3 .",Multi-dimensional attention is a natural extension of additive attention ( or MLP attention ) at the feature level .,method
natural_language_inference,29,"To tackle this task , we propose an end - to - end learning method to extract fact that is helpful for answering questions from reviews and attributes and then generate answer text .",introduction,introduction,0,49,37,37,0,introduction : introduction,0.13424657534246576,0.902439024390244,0.902439024390244,To tackle this task we propose an end to end learning method to extract fact that is helpful for answering questions from reviews and attributes and then generate answer text ,31,We propose a product - aware answer generation task .,"Due to the review is in an informal style with noise , we propose an attention based review reader and use the Wasserstein distance based adversarial learning method to learn to denoise the review text .",introduction
sentence_compression,1,We evaluate the baseline and our systems on the 200 - sentence test set in an experiment with human raters .,experiment,Experiments,0,132,2,2,0,experiment : Experiments,0.631578947368421,0.027777777777777776,0.027777777777777776,We evaluate the baseline and our systems on the 200 sentence test set in an experiment with human raters ,20, ,The raters were asked to rate readability and informativeness of compressions given the input which are the standard evaluation metrics for compression .,experiment
natural_language_inference,34,"The framework has four output dimensions , including 1 . supporting sentences , 2 . the start position of the answer , 3 . the end position of the answer , and 4 .",system description,Prediction,0,191,88,3,0,system description : Prediction,0.6474576271186441,0.8712871287128713,0.1875,The framework has four output dimensions including 1 supporting sentences 2 the start position of the answer 3 the end position of the answer and 4 ,27,We follow the same structure of prediction layers as .,the answer type .,method
semantic_role_labeling,4,"To take advantage of it , we introduce a variant of a mixture of experts ( MoE )",architecture,architecture,0,138,50,50,0,architecture : architecture,0.46,0.8620689655172413,0.8620689655172413,To take advantage of it we introduce a variant of a mixture of experts MoE ,16,Each base model trained with different random initializations has variance in span representations .,"Firstly , we combine span representations h",method
natural_language_inference,99,"where [ h , E ] is the concatenation function .",system description,Input Embedding Layer,0,77,35,26,0,system description : Input Embedding Layer,0.3055555555555556,1.0,1.0,where h E is the concatenation function ,8,"Therefore , the final representation of words are computed as follows :", ,method
temporal_information_extraction,0,"Based on the output of the previous sieve , we run a transitive reasoner layer , similar to CAEVO , in order to infer new temporal links among candidate pairs .",system,Temporal Reasoner,0,64,37,2,0,system : Temporal Reasoner,0.3386243386243386,0.5692307692307692,0.25,Based on the output of the previous sieve we run a transitive reasoner layer similar to CAEVO in order to infer new temporal links among candidate pairs ,28, ,"This alleviates the issue of high precision and low recall , typical of the rule - based sieve .",method
natural_language_inference,6,Number of training languages,training,Number of training languages,0,178,1,1,0,training : Number of training languages,0.717741935483871,1.0,1.0,Number of training languages,4, , ,experiment
natural_language_inference,2,Variation on the number of factors ( m ) and q ma shows the performance of AMANDA for different values of m .,result,Effectiveness of the Model Components,0,225,34,12,0,result : Effectiveness of the Model Components,0.8395522388059702,0.7727272727272727,0.5454545454545454,Variation on the number of factors m and q ma shows the performance of AMANDA for different values of m ,21,shows that question - dependent passage encoding has the highest impact on performance .,We use 4 factors for all the experiments as it gives the highest F1 score .,result
natural_language_inference,94,"Thus we score c ? {c 2 , c 3 , c 5 } by : score ( c ) = count ( c ) / | C| , where | C| is the context length and count ( ) is the number of times a concept appears in the context .",method,method,0,164,96,96,0,method : method,0.4281984334203655,0.7559055118110236,0.7559055118110236,Thus we score c c 2 c 3 c 5 by score c count c C where C is the context length and count is the number of times a concept appears in the context ,36,"We approximate importance and saliency for concepts in the context by their termfrequency , under the heuristic that important concepts occur more frequently .","In , this ensures that concepts like daughter are scored highly due to their frequency in the context .",method
natural_language_inference,89,"Next , we do an ablation study on the SQuAD 2.0 development set to show the effects of our proposed methods for each individual component .",ablation,Ablation Study,0,191,2,2,0,ablation : Ablation Study,0.7346153846153847,0.08695652173913042,0.08695652173913042,Next we do an ablation study on the SQuAD 2 0 development set to show the effects of our proposed methods for each individual component ,26, ,first shows the ablation results of different auxiliary losses on the reader .,result
part-of-speech_tagging,5,We opted for a simple synchronous schedule outline in Algorithm,training,Training Schema,0,97,5,5,0,training : Training Schema,0.4801980198019802,0.2631578947368421,0.2631578947368421,We opted for a simple synchronous schedule outline in Algorithm,10,"Thus , it is in some sense a multitask learning model and we must define a schedule in which individual models are updated .",". Here , during each epoch , we update each of the models in sequence - character , word and meta-using the entire training data .",experiment
relation_extraction,8,These features are often derived from preexisting Natural Language Processing ( NLP ) tools .,introduction,introduction,0,29,16,16,0,introduction : introduction,0.10780669144981413,0.3555555555555556,0.3555555555555556,These features are often derived from preexisting Natural Language Processing NLP tools ,13,"Second , previous methods have typically applied supervised models to elaborately designed features when obtained the labeled data through distant supervision .","Since errors inevitably exist in NLP tools , the use of traditional features leads to error propagation or accumulation .",introduction
natural_language_inference,78,"where v c , v s , v m ? Rand Z ( . ) is the factorization operation .",model,Factorization Operation,0,134,59,20,0,model : Factorization Operation,0.4855072463768116,0.6781609195402298,0.9090909090909092,where v c v s v m Rand Z is the factorization operation ,14,"Let ( s , s ) represent an intra-aligned pair from either the premise or hypothesis , we compute the following operations :",Applying alignment factorization to intra-aligned representations produces another three scalar - valued features which are mapped to each word in the sequence .,method
natural_language_inference,30,"We compare various versions of our model against two versions of paralex , whose results were given in .",result,Results,0,200,4,4,0,result : Results,0.7751937984496124,0.0975609756097561,0.19047619047619047,We compare various versions of our model against two versions of paralex whose results were given in ,18,Reranking and present the results of the reranking experiments .,"First , we can see that multitasking with paraphrase data is essential since it improves F1 from 0.60 to 0.68 .",result
sentence_compression,3,"3 ) the dependency relations , ROOT , dobj , nsubj , pobj , should be retained as they are the skeletons of a sentence .",method,Comparison Methods,0,76,6,6,0,method : Comparison Methods,0.6129032258064516,0.3333333333333333,0.3333333333333333,3 the dependency relations ROOT dobj nsubj pobj should be retained as they are the skeletons of a sentence ,20,h( w i ) = 5 if w i is in the headline ; h( w i ) = 1 otherwise .,4 ) the sentence length should be over than ? but less than ?.,method
natural_language_inference,35,"Following previous studies , we used the summary setting for the comparisons with the reported baselines , where each question refers to one summary ( averaging 659 words ) , and there is no unanswerable questions .",model,Modeling Encoder Layer,0,169,130,5,0,model : Modeling Encoder Layer,0.6425855513307985,0.9285714285714286,0.3333333333333333,Following previous studies we used the summary setting for the comparisons with the reported baselines where each question refers to one summary averaging 659 words and there is no unanswerable questions ,32,We only describe the settings specific to this experiment .,Our model therefore did not use the passage ranker and answer possibility classifier .,method
natural_language_inference,88,"Recent approaches use two sequential LSTMs to encode the premise and the hypothesis respectively , and apply neural attention to reason about their logical relationship .",model,Natural Language Inference,0,215,30,8,0,model : Natural Language Inference,0.8704453441295547,0.5660377358490566,0.25806451612903225,Recent approaches use two sequential LSTMs to encode the premise and the hypothesis respectively and apply neural attention to reason about their logical relationship ,25,We performed lower - casing and tokenization for the entire dataset .,"Furthermore , show that a non-standard encoder - decoder architecture which processes the hypothesis conditioned on the premise results significantly boosts performance .",method
text_generation,3,"For example , given "" What 's your name "" as the input , the models generate "" I like it "" as the output .",introduction,introduction,0,17,7,7,0,introduction : introduction,0.12056737588652484,0.3181818181818182,0.3181818181818182,For example given What s your name as the input the models generate I like it as the output ,20,"promise for generating fluent responses , they still suffer from the poor semantic relevance between inputs and responses .","Recently , the neural attention mechanism has been proved successful in many tasks including neural machine translation and abstractive summarization , for its ability of capturing word - level dependency by associating a generated word with relevant words in the source - side context .",introduction
natural_language_inference,47,"All of the models are randomly initialized using standard techniques and trained using AdaDelta ( Zeiler , 2012 ) minibatch SGD until performance on the development set stops improving .",model,Sentence embeddings and NLI,0,158,44,13,1,model : Sentence embeddings and NLI,0.7348837209302326,0.8148148148148148,0.5652173913043478,All of the models are randomly initialized using standard techniques and trained using AdaDelta Zeiler 2012 minibatch SGD until performance on the development set stops improving ,27,"In addition , all of the models use an additional tanh neural network layer to map these 300d embeddings into the lower - dimensional phrase and sentence embedding space .","We applied L2 regularization to all models , manually tuning the strength coefficient ? for each , and additionally applied dropout to the inputs and outputs of the sen - tence embedding models ( though not to its internal connections ) with a fixed dropout rate .",method
natural_language_inference,72,the required comprehension skills poorly understood .,introduction,introduction,0,26,18,18,0,introduction : introduction,0.08333333333333333,0.375,0.375,the required comprehension skills poorly understood ,7,The passage has been shortened for clarity .,"With our work we hope to narrow this gap by proposing a new resource for reading comprehension in the clinical domain , and by analyzing the different types of comprehension skills thatare triggered while answering .",introduction
natural_language_inference,38,We set the hidden size as 100 for all the LSTM and GRU layers and apply dropout between layers with a dropout ratio as 0.2 .,experiment,Experiment Implementation Settings,1,116,9,9,0,experiment : Experiment Implementation Settings,0.6137566137566137,0.8181818181818182,0.8181818181818182,We set the hidden size as 100 for all the LSTM and GRU layers and apply dropout between layers with a dropout ratio as 0 2 ,27,"We use 100 one dimensional filters for CNN in the character level embedding , with width of 5 for each one .","We use the AdaDelta ( Zeiler , 2012 ) optimizer with a initial learning rate as 0.001 .",experiment
natural_language_inference,45,"In addition to different ways of combining word - level and character - level representations , we also compare two different ways of integrating documents and queries : GA refers to the gated attention reader and FG refers to our fine - grained gating described in Section 3.3 .",performance,CLOZE-STYLE QUESTIONS,0,163,11,8,0,performance : CLOZE-STYLE QUESTIONS,0.8190954773869347,0.3548387096774194,0.4705882352941176,In addition to different ways of combining word level and character level representations we also compare two different ways of integrating documents and queries GA refers to the gated attention reader and FG refers to our fine grained gating described in Section 3 3 ,45,The word embeddings are updated during training .,The results are reported in .,result
natural_language_inference,95,Recent years have seen rapid growth in the MRC community .,introduction,introduction,0,11,3,3,0,introduction : introduction,0.04700854700854702,0.05172413793103448,0.05172413793103448,Recent years have seen rapid growth in the MRC community ,11,"Machine reading comprehension ( MRC ) , empowering computers with the ability to acquire knowledge and answer questions from textual data , is believed to be a crucial step in building a general intelligent agent .","With the release of various datasets , the MRC task has evolved from the early cloze - style test to answer extraction from a single passage and to the latest more complex question answering on web data .",introduction
part-of-speech_tagging,3,"For each of the embeddings , we fine - tune initial embeddings , modifying them during gradient updates of the neural network model by back - propagating gradients .",training,Optimization Algorithm,1,106,26,11,0,training : Optimization Algorithm,0.5221674876847291,0.7428571428571429,0.5789473684210527,For each of the embeddings we fine tune initial embeddings modifying them during gradient updates of the neural network model by back propagating gradients ,25,"The "" best "" parameters appear at around 50 epochs , according to our experiments .",The effectiveness of this method has been previously explored in sequential and structured prediction problems .,experiment
text_generation,1,"We randomly select 80 , 000 captions as the training set , and select 5 , 000 captions to form the validation set .",experiment,Results on COCO image captions,0,244,80,5,0,experiment : Results on COCO image captions,0.8905109489051095,0.7766990291262136,0.25,We randomly select 80 000 captions as the training set and select 5 000 captions to form the validation set ,21,"The captions are the narrative sentences written by human , and each sentence is at least 8 words and at most 20 words .",We replace the words appeared less than 5 times with UNK character .,experiment
natural_language_inference,41,"During generation , we set beam size as 5 , remove duplicated trigrams in beam search , and tuned the model with min-len , max - len , length penalty on the validation set .",experiment,Generation Tasks,0,172,20,6,0,experiment : Generation Tasks,0.669260700389105,0.625,1.0,During generation we set beam size as 5 remove duplicated trigrams in beam search and tuned the model with min len max len length penalty on the validation set ,30,"During finetuning we use a label smoothed cross entropy loss , with the smoothing parameter set to 0.1 .","To provide a comparison with the state - of - the - art in summarization , we present results on two summarization datasets , CNN / DailyMail and XSum , which have distinct properties .",experiment
relation_extraction,2,It computes the macro-averaged F1 - scores for the nine actual relations ( excluding Other ) and considers directionality .,dataset,Dataset and Evaluation Metric,0,92,9,9,0,dataset : Dataset and Evaluation Metric,0.6814814814814815,1.0,1.0,It computes the macro averaged F1 scores for the nine actual relations excluding Other and considers directionality ,18,We evaluate our solution by using the SemEval - 2010 Task 8 official scorer script ., ,experiment
text_summarization,3,where j represents the selected concept index .,system description,Concept Pointer Generator,0,81,26,14,0,system description : Concept Pointer Generator,0.3347107438016529,0.1984732824427481,0.8235294117647058,where j represents the selected concept index ,8,"In random selection , each of the concept candidates could be trained randomly to update the parameters :","Considering the above baseline generation network and both the pointer networks , our final output distribution is",method
natural_language_inference,61,hyper-parameters of aESIM model are listed as follows .,model,Setting,0,120,59,3,0,model : Setting,0.7692307692307693,0.8676470588235294,0.25,hyper parameters of aESIM model are listed as follows ,10,We use the validation set to select models for testing .,We use the Adam method for optimization .,method
question_answering,3,"The key competitors are the Stanford Attention Reader ( Stanford AR ) , Gated Attention Reader ( GA ) , and Dynamic Fusion Networks ( DFN ) .",method,RACE,1,179,4,2,0,method : RACE,0.6392857142857142,0.1176470588235294,0.1111111111111111,The key competitors are the Stanford Attention Reader Stanford AR Gated Attention Reader GA and Dynamic Fusion Networks DFN ,20, ,GA incorporates a multi-hop attention mechanism that helps to refine the answer representations .,method
sentiment_analysis,41,"In this paper , we deal with aspect - level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect .",introduction,introduction,1,15,4,4,0,introduction : introduction,0.06726457399103139,0.16,0.16,In this paper we deal with aspect level sentiment classification and we find that the sentiment polarity of a sentence is highly dependent on both content and aspect ,29,Aspect - level sentiment analysis is a fine - grained task that can provide complete and in - depth results .,"For example , the sentiment polarity of "" Staffs are not that friendly , but the taste covers all . "" will be positive if the aspect is food but negative when considering the aspect service .",introduction
natural_language_inference,81,Archaeology can be considered both asocial science and a branch of the humanities .,APPENDIX,Answer town,0,312,104,24,0,APPENDIX : Answer town,0.7410926365795725,0.4882629107981221,0.18045112781954886,Archaeology can be considered both asocial science and a branch of the humanities ,14,"The archaeological record consists of artifacts , architecture , biofacts or ecofacts , and cultural landscapes .","In North America , archaeology is considered a sub-field of anthropology , while in Europe archaeology is often viewed as either a discipline in its own right or a sub-field of other disciplines .",others
text_summarization,7,The encoder receives input X and returns a list of final hidden states H s = ( h s i ) I i = 1 :,system description,Baseline RNN-based EncDec Model,0,34,13,13,0,system description : Baseline RNN-based EncDec Model,0.2251655629139073,0.15853658536585366,0.52,The encoder receives input X and returns a list of final hidden states H s h s i I i 1 ,22,Encoder : Let ? s ( ) denote the over all process of our 2 - layer bidirectional LSTM encoder .,We employ a K - best beam - search decoder to find the ( approximated ) best output ? given input X. shows a typical Kbest beam search algorithm used in the decoder of EncDec approach .,method
natural_language_inference,80,"However , the model would suffer from the absence of similarity and closeness measures .",model,model,0,92,36,36,0,model : model,0.31724137931034485,0.5625,0.5625,However the model would suffer from the absence of similarity and closeness measures ,14,"To further enrich the collected attentional information , a trivial next step would be to pass the concatenation of the tuples ( i ,? i ) or ( v j , ? j ) which provides a linear relationship between them .","Therefore , we calculate the difference and element - wise product for the tuples ( i ,? i ) and ( v j ,? j ) that represent the similarity and closeness information respectively .",method
natural_language_inference,33,STN + Fr + De + NLI +L + STP + Par :,evaluation,MULTI-TASK MODEL DETAILS,0,208,84,14,0,evaluation : MULTI-TASK MODEL DETAILS,0.7969348659003831,0.6131386861313869,0.7777777777777778,STN Fr De NLI L STP Par ,8,The sentence representation h x is the concatenation of the final hidden vectors from a 2 - layer bidirectional GRU with 2048 - dimensional hidden vectors and a 1 - layer bidirectional GRU with 2048 - dimensional hidden vectors trained without STP .,The sentence representation h x is the concatenation of the final hidden vectors from a bidirectional GRU with 2048 - dimensional hidden vectors and another bidirectional GRU with 2048 - dimensional hidden vectors trained without Par.,result
sentiment_analysis,43,We propose a novel multi-grained attention network ( MGAN ) model for aspect level sentiment classification .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.016666666666666666,0.2222222222222222,0.2222222222222222,We propose a novel multi grained attention network MGAN model for aspect level sentiment classification ,16, ,"Existing approaches mostly adopt coarse - grained attention mechanism , which may bring information loss if the aspect has multiple words or larger context .",abstract
text-classification,1,Our findings are threefold .,introduction,introduction,0,39,28,28,0,introduction : introduction,0.15234375,0.8,0.8,Our findings are threefold ,5,"Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .","First , in the supervised setting , our simplification strategy leads to higher accuracy and faster training than previous LSTM .",introduction
natural_language_inference,50,Our qualitative studies enable us to gain a better intuition pertaining to the good performance of our model .,abstract,abstract,0,48,46,46,0,abstract : abstract,0.15141955835962145,1.0,1.0,Our qualitative studies enable us to gain a better intuition pertaining to the good performance of our model ,19,"Due to its compositional nature , we find that our model learns to self - organize not only at the QA level but also at the word - level .", ,abstract
natural_language_inference,71,"The GORU is able to successfully outperform GRU , LSTM and EURNN in terms of both learning speed and final performances as shown in .",experiment,Parenthesis Task,1,146,47,11,0,experiment : Parenthesis Task,0.6790697674418604,0.7833333333333333,0.6875,The GORU is able to successfully outperform GRU LSTM and EURNN in terms of both learning speed and final performances as shown in ,24,This task requires learning long - term dependencies and forgetting of the noisy data .,We also analyzed the activations of the update gates for GORU and GRU .,experiment
text-classification,0,"We tried our best to choose models that can provide comparable and competitive results , and the results are reported faithfully without any model selection .",model,Comparison Models,0,102,32,3,0,model : Comparison Models,0.4493392070484581,1.0,1.0,We tried our best to choose models that can provide comparable and competitive results and the results are reported faithfully without any model selection ,25,"To offer fair comparisons to competitive models , we conducted a series of experiments with both traditional and deep learning methods .", ,method
sentiment_analysis,13,We believe that such training examples are rare .,analysis,analysis,0,273,26,26,0,analysis : analysis,0.9820143884892086,1.0,1.0,We believe that such training examples are rare ,9,"Also , BERT - PT has the problem of dealing with one sentence with two opposite opinions ( "" The screen is good but not for windows . "" ) .", ,result
question_answering,2,Our method achieves the state - of - the - art results on two VQA benchmarks .,introduction,introduction,0,48,38,38,0,introduction : introduction,0.17328519855595667,1.0,1.0,Our method achieves the state of the art results on two VQA benchmarks ,14,We quantitatively verify that the evidence produced by our method are more correlated to that of human annotators ., ,introduction
sentiment_analysis,5,"However , comparing it with the subjectindependent confusion matrix in , we can see that the recognition rate of sad decreases significantly , which is same as the case observed in the above ( i.e. , point ) .",experiment,Confusion matrix,0,208,73,8,0,experiment : Confusion matrix,0.7849056603773585,0.8390804597701149,0.8,However comparing it with the subjectindependent confusion matrix in we can see that the recognition rate of sad decreases significantly which is same as the case observed in the above i e point ,34,"For the subject - dependent experimental result in , we can find that the emotions funny , neutral and sad are much easier to be recognized than the other four emotions .",It is possibly because the pattern of emotion sad varies considerably from one subject to another .,experiment
sentence_compression,3,"To further analyze the Evaluator - SLM performance , we used an example sentence , "" The Dalian shipyard has built two new huge ships "" to observe how a language model scores different word deletion operations .",analysis,Evaluator Analysis,0,116,2,2,0,analysis : Evaluator Analysis,0.935483870967742,0.3333333333333333,0.3333333333333333,To further analyze the Evaluator SLM performance we used an example sentence The Dalian shipyard has built two new huge ships to observe how a language model scores different word deletion operations ,33, ,"We converted the reward function R SLM toe ? logR SLM for a better observation ( sim - ilar to "" sentence perplexity "" , the higher the score is , the worse is the sentence ) .",result
natural_language_inference,42,Note that in ( 12 b ) zero - padding is applied so that logits conv maintains the same dimension of logits .,system description,Convolutional Attention,0,126,86,9,0,system description : Convolutional Attention,0.4359861591695502,0.4914285714285714,1.0,Note that in 12 b zero padding is applied so that logits conv maintains the same dimension of logits ,20,"where Conv represents a single convolutional layer with a trainable kernel H ? R hwn heads n heads that has height h , width w , and number of filters and channels both equal ton heads .", ,method
natural_language_inference,61,Adaptive word direction layer,model,aESIM model,0,90,29,17,0,model : aESIM model,0.5769230769230769,0.4264705882352941,0.5151515151515151,Adaptive word direction layer,4,"where u il is obtained a er one - layer MLP for the input f il , ? il is the importance of word l , is calculated by the So Max unit on the context vector u w of the sentence i which is randomly initialized and modi ed during the training , s il is the a ention enhanced vector through multiplying the weight ? il and original vector f il , where","In traditional Bi - LSTM model , the forward and the backward vectors of a word are considered to have equal importance on the word representation .",method
paraphrase_generation,1,"The dimension of the embedding vector is set to 300 , the dimension of both encoder and decoder is 600 , and the latent space dimension is 1100 .",experiment,Experimental Setup,1,131,5,5,0,experiment : Experimental Setup,0.5927601809954751,0.38461538461538464,0.38461538461538464,The dimension of the embedding vector is set to 300 the dimension of both encoder and decoder is 600 and the latent space dimension is 1100 ,27,"In our setup , we do not use any external word embeddings such as Glove ; rather we train these as part of the model - training .",The number of layers in the encoder is 1 and in decoder,experiment
relation-classification,0,11 This has two LSTM - RNNs for the left and right subpaths of the shortest path .,experiment,End-to-end Relation Extraction Results,0,192,40,29,0,experiment : End-to-end Relation Extraction Results,0.8495575221238938,0.9302325581395348,0.90625,11 This has two LSTM RNNs for the left and right subpaths of the shortest path ,17,SPX u is our adaptation of the shortest path LSTM - RNN proposed by to match our sequence - layer based model .,"We first calculate the max pooling of the LSTM units for each of these two RNNs , and then concatenate the outputs of the pooling for the relation candidate .",experiment
sentiment_analysis,50,2 ) The numbers of neutral examples in the test sets of D3 and D4 are very small .,model,Model Comparison,1,105,7,7,0,model : Model Comparison,0.6481481481481481,0.4666666666666667,0.4666666666666667,2 The numbers of neutral examples in the test sets of D3 and D4 are very small ,18,There are two main reasons why the improvements of macro - F1 scores are more significant on D3 and D4 than on D1 : without any external knowledge might still be able to learn some neutral - related features on D1 but it is very hard to learn from D3 and D4 .,"Thus , the precision and recall on neutral class will be largely affected by even a small prediction difference ( e.g. , with 5 more neutral examples correctly identified , recall is increased by more than 10 % on both datasets ) .",method
natural_language_inference,81,si is the ith position of the coattention context .,system description,COARSE-GRAIN MODULE,0,73,16,16,0,system description : COARSE-GRAIN MODULE,0.17339667458432306,0.3404255319148936,0.6153846153846154,si is the ith position of the coattention context ,10,"2 , b 2 , W 1 , and b 1 are parameters for the MLP scorer .","We abbreviate self - attention , which takes as input a sequence U sand produces the summary conditioned on the query G s , as",method
relation-classification,9,"On every dataset except BC5 CDR and SciCite , BERT - Base with finetuning outperforms ( or performs similarly to ) a model using frozen SCIBERT embeddings .",Effect of Finetuning,Effect of Finetuning,0,130,4,4,0,Effect of Finetuning : Effect of Finetuning,0.8843537414965986,0.4,1.0,On every dataset except BC5 CDR and SciCite BERT Base with finetuning outperforms or performs similarly to a model using frozen SCIBERT embeddings ,24,"For each scientific domain , we observe the largest effects of finetuning on the computer science ( + 5.59 F1 with SCIB - ERT and + 3.17 F1 with BERT - Base ) and biomedical tasks ( + 2.94 F1 with SCIBERT and + 4.61 F1 with BERT - Base ) , and the smallest effect on multidomain tasks ( + 0.7 F1 with SCIBERT and + 1.14 F1 with BERT - Base ) .", ,others
natural_language_inference,43,"In Section 4 we evaluate the performance of a simple QA model for each compositionality type , and find that N - ARY questions are handled well by our web - based QA system .",system description,Type,0,77,35,15,0,system description : Type,0.4967741935483871,1.0,1.0,In Section 4 we evaluate the performance of a simple QA model for each compositionality type and find that N ARY questions are handled well by our web based QA system ,32,illustrates that COMPLEXQUESTIONS is dominated by N - ARY questions that involve an event with multiple entities ., ,method
natural_language_inference,100,We use the skip - gram model with window size 5 and filter words with frequency less than 5 following the common practice in many neural embedding models .,experiment,experiment,0,244,6,6,0,experiment : experiment,0.6666666666666666,0.6,0.6,We use the skip gram model with window size 5 and filter words with frequency less than 5 following the common practice in many neural embedding models ,28,This data set is one of the most widely used benchmarks for answer with the English Wikipedia dump .,"For the word vector dimension , we tune it as a hyper - parameter on the validation data starting from 200 to 1000 .",experiment
natural_language_inference,38,Then we use the passage representation along with the initialized hidden state to predict the indices that represent the answer 's location in the passage :,model,Output layer,0,104,63,5,0,model : Output layer,0.5502645502645502,0.9545454545454546,0.625,Then we use the passage representation along with the initialized hidden state to predict the indices that represent the answer s location in the passage ,26,"where s T , W Q and b Q are parameters , l 0 is the initial hidden state of the pointer network .","pk = argmax ( a k 1 , ... , a kn ) where W h is parameter , k = 1 , 2 that respectively represent the start point and the endpoint of the answer , O j is the vector that represents j- th word in the passage of the final output of the memory networks .",method
natural_language_inference,3,"Among these tasks , a common problem is modelling the relevance / similarity of the sentence pair , which is also called text semantic matching .",introduction,introduction,1,12,3,3,0,introduction : introduction,0.0576923076923077,0.21428571428571427,0.21428571428571427,Among these tasks a common problem is modelling the relevance similarity of the sentence pair which is also called text semantic matching ,23,"Distributed representations of words or sentences have been widely used in many natural language processing ( NLP ) tasks , such as text classification , question answering and machine translation and soon .","Recently , deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses .",introduction
text_summarization,5,"In addition , we find seq2seq models usually focus on copying source words in order , without any actual "" summarization "" .",introduction,introduction,0,20,11,11,0,introduction : introduction,0.07936507936507936,0.3055555555555556,0.3055555555555556,In addition we find seq2seq models usually focus on copying source words in order without any actual summarization ,19,These results largely reduce the informativeness and readability of the generated summaries .,"Therefore , we argue that , the free generation based on the source sentence is not enough for a seq2seq model .",introduction
sentiment_analysis,23,We summarize the mean and standard deviation in .,model,Model,0,243,7,7,0,model : Model,0.8321917808219178,0.14,0.5384615384615384,We summarize the mean and standard deviation in ,9,Each configuration was run five times with different random initializations .,"As the results imply , complicated pooling is better than global pooling to some degree for both model variants .",method
text_summarization,8,Distillation experiments in which we tried to use the output of the contentselection as training - input to abstractive models showed a drastic decrease in model performance .,analysis,Analysis and Discussion,0,248,21,21,0,analysis : Analysis and Discussion,0.8671328671328671,0.6176470588235294,0.6176470588235294,Distillation experiments in which we tried to use the output of the contentselection as training input to abstractive models showed a drastic decrease in model performance ,27,"However , we also note that the abstractive system requires access to the full source document .","Analysis of Copying While Pointer - Generator models have the ability to abstract in summary , the use of a copy mechanism causes the summaries to be mostly extractive .",result
natural_language_inference,71,We test each RNN on character - level language modeling .,model,Language Modeling: Character-level Prediction,0,183,24,2,0,model : Language Modeling: Character-level Prediction,0.8511627906976744,0.4897959183673469,0.1111111111111111,We test each RNN on character level language modeling ,10, ,The RNN is fed by one character each step from are al context and supposed to output the prediction for the next character .,method
sentiment_analysis,2,"Although both IAN and MemNet models performance better than other methods , they all perform less competitive than our PBAN both on Restaurant and Laptop datasets .",dataset,Datasets,0,153,22,22,0,dataset : Datasets,0.6740088105726872,0.7586206896551724,0.7586206896551724,Although both IAN and MemNet models performance better than other methods they all perform less competitive than our PBAN both on Restaurant and Laptop datasets ,26,"MemNet ( 9 ) utilizes a more complex structure that containing nine computational layers , and it achieves better results compared to IAN since MemNet reads the useful information from external memory repeatedly .","For IAN model , it interactively learns the attentions between the aspect term and its corresponding sentence , but this attention mechanism is coarse - grained and it does not fully consider the influence of different words in aspect term on the sentence .",experiment
sarcasm_detection,0,"To reduce noise , we use several filters to remove noisy and uninformative comments .",system description,Constructing SARC,0,37,12,4,0,system description : Constructing SARC,0.2102272727272728,0.4,0.3333333333333333,To reduce noise we use several filters to remove noisy and uninformative comments ,14,"For each comment we provide a sarcasm label , author , the subreddit it appeared in , the comment score as voted on by users , the date of the comment , and identifiers linking back to the original dataset of all comments .",Many of these are standard preprocessing steps such as excluding URLs and limiting characters to be ASCII .,method
named-entity-recognition,8,"For each task , we selected the best fine - tuning learning rate ( among 5 e - 5 , 4 e - 5 , 3 e - 5 , and 2 e - 5 ) on the Dev set .",experiment,GLUE,1,171,19,17,0,experiment : GLUE,0.4418604651162791,0.2676056338028169,0.2463768115942029,For each task we selected the best fine tuning learning rate among 5 e 5 4 e 5 3 e 5 and 2 e 5 on the Dev set ,30,We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .,"Additionally , for BERT LARGE we found that finetuning was sometimes unstable on small datasets , so we ran several random restarts and selected the best model on the Dev set .",experiment
part-of-speech_tagging,3,It demonstrates the effectiveness of dropout in reducing overfitting .,system description,Word Embeddings,0,164,29,19,0,system description : Word Embeddings,0.8078817733990148,0.90625,1.0,It demonstrates the effectiveness of dropout in reducing overfitting ,10,We observe a essential improvement for both the two tasks ., ,method
natural_language_inference,60,"Multi-modality Multi-modal information has proven useful in many tasks , yet the question of multi-modal fusion remains an open problem .",introduction,introduction,0,29,22,22,0,introduction : introduction,0.15025906735751296,0.9565217391304348,0.9565217391304348,Multi modality Multi modal information has proven useful in many tasks yet the question of multi modal fusion remains an open problem ,23,"With our method , embeddings from different domains can be combined , optionally while taking into account contextual information .",Our method offers a straightforward solution for combining information from different modalities .,introduction
sentiment_analysis,11,"For fair comparison in an end - toend learning paradigm , we remove the penultimate SVM of this model .",baseline,bc-LSTM:,0,264,10,5,0,baseline : bc-LSTM:,0.7674418604651163,0.3125,0.1851851851851852,For fair comparison in an end toend learning paradigm we remove the penultimate SVM of this model ,18,The model uses context features from unimodal LSTMs and its concatenation is fed to a final LSTM for classification .,The model does n't accommodate inter-speaker dependencies .,result
natural_language_inference,52,"Accordingly , we evaluate our system both in terms of it 's ability to correctly answer questions ( Section 6.1 ) , as well as provide highquality justifications for those answers ( 6.2 ) .",result,Results,0,169,3,3,0,result : Results,0.6475095785440613,0.75,0.75,Accordingly we evaluate our system both in terms of it s ability to correctly answer questions Section 6 1 as well as provide highquality justifications for those answers 6 2 ,31,"Rather than seeking to outperform all other systems at selecting the correct answer to a question , here we aimed to construct a system system that can produce substantially better justifications for why the answer choice is correct to a human user , without unduly sacrificing accuracy on the answer selection task .","Additionally , we perform an error analysis ( Section 6.3 ) , taking advantage of the insight the reranked justifications provide into what the model is learning .",result
relation_extraction,5,"In contrast , the masked model is unaffected by unseen entity mentions and achieves a stable dev F 1 of 74.7 .",model,Entity Bias in the SemEval Dataset,0,218,20,12,0,model : Entity Bias in the SemEval Dataset,0.8288973384030418,0.9090909090909092,0.8571428571428571,In contrast the masked model is unaffected by unseen entity mentions and achieves a stable dev F 1 of 74 7 ,22,"While the unmasked model achieves a 83.6 F 1 on the original SemEval dev set , F 1 drops drastically to 62.4 if we replace dev set entity mentions with a special < UNK > token to simulate the presence of unseen entities .",This suggests that models trained without entities masked generalize poorly to new examples with unseen entities .,method
natural_language_inference,7,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,title,title,1,2,1,1,0,title : title,0.011235955056179777,1.0,1.0,LEARNING RECURRENT SPAN REPRESENTATIONS FOR EXTRACTIVE QUESTION ANSWERING,8, , ,title
text_summarization,11,It is described below :,system description,Convolutional Gated Unit,0,44,15,6,0,system description : Convolutional Gated Unit,0.3055555555555556,0.36585365853658536,0.1875,It is described below ,5,"The hidden state of the decoder st and the en-coders output hi at each time step i of the encoding process are computed with a weight matrix W a to obtain the global attention ? t , i and the context vector ct .","where C refers to the cell state in the LSTM , and g ( ) refers to a non-linear function .",method
sentiment_analysis,31,"where represents element - wise product , b ? R is a bias term and f is a non-linear function .",system description,Convolutional Neural Networks,0,61,17,11,0,system description : Convolutional Neural Networks,0.3836477987421384,0.3035714285714285,0.5789473684210527,where represents element wise product b R is a bias term and f is a non linear function ,19,"Specifically , for a word window v i:i+h?1 ? R hk , a feature c i is generated by","where represents element - wise product , b ? R is a bias term and f is a non-linear function .",method
machine-translation,5,"Therefore , for WMT 2018 , Tilde submitted NMT systems that were trained using Transformer models .",introduction,introduction,0,21,14,14,0,introduction : introduction,0.14685314685314685,0.875,0.875,Therefore for WMT 2018 Tilde submitted NMT systems that were trained using Transformer models ,15,"In this paper , we compare the MLSTMbased models with Transformer models for English - Estonian and Estonian - English and we show that the state - of - the - art of WMT 2017 is well behind the new models .",The paper is further structured as follows :,introduction
named-entity-recognition,6,Those features have been shown to be crucial for stateof - the - art performance .,model,Features,0,109,5,3,0,model : Features,0.5141509433962265,0.25,1.0,Those features have been shown to be crucial for stateof the art performance ,14,"In addition to the LS vector , we incorporate publicly available pre-trained embeddings , as well as character - level , and capitalization features .", ,method
natural_language_inference,36,"For predicate identification , we use spa",model,Downstream Model,0,105,5,5,0,model : Downstream Model,0.5,0.16129032258064516,0.16129032258064516,For predicate identification we use spa,6,"Thus , our SRL module has to be end -toend , predicting all predicates and corresponding arguments in one shot .",Cy 1 to tokenize the input sentence with part - of - speech ( POS ) tags and the verbs are marked as the binary predicate indicator for whether the word is the verb for the sentence .,method
text_summarization,14,Sentence summarization is a well - studied task that creates a condensed version of along source sentence .,introduction,introduction,0,13,2,2,0,introduction : introduction,0.05701754385964913,0.08333333333333333,0.08333333333333333,Sentence summarization is a well studied task that creates a condensed version of along source sentence ,17, ,Sequence - to - sequence ( seq2seq ) model that encodes a source sequence into a latent representation and outputs another sequence is the dominating framework for sentence summarization .,introduction
natural_language_inference,62,"Then by constructing ac w - attended summary of Z , we obtain the matching vector c + w ( if E w = ? , which makes Z = {} , we will set c + w = 0 ) :",architecture,Knowledge Aided Mutual Attention,0,146,51,12,0,architecture : Knowledge Aided Mutual Attention,0.6517857142857143,0.6623376623376623,0.5217391304347826,Then by constructing ac w attended summary of Z we obtain the matching vector c w if E w which makes Z we will set c w 0 ,29,"Specifically , for each word w , whose context embedding is cw , to construct its enhanced context embedding c * w , first recall that we have extracted a set E w , which includes the positions of the passage words that w is semantically connected to , thus by gathering the columns in C P whose indexes are given by E w , we obtain the matching context embeddings Z ? R d| Ew | .","where v c , W c , and U care trainable parameters ; z i represents the i - th column in Z .",method
sentiment_analysis,44,The Relation Neural Net is the same in structure as the Structure Neural Net in .,model,Neural Net Models,0,141,36,13,0,model : Neural Net Models,0.6409090909090909,0.4444444444444444,0.8125,The Relation Neural Net is the same in structure as the Structure Neural Net in ,16,Net takes as input the compressed vector representation for two Discourse Units thatare determined to be connected in the Discourse Tree and learns the relation label for the parent node .,Net takes as input the compressed vector representation for two Dis - course Unit thatare determined to be connected in the Discourse Tree and learns the sentiment label for the parent node .,method
natural_language_inference,80,shows that removing any part from our model hurts the development set accuracy which indicates the effectiveness of these components .,ablation,ablation,0,196,7,7,0,ablation : ablation,0.6758620689655173,0.4117647058823529,0.4117647058823529,shows that removing any part from our model hurts the development set accuracy which indicates the effectiveness of these components ,21,We can see that all modifications lead to a new model and their differences are statistically significant with a p-value of < 0.001 over Chi square test .,"Among all components , three of them have noticeable influences : max pooling , difference in the attention stage , and dependent reading .",result
natural_language_inference,69,"Mumbai is the most populous city in MASK7 . """,model,Lexical Abstraction: Candidate Masking,0,246,35,6,0,model : Lexical Abstraction: Candidate Masking,0.7130434782608696,0.7291666666666666,0.75,Mumbai is the most populous city in MASK7 ,9,"To evaluate the ability of models to rely on context alone , we created masked versions of the datasets : we replace any candidate expression randomly using 100 unique placeholder tokens , e.g.","Masking is consistent within one sample , but generally different for the same expression across samples .",method
natural_language_inference,89,Model - I follows a procedure of unsupervised pre-training and supervised fine - tuning .,training,Training and Inference,0,162,3,3,0,training : Training and Inference,0.6230769230769231,0.2307692307692308,0.2307692307692308,Model I follows a procedure of unsupervised pre training and supervised fine tuning ,14,"Our no -answer reader is trained on context passages , while the answer verifier is trained on oracle answer sentences .","That is , the model is first optimized with a language modeling objective on a large unlabeled text corpus to initialize its parameters .",experiment
part-of-speech_tagging,6,Dependency relation types are predicted in a similar manner .,model,BiLSTM-based latent feature representations:,0,43,22,17,0,model : BiLSTM-based latent feature representations:,0.2885906040268457,0.6470588235294118,0.8095238095238095,Dependency relation types are predicted in a similar manner ,10,We then compute a marginbased hinge loss L arc with loss - augmented inference to maximize the margin between the gold unlabeled parse tree and the highest scoring incorrect tree .,We use another MLP on top of the BiLSTM ctx for predicting relation type of an head - modifier arc .,method
question_answering,2,Focal Visual - Text Attention,architecture,Focal Visual-Text Attention,0,93,14,1,0,architecture : Focal Visual-Text Attention,0.3357400722021661,0.12844036697247707,0.05,Focal Visual Text Attention,4, , ,method
text-classification,4,"In this manner , the learned filters vary from sentence to sentence and allow for more fine - grained feature abstraction .",introduction,introduction,1,29,19,19,0,introduction : introduction,0.13122171945701358,0.7916666666666666,0.7916666666666666,In this manner the learned filters vary from sentence to sentence and allow for more fine grained feature abstraction ,20,"Specifically , we introduce a meta network to generate a set of contextsensitive filters , conditioned on specific input sentences ; these filters are adaptively applied to either the same ( Section 3.2 ) or different ( Section 3.3 ) text sequences .","Moreover , since the generated filters in our framework can adapt to different conditional information available ( labels or paired sentences ) , they can be naturally generalized to model sentence pairs .",introduction
temporal_information_extraction,1,shows the actual human annotations provided by TE3 .,introduction,introduction,0,34,25,25,0,introduction : introduction,0.13229571984435798,0.5681818181818182,0.5681818181818182,shows the actual human annotations provided by TE3 ,9,The inter-annotator agreement on TLINKs is usually about 50 % - 60 % .,"Among all the ten possible pairs of nodes , only three TLINKs were annotated .",introduction
semantic_role_labeling,3,The successes of neural networks root in its highly flexible nonlinear transformations .,system description,Nonlinear Sub-Layers,0,92,54,2,0,system description : Nonlinear Sub-Layers,0.3484848484848485,0.4954128440366973,0.08695652173913042,The successes of neural networks root in its highly flexible nonlinear transformations ,13, ,"Since attention mechanism uses weighted sum to generate output vectors , its representational power is limited .",method
sentiment_analysis,9,"The default SRD setting for all experiments is 5 , with additional instructions for experiments with different SRD .",dataset,Datasets and Hyperparameters Setting,0,188,16,16,0,dataset : Datasets and Hyperparameters Setting,0.6787003610108303,1.0,1.0,The default SRD setting for all experiments is 5 with additional instructions for experiments with different SRD ,18,The superior hyperparameters are listed in ., ,experiment
natural_language_inference,97,Machine comprehension ( MC ) is evaluated by posing a set of questions based on a text passage ( akin to the reading tests we all took in school ) .,introduction,introduction,1,16,5,5,0,introduction : introduction,0.0547945205479452,0.13513513513513514,0.13513513513513514,Machine comprehension MC is evaluated by posing a set of questions based on a text passage akin to the reading tests we all took in school ,27,It has garnered significant attention from the machine learning research community in recent years .,"Such tests are objectively gradable and can be used to assess a range of abilities , from basic understanding to causal reasoning to inference .",introduction
sentiment_analysis,22,"Aspect - level sentiment analysis is a fine - grained task in sentiment analysis , which aims to identify the sentiment polarity ( i.e. , negative , neutral , or positive ) of a specific opinion target expressed in a comment / review by a reviewer .",introduction,introduction,1,12,2,2,0,introduction : introduction,0.052401746724890834,0.060606060606060615,0.060606060606060615,Aspect level sentiment analysis is a fine grained task in sentiment analysis which aims to identify the sentiment polarity i e negative neutral or positive of a specific opinion target expressed in a comment review by a reviewer ,39, ,"For example , given a sentence "" The price is reasonable although the service is poor "" , the sentiment polarity for aspects "" price "" and "" service "" are positive and negative respectively .",introduction
natural_language_inference,97,"Our implementation of the Parallel - Hierarchical model , using the Keras framework , is available on Github .",model,Combining Perspectives,0,193,111,7,0,model : Combining Perspectives,0.660958904109589,0.9910714285714286,0.875,Our implementation of the Parallel Hierarchical model using the Keras framework is available on Github ,16,This approach worked better than comparing the correct answer to the incorrect answers individually as in ., ,method
natural_language_inference,46,"Although the answers are not just single word / entity answers , many plausible questions for assessing RC can not be asked because no document span would contain its answer .",system description,FRANK (to the baby),0,69,53,47,0,system description : FRANK (to the baby),0.2323232323232323,0.6309523809523809,0.6025641025641025,Although the answers are not just single word entity answers many plausible questions for assessing RC can not be asked because no document span would contain its answer ,29,"large number of a questions and answers are provided for a set of documents , where the answers are spans of the context document , i.e. contiguous sequences of words from the document .","While they provide a large number of questions , these are from a relatively small number of documents , which are themselves fairly short , thereby limiting the lexical and topical diversity models trained on this data can cope with .",method
natural_language_inference,23,"This is because "" Belgium "" and "" France "" are similar European countries .",model,MULTI-LEVEL ATTENTION VISUALIZATION,0,508,115,7,0,model : MULTI-LEVEL ATTENTION VISUALIZATION,0.9902534113060428,0.9583333333333334,0.5833333333333334,This is because Belgium and France are similar European countries ,11,"If only the high - level attention is considered ( which is common in most previous architectures ) , we can see from the high - level attention map in the right hand side of that the added sentence "" The proclamation of the Central Park abolished protestantism in Belgium "" matches well with the question "" What proclamation abolished protestantism in France ? ""","Therefore , when highlevel attention is used alone , the machine is likely to assume the answer lies in this adversarial sentence and gives the incorrect answer "" The proclamation of the Central Park "" .",method
relation_extraction,10,"And yet , our method covers 99.99 % and 99.96 % of all Entities and Relations respectively in the Train Set of ACE2005 .",model,Entity Mention Detection,0,177,33,9,0,model : Entity Mention Detection,0.921875,0.7674418604651163,0.4736842105263158,And yet our method covers 99 99 and 99 96 of all Entities and Relations respectively in the Train Set of ACE2005 ,23,Note how our chosen policy is more than 40x more memory efficient than a policy which considers all spans in the doc .,Results shows the results for RE .,method
natural_language_inference,8,"The answer sentence selection dataset contains a set of factoid questions , with a list of answer sentences corresponding to each question .",dataset,dataset,0,138,2,2,0,dataset : dataset,0.6540284360189573,0.16666666666666666,0.16666666666666666,The answer sentence selection dataset contains a set of factoid questions with a list of answer sentences corresponding to each question ,22, ,"Wang et al. created this dataset from Text REtrieval Conference ( TREC ) QA track data , with candidate answers automatically selected from each question 's document pool .",experiment
sentiment_analysis,20,For training we use the available data from prior years ( only tweets ) .,training,Training,0,150,5,5,0,training : Training,0.8064516129032258,0.29411764705882354,0.7142857142857143,For training we use the available data from prior years only tweets ,13,"We use Adam for tuning the learning rate and we clip the norm of the gradients at 5 , as an extra safety measure against exploding gradients .",shows the statistics of the data we used .,experiment
text_summarization,4,We posit that this is due to the fact that the article title does not correspond to the summary of the first sentence .,result,Results,0,213,19,19,0,result : Results,0.8255813953488372,0.3275862068965517,0.3275862068965517,We posit that this is due to the fact that the article title does not correspond to the summary of the first sentence ,24,"Interestingly , the CNN variant produces better ( but with no significant difference ) summaries than the gold summaries .",Selective dis ambiguation of entities,result
natural_language_inference,23,"We call this a fusion process , where set B is fused into set A. Fusion processes are commonly based on attention , but some are not .",system description,CONCEPTUAL ARCHITECTURE FOR MACHINE READING COMPREHENSION,0,68,18,4,0,system description : CONCEPTUAL ARCHITECTURE FOR MACHINE READING COMPREHENSION,0.13255360623781676,0.8181818181818182,0.5,We call this a fusion process where set B is fused into set A Fusion processes are commonly based on attention but some are not ,26,"Given two sets of vectors , A and B , we enhance or modify every single vector in set A with the information from set B .",Major improvements in recent MRC work lie in how the fusion process is designed .,method
natural_language_inference,65,"? R Hd , U ? R HH and b ? R H1 are the network parameters .",system description,Convolution,0,70,29,17,0,system description : Convolution,0.3043478260869565,0.725,0.6071428571428571, R Hd U R HH and b R H1 are the network parameters ,15,"Finally , the forget gate allows the cell to remember or forget its previous state .","? R Hd , U ? R HH and b ? R H1 are the network parameters .",method
natural_language_inference,95,"As is described above , we define three objectives for the reading comprehension model over multiple passages :",training,Joint Training and Prediction,0,119,2,2,0,training : Joint Training and Prediction,0.5085470085470085,0.14285714285714285,0.14285714285714285,As is described above we define three objectives for the reading comprehension model over multiple passages ,17, ,. finding the boundary of the answer ; 2 . predicting whether each word should be included in the content ; 3 . selecting the best answer via cross - passage answer verification .,experiment
sentiment_analysis,29,Neural network based approaches automatically learn feature representations and do not require intensive feature engineering .,introduction,introduction,0,41,33,33,0,introduction : introduction,0.23563218390804605,0.6226415094339622,0.6226415094339622,Neural network based approaches automatically learn feature representations and do not require intensive feature engineering ,16,"In the recent years , sentiment classification has been advanced by neural networks significantly .",Researchers proposed a variety of neural network architectures .,introduction
natural_language_inference,27,"We conduct ablation studies on components of the proposed model , and investigate the effect of augmented data .",analysis,ABALATION STUDY AND ANALYSIS,0,245,2,2,0,analysis : ABALATION STUDY AND ANALYSIS,0.7248520710059172,0.054054054054054064,0.09090909090909093,We conduct ablation studies on components of the proposed model and investigate the effect of augmented data ,18, ,The validation scores on the development set are shown in .,result
sentiment_analysis,50,These vectors are also used for initializing E in the pretraining phase .,dataset,Datasets and Experimental Settings,1,88,12,12,0,dataset : Datasets and Experimental Settings,0.5432098765432098,0.5454545454545454,0.5454545454545454,These vectors are also used for initializing E in the pretraining phase ,13,"In all experiments , 300 - dimension Glo Ve vectors are used to initialize E and E when pretraining is not conducted for weight initialization .",Values for hyperparameters are obtained from experiments on development sets .,experiment
natural_language_inference,61,shows several samples of natural language inference from SNLI ( Stanford Natural Language Inference ) corpus .,introduction,introduction,0,11,4,4,0,introduction : introduction,0.07051282051282051,0.0784313725490196,0.0784313725490196,shows several samples of natural language inference from SNLI Stanford Natural Language Inference corpus ,15,"It concerns whether a hypothesis can be inferred from a premise , requiring understanding of the semantic similarity between the hypothesis and the premise to discriminate their relation .","In the literature , the task of NLI is usually viewed as a relation classi cation .",introduction
natural_language_inference,64,"For the experiments with one fine - tuning step ( baseline ) , we again choose values between 1 e ? 6 and 2 e ? 5 , i.e , following the common practice of BERT fine - tuning .",training,training,0,162,13,13,0,training : training,0.6454183266932271,1.0,1.0,For the experiments with one fine tuning step baseline we again choose values between 1 e 6 and 2 e 5 i e following the common practice of BERT fine tuning ,32,It should be noted that the optimality of a larger learning rate for the first step and a smaller learning rate for the second step supports our claim of considering the second step of TANDA as domain adaptation process ., ,experiment
text-classification,1,Note that being able to naturally combine several tv-embeddings is a strength of 2100 - dim LSTM tv-embed .,system description,Comparison with the previous best results on 20NG,0,192,146,9,0,system description : Comparison with the previous best results on 20NG,0.75,0.73,0.14285714285714285,Note that being able to naturally combine several tv embeddings is a strength of 2100 dim LSTM tv embed ,20,We will empirically confirm these conjectures in the experiments below .,6.08 9.24 5 oh -CNN 1200 - dim CNN tv-embed .,method
sentiment_analysis,2,"MemNet : MemNet applies attention multiple times on the word embedding , so that more abstractive evidences could be selected from the external memory .",dataset,Datasets,1,143,12,12,0,dataset : Datasets,0.6299559471365639,0.4137931034482759,0.4137931034482759,MemNet MemNet applies attention multiple times on the word embedding so that more abstractive evidences could be selected from the external memory ,23,"Finally , it concatenates the aspect term representation and context representation for predicting the sentiment polarity of the aspect terms within its contexts .",The output of the last attention layer is fed to a softmax layer for predictions .,experiment
text_generation,0,"In case of some specific tasks , one may design a classifier to provide intermediate reward signal to enhance the performance of our framework .",implementation,Model Implementations,0,289,20,20,0,implementation : Model Implementations,0.8919753086419753,0.4081632653061225,0.5405405405405406,In case of some specific tasks one may design a classifier to provide intermediate reward signal to enhance the performance of our framework ,24,"As far as we know , except for some specific tasks , most discriminative models can only perform classification well for a whole sequence rather than the unfinished one .","But to make it more general , we focus on the situation where discriminator can only provide final reward , i.e. , the probability that a finished sequence was real .",experiment
sentiment_analysis,16,"To address this problem , we propose target - sensitive memory networks ( TMNs ) , which can capture the sentiment interaction between targets and contexts .",introduction,introduction,1,51,39,39,0,introduction : introduction,0.16037735849056606,0.975,0.975,To address this problem we propose target sensitive memory networks TMNs which can capture the sentiment interaction between targets and contexts ,22,"To distinguish it from the aforementioned targetedcontext detection problem as shown by sentence ( 1 ) , we refer to the problem in ( 2 ) , ( 3 ) and ( 4 ) as the target - sensitive sentiment ( or target - dependent sentiment ) problem , which means that the sentiment polarity of a detected / attended context word is conditioned on the target and can not be directly inferred from the context word alone , unlike "" excellent "" and "" ridiculous "" .",We present several approaches to implementing TMNs and experimentally evaluate their effectiveness .,introduction
text_summarization,1,"In question generation ( SQuAD ) and abstractive summarization ( CNN - DM ) , our method demonstrates significant improvements in accuracy , diversity and training efficiency , including state - of - the - art top - 1 accuracy in both datasets , 6 % gain in top - 5 accuracy , and 3.7 times faster training over a state - of - the - art model .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.0375,0.875,0.875,In question generation SQuAD and abstractive summarization CNN DM our method demonstrates significant improvements in accuracy diversity and training efficiency including state of the art top 1 accuracy in both datasets 6 gain in top 5 accuracy and 3 7 times faster training over a state of the art model ,51,"Due to the non-differentiable nature of discrete sampling and the lack of ground truth labels for binary mask , we leverage a proxy for ground - truth mask and adopt stochastic hard - EM for training .",Our code is publicly available at https://github.com/ clovaai/FocusSeq2Seq.,abstract
natural_language_inference,9,"Hence it is well - suited for sequential data with both local and global interactions ( note that QRN is not the replacement of RNN , which is arguably better for modeling complex local interactions ) .",introduction,introduction,0,37,29,29,0,introduction : introduction,0.11212121212121212,0.90625,0.90625,Hence it is well suited for sequential data with both local and global interactions note that QRN is not the replacement of RNN which is arguably better for modeling complex local interactions ,33,"Second , QRN is situated between the attention mechanism and RNN , effectively handling time dependency and long - term dependency problems of each technique , respectively .","Third , unlike most RNN - based models , QRN can be parallelized overtime by computing candidate reduced queries ( h t ) directly from local input queries ( q t ) and context sentence vectors ( x t ) .",introduction
natural_language_inference,92,"Pred Z ( ordered by P ( z | x ; ?t ) ) 1 k 10- 9 10-6 41-37 40-36 41-37 2 k 37-36 40-36 41-37 41- 37 10 - 6 4 k 40-36 40-36 41-37 41-37 10 - 6 8 k 40-36 40-36 41-37 41-37 10- 6 16 k 37-36 40-36 41-37 41- 37 10 - 6 32 k 40-36 40-36 41-37 41- 37 10 -6 : An example from DROP num ( same as and , with it s answer text ' 4 ' and a subset of the solution set ( Z ) , containing two of ' 41 - 38 ' ( which ' 41 ' come from different mentions ; one denoted by for distinction ) , ' 40 - 36 ' and ' 10 - 4 ' .",analysis,Analysis,0,195,16,16,0,analysis : Analysis,0.6770833333333334,0.4571428571428571,0.4571428571428571,Pred Z ordered by P z x t 1 k 10 9 10 6 41 37 40 36 41 37 2 k 37 36 40 36 41 37 41 37 10 6 4 k 40 36 40 36 41 37 41 37 10 6 8 k 40 36 40 36 41 37 41 37 10 6 16 k 37 36 40 36 41 37 41 37 10 6 32 k 40 36 40 36 41 37 41 37 10 6 An example from DROP num same as and with it s answer text 4 and a subset of the solution set Z containing two of 41 38 which 41 come from different mentions one denoted by for distinction 40 36 and 10 4 ,124,We analyze the top 1 prediction and the likelihood of z ? Z assigned by the model on DROP num with different number of training iterations ( steps from 1 k to 32 k ) .,"For each training step t , the top 1 prediction and Z ordered by P ( z |x ; ? t ) , a probability of z ? Z with respect to the model at t through training procedure are shown .",result
question_generation,1,where values of a =1500 and b=1250 are set empirically .,Supplementary Material,Training Configuration,0,250,21,5,0,Supplementary Material : Training Configuration,0.6377551020408163,1.0,1.0,where values of a 1500 and b 1250 are set empirically ,12,"In order to train a triplet model , we have used RM - SPROP to optimize the triplet model model parameter and configure hyper - parameter values to be : learning rate = 0.001 , batch size = 200 , ? = 0.9 , = 1 e ? 8 . We also used learning rate decay to decrease the learning rate on every epoch by a factor given by :", ,others
sentiment_analysis,5,"Subsequently , we characterize the relationship between the identified paired electrodes on two hemispheres , and generate a more discriminative and higher - level discrepancy feature for final classification .",model,The BiHDM model,0,70,6,6,0,model : The BiHDM model,0.2641509433962264,0.08450704225352113,0.1,Subsequently we characterize the relationship between the identified paired electrodes on two hemispheres and generate a more discriminative and higher level discrepancy feature for final classification ,27,"First , we obtain the deep representations of all the electrodes ' data .","Third , we leverage a classifier and a discriminator to corporately induce the above process to generate the emotion - related but domain - invariant features .",method
semantic_parsing,1,"We evaluate TRANX on both general - purpose ( Python , DJANGO ) and domain - specific ( SQL , WIKISQL ) code generation tasks .",system description,Code Generation,0,114,8,2,0,system description : Code Generation,0.7972027972027972,0.5,0.2,We evaluate TRANX on both general purpose Python DJANGO and domain specific SQL WIKISQL code generation tasks ,18, ,"The DJANGO dataset consists of 18,805 lines of Python source code extracted from the Django Web framework , with each line paired with an NL description .",method
text-classification,3,This experiment aims at studying the impact of using different word embeddings ( with differently preprocessed training corpora ) on tokenized datasets ( vanilla setting ) .,experiment,Experiment 2: Cross-preprocessing,0,98,37,2,0,experiment : Experiment 2: Cross-preprocessing,0.7967479674796748,0.7254901960784313,0.125,This experiment aims at studying the impact of using different word embeddings with differently preprocessed training corpora on tokenized datasets vanilla setting ,23, ,shows the results for this experiment .,experiment
sentiment_analysis,27,Given an input context consists of N words .,methodology,Methodology,0,80,3,3,0,methodology : Methodology,0.2898550724637681,0.035294117647058816,0.2727272727272727,Given an input context consists of N words ,9,Aspect - level sentiment classification can be formulated as follows .,An example to illustrate the usefulness of the sentiment dependencies between multiple aspects .,method
relation-classification,3,We use the relaxed evaluation similar to ; on the EC task .,experiment,Experimental setup,0,112,33,33,0,experiment : Experimental setup,0.8175182481751825,0.7021276595744681,0.7021276595744681,We use the relaxed evaluation similar to on the EC task ,12,"For the CoNLL04 dataset , we use two evaluation settings .","The baseline model outperforms the state - of - the - art models that do not rely on manually extracted features ( > 4 % improvement for both tasks ) , since we directly model the whole sentence , instead of just considering pairs of entities .",experiment
text-classification,9,"Using topics as additional context also decreases the performance of the CNN classifier on most data sets , giving an adverse effect to the CNN classifier .",result,Results and Discussion,0,191,25,25,0,result : Results and Discussion,0.7579365079365079,1.0,1.0,Using topics as additional context also decreases the performance of the CNN classifier on most data sets giving an adverse effect to the CNN classifier ,26,"When using a single context ( i.e. TopCNN word , TopCNN sent , and our models when N = 1 ) , translations always outperform topics even when using the baseline methods .", ,result
named-entity-recognition,0,"The updating rule for ? Similar to the iterative thresholding ( IT ) in , the degree of violations of the L N equality constraints are used to update the Lagrangian multiplier :",system description,Accelerated Robust Subset Selection (ARSS),0,133,111,35,0,system description : Accelerated Robust Subset Selection (ARSS),0.4907749077490775,0.6235955056179775,0.3431372549019608,The updating rule for Similar to the iterative thresholding IT in the degree of violations of the L N equality constraints are used to update the Lagrangian multiplier ,29,"In the following , a highly efficient solver will be given .","The updating rule for ? Similar to the iterative thresholding ( IT ) in , the degree of violations of the L N equality constraints are used to update the Lagrangian multiplier :",method
sentiment_analysis,13,We detail the hyper - parameter settings of this algorithm in Sec. 5.3 .,system description,Post-training,0,176,100,41,0,system description : Post-training,0.6330935251798561,1.0,1.0,We detail the hyper parameter settings of this algorithm in Sec 5 3 ,14,Only the gradients ? ? are kept throughout all iterations and used to update parameters ( based on the chosen optimizer ) inline 8 ., ,method
paraphrase_generation,1,"From the five captions accompanying each image , we randomly omit one caption , and use the other four as training instances ( by creating two source - reference pairs ) .",dataset,Datasets,0,105,9,9,0,dataset : Datasets,0.4751131221719457,0.42857142857142855,0.42857142857142855,From the five captions accompanying each image we randomly omit one caption and use the other four as training instances by creating two source reference pairs ,27,Train 2014 contains over 82K images and Val 2014 contains over 40K images .,"Because of the free form nature of the caption generation task , some captions were very long .",experiment
named-entity-recognition,5,"Considering efficiency , we choose a window size of 1 for the remaining experiments , setting the number of recurrent steps to 9 according to .",model,Model,0,57,4,4,0,model : Model,0.2727272727272727,0.2222222222222222,0.2222222222222222,Considering efficiency we choose a window size of 1 for the remaining experiments setting the number of recurrent steps to 9 according to ,24,"This can be be explained by the intuition that information exchange between distant nodes can be achieved using more recurrent steps under a smaller window size , as can be achieved using fewer steps under a larger window size .","S- LSTM vs BiLSTM : As shown in , BiLSTM gives significantly better accuracies compared to uni-directional LSTM 2 , with the training time per epoch growing from 67 seconds to 106 seconds .",method
natural_language_inference,60,We observe that different embeddings are preferred for different words .,result,Discussion & Analysis,0,135,22,7,0,result : Discussion & Analysis,0.6994818652849741,0.9565217391304348,0.875,We observe that different embeddings are preferred for different words ,11,The sentence is from the SNLI validation set .,"The figure is meant to illustrate possibilities for analysis , which we turn to in the next section .",result
text_generation,5,"The semi-supervised VAE fits a discriminative network q ( y|x ) , an inference network q ( z|x , y) and a generative network p ( x |y , z ) jointly as part of optimizing a variational lower bound similar that of basic VAE .",system description,Semi-supervised VAE,0,119,51,6,0,system description : Semi-supervised VAE,0.4033898305084746,0.6710526315789473,0.1935483870967742,The semi supervised VAE fits a discriminative network q y x an inference network q z x y and a generative network p x y z jointly as part of optimizing a variational lower bound similar that of basic VAE ,41,"Given the labeled set ( x , y) ? D Land the unlabeled set x ? D U , proposed a model whose latent representation contains continuous vector z and discrete label y:","For labeled data ( x , y ) , this bound is :",method
relation-classification,0,"We also concatenate the hidden state vectors of the two directions ' LSTM units corresponding to each word ( denoted as ? ? ht and ? ? ht ) as its output vector , st = ? ? ht ; ? ? ht , and pass it to the subsequent layers .",model,Entity Detection,0,70,22,2,0,model : Entity Detection,0.3097345132743363,0.2857142857142857,0.14285714285714285,We also concatenate the hidden state vectors of the two directions LSTM units corresponding to each word denoted as ht and ht as its output vector st ht ht and pass it to the subsequent layers ,37, ,"We also concatenate the hidden state vectors of the two directions ' LSTM units corresponding to each word ( denoted as ? ? ht and ? ? ht ) as its output vector , st = ? ? ht ; ? ? ht , and pass it to the subsequent layers .",method
sentiment_analysis,12,"Besides , the performance gap between MN ( + AS a ) and MN ( + AS m ) is larger than that between two variants of TNet - ATT .",result,result,0,180,11,11,0,result : result,0.8035714285714286,0.7333333333333333,0.7333333333333333,Besides the performance gap between MN AS a and MN AS m is larger than that between two variants of TNet ATT ,23,This is because the proportion of correctly predicted training instances is larger than that of incorrectly ones .,"One underlying reason is that the performance of TNet - ATT is better than MN , which enables TNet - ATT to produce more correctly predicted training instances .",result
temporal_information_extraction,1,"for all distinct events i , j , and k , where E = { ij | sentence dist ( i , j ) ? 1 } ,r is the reverse of r , and N is the number of possible relations for r 3 when r 1 and r 2 are true .",training,Inference,0,107,32,25,0,training : Inference,0.4163424124513619,0.3018867924528302,0.5681818181818182,for all distinct events i j and k where E ij sentence dist i j 1 r is the reverse of r and N is the number of possible relations for r 3 when r 1 and r 2 are true ,42,Then the ILP objective for global inference is formulated as fol -,"for all distinct events i , j , and k , where E = { ij | sentence dist ( i , j ) ? 1 } ,r is the reverse of r , and N is the number of possible relations for r 3 when r 1 and r 2 are true .",experiment
relation_extraction,12,"We propose the novel AGGCNs that learn a "" soft pruning "" strategy in an end - to - end fashion , which learns how to select and discard information .",introduction,introduction,0,51,36,36,0,introduction : introduction,0.15501519756838905,0.9,0.9,We propose the novel AGGCNs that learn a soft pruning strategy in an end to end fashion which learns how to select and discard information ,26,Our contributions are summarized as follows :,"Combining with dense connections , our AGGCN model is able to learn a better graph representation .",introduction
text_summarization,3,"To evaluate the relative impact of each training strategy with the model , we tested different combinations for comparison with each other and against the baselines .",training,Analysis on Training Strategies,0,212,2,2,0,training : Analysis on Training Strategies,0.8760330578512396,0.14285714285714285,0.14285714285714285,To evaluate the relative impact of each training strategy with the model we tested different combinations for comparison with each other and against the baselines ,26, ,Context - aware Conceptualization :,experiment
natural_language_inference,41,"We pre-train a large model with 12 layers in each of the encoder and decoder , and a hidden size of 1024 .",experiment,Experimental Setup,1,154,2,2,0,experiment : Experimental Setup,0.5992217898832685,0.0625,0.15384615384615385,We pre train a large model with 12 layers in each of the encoder and decoder and a hidden size of 1024 ,23, ,"Following RoBERTa , we use a batch size of 8000 , and train the model for 500000 steps .",experiment
natural_language_inference,69,Both neural RC models are able to largely retain or even improve their strong performance when answers are masked : they are able to leverage the textual context of the candidate expressions .,result,Results and Discussion,1,267,8,8,0,result : Results and Discussion,0.7739130434782608,0.2857142857142857,0.6153846153846154,Both neural RC models are able to largely retain or even improve their strong performance when answers are masked they are able to leverage the textual context of the candidate expressions ,32,"Especially on MEDHOP , where dataset sub - sampling is not a viable option , masking proves to be a valuable alternative , effectively circumventing spurious statistical correlations that RC models can learn to exploit .","To understand differences in model behavior between WIK - IHOP and MEDHOP , it is worth noting that drug mentions in MEDHOP are normalized to a unique single - word identifier , and performance drops under masking .",result
natural_language_inference,72,"This is because entity linking with CUIs can be noisy , and only apart of a word phrase maybe linked to the ontology .",evaluation,Evaluation,0,189,8,8,0,evaluation : Evaluation,0.6057692307692307,0.3333333333333333,0.3333333333333333,This is because entity linking with CUIs can be noisy and only apart of a word phrase maybe linked to the ontology ,23,"However , in that case , it would mean that sometimes the original word phrase is lost .","In the current setup , we are able to keep both the original word phrase as well as the extended answers .",result
named-entity-recognition,8,"In this section , we explore the effect of model size on fine - tuning task accuracy .",model,model,0,251,2,2,0,model : model,0.648578811369509,0.07142857142857142,0.07142857142857142,In this section we explore the effect of model size on fine tuning task accuracy ,16, ,"We trained a number of BERT models with a differing number of layers , hidden units , and attention heads , while otherwise using the same hyperparameters and training procedure as described previously .",method
natural_language_inference,29,"To generate sentences which are more consistent with the facts , we add a discriminator to provide additional training signals for the answer generator .",system description,Consistency discriminator,0,192,97,2,0,system description : Consistency discriminator,0.5260273972602739,0.60625,0.045454545454545456,To generate sentences which are more consistent with the facts we add a discriminator to provide additional training signals for the answer generator ,24, ,We propose a convolutional neural network ( CNN ) based classifier as discriminator .,method
text_summarization,0,"More specifically , we optimize the parameters of discriminator module according to the loss function L dc calculated in Equation 23 .",training,Model training,0,198,7,7,0,training : Model training,0.6578073089700996,1.0,1.0,More specifically we optimize the parameters of discriminator module according to the loss function L dc calculated in Equation 23 ,21,"Next , we train the discriminator module to maximize the probability of assigning the correct label to both generated aspect mt and reader focused aspect u .", ,experiment
natural_language_inference,11,"https://rajpurkar.github.io/SQuAD-explorer/ refinement module is that we are enabling more computation over the information present in the inputs , that is , we are effectively using a deeper architecture .",model,Model,0,143,8,8,0,model : Model,0.5181159420289855,0.3076923076923077,0.3076923076923077,https rajpurkar github io SQuAD explorer refinement module is that we are enabling more computation over the information present in the inputs that is we are effectively using a deeper architecture ,32,One potential explanation for the improvement obtained using the We do not report test set results for SQuAD due to restrictions on code sharing .,"To test whether this might be the case , we also ran an experiment with a 2 - layer BiLSTM ( + liq ) .",method
sentence_compression,2,These models train task - specific classifiers on the output of deep networks ( informed by the task - specific losses ) .,system description,Words FIRST PASS REGRESSIONS,0,67,48,23,0,system description : Words FIRST PASS REGRESSIONS,0.6633663366336634,0.979591836734694,0.9583333333333334,These models train task specific classifiers on the output of deep networks informed by the task specific losses ,19,"did multitask learning by doing parameter sharing across several deep networks , letting them share hidden layers ; a technique also used by for various NLP tasks .",We extend their models by moving to sequence prediction and allowing the task - specific sequence models to also be deep models .,method
semantic_role_labeling,1,detailed description of hyperparameter settings and data pre-processing can be found in Appendix A.,experiment,Experimental results,0,155,6,6,0,experiment : Experimental results,0.7110091743119266,0.15,0.3,detailed description of hyperparameter settings and data pre processing can be found in Appendix A ,16,We convert constituencies to dependencies using the Stanford head rules v 3.5 .,We compare our LISA models to four strong baselines :,experiment
sentence_classification,0,"Therefore , our dataset provides a concise annotation scheme that is useful for navigating research topics and machine reading of scientific papers .",dataset,SciCite dataset,0,124,9,9,0,dataset : SciCite dataset,0.4644194756554307,0.8181818181818182,0.8181818181818182,Therefore our dataset provides a concise annotation scheme that is useful for navigating research topics and machine reading of scientific papers ,22,More interesting intent categories are a direct use of a method or comparison of results .,"We consider three intent categories outlined in : BACK - GROUND , METHOD and RESULTCOMPARISON .",experiment
machine-translation,6,"Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity : it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground - truth , in terms of Spearman 's rank correlation .",experiment,Settings,1,177,21,3,0,experiment : Settings,0.6082474226804123,0.40384615384615385,0.08823529411764706,Word Similarity evaluates the performance of the learned word embeddings by calculating the word similarity it evaluates whether the most similar words of a given word in the embedding space are consistent with the ground truth in terms of Spearman s rank correlation ,44,We conduct experiments on the following tasks .,"We use the skip - gram model as our baseline model , and train the embeddings using Enwik9 6 .",experiment
relation_extraction,9,"Given our newly generated input attention - based representation R , we accordingly apply a filter of size dc as a weight matrix W f of size dc k ( d w + 2 d p ) .",model,Convolutional Max-Pooling with Secondary Attention,0,123,76,5,0,model : Convolutional Max-Pooling with Secondary Attention,0.5885167464114832,0.8636363636363636,0.29411764705882354,Given our newly generated input attention based representation R we accordingly apply a filter of size dc as a weight matrix W f of size dc k d w 2 d p ,33,"convolutional layer may , for instance , learn to recognize short phrases such as trigrams .","Then we add a linear bias B f , followed by a non-linear hyperbolic tangent transformation to represent features as follows :",method
natural_language_inference,30,"This paper addresses the challenging problem of open - domain question answering , which consists of building systems able to answer questions from any domain .",introduction,introduction,1,12,2,2,0,introduction : introduction,0.04651162790697674,0.06451612903225806,0.06451612903225806,This paper addresses the challenging problem of open domain question answering which consists of building systems able to answer questions from any domain ,24, ,Any advance on this difficult topic would bring a huge leap forward in building new ways of accessing knowledge .,introduction
natural_language_inference,81,"GRUs , and the replacement of encoder GRUs with projection over word embeddings .",system description,RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA,0,137,4,4,0,system description : RERANKING EXTRACTIVE QUESTION ANSWERING ON TRIVIAQA,0.3254156769596199,0.21052631578947367,0.21052631578947367,GRUs and the replacement of encoder GRUs with projection over word embeddings ,13,"The rows respectively correspond to the removal of coarse - grain module , the removal of finegrain module , the replacement of self - attention with average pooling , the replacement of bidir. with unidir.","We address the first subtask using BiDAF ++ , a competitive span extraction question answering model by and the second subtask using the CFC .",method
question_answering,2,The results show that the proposed attention can capture salient information for answering the question .,implementation,Comparison to the state-of-the-art,0,235,31,8,0,implementation : Comparison to the state-of-the-art,0.8483754512635379,0.6595744680851063,0.3333333333333333,The results show that the proposed attention can capture salient information for answering the question ,16,FVTA outperforms other attention models on finding the relevant photos for the question .,"For qualitative comparison , we select some representative questions and show both the answer and the retrieved top images based on the attention weights in .",experiment
sentiment_analysis,5,"To evaluate the proposed BiHDM model adequately , we design two kinds of experiments including the subject - dependent and subject - independent ones .",experiment,experiment,0,157,22,22,0,experiment : experiment,0.5924528301886792,0.25287356321839083,0.7333333333333333,To evaluate the proposed BiHDM model adequately we design two kinds of experiments including the subject dependent and subject independent ones ,22,There are totally 3360 samples in one subject .,"We use the released handcrafted features , i.e. , the differential entropy ( DE ) in SEED and SEED - IV , and the Short - Time Fourier Transform ( STFT ) in MPED , as the input to feed our model .",experiment
machine-translation,5,"For the tilde - nc - nmt ( unconstrained NMT ) systems , we performed model averaging of the best four models .",system,Full Workflow,0,47,24,11,0,system : Full Workflow,0.32867132867132864,0.32432432432432434,0.9166666666666666,For the tilde nc nmt unconstrained NMT systems we performed model averaging of the best four models ,18,"For the tilde - c - nmt ( constrained NMT ) systems , we performed model averaging of the best four models ( according to perplexity ) of the three different run NMT systems and deployed the averaged models in an ensemble .","For the tilde - c - nmt - comb Estonian - English system , we performed majority voting ( see Section 4.3 ) of translations produced by six different runs of different constrained systems ( using best BLEU models , averaged models , ensembled averaged models , ensembled models , and larger beam search ( 10 instead of 5 ) ) .",method
natural_language_inference,61,All vectors are updated during training .,model,Setting,0,129,68,12,0,model : Setting,0.8269230769230769,1.0,1.0,All vectors are updated during training ,7,Out - of - vocabulary ( OOV ) words are initialized randomly with Gaussian samples ., ,method
sentiment_analysis,10,"Also , u t ? R Dm is the utterance representation , obtained using feature extractors described below .",methodology,Problem Definition,0,51,5,4,0,methodology : Problem Definition,0.19844357976653693,0.35714285714285715,1.0,Also u t R Dm is the utterance representation obtained using feature extractors described below ,16,"The task is to predict the emotion labels ( happy , sad , neutral , angry , excited , and frustrated ) of the constituent utterances u 1 , u 2 , . . . , u N , where utterance u t is uttered by party p s ( ut ) , while s being the mapping between utterance and index of its corresponding party .", ,method
natural_language_inference,15,"The bottleneck component denoted as AE , inserted to prevent the ever - growing size of a feature vector , is optional for each repetition .",introduction,introduction,0,37,26,26,0,introduction : introduction,0.16371681415929204,0.8666666666666667,0.8666666666666667,The bottleneck component denoted as AE inserted to prevent the ever growing size of a feature vector is optional for each repetition ,23,"DRCN is , to our best knowledge , the first generalized version of DenseRNN which is expandable to deeper layers with the property of Dashed arrows indicate that a group of RNN - layer , concatenation and AE can be repeated multiple ( N ) times ( like a repeat mark in a music score ) .",The upper right diagram is our specific architecture for experiments with 5 RNN layers ( N = 4 ) .,introduction
natural_language_inference,72,"The progress in machine comprehension heavily depends on the introduction of new datasets , which encourages the development of new algorithms and deepens our understanding of the ( linguistic ) challenges that can or can not be tackled well by these algorithms .",introduction,introduction,0,11,3,3,0,introduction : introduction,0.035256410256410256,0.0625,0.0625,The progress in machine comprehension heavily depends on the introduction of new datasets which encourages the development of new algorithms and deepens our understanding of the linguistic challenges that can or can not be tackled well by these algorithms ,40,Machine comprehension is a task in which a system reads a text passage and then answers questions about it .,"Recently , a number of reading comprehension datasets have been proposed ( 2 ) , differing in various aspects such as mode of construction , answer - query formulation and required understanding skills .",introduction
natural_language_inference,81,In nineteenth century France an architect posing as a tramp falls in love with a woman .,APPENDIX,ATTENTION MAPS,0,277,69,54,0,APPENDIX : ATTENTION MAPS,0.6579572446555819,0.323943661971831,0.9,In nineteenth century France an architect posing as a tramp falls in love with a woman ,17,"The Beloved Vagabond is a 1936 British musical drama film directed by Curtis Bernhardt and starring Maurice Chevalier , Betty Stockfeld , Margaret Lockwood and Austin Trevor .",The film was made at Ealing Studios by the independent producer Ludovico Toeplitz .,others
sentiment_analysis,46,"Most existing studies setup sentiment classifiers using supervised machine learning approaches , such as support vector machine ( SVM ) , convolutional neural network ( CNN ) , long short - term memory ( LSTM ) , Tree - LSTM , and attention - based methods .",introduction,introduction,0,11,4,4,0,introduction : introduction,0.09016393442622953,0.25,0.25,Most existing studies setup sentiment classifiers using supervised machine learning approaches such as support vector machine SVM convolutional neural network CNN long short term memory LSTM Tree LSTM and attention based methods ,33,It has obtained considerable attention due to its broad applications in natural language processing .,"Despite the remarkable progress made by the previous work , we argue that sentiment analysis still remains a challenge .",introduction
paraphrase_generation,0,are not fixed constants .,method,Overview,0,67,7,4,0,method : Overview,0.28270042194092826,0.1129032258064516,0.25,are not fixed constants ,5,"Task : In the paraphrase generation problem , given an input sequence of words X = [ x 1 , ... , x L ] , we need to generate another output sequence of words Y = [ q 1 , ... , q T ] that has the same meaning as X .",Our training data consists of M pairs of paraphrases,method
natural_language_inference,46,"We observe that the AS Reader tends to copy subsequent tokens from the context , thus behaving like a span prediction model .",experiment,experiment,0,204,17,17,0,experiment : experiment,0.6868686868686869,0.3953488372093023,0.3953488372093023,We observe that the AS Reader tends to copy subsequent tokens from the context thus behaving like a span prediction model ,22,"Both the plain sequence to sequence model and the AS Reader , successfully applied to the CNN / DailyMail reading comprehension task , also perform well on this task .",An additional inductive bias results in higher performance for the span prediction model .,experiment
text-classification,0,They are both 9 layers deep with 6 convolutional layers and 3 fully - connected layers .,model,Model Design,0,64,3,3,0,model : Model Design,0.28193832599118945,0.3333333333333333,0.75,They are both 9 layers deep with 6 convolutional layers and 3 fully connected layers ,16,We designed 2 ConvNets - one large and one small .,gives an illustration .,method
natural_language_inference,41,BART is trained by corrupting documents and then optimizing a reconstruction loss - the cross - entropy between the decoder 's output and the original document .,architecture,Pre-training BART,0,45,7,2,0,architecture : Pre-training BART,0.17509727626459146,0.07865168539325842,0.1,BART is trained by corrupting documents and then optimizing a reconstruction loss the cross entropy between the decoder s output and the original document ,25, ,"Unlike existing denoising autoencoders , which are tailored to specific noising schemes , BART allows us to apply any type of document corruption .",method
named-entity-recognition,2,We apply dropout to the raw inputs x t and to each block 's output b t ( b ) to help prevent overfitting .,training,Training,0,120,16,16,0,training : Training,0.5633802816901409,0.7619047619047619,0.7619047619047619,We apply dropout to the raw inputs x t and to each block s output b t b to help prevent overfitting ,23,"Such an approach has been applied in a variety of contexts for training very deep networks in computer vision , but not to our knowledge in NLP .",The version of dropout typically used in practice has the undesirable property that the randomized predictor used at train time differs from the fixed one used at test time .,experiment
negation_scope_resolution,0,"For both stages , we use Google 's Bidirectional Encoder Representation for Transformers ) ( BERT - base ) with a classification layer on top of it .",methodology,Methodology,0,171,3,3,0,methodology : Methodology,0.7434782608695653,0.15789473684210525,0.15789473684210525,For both stages we use Google s Bidirectional Encoder Representation for Transformers BERT base with a classification layer on top of it ,23,We approach the task in the typical 2 - stage fashion : negation cue detection performed before scope resolution .,"We use huggingface 's PyTorch implementation of BERT , and finetune the bert - base uncased model ( 110 million parameters ) to the training sets .",method
natural_language_inference,62,"We tokenize the MRC dataset with spa Cy 2.0.13 , manipulate WordNet 3.0 with NLTK 3.3 , and implement KAR with TensorFlow 1.11.0 .",experiment,Experimental Settings,1,181,9,8,0,experiment : Experimental Settings,0.8080357142857143,0.2368421052631579,0.5714285714285714,We tokenize the MRC dataset with spa Cy 2 0 13 manipulate WordNet 3 0 with NLTK 3 3 and implement KAR with TensorFlow 1 11 0 ,28,"Specifically , each passage in AddSent contains several sentences thatare similar to the question but not contradictory to the answer , while each passage in AddOneSent contains a human - approved random sentence that maybe unrelated to the passage .","For the data enrichment method , we set the hyper - parameter ? to 3 .",experiment
sentiment_analysis,10,"Across all the occurrences of these emotional - shifts in the testing set , our model correctly predicts 47.5 % instances .",analysis,Error Analysis,0,240,9,9,0,analysis : Error Analysis,0.9338521400778208,0.75,0.75,Across all the occurrences of these emotional shifts in the testing set our model correctly predicts 47 5 instances ,20,"At the dialogue level , we observe that a significant amount of errors occur at turns having a change of emotion from the previous turn of the same party .",This stands less as compared to the 69.2 % success that it achieves at regions of no emotional - shift .,result
sentiment_analysis,9,The loss function for ATE task is,training,Training Details,0,169,11,11,0,training : Training Details,0.6101083032490975,0.7857142857142857,0.7857142857142857,The loss function for ATE task is,7,"The cross - entropy loss is adopted for APC and ATE subtask and the 2 regularization is applied in LCF - ATEPC , here is the loss function for APC task , where is the number of polarity categories , is the 2 regularization parameter , and ? is the parameter - set of the LCF - ATEPC .",where is the number of token classes and is the sum of the tokens in each input sequence .,experiment
text_generation,0,"To get a good and stable performance , we decrease ? by 0.002 for every training epoch .",training,Training Setting,0,198,15,15,0,training : Training Setting,0.6111111111111112,0.9375,0.9375,To get a good and stable performance we decrease by 0 002 for every training epoch ,17,curriculum rate ? is used to control the probability of replacing the true tokens with the generated ones .,"In the PG - BLEU algorithm , we use BLEU , a metric measuring the similarity between a generated sequence and references ( training data ) , to score the finished samples from Monte Carlo search .",experiment
natural_language_inference,8,"Beyond it s role in open domain question answering , answer sentence selection is also a stand - alone task with applications in knowledge base construction and information extraction .",introduction,introduction,0,18,9,9,0,introduction : introduction,0.08530805687203792,0.2571428571428571,0.2571428571428571,Beyond it s role in open domain question answering answer sentence selection is also a stand alone task with applications in knowledge base construction and information extraction ,28,"In this paper , we focus on answer sentence selection , the task that selects the correct sentences answering a factual question from a set of candidate sentences .","The correct sentence may not answer the question directly and perhaps also contain extraneous information , for example :",introduction
natural_language_inference,88,After that we compute the adaptive representation of the source memory tape ? t and hidden tape ? t as :,system description,Deep Attention Fusion Deep fusion combines,0,132,60,9,0,system description : Deep Attention Fusion Deep fusion combines,0.5344129554655871,0.9090909090909092,0.6,After that we compute the adaptive representation of the source memory tape t and hidden tape t as ,19,We compute inter-attention between the input at time step t and tokens in the entire source sequence as follows :,After that we compute the adaptive representation of the source memory tape ? t and hidden tape ? t as :,method
natural_language_inference,1,This Subgraph model is trained similarly as our main approach and only the results of a post -hoc ensemble combination of the two models ( where the scores are added ) are presented .,experiment,Experimental setup,0,237,12,12,0,experiment : Experimental setup,0.8681318681318682,0.9230769230769232,0.9230769230769232,This Subgraph model is trained similarly as our main approach and only the results of a post hoc ensemble combination of the two models where the scores are added are presented ,32,"In that setting , called Subgraph , each object o of a fact is itself represented as a bag - of - entities that encodes the immediate neighborhood of o .",We also report the results obtained by an ensemble of the 5 best models on validation ( subgraph excepted ) ; this is denoted 5 models .,experiment
sentiment_analysis,6,"number of videos , comprising of its constituent utterances , serve as the input .",method,Method,0,61,3,3,0,method : Method,0.2110726643598616,0.045454545454545456,0.6,number of videos comprising of its constituent utterances serve as the input ,13,"In this work , we propose a LSTM network that takes as input the sequence of utterances in a video and extracts contextual unimodal and multimodal features by modeling the dependencies among the input utterances .","We represent the dataset as U = u 1 , u 2 , u 3 ... , u M and each u i = u i , 1 , u i , 2 , ... , u i , Li where Li is the number of utterances in video u i .",method
natural_language_inference,81,Attention maps of more examples can be found in A.3 of the Appendix .,analysis,Figure 6:,0,170,17,9,0,analysis : Figure 6:,0.4038004750593824,0.5483870967741935,1.0,Attention maps of more examples can be found in A 3 of the Appendix ,15,"illustrates an example of this in which the coattention focuses on the relationship between the mentions and the phrase "" located in the administrative territorial entity "" .", ,result
natural_language_inference,16,"First , under the Siamese framework , we implement two baseline models : "" Siamese - CNN "" and "" Siamese - LSTM "" .",experiment,experiment,1,167,6,6,0,experiment : experiment,0.7840375586854459,0.1875,0.1875,First under the Siamese framework we implement two baseline models Siamese CNN and Siamese LSTM ,16,"Therefore , we implemented three types of baseline models .","Both of the two models encode two input sentences into sentence vectors with a neural network encoder , and make a decision based on the cosine similarity between the two sentence vectors .",experiment
natural_language_inference,17,The attentive information can only flow to the subsequent layer through the hidden representation .,architecture,Alignment Architecture for MRC,0,69,26,26,0,architecture : Alignment Architecture for MRC,0.2653846153846154,0.1897810218978102,0.2,The attentive information can only flow to the subsequent layer through the hidden representation ,15,"However , one problem is that each alignment is not directly aware of previous alignments in such architecture .",This can cause two problems :,method
natural_language_inference,91,We did not find scheduled sampling ) - having the model use its own transition decisions sometimes at training time - to help .,system description,Parsing: Predicting transitions,0,94,41,11,0,system description : Parsing: Predicting transitions,0.4034334763948498,1.0,1.0,We did not find scheduled sampling having the model use its own transition decisions sometimes at training time to help ,21,"For SNLI , we use the binary Stanford PCFG Parser parses thatare included with the corpus .", ,method
natural_language_inference,96,"All models substantially underperform humans , although performance increases as more context is provided ( left to right ) .",model,Other models,0,225,34,7,0,model : Other models,0.5769230769230769,0.8717948717948718,0.5833333333333334,All models substantially underperform humans although performance increases as more context is provided left to right ,17,"As our task requires world knowledge , we tried a rule - based system on top of the : Performance of all models in accuracy ( % ) .","We optionally train on found endings only , or found and human - validated generated endings ( found + gen ) .",method
relation-classification,1,"The LSTM architecture consists of a set of recurrently connected subnets , known as memory blocks .",method,The End-to-end Model,0,109,44,13,0,method : The End-to-end Model,0.44308943089430897,0.6285714285714286,0.3333333333333333,The LSTM architecture consists of a set of recurrently connected subnets known as memory blocks ,16,"After word embedding layer , there are two parallel LSTM layers : forward LSTM layer and backward LSTM layer .",Each time - step is a LSTM memory block .,method
sentiment_analysis,38,"For instance , Yelp reviews are of businesses , where details like hospitality , location , and atmosphere are important .",analysis,Capacity Ceiling,0,132,34,11,0,analysis : Capacity Ceiling,0.7857142857142857,0.6415094339622641,0.6470588235294118,For instance Yelp reviews are of businesses where details like hospitality location and atmosphere are important ,17,"Since our model is trained only on Amazon reviews , it is does not appear to be sensitive to concepts specific to other domains .",But these ideas are not present in reviews of products .,result
machine-translation,6,"RCNN contains both recurrent and convolutional layers to catch the key components in texts , and is widely used in text classification tasks .",AWD-LSTM + MoS,Models Description,0,280,13,12,0,AWD-LSTM + MoS : Models Description,0.9621993127147768,0.5416666666666666,1.0,RCNN contains both recurrent and convolutional layers to catch the key components in texts and is widely used in text classification tasks ,23,"Based on the co-occurrence of words , it produces distributed representations of words ( word embeddings ) .", ,others
relation_extraction,11,"Although Bi - GRU is capable of capturing local context , it fails to capture long - range dependencies which can be captured through dependency edges .",system description,Syntactic Sentence Encoding,0,126,59,8,0,system description : Syntactic Sentence Encoding,0.5080645161290323,0.4609375,0.3076923076923077,Although Bi GRU is capable of capturing local context it fails to capture long range dependencies which can be captured through dependency edges ,24,Bi - GRUs have been found to be quite effective in encoding the context of tokens in several tasks .,Prior works have exploited features from syntactic dependency trees for improving relation extraction .,method
natural_language_inference,91,"To do this , we reimplement the feedforward 1 of SPINN - PI - NT and an LSTM RNN baseline in C ++ / CUDA , and compare that implementation with a CPU - based C ++ / Eigen TreeRNN implementation from Irsoy and Cardie , which we modified to perform exactly the same computations as SPINN - PI - NT .",implementation,Inference speed,0,139,45,4,0,implementation : Inference speed,0.5965665236051502,0.8035714285714286,0.26666666666666666,To do this we reimplement the feedforward 1 of SPINN PI NT and an LSTM RNN baseline in C CUDA and compare that implementation with a CPU based C Eigen TreeRNN implementation from Irsoy and Cardie which we modified to perform exactly the same computations as SPINN PI NT ,50,"While the full models evaluated below are implemented and trained using Theano ( Theano Development Team , 2016 ) , which is reasonably efficient but not perfect for our model , we wish to compare well - optimized implementations of all three models .",TreeRNNs like this can only operate on a single example at a time and are thus poorly suited for GPU computation .,experiment
natural_language_inference,40,"In each task the agent is presented with a simple story about entities in operating in an environment , followed by a question about the final state of that environment .",performance,Performance Comparison,0,173,4,4,0,performance : Performance Comparison,0.6245487364620939,0.13333333333333333,0.13333333333333333,In each task the agent is presented with a simple story about entities in operating in an environment followed by a question about the final state of that environment ,30,"QA : Our first benchmark is the bAbi dataset from , a set of 20 toy tasks aimed at measuring the ability of agents to reason about natural language .","Different tasks measure different reasoning abilities on the part of the agent , including chaining facts , counting , deduction , induction and others .",result
semantic_parsing,1,Grey nodes denote primitive identifier fields .,methodology,Modeling ASTs using ASDL Grammar,0,60,35,24,0,methodology : Modeling ASTs using ASDL Grammar,0.4195804195804196,0.8536585365853658,0.8,Grey nodes denote primitive identifier fields ,7,Purple squares denote fields with sequential cardinality .,Fields are labeled with time steps at which they are generated .,method
sentiment_analysis,48,Then the classifier is re-trained with the new labeled data .,result,Main Results,0,191,11,11,0,result : Main Results,0.809322033898305,0.5,0.5,Then the classifier is re trained with the new labeled data ,12,"At each epoch , we select the 1 K samples with the best confidence and give them pseudo labels using the prediction .",The procedure loops until all the unlabeled samples are labeled .,result
natural_language_inference,7,We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.0449438202247191,0.4615384615384616,0.4615384615384616,We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers ,28,"In this paper , we focus on this answer extraction task , presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network .",Our approach improves upon the best published results of Wang & Jiang ( 2016 ) by 5 % and decreases the error of Rajpurkar et al. 's baseline by > 50 %.,abstract
natural_language_inference,56,"Finally we outperform Park which treats the Sudoku as a 9x9 image , uses 10 convolutional layers , iteratively picks the most probable digit , and evaluate on easier Sudokus with 24 - 36 givens .",experiment,Sudoku,1,176,97,32,0,experiment : Sudoku,0.5238095238095238,0.9065420560747663,0.9411764705882352,Finally we outperform Park which treats the Sudoku as a 9x9 image uses 10 convolutional layers iteratively picks the most probable digit and evaluate on easier Sudokus with 24 36 givens ,32,"We tried a few variants including a single step as presented and 32 steps with and without a loss on every step , but could not get it to solve any 17 given Sudokus .",We also tried to train a version of our network that only had a loss at the last step .,experiment
natural_language_inference,47,Our data as a platform for evaluation,system description,Our data as a platform for evaluation,0,110,75,1,0,system description : Our data as a platform for evaluation,0.5116279069767442,0.9493670886075948,0.2,Our data as a platform for evaluation,7, , ,method
text_generation,5,We fix the number of labeled samples to be 500 and report both classification accuracy and NLL of the test set of Yahoo data set in 5 .,result,Semi-supervised VAE results,0,215,31,6,0,result : Semi-supervised VAE results,0.7288135593220338,0.3406593406593407,0.2222222222222222,We fix the number of labeled samples to be 500 and report both classification accuracy and NLL of the test set of Yahoo data set in 5 ,28,"At first , we would like to explore the effect of different decoders for semi-supervised classification .",We can see that SCNN - VAE - Semi has the best classification accuracy of 65.5 .,result
sentiment_analysis,48,"In VAE , the data distribution is modeled by optimizing the evidence lower bound ( ELBO ) of data log-likelihood , which leads to two objectives for labeled data and unlabeled data respectively .",method,Variational Inference,0,82,20,3,0,method : Variational Inference,0.3474576271186441,0.3076923076923077,0.1875,In VAE the data distribution is modeled by optimizing the evidence lower bound ELBO of data log likelihood which leads to two objectives for labeled data and unlabeled data respectively ,31,"Using generative models is a common approach for semi-supervised learning , which tries to extract the information from the unlabeled data by modeling the data distribution .","For the labeled data , VAE maximizes the ELBO of p ( x , y|a ) .",method
natural_language_inference,18,There is only O ( LK 2 ) time increase by introducing an inference network for NASM when compared to LSTM + Att .,t-SNE Visualis ation of Document Representations,Computational Complexity,0,273,22,15,0,t-SNE Visualis ation of Document Representations : Computational Complexity,1.0,1.0,1.0,There is only O LK 2 time increase by introducing an inference network for NASM when compared to LSTM Att ,21,"In addition , the computational complexity of LSTM + Att , the deterministic counterpart of NASM , is also O ( ( L +S ) K 2 + SW ) .", ,others
natural_language_inference,35,The reader module is shared among multiple answer styles and the three task - specific modules .,model,Question-Passages Reader,0,50,11,2,0,model : Question-Passages Reader,0.19011406844106465,0.07857142857142857,1.0,The reader module is shared among multiple answer styles and the three task specific modules ,16, , ,method
natural_language_inference,52,Significance indicated as in the top - scoring justifications as re-ranked by our model .,model,Model,0,222,3,3,0,model : Model,0.8505747126436781,0.1875,0.1875,Significance indicated as in the top scoring justifications as re ranked by our model ,15,"Good@1 Good@5 NDC G@5 IR Baseline 0.52 0.64 0.55 Our Approach 0.61 0.74 0.62 * *: Percentage of questions that have at least one good justification within the top 1 ( Good@1 ) and the top 5 ( Good@5 ) justifications , as well as the normalized discounted cumulative gain at 5 ( NDCG@5 ) of the ranked justifications .","Each of these justifications was composed of a single sentence from our corpus , though a future version could use multi-sentence passages , or aggregate several sentences together , as in .",method
natural_language_inference,5,"This approach applies KL - divergence loss to train the model as follows : score i = model ( Q , A i ) , S = softmax ( [ score 1 , ... , score i ] ) , loss = N n=1 KL ( S n | |y n ) , where i is the number of answer candidates for the given question and N is the total number of samples employed during training .",approach,Pretrained Language Model (LM):,0,99,27,24,0,approach : Pretrained Language Model (LM):,0.6734693877551021,0.7941176470588235,0.7741935483870968,This approach applies KL divergence loss to train the model as follows score i model Q A i S softmax score 1 score i loss N n 1 KL S n y n where i is the number of answer candidates for the given question and N is the total number of samples employed during training ,57,"With a dataset that consists of a question , Q , a related answer set , A = { A 1 , ... , A N } , and a target label , y = {y 1 , ... , y N } , a matching score is computed using equation .","This approach applies KL - divergence loss to train the model as follows : score i = model ( Q , A i ) , S = softmax ( [ score 1 , ... , score i ] ) , loss = N n=1 KL ( S n | |y n ) , where i is the number of answer candidates for the given question and N is the total number of samples employed during training .",method
sentiment_analysis,11,"On the other hand , our model CMN is content - based which enables it to mine intricate patterns from the utterance histories .",system description,Emotional Influence Patterns,0,243,99,19,0,system description : Emotional Influence Patterns,0.7063953488372093,1.0,1.0,On the other hand our model CMN is content based which enables it to mine intricate patterns from the utterance histories ,22,"Needless to say , existence of some false positive patterns at the label level is imminent .", ,method
sentiment_analysis,9,"In this review , the consumers ' opinions on "" dessert "" and "" service "" are not consistent , with positive and negative sentiment polarity respectively .",introduction,introduction,0,19,5,5,0,introduction : introduction,0.06859205776173287,0.13157894736842105,0.13157894736842105,In this review the consumers opinions on dessert and service are not consistent with positive and negative sentiment polarity respectively ,21,"The dessert at this restaurant is delicious but the service is poor , "" the full - designed model for ABSA needs to extract the aspects "" dessert "" and "" service "" and correctly reason about their polarity .","Generally , aspects and their polarity need to be manually labeled before running the aspect polarity classification procedure in the supervised deep learning models .",introduction
natural_language_inference,83,"The dataset also contains an additional set of 3 , 742 four - sentences long stories ( context ) with two ending options , only one of which is correct .",dataset,Dataset,0,165,8,8,1,dataset : Dataset,0.5392156862745098,0.6153846153846154,0.6153846153846154,The dataset also contains an additional set of 3 742 four sentences long stories context with two ending options only one of which is correct ,26,"Also , the workers were asked to limit the length of individual sentences to 70 characters which yielded short and succinct sentences , and to not use informal language or quotations .",Each instance is annotated with this correctness information .,experiment
sentiment_analysis,26,"While in computer vision , weights pre-trained on ImageNet are often used for transfer learning , the most popular way to incorporate external information into deep neural networks for text is to draw on word embeddings trained on vast amounts of word context information .",approach,Sentiment Embedding Computation,0,25,4,3,0,approach : Sentiment Embedding Computation,0.1020408163265306,0.047619047619047616,0.08108108108108109,While in computer vision weights pre trained on ImageNet are often used for transfer learning the most popular way to incorporate external information into deep neural networks for text is to draw on word embeddings trained on vast amounts of word context information ,44,Our goal is to incorporate external cues into a deep neural network such that the network is able to generalize better even when training data is scarce .,"Indeed , the semantic relatedness signals provided by such representations often lead to slightly improved results in polarity classification tasks .",method
relation_extraction,8,The piecewise max pooling procedure can be expressed as follows :,methodology,Piecewise Max Pooling,0,151,67,14,0,methodology : Piecewise Max Pooling,0.5613382899628253,0.6504854368932039,0.7,The piecewise max pooling procedure can be expressed as follows ,11,"As shown in , the output of each convolutional filter c i is divided into three segments {c i 1 , c i 2 , c i3 } by Kojo Annan and Kofi Annan .","For the output of each convolutional filter , we can obtain a 3 - dimensional vector pi = {p i 1 , p i 2 , p i3 }.",method
relation_extraction,9,"Experiments show that our model outperforms previous state - of - the - art methods , including those relying on much richer forms of prior knowledge .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03349282296650718,1.0,1.0,Experiments show that our model outperforms previous state of the art methods including those relying on much richer forms of prior knowledge ,23,"This architecture enables endto - end learning from task - specific labeled data , forgoing the need for external knowledge such as explicit dependency structures .", ,abstract
natural_language_inference,7,The input of both of these components is analyzed qualitatively in Section 6 .,model,MODEL VARIATIONS,0,134,9,9,0,model : MODEL VARIATIONS,0.7528089887640449,0.1956521739130435,1.0,The input of both of these components is analyzed qualitatively in Section 6 ,14,"The passage - independent question representation over the BiLSTM is less important , but it still accounts for over 3 % exact match and F 1 .", ,method
question_answering,0,"We define five models for our experiments , including three baselines and two graph models .",model,Models,0,164,2,2,0,model : Models,0.5559322033898305,0.09523809523809523,0.18181818181818185,We define five models for our experiments including three baselines and two graph models ,15, ,"We use cosine similarity between the question representation and the semantic graph representation as a reward function that judges whether the semantic graph is the correct parse of the question : ? = cos ( v s , v g ) . ) .",method
text_generation,2,"For synthetic data , Leak GAN obtains much lower negative log - likelihood than previous models with sequence length set to 20 and 40 .",introduction,introduction,0,43,31,31,0,introduction : introduction,0.12285714285714285,0.8157894736842105,0.8157894736842105,For synthetic data Leak GAN obtains much lower negative log likelihood than previous models with sequence length set to 20 and 40 ,23,We conduct extensive experiments based on synthetic and real data .,"For real data , we use the text in EMNLP 2017 WMT News , COCO Image Caption and Chinese Poems as the long , mid-length and short text corpus , respectively .",introduction
natural_language_inference,64,"i ) answer sentence selection ( AS2 ) , which , given a question and a set of answer sentence candidates , consists in selecting sentences ( e.g. , retrieved by a search engine ) correctly answering the question ; and ( ii ) machine reading ( MR ) or reading comprehension , which , given a question and a reference text , consists in finding a text span answering it .",introduction,introduction,1,18,5,5,0,introduction : introduction,0.07171314741035857,0.8333333333333334,0.8333333333333334,i answer sentence selection AS2 which given a question and a set of answer sentence candidates consists in selecting sentences e g retrieved by a search engine correctly answering the question and ii machine reading MR or reading comprehension which given a question and a reference text consists in finding a text span answering it ,56,"This has renewed the research interest in Question Answering ( QA ) and , in particular , in two main tasks :","Even though the latter is gaining more and more popularity , AS2 is more relevant to a production scenario since , a combination of a search engine and an AS2 model already implements an initial QA system .",introduction
natural_language_inference,77,"Other error types are mostly related to annotation preferences , e.g. , answer is good but there is a better , more specific one , or ambiguities within the question or context .",analysis,analysis,0,197,5,5,0,analysis : analysis,0.7216117216117216,0.4166666666666667,0.4166666666666667,Other error types are mostly related to annotation preferences e g answer is good but there is a better more specific one or ambiguities within the question or context ,30,We found that most errors are based on alack of either syntactic understanding or a fine - grained semantic distinction between lexemes with similar meanings .,prominent type of mistake is alack of finegrained understanding of certain answer types ( Ex. 1 ) .,result
natural_language_inference,61,dimensions of all hidden states of Bi - aLSTM and word embedding are 300 .,model,Setting,1,124,63,7,0,model : Setting,0.7948717948717948,0.9264705882352942,0.5833333333333334,dimensions of all hidden states of Bi aLSTM and word embedding are 300 ,14,"initial learning rate is set to 0.0005 , and the batch size is 128 .",We employ non-linearity function f = selu replacing recti ed linear unit ReLU on account of its faster convergence rate .,method
semantic_parsing,0,Our dataset and task are publicly available at https://yale-lily. github.io/ spider .,abstract,abstract,1,11,9,9,0,abstract : abstract,0.039568345323741004,1.0,1.0,Our dataset and task are publicly available at https yale lily github io spider ,15,This shows that Spider presents a strong challenge for future research ., ,abstract
machine-translation,6,"In this subsection , we provide the experimental results of all tasks .",experiment,Settings,0,207,51,33,0,experiment : Settings,0.711340206185567,0.9807692307692308,0.9705882352941176,In this subsection we provide the experimental results of all tasks ,12,We list other hyper - parameters related to different task - specific models in the supplementary material ( part A ) .,"For simplicity , we use "" with FRAGE "" as our proposed method in the tables .",experiment
natural_language_inference,52,This demonstrates not only the difficulty of the ques - Type :,analysis,Type:,0,250,15,7,0,analysis : Type:,0.9578544061302682,0.6818181818181818,0.5,This demonstrates not only the difficulty of the ques Type ,11,The second largest source of errors came from questions requiring complex inference as with the question shown in .,If an object traveling to the right is acted upon by an unbalanced force from behind it the object will .,result
text_summarization,4,We decide k empirically ( see Section 5 ) .,model,Pooling submodule,0,145,75,19,0,model : Pooling submodule,0.562015503875969,0.8152173913043478,1.0,We decide k empirically see Section 5 ,8,"Moreover , when k increases towards infinity , firm attention becomes soft attention .", ,method
machine-translation,9,"In machine translation tasks , the loss - free compression rate reaches 94 % ? 99 % . We propose a direct learning approach for the codes in an end - to - end neural network , with a Gumbel - softmax layer to encourage the discreteness .",introduction,introduction,0,64,51,51,0,introduction : introduction,0.2229965156794425,0.9622641509433962,0.9622641509433962,In machine translation tasks the loss free compression rate reaches 94 99 We propose a direct learning approach for the codes in an end to end neural network with a Gumbel softmax layer to encourage the discreteness ,38,"In the experiments , we show that over 98 % of the embedding parameters can be eliminated in sentiment analysis task without affecting performance .",The neural network for learning codes will be packaged into a tool .,introduction
natural_language_inference,9,Visualization of Dialog .,model,VISUALIZATIONS,0,326,39,32,0,model : VISUALIZATIONS,0.987878787878788,0.9069767441860463,0.8888888888888888,Visualization of Dialog ,4,"Note that we only show some of recent sentences here , even the dialog has more sentences .",shows visualization of models for dialog tasks .,method
question-answering,7,Word association or composition graphs produced by NSE memory access .,analysis,Machine Translation,0,236,37,16,0,analysis : Machine Translation,0.8581818181818182,0.578125,0.6666666666666666,Word association or composition graphs produced by NSE memory access ,11,"We merged the dev2010 and dev2012 sets for development and the tst2010 , tst2011 and tst 2012 sets for test data :",The directed arcs connect the words thatare composed via compose module .,result
question_generation,1,"More formally , given an image xi we obtain an embedding g i using a CNN that we parameterize through a function G ( x i , W c ) where W care the weights of the CNN .",ablation,Differential Image Network,0,259,9,6,0,ablation : Differential Image Network,0.6607142857142857,0.06338028169014084,0.6666666666666666,More formally given an image xi we obtain an embedding g i using a CNN that we parameterize through a function G x i W c where W care the weights of the CNN ,35,The aim is to obtain latent weight vectors that bring the supporting exemplar close to the target image and enhances the difference between opposing examples .,This is illustrated in figure 11 . : Full State - of - the - Art comparison on VQG - COCO Dataset .,result
natural_language_inference,83,"We can see that the features extracted from the aspect analyzing the event - sequence have the strongest predictive power , followed by those characterizing Sentiment - trajectory .",ablation,Ablation Study,1,198,4,4,0,ablation : Ablation Study,0.6470588235294118,0.19047619047619047,0.19047619047619047,We can see that the features extracted from the aspect analyzing the event sequence have the strongest predictive power followed by those characterizing Sentiment trajectory ,26,shows the performance of a logistic regression model trained using all the features ( All ) and then using individual feature - groups .,The features measuring top - ical consistency result in lowest accuracy but they still perform better than random on the task .,result
sentiment_analysis,43,"Existing approaches train each aspect with its context separately , without considering the relationship among the aspects .",training,Aspect Alignment Loss,0,145,3,2,0,training : Aspect Alignment Loss,0.6041666666666666,0.15789473684210525,0.1111111111111111,Existing approaches train each aspect with its context separately without considering the relationship among the aspects ,17, ,"However , we observe the aspect - level interactions can bring extra valuable information .",experiment
sentiment_analysis,3,where the response utterance representation,model,Context-Response Cross-Attention,0,175,110,4,0,model : Context-Response Cross-Attention,0.5993150684931506,0.9016393442622952,0.25,where the response utterance representation,5,"Finally , a context - aware concept - enriched response representation R i ? R md for conversation X i is learned by cross-attention , which selectively attends to the concept - enriched context representation as follows :",i j ? R md is obtained via the MH layer :,method
sentiment_analysis,32,"where T is the number of correctly predicted samples , N is the total number of samples .",evaluation,Evaluation Metric,0,121,4,4,0,evaluation : Evaluation Metric,0.5260869565217391,0.6666666666666666,0.6666666666666666,where T is the number of correctly predicted samples N is the total number of samples ,17,"we adopt the Accuracy metric , which is defined as :",Accuracy measures the percentage of correct predicted samples in all samples .,result
natural_language_inference,50,"Overall , we summarize the key findings of our experiments .",analysis,Overall analysis.,0,252,29,2,0,analysis : Overall analysis.,0.7949526813880127,0.3670886075949367,0.125,Overall we summarize the key findings of our experiments ,10, ,"It is possible to achieve very competitive performance with small parameterization , and no word matching or interaction layers .",result
text_generation,4,Skip - Thought sentence embeddings can be used to generate images with GANs conditioned on text vectors for text - to - image conversion tasks like those achieved in .,training,Further Work,0,108,34,4,0,training : Further Work,0.9908256880733946,0.9714285714285714,0.8,Skip Thought sentence embeddings can be used to generate images with GANs conditioned on text vectors for text to image conversion tasks like those achieved in ,27,Simple CFG 4 and more complex ones like Penn Treebank CFG generate samples which are used as input to GAN and the model is evaluated by computing the diversity and accuracy of generated samples conforming to the given CFG .,These embeddings have also been used to Models like neuralstoryteller 5 which use these sentence embeddings can be experimented with generative adversarial networks to generate unique samples .,experiment
temporal_information_extraction,0,The set of labels for the TLINK classifiers ( Section 4.2.3 ) is also adjusted accordingly following the labels in the TimeBank - Dense training data .,dataset,dataset,0,145,16,16,0,dataset : dataset,0.7671957671957672,1.0,1.0,The set of labels for the TLINK classifiers Section 4 2 3 is also adjusted accordingly following the labels in the TimeBank Dense training data ,26,"Since the set of TLINK types used in the TimeBank - Dense corpus is slightly different from the one used in TempEval - 3 , 12 we map the relation types of TLINKs labelled by the rule - based sieve of CATENA ( Section 4.2.1 ) as follows : ( i ) BEGINS , ENDED BY ? BEFORE , ( ii ) BEGUN BY , ENDS ? AFTER , and ( iii ) DURING , IDENTITY ? SIMULTANEOUS .", ,experiment
sentiment_analysis,40,"At each time step t , the forward LSTM not only outputs the hidden state ? ? h l tat it s layer l ( ? ? h 0 t = v t ) but also maintains a memory ? ? cl t inside it s hidden cell .",model,BLSTM for Memory Building,0,82,17,5,0,model : BLSTM for Memory Building,0.36771300448430494,0.27419354838709675,0.3125,At each time step t the forward LSTM not only outputs the hidden state h l tat it s layer l h 0 t v t but also maintains a memory cl t inside it s hidden cell ,39,"In this paper , we use Deep Bidirectional LSTM ( DBLSTM ) to build the memory which records all information to be read in the subsequent modules .","At each time step t , the forward LSTM not only outputs the hidden state ? ? h l tat it s layer l ( ? ? h 0 t = v t ) but also maintains a memory ? ? cl t inside it s hidden cell .",method
paraphrase_generation,0,Sentiment Analysis with Stanford Sentiment Treebank ( SST ) Dataset,model,Sentiment Analysis with Stanford Sentiment Treebank (SST) Dataset,0,178,10,1,0,model : Sentiment Analysis with Stanford Sentiment Treebank (SST) Dataset,0.7510548523206751,1.0,1.0,Sentiment Analysis with Stanford Sentiment Treebank SST Dataset,8, , ,method
relation_extraction,1,"Recently , the NLP community has seen excitement around neural models that make heavy use of pretraining based on language modeling .",introduction,introduction,0,20,12,12,0,introduction : introduction,0.2247191011235955,0.7058823529411765,0.7058823529411765,Recently the NLP community has seen excitement around neural models that make heavy use of pretraining based on language modeling ,21,"Although syntactic features are no doubt helpful , a known challenge is that parsers are not available for every language , and even when available , they may not be sufficiently robust , especially for out - of - domain text , which may even hurt performance .","The latest development is BERT , which has shown impressive gains in a wide variety of natural language tasks ranging from sentence classification to sequence labeling .",introduction
sentiment_analysis,28,"To deal with the label unreliability issue , we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels .",introduction,introduction,1,28,18,18,0,introduction : introduction,0.15555555555555556,0.6206896551724138,0.6206896551724138,To deal with the label unreliability issue we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels ,25,"Specifically , our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words .",We also apply pre-trained BERT to this task and show our model enhances the performance of basic BERT model .,introduction
sentiment_analysis,0,"In this paper , we propose a novel deep dual recurrent encoder model that simultaneously utilizes audio and text data in recognizing emotions from speech .",introduction,introduction,1,27,18,18,0,introduction : introduction,0.15168539325842698,0.5625,0.5625,In this paper we propose a novel deep dual recurrent encoder model that simultaneously utilizes audio and text data in recognizing emotions from speech ,25,"Thus , we hypothesize that the speech emotion recognition model will be benefit from the incorporation of high - level textual input .",Extensive experiments are conducted to investigate the efficacy and properties of the proposed model .,introduction
natural_language_inference,85,The model marked with is LSTMs enforcing the so called word - by - word attention .,result,Results,0,187,13,13,0,result : Results,0.7957446808510639,0.5652173913043478,0.5652173913043478,The model marked with is LSTMs enforcing the so called word by word attention ,15,"8 ) - ( 15 ) , are inter-sentence attention - based model .",The model of extends this idea to explicitly enforce word - by - word matching between the hypothesis and the premise .,result
natural_language_inference,31,An embedding layer first embeds discrete tokens .,introduction,introduction,1,28,21,21,0,introduction : introduction,0.10144927536231883,0.7241379310344828,0.7241379310344828,An embedding layer first embeds discrete tokens ,8,The general architecture of RE2 is illustrated in .,"Several same - structured blocks consisting of encoding , alignment and fusion layers then process the sequences consecutively .",introduction
natural_language_inference,73,Both variants entirely discard the information of the unselected tokens and hence are more time - efficient .,model,Applications of the Proposed Models,0,141,59,5,0,model : Applications of the Proposed Models,0.5381679389312977,0.9833333333333332,0.8333333333333334,Both variants entirely discard the information of the unselected tokens and hence are more time efficient ,17,"Further , we propose two simplified variants of ReSAN with a simpler structure or fewer parameters , i.e. , 1 ) ReSAN w/ o unselected heads which only applies the soft self - attention to the selected head and dependent tokens , and 2 ) Re SAN w/o dependency restricted which use only one RSS to select tokens for both heads and dependents .","However , neither can be used for context fusion , because the input and output sequences are not equal in length .",method
text-to-speech_synthesis,0,"It can be seen that our method on 6 - layer encoder and 6 - layer decoder Transformer achieves the new state - of - the - art result of 19.88 % WER , outperforming NSGD by 4.22 % WER .",result,Achieving State-Of-The-Art Accuracy,1,136,7,6,0,result : Achieving State-Of-The-Art Accuracy,0.8395061728395061,0.5,0.4615384615384616,It can be seen that our method on 6 layer encoder and 6 layer decoder Transformer achieves the new state of the art result of 19 88 WER outperforming NSGD by 4 22 WER ,35,The convolutional sequence to sequence model with non-sequential greedy decoding ( NSGD ) is the previous state - of - the - art on CMUDict 0.7 b dataset .,Reducing Model Size by 6 x,result
natural_language_inference,16,"In this work , we propose a bilateral multi-perspective matching ( BiMPM ) model .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.028169014084507043,0.3636363636363637,0.3636363636363637,In this work we propose a bilateral multi perspective matching BiMPM model ,13,Previous approaches either match sentences from a single direction or only apply single granular ( wordby - word or sentence - by- sentence ) matching .,"Given two sentences P and Q , our model first encodes them with a BiL - STM encoder .",abstract
sentiment_analysis,20,"We collected a big dataset of 330M English Twitter messages , gathered from 12/2012 to 07/2016 , which is used ( 1 ) for calculating words statistics needed by our text processor and ( 2 ) for training our word embeddings .",system description,Overview,0,33,8,8,0,system description : Overview,0.1774193548387097,0.1568627450980392,0.6666666666666666,We collected a big dataset of 330M English Twitter messages gathered from 12 2012 to 07 2016 which is used 1 for calculating words statistics needed by our text processor and 2 for training our word embeddings ,38,"In Subtasks D , E ( quantification ) we are given a set of messages about a set of topics and must estimate the distribution of the tweets across 2 - point scale ( Subtask D ) and a 5 - point scale ( Subtask E ) .",Pre-trained Word Embeddings .,method
natural_language_inference,94,"This type of multi-step reasoning also often requires understanding implicit relations , which humans resolve via external , background commonsense knowledge .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.015665796344647518,0.2,0.2,This type of multi step reasoning also often requires understanding implicit relations which humans resolve via external background commonsense knowledge ,21,"We instead focus on a more challenging multihop generative task ( NarrativeQA ) , which requires the model to reason , gather , and synthesize disjoint pieces of information within the context to generate an answer .",We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer - generator decoder to synthesize the answer .,abstract
natural_language_inference,30,"While this is often fine , this is also very limited and caused many incoherences in the data .",training,Questions Generation,0,103,29,18,0,training : Questions Generation,0.3992248062015504,0.6444444444444445,0.5806451612903226,While this is often fine this is also very limited and caused many incoherences in the data ,18,"their suffixes , i.e. the string representing winston - churchill.e is simply winston churchill .","Generating questions with a richer KB than ReVerb , such as Freebase or DBpedia , would lead to better quality because typing and better lexicons could be used .",experiment
sentiment_analysis,42,It is somewhat dis appointing that incorporating attention model over TDLSTM does not bring any improvement .,method,Comparison to Other Methods,0,174,19,19,0,method : Comparison to Other Methods,0.6904761904761905,0.7307692307692307,0.7307692307692307,It is somewhat dis appointing that incorporating attention model over TDLSTM does not bring any improvement ,17,"This is reason - able as the sentiment polarity of a sentence towards different aspects ( e.g. "" food "" and "" service "" ) might be different .",We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position .,method
sentiment_analysis,27,Experiments show that our model outperforms the state - of - the - art methods .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.036231884057971016,0.8888888888888888,0.8888888888888888,Experiments show that our model outperforms the state of the art methods ,13,We evaluate the proposed approach on the SemEval 2014 datasets .,"We also conduct experiments to evaluate the effectiveness of GCN module , which indicates that the dependencies between different aspects is highly helpful in aspect - level sentiment classification .",abstract
sentiment_analysis,47,"However , there are two important issues that still remain to be further studied , i.e. , 1 ) how to efficiently represent the target especially when the target contains multiple words ; 2 ) how to utilize the interaction between target and left / right contexts to capture the most important words in them .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.02325581395348837,0.25,0.25,However there are two important issues that still remain to be further studied i e 1 how to efficiently represent the target especially when the target contains multiple words 2 how to utilize the interaction between target and left right contexts to capture the most important words in them ,50,Deep learning techniques have achieved success in aspect - based sentiment analysis in recent years .,"In this paper , we propose an approach , called left - centerright separated neural network with rotatory attention ( LCR - Rot ) , to better address the two problems .",abstract
sentiment_analysis,11,The above equations can be summarized as :,system description,Single Layer,0,172,28,14,0,system description : Single Layer,0.5,0.2828282828282828,0.3333333333333333,The above equations can be summarized as ,8,"Here , V , W and bare parameter matrices and vector and ? represents element - wise multiplication .","For each ? ? { a , b} , a memory representation M ? = [ m 1 ? , ... , m K ? ] for hist ? is generated using a GRU .",method
natural_language_inference,27,The speed - up gain allows us to train the model with much more data .,abstract,abstract,0,10,7,7,0,abstract : abstract,0.02958579881656805,0.7777777777777778,0.7777777777777778,The speed up gain allows us to train the model with much more data ,15,"On the SQuAD dataset , our model is 3x to 13x faster in training and 4x to 9 x faster in inference , while achieving equivalent accuracy to recurrent models .",We hence combine our model with data generated by backtranslation from a neural machine translation model .,abstract
natural_language_inference,48,It s workflow has three steps .,system description,Alternating Iterative Attention,0,46,17,3,0,system description : Alternating Iterative Attention,0.2081447963800905,0.2463768115942029,0.375,It s workflow has three steps ,7,Our model is represented in .,"First is the encoding phase , in which we compute a set of vector representations , acting as a memory of the content of the input document and query .",method
sentiment_analysis,3,"For each g ( t ) , we remove concepts thatare stopwords or not in our vocabulary .",model,Knowledge Retrieval,0,94,29,26,0,model : Knowledge Retrieval,0.3219178082191781,0.2377049180327869,0.8125,For each g t we remove concepts thatare stopwords or not in our vocabulary ,15,"In general , for each non-stopword token tin X i j , we retrieve a connected knowledge graph g (t ) comprising its immediate neighbors from Con -cept Net .",We further remove concepts with confidence scores less than 1 to reduce annotation noises .,method
natural_language_inference,54,"Similar to the syntactic encoding of C - Tree , we take the last hidden states as its embedding .",system description,Structural Embedding of Dependency Trees (SEDT),0,93,38,10,0,system description : Structural Embedding of Dependency Trees (SEDT),0.4061135371179039,0.6666666666666666,0.9090909090909092,Similar to the syntactic encoding of C Tree we take the last hidden states as its embedding ,18,"In such as a sub-tree , since children are directly linked to the root , they are position according to the original sequence in the sentence .","Similar to SECT , we use a window of size l to limit the amount of syntactic information for the learning models by choosing only the l-nearest dependents , which is again reported in Section 4.5 .",method
natural_language_inference,95,We first evaluate the answer verification by ablating the cross - passage verification model so that the verification loss and verification score will not be used during training and testing .,ablation,Ablation Study,0,181,4,4,0,ablation : Ablation Study,0.7735042735042735,0.125,0.3076923076923077,We first evaluate the answer verification by ablating the cross passage verification model so that the verification loss and verification score will not be used during training and testing ,30,"Following , we mainly focus on the ROUGE - L score that is averaged case by case .",Then we remove the content model in order to test the necessity of modeling the content of the answer .,result
natural_language_inference,95,"While the answer given is in essence true , societies originally form for the express purpose to enhance . . . . . .",introduction,introduction,0,29,21,21,0,introduction : introduction,0.12393162393162395,0.3620689655172414,0.3620689655172414,While the answer given is in essence true societies originally form for the express purpose to enhance ,18,1 ] A culture is a society 's total way of living and a society is a group that live in a defined territory and participate in common culture .,There has been resurgence in the economic system known as capitalism during the past two decades .,introduction
question_answering,4,depicts a high - level overview of our proposed architecture .,system description,Densely Connected Attention Propagation (DECAPROP),0,88,45,6,0,system description : Densely Connected Attention Propagation (DECAPROP),0.3424124513618677,0.4326923076923077,1.0,depicts a high level overview of our proposed architecture ,10,"In this section , we describe our proposed model in detail .", ,method
natural_language_inference,24,shows the development set scores of SAN trained from initialization with different random seeds .,result,SAN:,0,164,32,22,0,result : SAN:,0.7008547008547008,0.4,0.3142857142857143,shows the development set scores of SAN trained from initialization with different random seeds ,15,We are interested in whether the proposed model is sensitive to different random initial conditions .,We observe that the SAN results are consistently strong regardless of the 10 different initializations .,result
natural_language_inference,47,"Third , a subset of the resulting sentences were sent to a validation task aimed at providing a highly reliable set of annotations over the same data , and at identifying are as of inferential uncertainty .",system description,new corpus for NLI,0,57,22,22,0,system description : new corpus for NLI,0.2651162790697674,0.27848101265822783,1.0,Third a subset of the resulting sentences were sent to a validation task aimed at providing a highly reliable set of annotations over the same data and at identifying are as of inferential uncertainty ,35,"Second , the prompt gave participants the freedom to produce entirely novel sentences within the task setting , which led to richer examples than we see with the more proscribed string - editing techniques of earlier approaches , without sacrificing consistency .", ,method
sentence_classification,0,"Our model achieves a new state - of the - art on an existing ACL anthology dataset ( ACL - ARC ) with a 13.3 % absolute increase in F1 score , without relying on external linguistic resources or hand - engineered features as done in existing methods .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.02247191011235955,0.6666666666666666,0.6666666666666666,Our model achieves a new state of the art on an existing ACL anthology dataset ACL ARC with a 13 3 absolute increase in F1 score without relying on external linguistic resources or hand engineered features as done in existing methods ,42,"We propose structural scaffolds , a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents .","In addition , we introduce a new dataset of citation intents ( Sci - Cite ) which is more than five times larger and covers multiple scientific domains compared with existing datasets .",abstract
natural_language_inference,21,"DiSAN surpasses the RNN / CNN based models with more complicated architecture and more parameters by large margins , e.g. , + 2.32 % to Bi - LSTM , + 1.42 % to Bi - LSTM with additive attention .",experiment,Word Embedding with s2t self-attention: DiSAN with,1,216,42,13,0,experiment : Word Embedding with s2t self-attention: DiSAN with,0.7448275862068966,0.8235294117647058,0.5909090909090909,DiSAN surpasses the RNN CNN based models with more complicated architecture and more parameters by large margins e g 2 32 to Bi LSTM 1 42 to Bi LSTM with additive attention ,33,"Compared to the results from the official leaderboard of SNLI in , DiSAN outperforms previous works and improves the best latest test accuracy ( achieved by a memory - based NSE encoder network ) by a remarkable margin of 1.02 % .","It even outperforms models with the assistance of a semantic parsing tree , e.g. , + 3.52 % to Tree - based CNN , + 2.42 % to SPINN - PI .",experiment
natural_language_inference,74,"test their model on both SNLI and MiltiNLI , and achieves competitive results .",result,Results,0,167,3,3,0,result : Results,0.7455357142857143,0.375,0.375,test their model on both SNLI and MiltiNLI and achieves competitive results ,13,"In proposed a simple baseline that uses LSTM to encode the whole sentences and feed them into a MLP classifier to predict the final inference relationship , they achieve an accuracy of 80.6 % on SNLI .","In the medium part , we show the results of other neural network models .",result
text_generation,1,bedroom has silver photograph desk .,experiment,RankGAN (Ours),0,231,67,3,0,experiment : RankGAN (Ours),0.843065693430657,0.6504854368932039,0.2727272727272727,bedroom has silver photograph desk ,6,Three people standing in front of some kind of boats .,The bears standing in front of a palm state park .,experiment
natural_language_inference,75,"However , Key - Value Memory Networks outperform all other methods on all three data source types .",model,WikiMovies,1,170,121,10,0,model : WikiMovies,0.8374384236453202,0.7857142857142857,0.38461538461538464,However Key Value Memory Networks outperform all other methods on all three data source types ,16,"The QA system of outperforms Supervised Embeddings and Memory Networks for KB and IE - based KB representations , but is designed to work with a KB , not with documents ( hence the N / A in that column ) .","Reading from Wikipedia documents directly ( Doc ) outperforms an IE - based KB ( IE ) , which is an encouraging result towards automated machine reading though a gap to a humanannotated KB still remains ( 93.9 vs. 76.2 ) .",method
part-of-speech_tagging,1,"We concatenate two hidden states to form the output of bi-directional LSTM [ ? ? ht , ? ? ht ] for capturing context information from both sides .",architecture,architecture,0,146,67,67,0,architecture : architecture,0.584,0.8375,0.8375,We concatenate two hidden states to form the output of bi directional LSTM ht ht for capturing context information from both sides ,23,"Forward LSTM and backward LSTM compute the representations of ? ? ht and ? ? ht for left and right context of the sentence , respectively .","We concatenate two hidden states to form the output of bi-directional LSTM [ ? ? ht , ? ? ht ] for capturing context information from both sides .",method
natural_language_inference,34,We propose a new metric to evaluate the quality of predicted reasoning chains and constructed entity graphs .,system description,Original Entity Graph,0,62,38,35,0,system description : Original Entity Graph,0.21016949152542366,0.8837209302325582,0.875,We propose a new metric to evaluate the quality of predicted reasoning chains and constructed entity graphs ,18,"Since the ground truth reasoning chain is very hard to define and label for open - domain corpus , we propose a feasible way to weakly supervise the mask learning .",Our contributions are summarized as follows :,method
natural_language_inference,94,"having failed to find such a person , lady coxon tells anvoy that upon her death the money will be left to her , and she must carry on the quest .. "" "" .. anvoy , having lost nearly all her wealth , has only the 13,000 pounds from lady coxon , with amoral but not legal obligation to give it away .. "" "" .. she awards the coxon fund to saltram , who lives off it exactly as he lived off his friends , producing nothing of intellectual value .. """,training,training,0,310,45,45,0,training : training,0.8093994778067886,0.3813559322033898,0.3813559322033898,having failed to find such a person lady coxon tells anvoy that upon her death the money will be left to her and she must carry on the quest anvoy having lost nearly all her wealth has only the 13 000 pounds from lady coxon with amoral but not legal obligation to give it away she awards the coxon fund to saltram who lives off it exactly as he lived off his friends producing nothing of intellectual value ,79,".. having made a promise to her now - deceased husband , lady coxon has for years been seeking to bestow a sum of 13,000 pounds upon a talented intellectual whose potential has been hampered by lack of money .","Answers she must giveaway the 13,000 pounds to an appropriate recipient .",experiment
question-answering,7,LSTM is powerful because it learns to control it s short term memories .,abstract,abstract,0,13,11,11,0,abstract : abstract,0.04727272727272727,0.3793103448275862,0.3793103448275862,LSTM is powerful because it learns to control it s short term memories ,14,"2 ] have achieved a notable success in sequential tasks [ 3 , 4 ] .","However , the short term memories in LSTM are a part of the training parameters .",abstract
paraphrase_generation,1,"Since we can not use the ground truth to find the best value , we instead use the metric between the input sentence and the variant to get the best variant , and then report the metric between the best variant and the ground truth .",result,Results,0,186,20,20,0,result : Results,0.8416289592760181,0.4081632653061225,0.4081632653061225,Since we can not use the ground truth to find the best value we instead use the metric between the input sentence and the variant to get the best variant and then report the metric between the best variant and the ground truth ,44,"Note that in this case , we would be using the ground truth to compute the best which is not available at test time .","Those numbers are reported in the Measure column with row best - BLEU / best - METEOR . , we report the results for MSCOCO dataset .",result
machine-translation,8,"In both cases , we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word .",model,MODELS,1,124,8,8,0,model : MODELS,0.37462235649546827,0.5714285714285714,0.5714285714285714,In both cases we use a multilayer network with a single maxout hidden layer to compute the conditional probability of each target word ,24,It s decoder has 1000 hidden units .,We use a minibatch stochastic gradient descent ( SGD ) algorithm together with Adadelta to train each model .,method
natural_language_inference,91,It modifies the shiftreduce formalism by using fixed length vectors to represent each entry in the stack and the buffer .,system description,Composition and representation,0,56,3,3,0,system description : Composition and representation,0.2403433476394849,0.07317073170731707,0.2,It modifies the shiftreduce formalism by using fixed length vectors to represent each entry in the stack and the buffer ,21,"SPINN is based on a shift - reduce parser , but it is designed to produce a vector representation of a sentence as its output , rather than a tree as in standard shift - reduce parsing .","Correspondingly , its reduce operation combines two vector representations from the stack into another vector using a neural network function .",method
natural_language_inference,42,"shows that shorter answers are easier for both models : while they reach more than 75 % F1 for answers that are shorter than four words , for answers longer than ten words these scores drop to 60.4 % and 67.3 % for FABIR and BiDAF , respectively .",evaluation,FABIR and BiDAF Statistics,1,256,31,3,0,evaluation : FABIR and BiDAF Statistics,0.8858131487889274,0.5849056603773585,0.12,shows that shorter answers are easier for both models while they reach more than 75 F1 for answers that are shorter than four words for answers longer than ten words these scores drop to 60 4 and 67 3 for FABIR and BiDAF respectively ,45,In this section we analyze the performance of FABIR and BiDAF in the different types of question in SQuAD .,"Long answers are not only more challenging but are also underrepresented in the dataset , which introduces a bias towards short responses .",result
natural_language_inference,81,"It is the second - longest river in Central and Western Europe ( after the Danube ) , at about , with an average discharge of about .",APPENDIX,ATTENTION MAPS,0,275,67,52,0,APPENDIX : ATTENTION MAPS,0.6532066508313539,0.3145539906103286,0.8666666666666667,It is the second longest river in Central and Western Europe after the Danube at about with an average discharge of about ,23,"The largest city on the river Rhine is Cologne , Germany , with a population of more than 1,050,000 people .","The Beloved Vagabond is a 1936 British musical drama film directed by Curtis Bernhardt and starring Maurice Chevalier , Betty Stockfeld , Margaret Lockwood and Austin Trevor .",others
paraphrase_generation,1,A Deep Generative Framework for Paraphrase Generation,title,title,1,2,1,1,0,title : title,0.00904977375565611,1.0,1.0,A Deep Generative Framework for Paraphrase Generation,7, , ,title
smile_recognition,0,"Nonetheless , deep neural networks are not a magic bullet and successful training is still heavily based on experimentation .",introduction,introduction,0,14,7,7,0,introduction : introduction,0.15555555555555556,0.5384615384615384,0.5384615384615384,Nonetheless deep neural networks are not a magic bullet and successful training is still heavily based on experimentation ,19,This has for example resulted in breakthroughs 3 in signal processing .,The Facial Action Coding System ( FACS ) 1 is a system to taxonomize any facial expression of a human being by their appearance on the face .,introduction
natural_language_inference,20,"To better understand what kind of inferential relationships our model is able to identify , we conducted an error analysis for the three datasets .",evaluation,Error Analysis of NLI Predictions,0,146,64,2,0,evaluation : Error Analysis of NLI Predictions,0.6008230452674898,0.6808510638297872,0.0625,To better understand what kind of inferential relationships our model is able to identify we conducted an error analysis for the three datasets ,24, ,We report the results below .,result
relation_extraction,3,"Compared to standard BERT 's self - attention , a V ij , a K ij ? R dz are extra , which could be viewed as the edge representation between the input element x i and x j .",approach,Entity-Aware Self-Attention based on Relative Distance,0,50,23,7,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.36496350364963503,0.6571428571428571,0.3684210526315789,Compared to standard BERT s self attention a V ij a K ij R dz are extra which could be viewed as the edge representation between the input element x i and x j ,35,"are the parameters of the model , and dz is the dimension of the output from the self - attention layer .","Compared to standard BERT 's self - attention , a V ij , a K ij ? R dz are extra , which could be viewed as the edge representation between the input element x i and x j .",method
named-entity-recognition,6,We observe that mentions often annotated by a given type in our resource tend to cluster around this entity type .,method,Embedding Words and Entity Types,0,74,23,22,0,method : Embedding Words and Entity Types,0.3490566037735849,0.46,0.7333333333333333,We observe that mentions often annotated by a given type in our resource tend to cluster around this entity type ,21,"In addition , words have the color associated with the most frequent type they were annotated within WiFiNE .","For instance , "" firefox "" is close to the type / product / software , while "" enzyme "" is close to the / biology entity type .",method
part-of-speech_tagging,7,"However , note that these results are not strictly comparable as they use the earlier UD v 1.1 version .",result,Results,0,84,14,14,0,result : Results,0.672,0.35,0.5384615384615384,However note that these results are not strictly comparable as they use the earlier UD v 1 1 version ,20,The only system we are aware of that evaluates on UD is ( last column ) .,The over all best system is the multi-task bi - LSTM FREQBIN ( it uses w + c and POLYGLOT initialization for w ) .,result
text_summarization,12,"For our training set , we use a parallel corpus which is constructed from the Annotated English Gigaword dataset as mentioned in .",training,training,0,133,2,2,0,training : training,0.5833333333333334,0.1,0.2857142857142857,For our training set we use a parallel corpus which is constructed from the Annotated English Gigaword dataset as mentioned in ,22, ,The parallel corpus is produced by pairing the first sentence and the headline in the news article with some heuristic rules .,experiment
sentiment_analysis,29,The L 2 regularization coefficient is set to 10 ? 4 and the dropout keep rate is set to 0.2 .,hyperparameters,Hyperparameters Setting,1,122,4,4,0,hyperparameters : Hyperparameters Setting,0.7011494252873564,0.4,0.4,The L 2 regularization coefficient is set to 10 4 and the dropout keep rate is set to 0 2 ,21,"All weight matrices are randomly initialized from uniform distribution U ( ?10 ?4 , 10 ?4 ) and all bias terms are set to zero .",The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,experiment
text_summarization,8,"Neural network - based methods for abstractive summarization produce outputs that are more fluent than other techniques , but perform poorly at content selection .",abstract,abstract,0,4,2,2,0,abstract : abstract,0.013986013986013986,0.2857142857142857,0.2857142857142857,Neural network based methods for abstractive summarization produce outputs that are more fluent than other techniques but perform poorly at content selection ,23, ,This work proposes a simple technique for addressing this issue : use a data- efficient content selector to over - determine phrases in a source document that should be part of the summary .,abstract
sentiment_analysis,25,Convolutional layers with multiple filters can efficiently extract n-gram features at many granularities on each receptive field .,introduction,introduction,0,35,23,23,0,introduction : introduction,0.15765765765765766,0.7666666666666667,0.7666666666666667,Convolutional layers with multiple filters can efficiently extract n gram features at many granularities on each receptive field ,19,"For ACSA task , our model has two separate convolutional layers on the top of the embedding layer , whose outputs are combined by novel gating units .","The proposed gating units have two nonlinear gates , each of which is connected to one convolutional layer .",introduction
natural_language_inference,97,"Clearly , MCTest - 160 is easier .",training,Training and Model Details,0,240,26,26,0,training : Training and Model Details,0.821917808219178,1.0,1.0,Clearly MCTest 160 is easier ,6,"Accuracy scores are divided among questions whose evidence lies in a single sentence ( single ) and across multiple sentences ( multi ) , and among the two variants .", ,experiment
semantic_role_labeling,1,The basis for our model is the Transformer encoder introduced by : we transform word embeddings into contextually - encoded token representations using stacked multi-head self - attention and feedforward layers ( 2.1 ) .,model,Model,0,39,6,6,0,model : Model,0.17889908256880735,0.06818181818181818,0.3,The basis for our model is the Transformer encoder introduced by we transform word embeddings into contextually encoded token representations using stacked multi head self attention and feedforward layers 2 1 ,32,depicts the over all architecture of our model .,"To incorporate syntax , one self - attention head is trained to attend to each token 's syntactic parent , allowing the model to use this attention head as an oracle for syntactic dependencies .",method
sentence_compression,2,"Also , CASCADED - LSTM is consistently better than MULTITASK - LSTM . : Results ( F1 ) .",result,Results and discussion,1,94,4,4,0,result : Results and discussion,0.9306930693069309,0.3636363636363637,0.3636363636363637,Also CASCADED LSTM is consistently better than MULTITASK LSTM Results F1 ,12,"We observe that across all three datasets , including all three annotations of BROADCAST , gaze features lead to improvements over our baseline 3 - layer bi - LSTM .","For all three datasets , the inclusion of gaze measures ( first pass duration ( FP ) and regression duration ( Regr. ) ) leads to improvements over the baseline .",result
natural_language_inference,46,"The data is human generated , and the answers can be phrases or sentences .",system description,FRANK (to the baby),0,58,42,36,0,system description : FRANK (to the baby),0.19528619528619529,0.5,0.4615384615384616,The data is human generated and the answers can be phrases or sentences ,14,"While this could be used as a QA task , the MCTest corpus is in fact intended as an answer selection corpus .","The main limitation of this dataset is that it serves more as a an evaluation challenge than as the basis for end - to - end training of models , due to its relatively small size .",method
sentiment_analysis,2,"Therefore , according to these three experimental results , we can prove the importance of the position information in aspect - level sentiment analysis task .",dataset,Dataset,0,175,5,5,0,dataset : Dataset,0.7709251101321586,0.4166666666666667,0.4166666666666667,Therefore according to these three experimental results we can prove the importance of the position information in aspect level sentiment analysis task ,23,"Compared with ATAE - Bi - GRU , the most difference is that PAN utilizes the position embedding to calculate the attention weights rather than the aspect term embedding like ATAE - Bi - GRU .","As for BAN model , it outperforms IAN model while performs worse than PBAN model .",experiment
sentiment_analysis,17,The input word at each node depends on the tree structure used for the network .,system description,Tree-Structured LSTMs,0,86,47,15,0,system description : Tree-Structured LSTMs,0.38222222222222224,0.6025641025641025,0.9375,The input word at each node depends on the tree structure used for the network ,16,"In our applications , each x j is a vector representation of a word in a sentence .","For instance , in a Tree - LSTM over a dependency tree , each node in the tree takes the vector corresponding to the headword as input , whereas in a Tree - LSTM over a constituency tree , the leaf nodes take the corresponding word vectors as input .",method
sentiment_analysis,9,"AEN - is an attentional encoder network based on the pretrained BERT model , which aims to solve the aspect polarity classification .",method,Compared Methods,1,201,13,13,0,method : Compared Methods,0.7256317689530686,0.52,0.52,AEN is an attentional encoder network based on the pretrained BERT model which aims to solve the aspect polarity classification ,21,GANN obtained the state - of - the - art APC performance on the Chinese review datasets .,"BERT - is a BERT - adapted model for Review Reading Comprehension ( RRC ) task , a task inspired by machine reading comprehension ( MRC ) , it could be adapted to aspect - level sentiment classification task .",method
sentiment_analysis,6,The higher performance improvement on the IEMO - CAP dataset indicates the necessity of modeling long - range dependencies among the utterances as continuous emotion recognition is a multiclass sequential problem where a person does not frequently change emotions .,baseline,baseline,0,250,4,4,0,baseline : baseline,0.8650519031141869,0.2,0.2,The higher performance improvement on the IEMO CAP dataset indicates the necessity of modeling long range dependencies among the utterances as continuous emotion recognition is a multiclass sequential problem where a person does not frequently change emotions ,38,These results prove our initial hypothesis that modeling the contextual dependencies among utterances ( which uni - SVM can not do ) improves the classification .,We have implemented and compared with the current state - of - the - art approach proposed by .,result
text_generation,1,"Following the evaluation protocol in , we first carry out experiments on the data and simulator proposed in .",experiment,Experimental results,0,166,2,2,0,experiment : Experimental results,0.6058394160583942,0.019417475728155338,0.6666666666666666,Following the evaluation protocol in we first carry out experiments on the data and simulator proposed in ,18, ,"Then , we compare the performance of RankGAN with other state - of - the - art methods on multiple public language datasets including Chinese poems , COCO captions , and Shakespear 's plays .",experiment
sentiment_analysis,11,Cases represent scenarios when the emotion of current utterance is influenced by self or the other person respectively .,system description,Emotional Influence Patterns,0,232,88,8,0,system description : Emotional Influence Patterns,0.6744186046511628,0.8888888888888888,0.42105263157894735,Cases represent scenarios when the emotion of current utterance is influenced by self or the other person respectively ,19,"For all utterances in the dataset , we sample their histories by setting K = 5 , i.e. , five previous utterances ( as per availability ) from both speakers .","In case 3 , the utterance has relevant content in the histories that do not precede immediately .",method
question_answering,3,"In our experiments , we focus on both multiple - choice based ( RACE ) and span prediction RC tasks ( SearchQA , Narrative QA ) .",model,Overall Model Architectures,0,105,3,3,0,model : Overall Model Architectures,0.375,0.05555555555555555,0.5,In our experiments we focus on both multiple choice based RACE and span prediction RC tasks SearchQA Narrative QA ,20,This section describes the over all model architectures that utilize DCU encoders .,"Since the core focus of this paper is our encoder , we briefly provide the high - level details 4 of our vanilla Bi - Attentive model .",method
natural_language_inference,81,We evaluate the CFC on two tasks to evaluate its effectiveness .,experiment,EXPERIMENTS,0,106,2,2,0,experiment : EXPERIMENTS,0.2517814726840855,0.06896551724137931,0.3333333333333333,We evaluate the CFC on two tasks to evaluate its effectiveness ,12, ,The first task is multi-evidence question answering on the unmasked and masked version of the WikiHop dataset .,experiment
natural_language_inference,54,"We found that the models have higher performance on the test set than the development set , which coincides with the previous results on the same data set .",performance,Predictive Performance,0,151,13,13,0,performance : Predictive Performance,0.6593886462882096,0.52,1.0,We found that the models have higher performance on the test set than the development set which coincides with the previous results on the same data set ,28,We compared these models on both the development set and official test set and reported the results in ., ,result
relation-classification,0,"We followed the official task setting , and report the official macro -averaged F1 - score ( Macro - F1 ) on the 9 relation types .",result,Data and Task Settings,0,151,18,17,0,result : Data and Task Settings,0.668141592920354,0.9473684210526316,0.9444444444444444,We followed the official task setting and report the official macro averaged F1 score Macro F1 on the 9 relation types ,22,We randomly selected 800 sentences from the training set as our development set .,"For more details of the data and task settings , please refer to the supplementary material .",result
relation_extraction,13,Fixed length relation representation,model,Fixed length relation representation,0,88,23,1,0,model : Fixed length relation representation,0.4131455399061033,0.4339622641509434,0.14285714285714285,Fixed length relation representation,4, , ,method
sentiment_analysis,11,Gated Recurrent Unit : GRUs are a gating mechanism in recurrent neural networks introduced by .,system description,Single Layer,0,163,19,5,0,system description : Single Layer,0.4738372093023256,0.1919191919191919,0.11904761904761905,Gated Recurrent Unit GRUs are a gating mechanism in recurrent neural networks introduced by ,15,"First , we define the GRU cell .","Similar to an LSTM ( Hochreiter and Schmidhuber , 1997 ) , GRU provides a simpler computation with similar performance .",method
natural_language_inference,36,Our semantics enhanced model takes input of concatenating the token embedding with SRL embeddings .,model,Machine Reading Comprehension Our baseline MRC model is an enhanced version of Bidirectional,0,153,11,5,0,model : Machine Reading Comprehension Our baseline MRC model is an enhanced version of Bidirectional,0.7285714285714285,0.6470588235294118,0.5555555555555556,Our semantics enhanced model takes input of concatenating the token embedding with SRL embeddings ,15,"The token embedding is the concatenation of pre-trained Glo Ve word vectors , a character - level embedding from a convolutional neural network with max - pooling and pre-trained ELMo embeddings .","The embeddings of document and question are passed through a shared bi-directional GRU , followed by a BiDAF attention .",method
relation_extraction,13,"For the few - shot task , we use the dot product between relation representation of the query statement and each of the candidate statements as a similarity score .",model,Entity mention pooling,0,111,46,17,0,model : Entity mention pooling,0.5211267605633803,0.8679245283018868,0.7083333333333334,For the few shot task we use the dot product between relation representation of the query statement and each of the candidate statements as a similarity score ,28,The classification loss is the standard cross entropy of the softmax of hr W T with respect to the true relation type .,"In this case , we also apply across entropy loss of the softmax of similarity scores with respect to the true class .",method
sentiment_analysis,11,Models with hops in the range of 3 ? 10 outperform the single layer variant .,result,Results,0,300,14,14,0,result : Results,0.8720930232558141,0.2641509433962264,0.8235294117647058,Models with hops in the range of 3 10 outperform the single layer variant ,15,The attention - based filtering in each hop provides a refined context representation of the histories .,Models with hops in the range of 3 ? 10 outperform the single layer variant .,result
natural_language_inference,92,"Note that at inference time Z is not given , so top 1 prediction is not necessarily an element of Z. the model first begins by assigning a small , uniform probability distribution to Z , but gradually learns to favor the true solution .",analysis,Analysis,0,198,19,19,0,analysis : Analysis,0.6875,0.5428571428571428,0.5428571428571428,Note that at inference time Z is not given so top 1 prediction is not necessarily an element of Z the model first begins by assigning a small uniform probability distribution to Z but gradually learns to favor the true solution ,42,"For each training step t , the top 1 prediction and Z ordered by P ( z |x ; ? t ) , a probability of z ? Z with respect to the model at t through training procedure are shown .","The model sometimes gives the wrong prediction - for example , at t = 16 k , and changes its prediction from the true solution to the wrong solution , ' 37 - 36 ' - but again changes its prediction to be a true solution afterward .",result
named-entity-recognition,9,We used full abstracts ( PMIDs ) and related questions and answers provided by the BioASQ organizers .,dataset,Datasets,0,110,13,13,0,dataset : Datasets,0.5527638190954773,0.65,0.65,We used full abstracts PMIDs and related questions and answers provided by the BioASQ organizers ,16,"We used the BioASQ factoid datasets , which can be converted into the same format as the SQuAD dataset ) .",We have made the pre-processed BioASQ datasets publicly available .,experiment
question_answering,5,Our re-ranking is based on the entire matching representation .,method,EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0,125,74,45,0,method : EVIDENCE AGGREGATION FOR COVERAGE-BASED RE-RANKER,0.4528985507246377,0.8043478260869565,0.7142857142857143,Our re ranking is based on the entire matching representation ,11,Re-ranking Objective Function,"For each candidate answer a k , k ? [ 1 , K ] , we can get a matching representation h s k between the answer a k , question q and the union passagep k through Eqn. ( 1 - 5 ) .",method
text_generation,1,"Primarily , the GANs have difficulties in dealing with discrete data ( e.g. , text sequences ) .",introduction,introduction,0,23,13,13,0,introduction : introduction,0.08394160583941605,0.4814814814814815,0.4814814814814815,Primarily the GANs have difficulties in dealing with discrete data e g text sequences ,15,"However , GANs have limited progress with natural language processing .","In natural languages processing , the text sequences are evaluated as the discrete tokens whose values are nondifferentiable .",introduction
named-entity-recognition,9,"For computational efficiency , whenever the Wiki Books corpora were used for pre-training , we initialized BioBERT with the pre-trained BERT model provided by .",method,Pre-training BioBERT,0,68,22,8,0,method : Pre-training BioBERT,0.3417085427135678,0.44,0.5714285714285714,For computational efficiency whenever the Wiki Books corpora were used for pre training we initialized BioBERT with the pre trained BERT model provided by ,25,"The text corpora used for pre-training of BioBERT are listed in , and the tested combinations of text corpora are listed in .",We define BioBERT as a language representation model whose pre-training corpora includes biomedical corpora ( e.g. BioBERT ( PubMed ) ) .,method
natural_language_inference,8,"While empirically the inclusion of large amounts of semantic information has been shown to improve performance , such approaches rely on a significant amount of feature engineering and require expensive semantic resources which maybe difficult to obtain , particularly for resource - low languages .",introduction,introduction,0,27,18,18,0,introduction : introduction,0.12796208530805686,0.5142857142857142,0.5142857142857142,While empirically the inclusion of large amounts of semantic information has been shown to improve performance such approaches rely on a significant amount of feature engineering and require expensive semantic resources which maybe difficult to obtain particularly for resource low languages ,42,"Beyond syntactic information , some prior work has also included semantic features from resources such as WordNet , with the previous state - of - the - art model for this task relying on a variety of such lexical semantic resources .","second limitation of such feature - based semantic models is the difficulty of adapting to new domains , requiring separate feature extraction and resource development or identification steps for every domain .",introduction
text_generation,1,"In this paper , we propose a novel generative adversarial network , RankGAN , for generating highquality language descriptions .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.021897810218978103,0.5,0.5,In this paper we propose a novel generative adversarial network RankGAN for generating highquality language descriptions ,17,"However , the existing GANs restrict the discriminator to be a binary classifier , and thus limit their learning capacity for tasks that need to synthesize output with rich structures such as natural language descriptions .","Rather than training the discriminator to learn and assign absolute binary predicate for individual data sample , the proposed RankGAN is able to analyze and rank a collection of human- written and machinewritten sentences by giving a reference group .",abstract
natural_language_inference,92,"Then , where W 1 ? R h4 and W 2 ? R h|S|4 are learnable matrices .",model,Sequence Tagging model for discrete reasoning,0,249,31,8,0,model : Sequence Tagging model for discrete reasoning,0.8645833333333334,0.4428571428571429,0.6153846153846154,Then where W 1 R h4 and W 2 R h S 4 are learnable matrices ,17,"First , BERT encodings of the question and the paragraph is obtained vi ? where : indicates a concatenation , [ SEP ] is a special token , m is the length of Q , n is the length of P , and h is the hidden dimension of BERT .","Then , where W 1 ? R h4 and W 2 ? R h|S|4 are learnable matrices .",method
text_generation,2,"For the synthetic data experiment , the CNN kernel size ranges from 1 to T .",training,Training Settings,1,166,8,8,0,training : Training Settings,0.4742857142857143,0.42105263157894735,0.42105263157894735,For the synthetic data experiment the CNN kernel size ranges from 1 to T ,15,Note that one could design specific structure for different tasks to refine the CNN performance .,The number of each kernel is between 100 and 200 .,experiment
natural_language_inference,99,We set the ? ? 1 as in most cases the latter predicted answer is more accurate comparing to the former one .,model,Answer Selection with Checking Mechanism,0,171,94,31,0,model : Answer Selection with Checking Mechanism,0.6785714285714286,0.9791666666666666,0.9393939393939394,We set the 1 as in most cases the latter predicted answer is more accurate comparing to the former one ,21,where ? ? 1 is a weighted scalar controlling the proportion of the two predicted answers .,We set the ? ? 1 as in most cases the latter predicted answer is more accurate comparing to the former one .,method
question-answering,1,"The pooling then chooses , for each composition type , between two adjacent sliding windows , e.g. , between "" on the "" and "" the mat "" for feature maps group 2 from the rightmost two sliding windows .",model,Length Variability,0,58,32,13,0,model : Length Variability,0.3005181347150259,0.9696969696969696,1.0,The pooling then chooses for each composition type between two adjacent sliding windows e g between on the and the mat for feature maps group 2 from the rightmost two sliding windows ,33,"Different feature maps offer a variety of compositions , with confidence encoded in the values ( color coded in output of convolution layer in ) .", ,method
sentence_compression,1,bit indicating whether the parent word has already been seen but discarded ( 1 dimension ) .,baseline,Baseline,0,111,74,74,0,baseline : Baseline,0.5311004784688995,0.8604651162790697,0.8604651162790697,bit indicating whether the parent word has already been seen but discarded 1 dimension ,15,already been seen and kept in the compression ( 1 dimension ) .,bit indicating whether the parent word comes later in the input ( 1 dimension ) .,result
natural_language_inference,67,"In this section we modified a state - of - the - art RC system for cloze - style tasks for our answer extraction purpose , to see how much gap we have for the two type of tasks , and to inspire our end - to - end system in the next section .",system description,Baseline: Chunk-and-Rank Pipeline with,0,54,14,3,0,system description : Baseline: Chunk-and-Rank Pipeline with,0.2647058823529412,0.14893617021276595,0.2,In this section we modified a state of the art RC system for cloze style tasks for our answer extraction purpose to see how much gap we have for the two type of tasks and to inspire our end to end system in the next section ,47,Baseline : Chunk - and - Rank Pipeline with,"In order to make the cloze - style RC system to make chunk - level decision , we use the RC model to generate features for chunks , which are further used in a feature - based ranker like in ( Rajpurkar et al . ) .",method
sentiment_analysis,9,"The BERT - BASE model is trained on a large - scale general corpus , so the fine - tuning during process during training process is significant and inevitable for BERT - based models .",model,Model,0,225,4,4,0,model : Model,0.8122743682310469,0.3076923076923077,0.3076923076923077,The BERT BASE model is trained on a large scale general corpus so the fine tuning during process during training process is significant and inevitable for BERT based models ,30,"Twitter learning can not achieve as the best effect as single - task learning does , which is also the compromise of the multi-task learning model when dealing with multiple tasks .","Meanwhile , the ABSA datasets commonly benchmarked are generally small with the domain - specific characteristic , the effect of BERT - BASE model on the most ABSA datasets can be further improved through domain - adaption .",method
natural_language_inference,43,"While semantic parsers generally return a set , in COMPLEXQUESTIONS 87 % of the answers are a singleton set .",model,Candidate Ranking,0,93,16,9,0,model : Candidate Ranking,0.6,0.6956521739130435,0.5625,While semantic parsers generally return a set in COMPLEXQUESTIONS 87 of the answers are a singleton set ,18,"At test time , we return the most probable answers based on p ? ( a | q , R ) ( details in Section 4 ) .","candidate span a often has multiple mentions in the result set R. Therefore , our feature function ? ( ) computes the average of the features extracted from each mention .",method
semantic_role_labeling,0,"Recent high - performing SRL models are BIO - taggers , labeling argument spans for a single predicate at a time ( as shown in .",introduction,introduction,1,11,3,3,0,introduction : introduction,0.10476190476190476,0.21428571428571427,0.21428571428571427,Recent high performing SRL models are BIO taggers labeling argument spans for a single predicate at a time as shown in ,22,"Semantic role labeling ( SRL ) captures predicateargument relations , such as "" who did what to whom . ""","They are typically only evaluated with gold predicates , and must be pipelined with error - prone predicate identification models for deployment .",introduction
phrase_grounding,0,The phrase grounding task in the pointing game setting .,introduction,introduction,0,17,7,7,0,introduction : introduction,0.07555555555555556,0.3333333333333333,0.3333333333333333,The phrase grounding task in the pointing game setting ,10,crowd of onlookers on a tractor ride watch a farmer hard at work in the field .,"Given the sentence on top and the image on the left , the goal is to point ( illustrated by the stars here ) to the correct location of each natural language query ( colored text ) .",introduction
sentiment_analysis,2,"PBAN not only concentrates on the position information of aspect terms , but also mutually models the relation between aspect term and sentence by employing bidirectional attention mechanism .",abstract,abstract,0,9,7,7,0,abstract : abstract,0.039647577092511016,0.5,0.5,PBAN not only concentrates on the position information of aspect terms but also mutually models the relation between aspect term and sentence by employing bidirectional attention mechanism ,28,"Therefore , we propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional GRU .",The experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our proposed PBAN model .,abstract
sentiment_analysis,11,Literature consists of numerous fusion techniques for multimodal data .,approach,Fusion:,0,143,48,6,0,approach : Fusion:,0.4156976744186047,0.979591836734694,0.8571428571428571,Literature consists of numerous fusion techniques for multimodal data ,10,This multimodal representation is generated for all utterances in a conversation .,"Exploring these on CMN , however , is beyond the scope of this paper and left as a future work .",method
natural_language_inference,40,"As a result , the stories become longer , and relevant information about entities has to be tracked over longer distances .",performance,Performance Comparison,0,190,21,21,0,performance : Performance Comparison,0.6859205776173285,0.7,0.7,As a result the stories become longer and relevant information about entities has to be tracked over longer distances ,20,"Then we mixed the sentences in the two stories , in random order , and asked a question about one of the stories .","Answering the questions is still trivial for human readers , but .",result
machine-translation,2,"While for small values of d k the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of d k.",model,Scaled Dot-Product Attention,0,76,35,12,0,model : Scaled Dot-Product Attention,0.3392857142857143,0.3211009174311927,0.8,While for small values of d k the two mechanisms perform similarly additive attention outperforms dot product attention without scaling for larger values of d k ,27,"While the two are similar in theoretical complexity , dot-product attention is much faster and more space - efficient in practice , since it can be implemented using highly optimized matrix multiplication code .","We suspect that for large values of d k , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients",method
sentiment_analysis,37,"For example , the sentence "" Coffee is a better deal than overpriced cosi sandwiches "" contains two aspects : "" coffee "" and "" better "" .",result,Case Study,0,211,20,12,0,result : Case Study,0.83399209486166,0.4761904761904762,0.35294117647058826,For example the sentence Coffee is a better deal than overpriced cosi sandwiches contains two aspects coffee and better ,20,We suspect that the presence of multiple aspects in sentence makes IAN network perplexed as to the connection between aspect and the corresponding sentiment - bearing word in the sentence .,"Clearly , the sentiment behind aspect "" coffee "" comes from the word "" better "" and the same for aspect "" cosi sandwiches "" comes from "" overpriced "" .",result
sentiment_analysis,24,The IMN architecture is shown in .,method,Proposed Method,0,57,2,2,0,method : Proposed Method,0.1832797427652733,0.01492537313432836,0.1111111111111111,The IMN architecture is shown in ,7, ,"It accepts a sequence of tokens {x 1 , . . . , x n } as input into a feature extraction component f ? s that is shared among all tasks .",method
semantic_parsing,2,The model 's training objective is to maximize the log likelihood of the generated meaning representations given natural language expressions :,training,Training and Inference,0,111,2,2,0,training : Training and Inference,0.38144329896907214,0.01694915254237288,0.3333333333333333,The model s training objective is to maximize the log likelihood of the generated meaning representations given natural language expressions ,21, ,where D represents training pairs .,experiment
text-to-speech_synthesis,0,"3 ) Our method achieves the state - of - the - art accuracy on CMUdict dataset , outperforming the previous best result by 4.22 % WER .",introduction,introduction,0,33,22,22,0,introduction : introduction,0.2037037037037037,1.0,1.0,3 Our method achieves the state of the art accuracy on CMUdict dataset outperforming the previous best result by 4 22 WER ,23,"We are the first to use unlabeled words to boost the accuracy of grapheme - to - phoneme conversion , and also the first to introduce Transformer into this task and achieve better performance .", ,introduction
natural_language_inference,72,"We obtain the best results with a recurrent neural network ( RNN ) with gated attention , but a simple approach based on embedding similarity proves to be a strong baseline as well .",introduction,introduction,0,45,37,37,0,introduction : introduction,0.14423076923076922,0.7708333333333334,0.7708333333333334,We obtain the best results with a recurrent neural network RNN with gated attention but a simple approach based on embedding similarity proves to be a strong baseline as well ,31,We investigate different ways of representing medical entities in the text and how this affects the neural readers .,"Second , we look at how well humans perform on this task , by asking both a medical expert and a novice to answer a portion of the validation set .",introduction
relation_extraction,10,The span representations are used to perform entity mention detection on all spans in parallel .,introduction,introduction,1,37,29,29,0,introduction : introduction,0.1927083333333333,0.935483870967742,0.935483870967742,The span representations are used to perform entity mention detection on all spans in parallel ,16,We propose a simple bi - LSTM based model which generates span representations for each possible span .,The same span representations are then used to perform relation extraction on all pairs of detected entity mentions .,introduction
text_summarization,6,From the cases we can observe that DRGD can indeed capture some latent structures which are consistent with the golden summaries .,analysis,Summary Case Analysis,0,237,4,4,0,analysis : Summary Case Analysis,0.9045801526717556,0.5,0.5,From the cases we can observe that DRGD can indeed capture some latent structures which are consistent with the golden summaries ,22,"The source texts , golden summaries , and the generated summaries are shown in .","For example , our result for S ( 1 ) "" Wuhan wins men 's soccer title at Chinese city games "" matches the "" Who Action What "" structure .",result
natural_language_inference,6,"Instead , the combined performance seems mostly bounded by the accuracy of the language that performs worst when used alone .",training,Training data,0,218,30,30,0,training : Training data,0.8790322580645161,0.5,0.9375,Instead the combined performance seems mostly bounded by the accuracy of the language that performs worst when used alone ,20,We do not have evidence that distant languages perform considerably worse .,"For instance , Greek - Russian achieves very similar results to Bulgarian - Russian , two Slavic languages .",experiment
machine-translation,3,This suggests that the difficulty on this subset is not much different from that on the full set .,analysis,Unknown words,0,286,15,8,0,analysis : Unknown words,0.9137380191693292,0.5172413793103449,0.7272727272727273,This suggests that the difficulty on this subset is not much different from that on the full set ,19,"On this subset , the SMT model achieves 37.5 , which is similar to its score 37.0 on the full test set .",We therefore attribute the larger gap for Deep - att to the existence of unknown words .,result
semantic_parsing,2,"Sketches were extracted by substituting the original tokens with their token types , except delimiters ( e.g. , "" [ "" , and "" : "" ) , operators ( e.g. , "" + "" , and "" * "" ) , and built - in keywords ( e.g. , "" True "" , and "" while "" ) .",training,Natural Language to Source Code,0,158,49,6,0,training : Natural Language to Source Code,0.5429553264604811,0.4152542372881356,0.3333333333333333,Sketches were extracted by substituting the original tokens with their token types except delimiters e g and operators e g and and built in keywords e g True and while ,31,We used the built - in lexical scanner of Python 1 to tokenize the code and obtain token types .,"For instance , the expression "" if s [: 4 ] . lower ( ) == ' http ' : "" becomes "" if NAME [ : NUMBER ] . NAME ( ) == STRING : "" , with details about names , values , and strings being omitted .",experiment
question-answering,3,"The study is relevant to sockeye , rather than flounder .:",abstract,abstract,0,41,39,39,0,abstract : abstract,0.16141732283464566,0.7647058823529411,0.7647058823529411,The study is relevant to sockeye rather than flounder ,10,"The study is relevant to sockeye , instead of coho .","Examples for sentence similarity learning , where sockeye means "" red salmon "" , and coho means "" silver salmon "" .",abstract
sentiment_analysis,15,They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor - based composition function .,introduction,introduction,1,29,17,17,0,introduction : introduction,0.1074074074074074,0.7727272727272727,0.7727272727272727,They represent a phrase through word vectors and a parse tree and then compute vectors for higher nodes in the tree using the same tensor based composition function ,29,Recursive Neural Tensor Networks take as input phrases of any length .,"We compare to several supervised , compositional models such as standard recursive neural networks ( RNN ) , matrix - vector RNNs , and baselines such as neural networks that ignore word order , Naive Bayes ( NB ) , bi-gram NB and SVM .",introduction
machine-translation,6,"We develop three types of notations : embeddings , task - specific parameters / loss , and discriminator parameters / loss .",method,Our Method,0,116,8,8,0,method : Our Method,0.39862542955326463,0.16666666666666666,0.16666666666666666,We develop three types of notations embeddings task specific parameters loss and discriminator parameters loss ,16,We first define some notations and then introduce our algorithm .,"Denote ? emb ? R d|V | as the word embedding matrix to be learned , where d is the dimension of the embedding vectors and | V | is the vocabulary size .",method
sentiment_analysis,23,"Let t be the window size , and x 1 , , x t ? R ne be n e - dimensional word embeddings .",system description,Convolutional Neural Networks,0,41,5,5,0,system description : Convolutional Neural Networks,0.1404109589041096,0.047619047619047616,0.2777777777777778,Let t be the window size and x 1 x t R ne be n e dimensional word embeddings ,20,"set of fixed - width - window feature detectors slide over the sentence , and output the extracted features .","Let t be the window size , and x 1 , , x t ? R ne be n e - dimensional word embeddings .",method
text-classification,4,"Moreover , our method exhibits higher accuracy than n-grams , which is a very strong baseline as shown in .",experiment,Document Classification,0,180,9,8,0,experiment : Document Classification,0.8144796380090498,0.2571428571428571,0.5333333333333333,Moreover our method exhibits higher accuracy than n grams which is a very strong baseline as shown in ,19,"Although we only use one convolution layer for our ACNN model , it already outperforms other CNN baseline methods with much deeper architectures .",We attribute the superior performance of the ACNN framework to its stronger ( adaptive ) feature - extraction ability .,experiment
sentence_classification,2,"Revisiting the example above , to fix the vector of the English sentence "" The movie is terribly amazing "" , we use the Korean translation to move the vector towards the location where the vector "" The movie is greatly magnificent "" is .",introduction,introduction,0,41,31,31,0,introduction : introduction,0.1626984126984127,0.7380952380952381,0.7380952380952381,Revisiting the example above to fix the vector of the English sentence The movie is terribly amazing we use the Korean translation to move the vector towards the location where the vector The movie is greatly magnificent is ,39,"We posit that a can be used to fix b when a is used as a context of b , and vice versa","Based on these observations , we present a neural attentionbased multiple context fixing attachment ( MCFA ) .",introduction
machine-translation,9,"For the compositional approach with M codebooks and K codewords in each codebook , each code has M log 2 K bits , and the computation is a summation over M vectors .",system description,ADVANTAGE OF COMPOSITIONAL CODES,0,116,20,20,0,system description : ADVANTAGE OF COMPOSITIONAL CODES,0.4041811846689896,0.3333333333333333,0.7407407407407407,For the compositional approach with M codebooks and K codewords in each codebook each code has M log 2 K bits and the computation is a summation over M vectors ,31,"To support N basis vectors , a binary code will have N / 2 bits and the embedding computation is a summation over N / 2 vectors .",comparison of different coding approaches is summarized in .,method
sentiment_analysis,41,We experiment on the dataset of SemEval 2014 Task 4 2 .,dataset,Dataset,0,155,2,2,0,dataset : Dataset,0.695067264573991,0.3333333333333333,0.3333333333333333,We experiment on the dataset of SemEval 2014 Task 4 2 ,12, ,The dataset consists of customers reviews .,experiment
natural_language_inference,45,"However , they use aggregated representations of the query which may lead to loss of information .",introduction,introduction,0,40,31,31,0,introduction : introduction,0.20100502512562807,0.775,0.775,However they use aggregated representations of the query which may lead to loss of information ,16,The Gated - Attention Reader showed improved performance by replacing the inner-product with an element - wise product to allow for better matching at the semantic level .,In this work we use a fine - grained gating mechanism for each token in the paragraph and each token in the query .,introduction
text_summarization,4,Entity Commonsense Representation for Neural Abstractive Summarization,title,title,1,2,1,1,0,title : title,0.007751937984496123,1.0,1.0,Entity Commonsense Representation for Neural Abstractive Summarization,7, , ,title
named-entity-recognition,0,"Thus , we use it for our problem as :",system description,Accelerated Robust Subset Selection (ARSS),0,145,123,47,0,system description : Accelerated Robust Subset Selection (ARSS),0.5350553505535055,0.6910112359550562,0.4607843137254902,Thus we use it for our problem as ,9,This algorithm is easy to implement and able to achieve more accurate solutions than current methods .,is obtained by solving the following equation :,method
sentiment_analysis,5,"Besides , comparing it with the results in ( a ) , which are based on feeding the model with less symmetric electrodes ' data , we can observe that results are comparable or even better than this experiment based on single hemisphere data .",The activity maps of the paired EEG electrodes,The performance based on single hemispheric EEG data,0,252,28,6,0,The activity maps of the paired EEG electrodes : The performance based on single hemispheric EEG data,0.9509433962264152,0.7777777777777778,0.75,Besides comparing it with the results in a which are based on feeding the model with less symmetric electrodes data we can observe that results are comparable or even better than this experiment based on single hemisphere data ,39,"The obtained experimental results are shown in , from which we can see the left hemisphere is superior to the right for EEG emotion recognition , especially in the experiments on SEED - IV and MPED datasets .",This verifies the effectiveness of discrepancy information for EEG emotion recognition from another aspect .,others
question_answering,0,Then we use a simple reward function ? to judge if a semantic graph is a correct semantic parse of the input question .,system description,Representation learning,0,93,44,3,0,system description : Representation learning,0.3152542372881356,0.4888888888888889,0.6,Then we use a simple reward function to judge if a semantic graph is a correct semantic parse of the input question ,23,We follow the state - of - the - art approaches for QA and learn representations for the question and every possible semantic graph .,Then we use a simple reward function ? to judge if a semantic graph is a correct semantic parse of the input question .,method
machine-translation,0,"Recently , however , there has been interest in training neural networks to score the translated sentence ( or phrase pairs ) using a representation of the source sentence as an additional input .",system description,Statistical Machine Translation,0,82,47,12,0,system description : Statistical Machine Translation,0.3744292237442922,0.573170731707317,0.9230769230769232,Recently however there has been interest in training neural networks to score the translated sentence or phrase pairs using a representation of the source sentence as an additional input ,30,"In many cases , neural networks have been used to rescore translation hypotheses ( n- best lists ) ( see , e.g. , ) .","See , e.g. , , and .",method
sentiment_analysis,39,The output state at the index of each location is then fed into a softmax layer to identify the sentiment class for the corresponding aspect .,baseline,Long Short-Term Memory (LSTM),0,184,32,8,0,baseline : Long Short-Term Memory (LSTM),0.7510204081632653,0.9411764705882352,0.8,The output state at the index of each location is then fed into a softmax layer to identify the sentiment class for the corresponding aspect ,26,Bidirectional LSTM outputs a representation for each token in the sentence .,"In this figure , LSTM is trained to identify the sentiment of aspect "" price "" .",result
text-classification,4,"We also consider the task of paraphrase identification with the Quora Question Pairs dataset , with the same data splits as in .",dataset,dataset,0,149,11,11,0,dataset : dataset,0.6742081447963801,0.9166666666666666,0.9166666666666666,We also consider the task of paraphrase identification with the Quora Question Pairs dataset with the same data splits as in ,22,"To facilitate comparison with existing results , we truncate the candidate answers to a maximum length of 40 tokens for all experiments on the WikiQA dataset .",summary of all datasets is presented in .,experiment
natural_language_inference,31,Results on WikiQA dataset are listed in .,result,Results on Answer Selection,1,160,5,2,0,result : Results on Answer Selection,0.5797101449275363,0.17857142857142858,0.2857142857142857,Results on WikiQA dataset are listed in ,8, ,"Note that some of the previous methods round their reported results to three decimal points , but we choose to align with the original paper and round our results to four decimal points .",result
sentiment_analysis,49,"In contrast to the state - of - the - art systems , our proposed model attains an improved accuracy of 82.31 % when we utilize all the three modalities , i.e. text , visual & acoustic .",analysis,Comparative Analysis,0,206,41,8,0,analysis : Comparative Analysis,0.8142292490118577,0.5857142857142857,0.4705882352941176,In contrast to the state of the art systems our proposed model attains an improved accuracy of 82 31 when we utilize all the three modalities i e text visual acoustic ,32,reported accuracies of 75.7 % ( LSTM ( A ) ) & 76.5 % ( GME - LSTM ( A ) ) for two variants of their model .,Our proposed system also obtains better performance as compared to the state - of - the - arts for bi-modal inputs .,result
sentiment_analysis,8,Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight - dimensional features achieves very low accuracy as compared to the end - to - end trained ARE .,result,RESULTS,1,203,7,7,0,result : RESULTS,0.8675213675213675,0.2333333333333333,0.3043478260869565,Performance of LSTM and ARE reveals that deep models indeed need a lot of information to learn features as the LSTM classifier trained on eight dimensional features achieves very low accuracy as compared to the end to end trained ARE ,41,Results are especially interesting for this setting .,"However , neither of them are able to beat the lighter E1 model ( Ensemble of RF , XGB and MLP ) which was trained on the eightdimensional audio feature vectors .",result
relation_extraction,5,"We resolve these issues by normalizing the activations in the graph convolution before feeding it through the nonlinearity , and adding self - loops to each node in the graph :",model,Graph Convolutional Networks over Dependency Trees,0,49,14,12,0,model : Graph Convolutional Networks over Dependency Trees,0.18631178707224336,0.20895522388059692,0.5217391304347826,We resolve these issues by normalizing the activations in the graph convolution before feeding it through the nonlinearity and adding self loops to each node in the graph ,29,"Furthermore , the information in h ( l?1 ) i is never carried over to hi , since nodes never connect to themselves in a dependency tree .","where = A + I with I being then n identity matrix , and d i = n j=1 ij is the degree of token i in the resulting graph . :",method
text_generation,1,denotes the norm operator .,architecture,Rank score,0,95,29,6,0,architecture : Rank score,0.3467153284671533,0.6304347826086957,0.2608695652173913,denotes the norm operator ,5,"where they u and y s are the embedded feature vectors of the reference and the input sequence , respectively .","Then , a softmax - like formula is used to compute the ranking score for a certain sequence s given a comparison set C:",method
natural_language_inference,24,"In summary , each token pi in the passage is represented as a 600 - dimensional vector and each token q j is represented as a 300 - dimensional vector .",system description,Proposed model: SAN,0,54,27,27,0,system description : Proposed model: SAN,0.2307692307692308,0.2967032967032967,0.42857142857142855,In summary each token pi in the passage is represented as a 600 dimensional vector and each token q j is represented as a 300 dimensional vector ,28,"Compared to the exact matching features , these embeddings encode soft alignments between similar but notidentical words .","Due to different dimensions for the passages and questions , in the next layer two different bidirectional LSTM ( BiLSTM ) maybe required to encode the contextual information .",method
natural_language_inference,17,We mainly focus on the SQuAD dataset to train and evaluate our model .,implementation,implementation,0,183,2,2,0,implementation : implementation,0.7038461538461539,0.14285714285714285,0.14285714285714285,We mainly focus on the SQuAD dataset to train and evaluate our model ,14, ,"SQuAD is a machine comprehension dataset , totally containing more than 100 , 000 questions manually annotated by crowdsourcing workers on a set of 536 Wikipedia articles .",experiment
natural_language_inference,34,"The score of a path is acquired by multiplying corresponding soft masks and attention scores along the path , i.e. score ( P ) .",result,Evaluation on Graph Construction and Reasoning Chains,0,243,23,10,0,result : Evaluation on Graph Construction and Reasoning Chains,0.823728813559322,0.30666666666666664,0.5263157894736842,The score of a path is acquired by multiplying corresponding soft masks and attention scores along the path i e score P ,23,"Path A path is a sequence of entities visited by the fusion blocks , denoting as P = [ e p 1 , . . . , e p t +1 ] ( suppose t- layer fusion blocks ) .","Hit Given a path and a supporting sentence , if at least one entity of the supporting sentence is visited by the path , we call this supporting sentence is hit 4 .",result
natural_language_inference,21,"pre-trained token embedding ( e.g. , word2vec or GloVe ) is applied to v and transforms all discrete tokens to a sequence of low - dimensional dense vector representations x = [ x 1 , x 2 , . . . , x n ] with x i ? R de .",system description,Sentence Encoding,0,50,3,3,0,system description : Sentence Encoding,0.1724137931034483,0.023622047244094488,0.3,pre trained token embedding e g word2vec or GloVe is applied to v and transforms all discrete tokens to a sequence of low dimensional dense vector representations x x 1 x 2 x n with x i R de ,40,"In the pipeline of NLP tasks , a sentence is denoted by a sequence of discrete tokens ( e.g. , words or characters ) v = [ v 1 , v 2 , . . . , v n ] , where vi could be a one - hot vector whose dimension length equals the number of distinct tokens N .","pre-trained token embedding ( e.g. , word2vec or GloVe ) is applied to v and transforms all discrete tokens to a sequence of low - dimensional dense vector representations x = [ x 1 , x 2 , . . . , x n ] with x i ? R de .",method
question_answering,4,The FM layer is defined as :,system description,Bidirectional Attention Connectors (BAC),0,79,36,36,0,system description : Bidirectional Attention Connectors (BAC),0.30739299610894943,0.3461538461538461,0.9230769230769232,The FM layer is defined as ,7,"To this end , we adopt factorization machines ( FM ) as G ( . ) .",is a scalar .,method
natural_language_inference,6,Bitext mining is another natural application for multilingual sentence embeddings .,training,BUCC: bitext mining,0,134,65,2,0,training : BUCC: bitext mining,0.5403225806451613,0.7065217391304348,0.13333333333333333,Bitext mining is another natural application for multilingual sentence embeddings ,11, ,"Given two comparable corpora in different languages , the task consists in identifying sentence pairs that are translations of each other .",experiment
relation-classification,9,"We split sentences using Scispa Cy , 2 which is optimized for scientific text .",system description,Corpus,0,39,6,6,0,system description : Corpus,0.2653061224489796,1.0,1.0,We split sentences using Scispa Cy 2 which is optimized for scientific text ,14,"The average paper length is 154 sentences ( 2,769 tokens ) resulting in a corpus size of 3.17B tokens , similar to the 3.3B tokens on which BERT was trained .", ,method
question_answering,2,The result is a vector of twice the hidden size :,architecture,Similarity between visual and text features,0,120,41,8,0,architecture : Similarity between visual and text features,0.4332129963898917,0.3761467889908257,1.0,The result is a vector of twice the hidden size ,11,"More specifically , we use the following equation to compute the similarity representation between two hidden state vectors v 1 and v 2 .", ,method
sentiment_analysis,19,"The best model ( SuBiLSTM ) achieves 88.2 % , at par with a more complex attention based model BiMPM .",system description,Paraphrase Detection,0,113,61,6,0,system description : Paraphrase Detection,0.7385620915032679,1.0,1.0,The best model SuBiLSTM achieves 88 2 at par with a more complex attention based model BiMPM ,18,"Even in this case , we observe gains over both single layer and 2 - layer BiLSTMs , although slightly lesser than the attention based models .", ,method
natural_language_inference,60,An alternative is to try to combine the strengths of different word embeddings .,introduction,introduction,0,15,8,8,0,introduction : introduction,0.07772020725388601,0.3478260869565217,0.3478260869565217,An alternative is to try to combine the strengths of different word embeddings ,14,"While this is often a sensible thing to do , the usefulness of word embeddings for downstream tasks tends to be hard to predict , as downstream tasks can be poorly correlated with word - level benchmarks .","Recent work in socalled "" meta- embeddings "" , which ensembles embedding sets , has been gaining traction .",introduction
natural_language_inference,72,"We explore different entity - annotation choices in the empirical part , where we refer to them as Ent ( entities marked ) and Anonym ( entities marked but anonymized ) .",system description,Task formulation,0,117,41,9,0,system description : Task formulation,0.375,0.9318181818181818,0.75,We explore different entity annotation choices in the empirical part where we refer to them as Ent entities marked and Anonym entities marked but anonymized ,26,"In our case , it is not clear how relevant the anonymization is since we deal with medical entities , which have different properties than proper name entities .",We further examine a more challenging setup in which the reader can not rely on entity markers as they are not present in the passage ( NoEnt ) .,method
natural_language_inference,31,The number of blocks is tuned in a range from 1 to 3 .,implementation,Implementation Details,1,141,16,16,0,implementation : Implementation Details,0.5108695652173914,0.5925925925925926,0.6956521739130435,The number of blocks is tuned in a range from 1 to 3 ,14,We scale the summation in augmented residual connections by 1 / ? 2 when n ? 3 to preserve the variance under the assumption that the two addends have the same variance .,The number of layers of the convolutional encoder is tuned from 1 to 3 .,experiment
sentiment_analysis,35,"We build a large - scale , multi-domain dataset named YelpAspect as source domains , which is obtained similarly as the Yelp recommendation dataset ( Bauman , .",dataset,dataset,0,170,3,3,0,dataset : dataset,0.6854838709677419,0.2727272727272727,0.2727272727272727,We build a large scale multi domain dataset named YelpAspect as source domains which is obtained similarly as the Yelp recommendation dataset Bauman ,24,2 ) We alternately optimize L src for the source network and L tar for the target network .,"Specifically , YelpAspect contains three domains : Restaurant ( R1 ) , Beautyspa ( B ) , and Hotel ( H ) .",experiment
text-classification,6,Training repeats ten times for each transfer task model with different randomly initialized weights and we report evaluation results by averaging across runs .,experiment,Experiments,0,104,5,5,0,experiment : Experiments,0.7027027027027027,0.5,0.5,Training repeats ten times for each transfer task model with different randomly initialized weights and we report evaluation results by averaging across runs ,24,"Otherwise , hyperparameters are tuned by crossvalidation on the task training data when available or the evaluation test data when neither training nor dev data are provided .",Transfer learning is critically important when training data for a target task is limited .,experiment
natural_language_inference,34,"For a case with m supporting sentences and h of them are hit , this case has a recall score of h/m .",result,ESP EM (Exact Match),0,258,38,6,0,result : ESP EM (Exact Match),0.8745762711864407,0.5066666666666667,0.2,For a case with m supporting sentences and h of them are hit this case has a recall score of h m ,23,The ESP EM score is the ratio of exactly matched cases .,The averaged recall of the whole dataset is the ESP Recall .,result
text_generation,2,"Intuitively , the loss function is to force the goal vector to match the transition in the feature space while achieving high reward .",methodology,Hierarchical Structure of G,0,118,48,26,0,methodology : Hierarchical Structure of G,0.33714285714285713,0.7868852459016393,0.6666666666666666,Intuitively the loss function is to force the goal vector to match the transition in the feature space while achieving high reward ,23,"cos represents the cosine similarity between the change of feature representation after cstep transitions , i.e. f t +c ? ft , and the goal vector gt (? m ) 1 produced by MANAGER as in Eq .","At the same time , the WORKER is trained to maximize the reward using the REINFORCE algorithm as is done in , We use gt ( ? m ) to explicitly show gt is parameterized by ?m .",method
text-classification,0,Our models accept a sequence of encoded characters as input .,system description,Character quantization,0,56,29,2,0,system description : Character quantization,0.24669603524229075,0.8529411764705882,0.2857142857142857,Our models accept a sequence of encoded characters as input ,11, ,"The encoding is done by prescribing an alphabet of size m for the input language , and then quantize each character using 1 - of - m encoding ( or "" one - hot "" encoding ) .",method
natural_language_inference,31,"parked next to "" is associated mostly with "" bike "" and "" door "" since there is a weaker direct connection between "" parked "" and "" chained "" .",analysis,Analysis,0,238,55,55,0,analysis : Analysis,0.8623188405797102,0.9016393442622952,0.9016393442622952,parked next to is associated mostly with bike and door since there is a weaker direct connection between parked and chained ,22,"In the first block , the alignment results are almost word - or phrase - level .","In the final block , the alignment results take consideration of the semantics and structures of the whole sentences .",result
sentiment_analysis,42,Evaluation metric is classification accuracy .,experiment,Experimental Setting,0,155,9,7,0,experiment : Experimental Setting,0.6150793650793651,1.0,1.0,Evaluation metric is classification accuracy ,6,will make the dataset extremely unbalanced ., ,experiment
natural_language_inference,46,"Reading comprehension ( RC ) - in contrast to information retrieval - requires integrating information and reasoning about events , entities , and their relations across a full document .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.013468013468013473,0.25,0.25,Reading comprehension RC in contrast to information retrieval requires integrating information and reasoning about events entities and their relations across a full document ,24, ,"Question answering is conventionally used to assess RC ability , in both artificial agents and children learning to read .",abstract
question_answering,2,Embeddings are encoded by LSTM and averaged to get the final context representation .,baseline,baseline,0,193,5,5,0,baseline : baseline,0.6967509025270758,0.3125,0.3125,Embeddings are encoded by LSTM and averaged to get the final context representation ,14,"Embedding + LSTM utilizes word embeddings and character embeddings , along with the same visual embeddings used in FVTA .",Embedding + LSTM + Concat concatenates the last LSTM output from different modalities to produce the final output .,result
sentiment_analysis,37,"However , IAN fails to make this association for the aspect "" cosi sandwiches "" , evident from the attention weights of IAN shown in where the emphasis is on "" better "" .",result,Case Study,0,213,22,14,0,result : Case Study,0.8418972332015809,0.5238095238095238,0.4117647058823529,However IAN fails to make this association for the aspect cosi sandwiches evident from the attention weights of IAN shown in where the emphasis is on better ,28,"Clearly , the sentiment behind aspect "" coffee "" comes from the word "" better "" and the same for aspect "" cosi sandwiches "" comes from "" overpriced "" .","This leads to imperfect aspectaware sentence representation generation , resulting misclassification of the target aspect to be positive .",result
sentiment_analysis,47,"Similar as Equation , we can get the right - aware target representation , r tr .",model,Model Training,0,110,40,4,0,model : Model Training,0.5116279069767442,0.8888888888888888,0.4444444444444444,Similar as Equation we can get the right aware target representation r tr ,14,which we call left - aware target representation .,"We name the combination of left - aware and right - aware target representations [r t l , r tr ] as the two - side representation of the target .",method
text_generation,3,"For dialogue generation , human evaluation is more convincing , so we also report human evaluation results on the test set .",result,Results,0,111,18,18,0,result : Results,0.7872340425531915,0.4864864864864865,0.4864864864864865,For dialogue generation human evaluation is more convincing so we also report human evaluation results on the test set ,20,"In our experimental results , the number of repetitive "" I do n't know "" in the AEM + Attention model is reduced by 50 % compared to the Seq2Seq model .","We randomly choose 100 utterances in daily communication style for the human evaluation , each of which is sent to different models to generate responses .",result
sentiment_analysis,40,"To model the sentiment of the above phraselike word sequence ( i.e. "" not wonderful enough "" ) , LSTM - based methods are proposed , such as target dependent LSTM ( TD - LSTM ) .",introduction,introduction,0,22,13,13,0,introduction : introduction,0.09865470852017937,0.38235294117647056,0.38235294117647056,To model the sentiment of the above phraselike word sequence i e not wonderful enough LSTM based methods are proposed such as target dependent LSTM TD LSTM ,28,Such complications usually hinder conventional approaches to aspect sentiment analysis .,"TD - LSTM might suffer from the problem that after it captures a sentiment feature far from the target , it needs to propagate the feature word byword to the target , in which case it 's likely to lose this feature , such as the feature "" cost - effective "" for "" the phone "" in "" My over all feeling is that the phone , after using it for three months and considering its price , is really cost - effective "" .",introduction
natural_language_inference,6,") translate the test data into English and apply the English NLI classifier , or 2 ) translate the English training data and train a separate NLI classifier for each language .",training,XNLI: cross-lingual NLI,0,116,47,22,0,training : XNLI: cross-lingual NLI,0.4677419354838709,0.5108695652173914,0.7333333333333333, translate the test data into English and apply the English NLI classifier or 2 translate the English training data and train a separate NLI classifier for each language ,30,This can be done in two ways :,"Note that we are not evaluating multilingual sentence embeddings anymore , but rather the quality of the MT system and a monolingual model .",experiment
semantic_role_labeling,3,"Again , a linear map is used to mix different channels from different heads :",system description,Self-Attention,0,80,42,24,0,system description : Self-Attention,0.30303030303030304,0.3853211009174312,0.7058823529411765,Again a linear map is used to mix different channels from different heads ,14,"Finally , all the vectors produced by parallel heads are concatenated together to form a single vector .",where M ? R nd and W ? R dd .,method
sentiment_analysis,15,"It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.025925925925925925,0.5,0.5,It includes fine grained sentiment labels for 215 154 phrases in the parse trees of 11 855 sentences and presents new challenges for sentiment compositionality ,26,"To remedy this , we introduce a Sentiment Treebank .","To address them , we introduce the Recursive Neural Tensor Network .",abstract
text-classification,2,"We also compare with Tagspace ( Weston et al. , 2014 ) , which is a tag prediction model similar to ours , but based on the Wsabie model of .",analysis,Tag prediction,0,80,28,12,1,analysis : Tag prediction,0.8602150537634409,0.8,0.631578947368421,We also compare with Tagspace Weston et al 2014 which is a tag prediction model similar to ours but based on the Wsabie model of ,26,We consider a frequency - based baseline which predicts the most frequent tag .,"While the Tagspace model is described using convolutions , we consider the linear version , which achieves comparable performance but is much faster .",result
machine-translation,2,"Similar to the encoder , we employ residual connections around each of the sub-layers , followed by layer normalization .",model,Encoder and Decoder Stacks,1,59,18,12,0,model : Encoder and Decoder Stacks,0.26339285714285715,0.1651376146788991,0.8571428571428571,Similar to the encoder we employ residual connections around each of the sub layers followed by layer normalization ,19,"In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub - layer , which performs multi-head attention over the output of the encoder stack .",We also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions .,method
part-of-speech_tagging,1,We propose a funnel - shaped wide convolutional neural architecture with no downsampling that focuses on learning a better internal structure of words .,introduction,introduction,0,49,37,37,0,introduction : introduction,0.196,0.8604651162790697,0.8604651162790697,We propose a funnel shaped wide convolutional neural architecture with no downsampling that focuses on learning a better internal structure of words ,23,We provide in - depth experiments and empirical comparisons of different convolutional models and explore the advantages and dis advantages of their components for learning character - to - word representations .,Our proposed compositional character - toword model combined with LSTM - CRF achieves state - of - the - art performance for various sequence labeling tasks .,introduction
sentiment_analysis,12,"As shown in Algorithm 1 , we first use the initial training corpus D to conduct model training , and then obtain the initial model parameters ? ( 0 ) ( Line 1 ) .",approach,Details of Our Approach,0,82,17,3,0,approach : Details of Our Approach,0.36607142857142855,0.2361111111111111,0.06382978723404255,As shown in Algorithm 1 we first use the initial training corpus D to conduct model training and then obtain the initial model parameters 0 Line 1 ,28,"Based on the above analysis , we propose a novel incremental approach to automatically mine influential context words from training instances , which can be then exploited as attention supervision information for neural ASC models .","As shown in Algorithm 1 , we first use the initial training corpus D to conduct model training , and then obtain the initial model parameters ? ( 0 ) ( Line 1 ) .",method
sentiment_analysis,1,Note that our adjacency matrix A obtained in ( 10 ) aims to represent the brain network which combines both local anatomical connectivity and emotion - related global functional connectivity .,system description,Adjacency Matrix in RGNN,0,200,128,23,0,system description : Adjacency Matrix in RGNN,0.5050505050505051,0.6183574879227053,0.8214285714285714,Note that our adjacency matrix A obtained in 10 aims to represent the brain network which combines both local anatomical connectivity and emotion related global functional connectivity ,28,"where ( i , j ) denotes the indices of empirically selected symmetric channel pairs that balance wiring cost and global efficiency : ( FP1 , FP2 ) , ( AF3 , AF4 ) , ( F5 , F6 ) , ( FC5 , FC6 ) , ( C5 , C6 ) , ( CP5 , CP6 ) , ( P5 , P6 ) , ( PO5 , PO6 ) , and ( O1 , O2 ) .",The last step in constructing the adjacency matrix is finding an optimal value of ? to regularize the weights of connections between local channels .,method
sentiment_analysis,4,Concatenation is one of the most common fusion methods .,methodology,Fusion,0,147,42,3,0,methodology : Fusion,0.45092024539877296,0.42857142857142855,0.75,Concatenation is one of the most common fusion methods ,10,We generate the final representation of an utterance u by concatenating all three multimodal features :,Its simplicity also allows us to emphasize the contribution of the remaining components of ICON .,method
machine-translation,7,This residual connection encourages gradient flow .,APPENDICES A LOAD-BALANCING LOSS,8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0,268,46,7,0,APPENDICES A LOAD-BALANCING LOSS : 8-MILLION-OPERATIONS-PER-TIMESTEP MODELS,0.7184986595174263,0.30463576158940403,0.6363636363636364,This residual connection encourages gradient flow ,7,"After dropout , the output of the previous layer is added to the layer output .","For the hierarchical MoE layers , the first level branching factor was 16 , corresponding to the number of GPUs in our cluster .",others
sentiment_analysis,51,"Sentiment classification is an important process in understanding people 's perception towards a product , service , or topic .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.02666666666666667,0.2857142857142857,0.2857142857142857,Sentiment classification is an important process in understanding people s perception towards a product service or topic ,18, ,Many natural language processing models have been proposed to solve the sentiment classification problem .,abstract
semantic_parsing,0,We report results on two different settings for all models :,experiment,Experimental Results and Discussion,0,242,6,6,0,experiment : Experimental Results and Discussion,0.8705035971223022,0.6,0.6,We report results on two different settings for all models ,11,"Restaurants , GeoQuery , Scholar , Academic , IMDB , and Yelp .","1 ) Example split where examples are randomly split into 8659 train , 1034 dev , 2147 test .",experiment
natural_language_inference,30,"Besides , for the strings representing entities and relationships in the questions , we simply used their names in ReVerb , replacingby spaces and stripping off .",training,Questions Generation,0,100,26,15,0,training : Questions Generation,0.3875968992248062,0.5777777777777777,0.4838709677419355,Besides for the strings representing entities and relationships in the questions we simply used their names in ReVerb replacingby spaces and stripping off ,24,"For instance , since the type of entities in ReVerb is unknown , a pattern like who does er ? can be chosen for a triple where the type of ? in ( ? , r , e ) is not a person , and similar for other types ( e.g. when ) .",Patterns for generating questions from ReVerb triples following .,experiment
sentiment_analysis,11,"Softmax is applied to this final vector to get the emotion predictions , Categorical cross - entropy is used as the loss :",system description,Single Layer,0,196,52,38,0,system description : Single Layer,0.5697674418604651,0.5252525252525253,0.9047619047619048,Softmax is applied to this final vector to get the emotion predictions Categorical cross entropy is used as the loss ,21,"To generate the predictions for the current utterance u i , we combine the output representations of both persons : o a and ob with u i 's representation q i and perform an affine transformation using matrix W o .","Softmax is applied to this final vector to get the emotion predictions , Categorical cross - entropy is used as the loss :",method
natural_language_inference,49,"Once we have the interaction tensor I , we use a convolution with 1 1 kernel to scale down the tensor in a ratio , ? , without following ReLU .",model,DENSELY INTERACTIVE INFERENCE NETWORK,0,108,62,38,0,model : DENSELY INTERACTIVE INFERENCE NETWORK,0.4251968503937008,0.9117647058823528,0.8636363636363636,Once we have the interaction tensor I we use a convolution with 1 1 kernel to scale down the tensor in a ratio without following ReLU ,27,ReLU activation function is applied after all convolution unless otherwise noted .,If the input channel is k then the output channel is f loor ( k ? ) .,method
text_generation,1,where ? ? is the partial differential operator .,training,Training,0,139,27,27,0,training : Training,0.5072992700729927,0.6,0.6,where is the partial differential operator ,7,"Refer to the proof in , the gradient of the objective function for generator G can be formulated as :",where ? ? is the partial differential operator .,experiment
natural_language_inference,56,The network never sees the tree .,experiment,Age arithmetic task details,0,311,34,13,0,experiment : Age arithmetic task details,0.925595238095238,0.576271186440678,0.7647058823529411,The network never sees the tree ,7,"Note , this graph is different from the tree used to generate the facts .",The input vector for each fact are the four fact integers and the question integer one - hot encoded and concatenated .,experiment
sentiment_analysis,18,"Indeed , we can understand the process shown by Eq. as an autoencoder , where we first reduce c s from d dimensions to K dimensions with softmax non-linearity .",model,Target Representation,0,100,34,14,0,model : Target Representation,0.41841004184100417,0.4594594594594595,0.6086956521739131,Indeed we can understand the process shown by Eq as an autoencoder where we first reduce c s from d dimensions to K dimensions with softmax non linearity ,29,"Therefore , we add an unsupervised objective function to ensure the quality of the aspect embeddings , which is jointly trained with the attention - based LSTM .","Only the dimensions that are relevant to the aspects are retained in qt , whereas the other dimensions are removed .",method
natural_language_inference,26,"However , SemBERT still outperforms the simple BERT + SRL model just like the latter outperforms the original BERT by a large performance margin , which shows that SemBERT works more effectively for integrating both plain contextual representation and contextual semantics at the same time .",ablation,Ablation Study,1,172,6,6,0,ablation : Ablation Study,0.8113207547169812,0.5,1.0,However SemBERT still outperforms the simple BERT SRL model just like the latter outperforms the original BERT by a large performance margin which shows that SemBERT works more effectively for integrating both plain contextual representation and contextual semantics at the same time ,43,"From the results , we observe that the concatenation would yield an improvement , verifying that integrating contextual semantics would be quite useful for language understanding .", ,result
relation-classification,5,"In addition , using the biaffine attention improves the performance compared to using the linear mechanism significantly .",introduction,introduction,0,33,27,27,0,introduction : introduction,0.2920353982300885,0.9642857142857144,0.9642857142857144,In addition using the biaffine attention improves the performance compared to using the linear mechanism significantly ,17,"Experimental results on the benchmark "" relation and entity recognition "" dataset CoNLL04 show that our model outperforms previous models , obtaining new stateof - the - art scores .",We also provide an ablation study to investigate effects of different contributing factors in our model .,introduction
natural_language_inference,23,This was Hereford 's conclusion .,model,FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0,441,48,25,0,model : FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0.8596491228070176,0.4,0.29411764705882354,This was Hereford s conclusion ,6,"Twigg produced the first major work to challenge the bubonic plague theory directly , and his doubts about the identity of the Black Death have been taken up by a number of authors , including .",Question : What was Shrewsbury 's conclusion ? Answer : contemporary accounts were exaggerations FusionNet Prediction : contemporary accounts were exaggerations BiDAF,method
natural_language_inference,9,Layer 1 Layer 2 Task 1 Issuing API calls z 1 ? ? r 1 ? ? r 1 z,model,VISUALIZATIONS,0,304,17,10,0,model : VISUALIZATIONS,0.9212121212121211,0.3953488372093023,0.2777777777777778,Layer 1 Layer 2 Task 1 Issuing API calls z 1 r 1 r 1 z,16,Which price range are you looking for .,Can you make a restaurant reservation for eight in a cheap price range in madrid 0.00 1.00 0.93 1.00 I 'm on it .,method
text-to-speech_synthesis,0,"Second , large or ensemble models are too costly to serve when deploying in the online systems .",introduction,introduction,0,20,9,9,0,introduction : introduction,0.12345679012345676,0.4090909090909091,0.4090909090909091,Second large or ensemble models are too costly to serve when deploying in the online systems ,17,This work was done while the first author was an intern at Microsoft. of G2P conversion .,How to reduce the model size but maintain high accuracy is essential .,introduction
natural_language_inference,100,The results were either similar or worse than combining a NMM score with QL .,result,Learning with Combining Additional Features,0,328,67,12,0,result : Learning with Combining Additional Features,0.8961748633879781,0.8170731707317073,0.5454545454545454,The results were either similar or worse than combining a NMM score with QL ,15,"We also tried to combine a NMM score with other additional features such as word overlap features , IDF weighted word overlap features and BM25 as in previous research .","For a NMM , the gains after combining additional features are not as large as neural network models like CNN in and LSTM in .",result
text-classification,1,"Instead , demerits were evident - more meta-parameters to tune , poor accuracy with lowdimensional word vectors , and slow training / testing with high - dimensional word vectors as they are dense .",system description,Elimination of the word embedding layer,0,87,41,7,0,system description : Elimination of the word embedding layer,0.33984375,0.205,0.7,Instead demerits were evident more meta parameters to tune poor accuracy with lowdimensional word vectors and slow training testing with high dimensional word vectors as they are dense ,29,"In the preliminary experiments under our framework , we were unable to improve accuracy over one - hot LSTM by inclusion of such a randomly initialized word embedding layer ; i.e. , random vectors failed to provide good prior effects .","If a word embedding is appropriately pre-trained with unlabeled data , its inclusion is a form of semi-supervised learning and could be useful .",method
sentiment_analysis,11,"At every hop , the query utterance u i 's representation q i is updated as : We perform experiments on the IEMOCAP dataset",system description,Multiple Layers,0,208,64,8,0,system description : Multiple Layers,0.6046511627906976,0.6464646464646465,0.3333333333333333,At every hop the query utterance u i s representation q i is updated as We perform experiments on the IEMOCAP dataset,22,This constraint of sharing parameters adjacently between layers is added for reduction in total parameters and ease of training .,. It is a multimodal data base of 10 speakers ( 5 male and 5 female ) involved in two - way dyadic conversations .,method
text_summarization,13,"Therefore , in order to scale attention models for this problem , we aim to prune down the length of the source sequence in an intelligent way .",introduction,introduction,1,16,9,9,0,introduction : introduction,0.0599250936329588,0.5294117647058824,0.5294117647058824,Therefore in order to scale attention models for this problem we aim to prune down the length of the source sequence in an intelligent way ,26,"However , it makes sense intuitively that not every word of the source will be necessary for generating a summary , and so we would like to reduce the amount of computation performed on the source .","Instead of naively attending to all the words of the source at once , our solution is to use a two - layer hierarchical attention .",introduction
natural_language_inference,91,"Each vector - valued variable listed is of dimension D except e , of the independent dimension D tracking .",system description,Composition and representation,0,66,13,13,0,system description : Composition and representation,0.2832618025751073,0.3170731707317073,0.8666666666666667,Each vector valued variable listed is of dimension D except e of the independent dimension D tracking ,18,"The result of this function , the pair h , c , is placed back on the stack .",The stack and buffer,method
text-to-speech_synthesis,2,"The network is trained to optimize a generalized end - to - end speaker verification loss , so that embeddings of utterances from the same speaker have high cosine similarity , while those of utterances from different speakers are far apart in the embedding space .",model,Speaker encoder,0,56,10,7,0,model : Speaker encoder,0.2285714285714285,0.2222222222222222,0.5384615384615384,The network is trained to optimize a generalized end to end speaker verification loss so that embeddings of utterances from the same speaker have high cosine similarity while those of utterances from different speakers are far apart in the embedding space ,42,"The network maps a sequence of log-mel spectrogram frames computed from a speech utterance of arbitrary length , to a fixed - dimensional embedding vector , known as d-vector .",The training dataset consists of speech audio examples segmented into 1.6 seconds and associated speaker identity labels ; no transcripts are used .,method
question-answering,3,"In the following experiments , we utilize filter types in win - 3 .",model,Model Properties,0,199,29,29,0,model : Model Properties,0.7834645669291339,0.4142857142857143,1.0,In the following experiments we utilize filter types in win 3 ,12,"Therefore , the trigram is the best granularity for our model .", ,method
text_generation,4,"Bias matrices C z , Cr , C are introduced for the update gate , reset gate and hidden state computation by the encoder .",system description,Skip-Thought Architecture,0,51,24,9,0,system description : Skip-Thought Architecture,0.4678899082568808,0.6,0.6923076923076923,Bias matrices C z Cr C are introduced for the update gate reset gate and hidden state computation by the encoder ,22,neural language model conditioned on the encoder output hi serves as the decoder .,"Two decoders are used in parallel , one each for sentences s i + 1 and s i ? 1 . The following equations are iterated over for decoding :",method
smile_recognition,0,"For both inputs , contains the final models selected .",experiment,experiment,0,78,18,18,0,experiment : experiment,0.8666666666666667,0.72,0.72,For both inputs contains the final models selected ,9,Each model was trained for 50 epochs in the model selection .,"For the mouth input , there is a preference to more convolutions and more hidden layers .",experiment
paraphrase_generation,0,"The objective of global loss is to maximize the similarity between predicted sentence f pi with the ground truth sentence f g i of i th sentence and minimize the similarity between i th predicted sentence , f pi , with j th ground truth sentence , f g j , in the batch .",method,Discriminative-LSTM,0,116,56,12,0,method : Discriminative-LSTM,0.489451476793249,0.9032258064516128,0.8571428571428571,The objective of global loss is to maximize the similarity between predicted sentence f pi with the ground truth sentence f g i of i th sentence and minimize the similarity between i th predicted sentence f pi with j th ground truth sentence f g j in the batch ,51,Similarly ground truth batch embeddings are e,The loss is defined as,method
relation_extraction,7,"In above equation , the t th word output hit ? R m of BGRU is the element - wise addition of the t th hidden states of forward GRU and backward one .",methodology,BGRU over STP,0,110,49,10,0,methodology : BGRU over STP,0.4247104247104247,0.4298245614035088,0.9090909090909092,In above equation the t th word output hit R m of BGRU is the element wise addition of the t th hidden states of forward GRU and backward one ,31,The following equation defines the operation mathematically .,"In above equation , the t th word output hit ? R m of BGRU is the element - wise addition of the t th hidden states of forward GRU and backward one .",method
sentiment_analysis,39,We then run this model on the test set and report the results .,experiment,experiment,0,193,7,7,0,experiment : experiment,0.7877551020408163,0.7,0.7,We then run this model on the test set and report the results ,14,We save the best model which has the lowest loss on the dev set over all the iterations .,We report results separately on both categories of single location sentences and sentences with two locations and over all the data in the test set .,experiment
semantic_parsing,1,"TRANX uses ASTs as the general - purpose , intermediate semantic representation for MRs .",methodology,Modeling ASTs using ASDL Grammar,0,38,13,2,0,methodology : Modeling ASTs using ASDL Grammar,0.2657342657342657,0.3170731707317073,0.06666666666666668,TRANX uses ASTs as the general purpose intermediate semantic representation for MRs ,13, ,"ASTs are commonly used to represent programming languages , and can also be used to represent other tree - structured MRs ( e.g. , ?- calculus ) .",method
natural_language_inference,48,"In both cases , the machine comprehension system is presented with an ablated query and the document to which the original query refers .",introduction,introduction,0,17,11,11,0,introduction : introduction,0.07692307692307693,0.4782608695652174,0.4782608695652174,In both cases the machine comprehension system is presented with an ablated query and the document to which the original query refers ,23,The abstractive nature of the summary is likely to demand a higher level of comprehension of the original text .,The missing word is assumed to appear in the document .,introduction
question-answering,6,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,model,MODEL DETAILS,1,213,33,33,0,model : MODEL DETAILS,0.6454545454545455,0.4177215189873418,0.9705882352941176,The learning rate is controlled by AdaGrad with the initial learning rate of 0 5 0 1 for QA 10 k ,22,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .","Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10 k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data .",method
natural_language_inference,57,Multimodal regularities found with embeddings learned for the caption - image retrieval task .,SUPPLEMENTARY MATERIAL,SUPPLEMENTARY MATERIAL,0,171,3,3,0,SUPPLEMENTARY MATERIAL : SUPPLEMENTARY MATERIAL,0.994186046511628,0.75,0.75,Multimodal regularities found with embeddings learned for the caption image retrieval task ,13,"Nearest non - query images in COCO train max ( "" man "" , "" cat "" ) max ( "" black dog "" , "" park "" ) :","Note that some images have been slightly cropped for easier viewing , but no relevant objects have been removed .",others
relation_extraction,11,BGWA : Bi - GRU based relation extraction model with word and sentence level attention ) .,baseline,baseline,1,203,8,8,0,baseline : baseline,0.8185483870967742,0.8888888888888888,0.8888888888888888,BGWA Bi GRU based relation extraction model with word and sentence level attention ,14,PCNN + ATT : A piecewise max - pooling over CNN based model which is used by to get sentence representation followed by attention over sentences .,"RESIDE : The method proposed in this paper , please refer Section 5 for more details .",result
natural_language_inference,84,"300 - dimensional ESIM performed best for the baseline and Glo Ve models , whereas using just 100 dimensions worked better for the models using auxiliary information .",system description,ENTAILMENT PREDICTION,0,168,101,14,0,system description : ENTAILMENT PREDICTION,0.7304347826086957,0.90990990990991,0.5833333333333334,300 dimensional ESIM performed best for the baseline and Glo Ve models whereas using just 100 dimensions worked better for the models using auxiliary information ,26,Our last model used pretrained GloVe embeddings .,All runs were repeated 3 times and scores are averaged .,method
text_summarization,4,This dataset receives the full news article as input and use the human - generated multiple sentence highlight as the gold standard summary .,dataset,dataset,0,171,8,8,0,dataset : dataset,0.6627906976744186,0.6666666666666666,0.6666666666666666,This dataset receives the full news article as input and use the human generated multiple sentence highlight as the gold standard summary ,23,"Second , we use the CNN dataset released in .",The original dataset has been modified and preprocessed specifically for the document summarization task .,experiment
natural_language_inference,33,lot of the recent success in natural language processing ( NLP ) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.01532567049808429,0.2,0.2,lot of the recent success in natural language processing NLP has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner ,30, ,These representations are typically used as general purpose features for words across a range of NLP problems .,abstract
natural_language_inference,46,http://deepmind.com/publications ing comprehension models .,system description,FRANK (to the baby),0,52,36,30,0,system description : FRANK (to the baby),0.1750841750841751,0.42857142857142855,0.38461538461538464,http deepmind com publications ing comprehension models ,8,"This ability to form new concepts based on the contexts of a text is a crucial aspect of reading comprehension , and is in part tested as part of the question answering tasks we present .",We summarize the key features of a collection of popular recent datasets in .,method
sentiment_analysis,36,We unfold the recursive form of Eq. 6 as follows :,model,Context-Preserving Mechanism,0,101,54,16,0,model : Context-Preserving Mechanism,0.40399999999999997,0.5744680851063829,0.5,We unfold the recursive form of Eq 6 as follows ,11,is the output of TST in this layer .,"we can see that the output of each layer will contain the contextualized word representations ( i.e. , h ( 0 ) i ) , thus , the context information is encoded into the transformed features .",method
machine-translation,5,The Moses truecasing script truecase .,system,Data Pre-processing,0,71,48,8,0,system : Data Pre-processing,0.4965034965034965,0.6486486486486487,0.5714285714285714,The Moses truecasing script truecase ,6,"Then , the data are tokenised using the Tilde MT regular expression - based tokeniser .",perl is used to truecase the first word of every sentence .,method
topic_models,0,"The topic distributions over vocabulary ? , however , still remained Discrete .",model,model,0,219,29,29,0,model : model,0.5315533980582524,0.7837837837837838,0.7837837837837838,The topic distributions over vocabulary however still remained Discrete ,10,The advantage is that the document vectors can model the correlations in topics .,"In Bayesian SMM , the topic - word distributions ( T ) are not Discrete , hence it can model the correlations between words and ( latent ) topics .",method
natural_language_inference,100,"We design an experiment to compare a NMM - 1 / a NMM - 2 with a NMM - IDF , which is a degenerate version of our model where we use IDF to directly replace the output of question attention network .",result,Question Term Importance,0,291,30,11,0,result : Question Term Importance,0.7950819672131147,0.36585365853658536,0.7333333333333333,We design an experiment to compare a NMM 1 a NMM 2 with a NMM IDF which is a degenerate version of our model where we use IDF to directly replace the output of question attention network ,38,An interesting question is how the learned term importance compare with traditional IR term weighting methods such as IDF .,"In this case , ? ( v qj ) in Equation 6 is replaced by the IDF of the j - th question term .",result
natural_language_inference,63,We add a residual connection between the controller 's input and output to mitigate the information morphing that can occur when interacting with memory .,model,Memory Controller,0,85,45,15,0,model : Memory Controller,0.4722222222222222,0.5921052631578947,0.6,We add a residual connection between the controller s input and output to mitigate the information morphing that can occur when interacting with memory ,25,The controller makes the output vector as a weighted sum v t of hidden state of the recurrent layer and memory vectors .,"As a result , we get a long - term dependency - aware output vector as follows :",method
natural_language_inference,29,The goal of this classifier is to distinguish whether a sentence is consistent with the given facts .,system description,Consistency discriminator,0,194,99,4,0,system description : Consistency discriminator,0.5315068493150685,0.61875,0.09090909090909093,The goal of this classifier is to distinguish whether a sentence is consistent with the given facts ,18,We propose a convolutional neural network ( CNN ) based classifier as discriminator .,So we can use the confidence of classifying a sentence as a training signal to encourage the answer generator to produce a better answer .,method
question-answering,7,The corpus consists of sentence - aligned translation of TED talks .,analysis,Machine Translation,0,233,34,13,0,analysis : Machine Translation,0.8472727272727273,0.53125,0.5416666666666666,The corpus consists of sentence aligned translation of TED talks ,11,We used the English - German translation corpus from the IWSLT 2014 evaluation campaign .,The data was pre-processed and lowercased with the Moses toolkit .,result
natural_language_inference,99,These results sufficiently prove the significant superiority of our proposed model .,result,Overall Results,0,210,9,9,0,result : Overall Results,0.8333333333333334,0.8181818181818182,0.8181818181818182,These results sufficiently prove the significant superiority of our proposed model ,12,"From the tables 1 and 2 we can see our single model achieves an EM score of 71.415 % and a F1 score of 80.160 % and the ensemble model improves to EM 75.989 % and F1 83. 475 % , which are both only after the r-net method at the time of submission .",We also compare our models on the recently proposed dataset Trivia QA. shows the performance comparison on the test set of Trivia QA .,result
sentiment_analysis,24,DS and + DD denote adding DS and DD with parameter sharing only .,analysis,Results and Analysis,0,240,27,27,0,analysis : Results and Analysis,0.7717041800643086,0.4655172413793103,0.4655172413793103,DS and DD denote adding DS and DD with parameter sharing only ,13,Message passing - a denotes propagating the outputs from aspect - level tasks only at each message passing iteration .,Message passing -d denotes involving the document - level information for message passing .,result
text_generation,2,We also train a ?-parameterized discriminative model D ? that provides a scalar guiding signal D ? ( s T ) for G ? to adjust it s parameters when the whole sentence s T has been generated .,methodology,Methodology,0,75,5,5,0,methodology : Methodology,0.21428571428571427,0.08196721311475409,0.4545454545454545,We also train a parameterized discriminative model D that provides a scalar guiding signal D s T for G to adjust it s parameters when the whole sentence s T has been generated ,34,"?-parameterized generative net G ? , which corresponds to a stochastic policy , maps st to a distribution over the whole vocabulary , i.e. G ? ( |s t ) , from which the action x t + 1 , i.e. the next word to select is sampled .",We also train a ?-parameterized discriminative model D ? that provides a scalar guiding signal D ? ( s T ) for G ? to adjust it s parameters when the whole sentence s T has been generated .,method
passage_re-ranking,0,"This is a somewhat surprising result because query expansion usually improves effectiveness in document retrieval , but this can likely be explained by the fact that both MS MARCO and CAR are precision oriented .",result,result,0,110,28,28,0,result : result,0.9016393442622952,0.875,0.875,This is a somewhat surprising result because query expansion usually improves effectiveness in document retrieval but this can likely be explained by the fact that both MS MARCO and CAR are precision oriented ,34,"As a contrastive condition , we find that query expansion with RM3 hurts in both datasets , whether applied to the unexpanded corpus ( BM25 + RM3 ) or the expanded version ( BM25 + Doc2query + RM3 ) .","This result shows that document expansion can be more effective than query expansion , most likely because there are more signals to exploit as documents are much longer .",result
text-to-speech_synthesis,2,We trained separate synthesis and vocoder networks for each of these two corpora .,experiment,experiment,0,104,13,13,0,experiment : experiment,0.4244897959183673,0.125,0.125,We trained separate synthesis and vocoder networks for each of these two corpora ,14,This process was only used on the synthesis target ; the original noisy speech was passed to the speaker encoder .,"Throughout this section , we used synthesis networks trained on phoneme inputs , in order to control for pronunciation in subjective evaluations .",experiment
natural_language_inference,95,"On the contrary , with the content probabilities , we pay more attention to the content part of the answer , which can provide more distinguishable information for verifying the correct answer .",ablation,Necessity of the Content Model,0,207,30,8,0,ablation : Necessity of the Content Model,0.8846153846153846,0.9375,0.8,On the contrary with the content probabilities we pay more attention to the content part of the answer which can provide more distinguishable information for verifying the correct answer ,30,"Since answer candidates usually have similar boundary words , if we compute the answer representation based on the boundary probabilities , it 's difficult to model the real difference among different answer candidates .","Furthermore , the content probabilities can also adjust the weights of the words within the answer span so that unimportant words ( e.g. "" and "" and "" . "" ) get lower weights in the final answer representation .",result
sentiment_analysis,8,"From , we see that an LSTM cell is able to keep track of hidden states at all time steps through the feedback mechanism .",model,model,0,152,17,17,0,model : model,0.6495726495726496,0.85,0.85,From we see that an LSTM cell is able to keep track of hidden states at all time steps through the feedback mechanism ,24,"where initial values are c 0 = 0 and h 0 = 0 and denotes the element - wise product , t denotes the time step ( each element in a sequence belongs to onetime step ) , x t refers to the input vector to the LSTM unit , ft is the forget gate 's activation vector , it refers to the input gate 's activation vector , o t refers to the output gate 's activation vector , ht is the hidden state vector ( which is typically used to map a vector from the feature space to a lowerdimensional latent space , ) ct is the cell state vector and W , U and bare weight and bias matrices which need to be learned during training .",shows the network implemented in this work .,method
topic_models,0,Using the results from,result,result,0,410,1,1,0,result : result,0.9951456310679612,0.3333333333333333,0.3333333333333333,Using the results from,4, ,Taking derivative with respect to shared precision matrix D and equating it to zero :,result
sentiment_analysis,40,"Because the datasets have unbalanced classes as shown in , Macro - averaged F- measure is also reported , as did in .",result,Main Results,0,166,3,3,0,result : Main Results,0.7443946188340808,0.0576923076923077,0.1875,Because the datasets have unbalanced classes as shown in Macro averaged F measure is also reported as did in ,20,"The first evaluation metric is Accuracy , which is used in .","As shown by the results in , our RAM consistently outperforms all compared methods on these four datasets .",result
sentiment_analysis,10,They are not aware of the speaker of a given utterance .,introduction,introduction,0,12,5,5,0,introduction : introduction,0.04669260700389105,0.19230769230769232,0.19230769230769232,They are not aware of the speaker of a given utterance ,12,"Current systems , including the state of the art , do not distinguish different parties in a conversation in a meaningful way .","In contrast , we model individual parties with party states , as the conversation flows , by relying on the utterance , the context , and current party state .",introduction
machine-translation,1,The ByteNet uses dilation in the convolutional layers to increase its receptive field .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.03980099502487562,0.6,0.6,The ByteNet uses dilation in the convolutional layers to increase its receptive field ,14,"To address the differing lengths of the source and the target , we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder .",The resulting network has two core properties : it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization .,abstract
semantic_parsing,2,"We employed the preprocessed DJANGO data provided by , where input expressions are tokenized by NLTK , and quoted strings in the input are replaced with place holders .",experiment,Experimental Setup,0,234,7,4,0,experiment : Experimental Setup,0.8041237113402062,0.3333333333333333,0.2222222222222222,We employed the preprocessed DJANGO data provided by where input expressions are tokenized by NLTK and quoted strings in the input are replaced with place holders ,27,We combined predicates and left brackets that indicate hierarchical structures to make meaning representations compact .,"WIK - ISQL was preprocessed by the script provided by , where inputs were lowercased and tokenized by Stanford CoreNLP .",experiment
natural_language_inference,17,"The intuition is that two words should be correlated if their attentions about same texts are highly overlapped , and be less related vice versa .",architecture,Alignment Architecture for MRC,0,79,36,36,0,architecture : Alignment Architecture for MRC,0.3038461538461538,0.26277372262773724,0.2769230769230769,The intuition is that two words should be correlated if their attentions about same texts are highly overlapped and be less related vice versa ,25,"To address these problems , we propose to temporally memorize past attentions and explicitly use them to refine current attentions .","For example , in , suppose that we have access to previous attentions , and then we can compute their dot product to obtain a "" similarity of attention "" .",method
named-entity-recognition,3,Importance of task specific RNN .,analysis,analysis,0,142,16,16,0,analysis : analysis,0.7675675675675676,0.64,0.64,Importance of task specific RNN ,6,"This result supports the hypothesis that adding language models help because they learn composition functions ( i.e. , the RNN parameters in the language model ) from much larger data compared to the composition functions in the baseline tagger , which are only learned from labeled data .",To understand the importance of including a task specific sequence RNN we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags .,result
relation-classification,2,"We measure the performance of our system using micro F 1 scores , Precision and Recall on both entities and relations .",dataset,Datasets and evaluation metrics,0,176,5,5,0,dataset : Datasets and evaluation metrics,0.5966101694915255,0.16129032258064516,0.16129032258064516,We measure the performance of our system using micro F 1 scores Precision and Recall on both entities and relations ,21,We obtained the preprocessing script from Miwa 's github codebase .,We treat an entity as correct when the entity type and the region of its head are correct .,experiment
sentiment_analysis,25,"Some existing work ) removed "" conflict "" labels from four sentiment labels , which makes their results incomparable to those from the workshop report .",dataset,Datasets and Experiment Preparation,0,127,3,3,0,dataset : Datasets and Experiment Preparation,0.5720720720720721,0.10714285714285714,0.10714285714285714,Some existing work removed conflict labels from four sentiment labels which makes their results incomparable to those from the workshop report ,22,"We conduct experiments on public datasets from SemEval workshops , which consist of customer reviews about restaurants and laptops .","We reimplemented the compared methods , and used hyper - parameter settings described in these references .",experiment
text_generation,5,Causal convolution can be seen a special case with d = 1 .,system description,Dilated Convolutional Decoders,0,97,29,12,0,system description : Dilated Convolutional Decoders,0.3288135593220339,0.3815789473684211,0.631578947368421,Causal convolution can be seen a special case with d 1 ,12,"With dilation d , the convolution is applied so that d ? 1 inputs are skipped each step .","With dilation , the effective receptive size grows exponentially with network depth .",method
named-entity-recognition,4,"For tasks where direct comparisons are possible , ELMo outperforms CoVe , which computes contextualized representations using a neural machine translation encoder .",introduction,introduction,0,24,17,17,0,introduction : introduction,0.08823529411764706,0.4358974358974359,0.4358974358974359,For tasks where direct comparisons are possible ELMo outperforms CoVe which computes contextualized representations using a neural machine translation encoder ,21,"The addition of ELMo representations alone significantly improves the state of the art in every case , including up to 20 % relative error reductions .","Finally , an analysis of both ELMo and CoVe reveals that deep representations outperform ar Xiv : 1802.05365v2 [ cs. CL ] 22 Mar 2018 those derived from just the top layer of an LSTM .",introduction
text_summarization,9,Generally the model produces more fluent summaries and is better able to capture the main actors of the input .,result,Results,0,136,31,31,0,result : Results,0.8888888888888888,0.7209302325581395,0.7209302325581395,Generally the model produces more fluent summaries and is better able to capture the main actors of the input ,20,The first two examples highlight typical improvements in the RAS model over ABS + .,"For instance in Sentence 1 , RAS - Elman correctly distinguishes the actions of "" pepe "" from "" ferreira "" , and in Sentence 2 it identifies the correct role of the "" think tank "" .",result
natural_language_inference,23,The comparison is shown in Figure 5 and 6 for EM and F1 score respectively .,abstract,abstract,0,13,11,11,0,abstract : abstract,0.025341130604288498,0.7857142857142857,0.7857142857142857,The comparison is shown in Figure 5 and 6 for EM and F1 score respectively ,16,"In this appendix , we compare with published state - of - the - art architectures on the SQuAD dev set .",The performance of FusionNet is shown under different training epochs .,abstract
natural_language_inference,3,Comparison of LC - LSTMs and word - by - word Attention LSTMs,model,Comparison of LC-LSTMs and word-by-word Attention LSTMs,0,91,68,1,0,model : Comparison of LC-LSTMs and word-by-word Attention LSTMs,0.4375,0.6017699115044248,0.2,Comparison of LC LSTMs and word by word Attention LSTMs,10, , ,method
text_generation,2,We formalize the text generation problem as a sequential decision making process .,methodology,Methodology,0,72,2,2,0,methodology : Methodology,0.2057142857142857,0.032786885245901634,0.18181818181818185,We formalize the text generation problem as a sequential decision making process ,13, ,"Specifically , at each timestep t , the agent takes the previously generated words as its current state , denoted ass t = ( x 1 , . . . , x i , . . . , x t ) , where xi represents a word token in the given vocabulary V .",method
natural_language_inference,95,3 ] A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies .,introduction,introduction,0,33,25,25,0,introduction : introduction,0.14102564102564102,0.4310344827586207,0.4310344827586207,3 A pure culture is one in which only one kind of microbial species is found whereas in mixed culture two or more microbial species formed colonies ,28,"As a result , some institutions are owned and maintained by . . .","Culture on the other hand , is the lifestyle that the people in the country . . .",introduction
natural_language_inference,24,"Further , during decoding , we reuse wisdom of masses instead of individual to achieve a better result .",system description,Experiment Setup,0,100,73,10,0,system description : Experiment Setup,0.4273504273504273,0.8021978021978022,0.35714285714285715,Further during decoding we reuse wisdom of masses instead of individual to achieve a better result ,17,"Intuitively , by applying dropout , it avoids a "" step bias problem "" ( where models places too much emphasis one particular step 's predictions ) and forces the model to produce good predictions at every individual step .","We call this method "" stochastic prediction dropout "" because dropout is being applied to the final predictive distributions .",method
natural_language_inference,28,We apply gradient clipping with maximal norm of the gradients equal to 1.0 . lists the hyper - parameters we use for our models .,CHARACTER LEVEL PENN TREEBANK TASK,CHARACTER LEVEL PENN TREEBANK TASK,0,268,4,4,0,CHARACTER LEVEL PENN TREEBANK TASK : CHARACTER LEVEL PENN TREEBANK TASK,0.9889298892988928,0.5714285714285714,0.5714285714285714,We apply gradient clipping with maximal norm of the gradients equal to 1 0 lists the hyper parameters we use for our models ,24,For training we use Adam optimization ( Kingma & Ba ) .,We embed the inputs into a higher - dimensional space .,others
natural_language_inference,35,The proposed model has two key characteristics .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.02281368821292776,0.4,0.4,The proposed model has two key characteristics ,8,"We propose a multi-style abstractive summarization model for question answering , called Masque .","First , unlike most studies on RC that have focused on extracting an answer span from the provided passages , our model instead focuses on generating a summary from the question and multiple passages .",abstract
text_summarization,9,Our model can be seen as an extension of their model .,system description,93,0,37,9,9,0,system description : 93,0.24183006535947715,1.0,1.0,Our model can be seen as an extension of their model ,12,Very recently proposed a neural attention model for this problem using a new data set for training and showing state - of - the - art performance on the DUC tasks ., ,method
sentiment_analysis,24,We employ a fully - connected layer with ReLu activation as the re-encoding function f ? re .,method,Message Passing Mechanism,0,121,66,6,0,method : Message Passing Mechanism,0.3890675241157556,0.4925373134328358,0.4615384615384616,We employ a fully connected layer with ReLu activation as the re encoding function f re ,17,where t > 0 and [:] denotes the concatenation operation .,"To update the shared representations , we incorporate ? , the outputs of AE and AS from the previous iteration , such that these information are available for both tasks in current round of computation .",method
sentence_compression,2,show that syntactic complexity ( measured as dependency locality ) is also an important predictor of reading time .,system description,Our contributions,0,26,7,7,0,system description : Our contributions,0.25742574257425743,0.14285714285714285,0.28,show that syntactic complexity measured as dependency locality is also an important predictor of reading time ,17,"These are also words that are likely to be replaced with simpler ones in sentence simplification , but it is not clear that they are words that would necessarily be removed in the context of sentence compression .","Phrases that are often removed in sentence compressionlike fronted phrases , parentheticals , floating quantifiers , etc. - are often associated with non-local dependencies .",method
sarcasm_detection,0,"Even humans will struggle with sarcastic comments drawn from unfamiliar topics , for instance , obscure hobbies or art forms .",evaluation,Politics,0,138,17,3,0,evaluation : Politics,0.7840909090909091,0.8947368421052632,0.6,Even humans will struggle with sarcastic comments drawn from unfamiliar topics for instance obscure hobbies or art forms ,19,The difficulty of detecting sarcasm rests not only on the need to understand the context of previous statements but also on understanding background information on the topic being discussed .,"Thus we also test human and machine performance on comments drawn solely from the politics subreddit , a topic for which all evaluators had sufficient background information .",result
natural_language_inference,28,"We find that LSTM fails to learn the task , because of its lack of sufficient memory capacity .",experiment,ASSOCIATIVE RECALL TASK,1,178,39,14,0,experiment : ASSOCIATIVE RECALL TASK,0.6568265682656826,0.7647058823529411,0.9333333333333332,We find that LSTM fails to learn the task because of its lack of sufficient memory capacity ,18,The optimizer is RMSProp with a learning rate 0.001 .,"NTM and Fast - weight RNN fail longer tasks , which means they can not learn to manipulate their memory efficiently .",experiment
sentiment_analysis,24,"Unlike conventional multi-task learning methods that rely on learning common features for the different tasks , IMN introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.02572347266881029,0.8571428571428571,0.8571428571428571,Unlike conventional multi task learning methods that rely on learning common features for the different tasks IMN introduces a message passing architecture where information is iteratively passed to different tasks through a shared set of latent variables ,38,"In this paper , we propose an interactive multi-task learning network ( IMN ) which is able to jointly learn multiple related tasks simultaneously at both the token level as well as the document level .",Experimental results demonstrate superior performance of the proposed method against multiple baselines on three benchmark datasets .,abstract
natural_language_inference,6,Tatoeba : results for unseen languages,training,Tatoeba: results for unseen languages,0,244,56,1,0,training : Tatoeba: results for unseen languages,0.9838709677419356,0.9333333333333332,0.2,Tatoeba results for unseen languages,5, , ,experiment
natural_language_inference,46,"That 's a good - looking kid you got there , Ms. Barrett .",system description,FRANK (to the baby),0,27,11,5,0,system description : FRANK (to the baby),0.09090909090909093,0.13095238095238096,0.0641025641025641,That s a good looking kid you got there Ms Barrett ,12,"What do you say , slugger ? FRANK ( to Dana )",Example question - answer pair .,method
natural_language_inference,6,"The dataset consists of up to 1,000 English - aligned sentence pairs for each language .",training,Tatoeba: similarity search,0,151,82,4,0,training : Tatoeba: similarity search,0.6088709677419355,0.8913043478260869,0.2857142857142857,The dataset consists of up to 1 000 English aligned sentence pairs for each language ,16,"So as to better assess the performance of our model in all these languages , we introduce a new test set of similarity search for 112 languages based on the Tatoeba corpus .",describes how the dataset was constructed in more details .,experiment
machine-translation,8,"The decoder is often trained to predict the next wordy t given the context vector c and all the previously predicted words {y 1 , , y t ?1 }.",system description,RNN ENCODER-DECODER,0,48,18,8,0,system description : RNN ENCODER-DECODER,0.14501510574018128,0.25,0.6153846153846154,The decoder is often trained to predict the next wordy t given the context vector c and all the previously predicted words y 1 y t 1 ,28,"used an LSTM as f and q ( {h 1 , , h T }) = h T , for instance .","In other words , the decoder defines a probability over the translation y by decomposing the joint probability into the ordered conditionals :",method
machine-translation,4,"In this experiment , we vary the number of weightsharing layers in the AEs from 0 to 4 .",analysis,Number of weight-sharing layers,0,194,4,3,0,analysis : Number of weight-sharing layers,0.8117154811715481,0.17391304347826084,0.13636363636363635,In this experiment we vary the number of weightsharing layers in the AEs from 0 to 4 ,18,We firstly investigate how the number of weightsharing layers affects the translation performance .,"Sharing one layer in AEs means sharing one layer for the encoders and in the meanwhile , sharing one layer for the decoders .",result
sentiment_analysis,11,This trend suggests that CMN is capable of capturing inter-speaker emotional influences which are often seen in the presence of such active emotions .,baseline,bc-LSTM:,0,286,32,27,0,baseline : bc-LSTM:,0.8313953488372093,1.0,1.0,This trend suggests that CMN is capable of capturing inter speaker emotional influences which are often seen in the presence of such active emotions ,25,"similar trend is seen with bc - LSTM , where our model does explicitly well for the active emotions happiness and anger .", ,result
topic_models,0,The remaining rows in show our baselines and proposed systems .,result,result,0,366,9,9,0,result : result,0.8883495145631068,0.3333333333333333,0.3333333333333333,The remaining rows in show our baselines and proposed systems ,11,"Although we have used the same training and test splits , May had slightly larger vocabulary than ours , and their best system is similar to our baseline TF - IDF based system .","We can see that our proposed systems achieve consistently better accuracies ; notably , GLCU which exploits the uncertainty in document embeddings has much lower cross - entropy than its counterpart , GLC .",result
sentiment_analysis,9,"Compared with the BERT - BASE model , BERT - SPC significantly improves the accuracy and F 1 score of aspect polarity classification .",analysis,Overall Performance Analysis,1,246,12,12,0,analysis : Overall Performance Analysis,0.8880866425992779,0.3428571428571429,0.8,Compared with the BERT BASE model BERT SPC significantly improves the accuracy and F 1 score of aspect polarity classification ,21,"Meanwhile , we implement the joint - task model based on BERT - SPC .","In addition , for the first time , BERT - SPC has increased the F 1 score of ATE subtask on three datasets up to 99 % .",result
text_summarization,1,Method prefixes are the numbers of generated summaries for each document .,result,Diversity vs. Number of Mixtures,0,218,12,5,0,result : Diversity vs. Number of Mixtures,0.9083333333333332,0.4615384615384616,0.2631578947368421,Method prefixes are the numbers of generated summaries for each document ,12,"While we observe a similar trend for SELECTOR in the question generation task , shows the opposite , and the rest are from our experiments using PG as a generator .",Best scores are bolded .,result
natural_language_inference,30,The automatically generated examples are useful to connect KB triples and natural language .,training,Questions Generation,0,107,33,22,0,training : Questions Generation,0.41472868217054265,0.7333333333333333,0.7096774193548387,The automatically generated examples are useful to connect KB triples and natural language ,14,"However , this would contradict one of our motivations which is to train a system with as little human intervention as possible ( and hence choosing ReVerb over hand - curated KBs ) .","However , they do not allow for a satisfactory modeling of English language because of their poor wording .",experiment
natural_language_inference,56,Daniel journeyed to the garden .,experiment,Experiments,0,85,6,6,0,experiment : Experiments,0.2529761904761905,0.05607476635514018,0.6,Daniel journeyed to the garden ,6,"Where is the milk ? "" is preceded by a number of facts in the form of short sentences , e.g.","Daniel put down the milk . """,experiment
paraphrase_generation,1,"MSCOCO : This dataset , also used previously to evaluate paraphrase generation methods , contains human annotated captions of over 120K images .",dataset,Datasets,0,99,3,3,0,dataset : Datasets,0.4479638009049774,0.14285714285714285,0.14285714285714285,MSCOCO This dataset also used previously to evaluate paraphrase generation methods contains human annotated captions of over 120K images ,20,"We evaluate our framework on two datasets , one of which ( MSCOCO ) is for the task of standard paraphrase generation and the other ( Quora ) is a newer dataset for the specific problem of question paraphrase generation .",Each image contains five captions from five different annotators .,experiment
text_generation,2,"In all those cases , LeakGAN shows significant improvements compared to previous models in terms of BLEU statistics and human Turing test .",introduction,introduction,0,45,33,33,0,introduction : introduction,0.12857142857142856,0.8684210526315791,0.8684210526315791,In all those cases LeakGAN shows significant improvements compared to previous models in terms of BLEU statistics and human Turing test ,22,"For real data , we use the text in EMNLP 2017 WMT News , COCO Image Caption and Chinese Poems as the long , mid-length and short text corpus , respectively .","We further provide a deep investigation on the interaction between MAN - AGER and WORKER , which indicates Leak GAN implicitly learns sentence structures , such as punctuation , clause structure and long suffix without any supervision . :",introduction
part-of-speech_tagging,2,The numbers are shown in .,performance,TRANSFER LEARNING PERFORMANCE,0,141,10,10,0,performance : TRANSFER LEARNING PERFORMANCE,0.7921348314606742,0.24390243902439024,0.4,The numbers are shown in ,6,"The numbers in the y-axes are accuracies for POS tagging , and chunk - level F 1 scores for chunking and NER .",We can see that our transfer learning approach consistently improved over the non-transfer results .,result
question_answering,1,The QA task requires the model to find a sub-phrase of the paragraph to answer the query .,model,Attention Flow,0,94,62,53,0,model : Attention Flow,0.29652996845425866,0.6138613861386139,0.7681159420289855,The QA task requires the model to find a sub phrase of the paragraph to answer the query ,19,"In section 5 , we use a slight modification of this output layer for cloze - style comprehension .",The phrase is derived by predicting the start and the end indices of the phrase in the paragraph .,method
natural_language_inference,34,"For optimization , we use Adam Optimizer with an initial learning rate of 1 e ?4 .",implementation,Implementation Details,1,218,11,11,0,implementation : Implementation Details,0.7389830508474576,0.8461538461538461,0.8461538461538461,For optimization we use Adam Optimizer with an initial learning rate of 1 e 4 ,16,We set the dropout rate for all hidden units of LSTM and dynamic graph attention to 0.3 and 0.5 respectively .,https://nlp.stanford.edu/software/,experiment
temporal_information_extraction,1,"However , it was overlooked that , although some r 1 and r 2 can not uniquely determiner 3 , they can still constrain the set of labels r 3 can take .",training,Inference,0,120,45,38,0,training : Inference,0.4669260700389105,0.4245283018867925,0.8636363636363636,However it was overlooked that although some r 1 and r 2 can not uniquely determiner 3 they can still constrain the set of labels r 3 can take ,30,") Previously , transitivity constraints were formulated as Ir 1 ( ij ) + Ir 2 ( jk ) ? Ir 3 ( ik ) ? 1 , which is a special case when N = 1 and can be understood as "" r 1 and r 2 determine a single r 3 "" .","For example , as shown in , when r 1 = before and r 2 =is included , r 3 is not determined but we know that r 3 ? { before , is included }",experiment
sentiment_analysis,36,"where g( * ) is a non-linear activation function and "" : "" denotes vector concatenation .",model,Context-Preserving Mechanism,0,90,43,5,0,model : Context-Preserving Mechanism,0.36,0.4574468085106383,0.15625,where g is a non linear activation function and denotes vector concatenation ,13,"Finally , the concatenation of r ? i and hi is fed into a fully - connected layer to obtain the i - th targetspecific word representationh i ( l ) :",? and b ? are the weights of the layer .,method
natural_language_inference,73,"The output of attention , s , is the expectation of sampling a token according to the categorical distribution p ( v|x , q ) , i.e. , Multi-dimensional ( multi-dim ) attention mechanism extends the vanilla one to a feature - wise level , i.e. , each feature of every token has an alignment score .",system description,Attention,0,57,10,10,0,system description : Attention,0.21755725190839686,0.2857142857142857,0.7142857142857143,The output of attention s is the expectation of sampling a token according to the categorical distribution p v x q i e Multi dimensional multi dim attention mechanism extends the vanilla one to a feature wise level i e each feature of every token has an alignment score ,50,"1 ) p ( v|x , q ) = softmax ( a ) .","The output of attention , s , is the expectation of sampling a token according to the categorical distribution p ( v|x , q ) , i.e. , Multi-dimensional ( multi-dim ) attention mechanism extends the vanilla one to a feature - wise level , i.e. , each feature of every token has an alignment score .",method
named-entity-recognition,9,"For instance , Long Short - Term Memory ( LSTM ) and Conditional Random Field ( CRF ) have greatly improved performance in biomedical named entity recognition ( NER ) over the last few years .",introduction,introduction,0,23,8,8,0,introduction : introduction,0.11557788944723618,0.4705882352941176,0.4705882352941176,For instance Long Short Term Memory LSTM and Conditional Random Field CRF have greatly improved performance in biomedical named entity recognition NER over the last few years ,28,Recent progress of biomedical text mining models was made possible by the advancements of deep learning techniques used in natural language processing ( NLP ) .,Other deep learning based models have made improvements in biomedical text mining tasks such as relation extraction ( RE ) and question answering ( QA ) .,introduction
temporal_information_extraction,1,"In fact , to make better use of global constraints , we should have allowed more event pairs in Eq..",training,Inference,0,113,38,31,0,training : Inference,0.4396887159533074,0.3584905660377358,0.7045454545454546,In fact to make better use of global constraints we should have allowed more event pairs in Eq ,19,") We restrict our event pairs ij to a smaller set E = { ij | sentence dist ( i , j ) ? 1 } where pairs thatare more than one sentence away are deleted for computational efficiency and ( usually ) for better performance .","However , fr ( ij ) is usually more reliable when i and j are closer in text .",experiment
natural_language_inference,57,"We tuned the number of layers , layer dimensionality and dropout rates to optimize performance on the development set , using the Adam optimizer .",result,RESULTS,0,109,7,7,0,result : RESULTS,0.6337209302325582,0.15555555555555556,0.5,We tuned the number of layers layer dimensionality and dropout rates to optimize performance on the development set using the Adam optimizer ,23,"Given pairs of sentence vectors u and v , the input to the network is the concatenation of u , v and the absolute difference |u ? v|.",Batch normalization and PReLU units were used .,result
relation_extraction,4,"In practice , however , due to the large amount of noise in the induced data , training relation extractors that perform well becomes very difficult .",introduction,introduction,0,76,66,66,0,introduction : introduction,0.3671497584541063,0.8048780487804879,0.8048780487804879,In practice however due to the large amount of noise in the induced data training relation extractors that perform well becomes very difficult ,24,One can further argue that it is easy to obtain a large amount of training data using distant supervision .,"For example , show that up to 31 % of the distantly supervised labels are wrong when creating training data from aligning Freebase to newswire text .",introduction
natural_language_inference,62,"To bridge the gap between Machine Reading Comprehension ( MRC ) models and human beings , which is mainly reflected in the hunger for data and the robustness to noise , in this paper , we explore how to integrate the neural networks of MRC models with the general knowledge of human beings .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.017857142857142856,0.3333333333333333,0.3333333333333333,To bridge the gap between Machine Reading Comprehension MRC models and human beings which is mainly reflected in the hunger for data and the robustness to noise in this paper we explore how to integrate the neural networks of MRC models with the general knowledge of human beings ,49, ,"On the one hand , we propose a data enrichment method , which uses WordNet to extract inter-word semantic connections as general knowledge from each given passage - question pair .",abstract
natural_language_inference,91,"Most words have multiple senses or meanings , and it is generally necessary to use the context in which a word occurs to determine which of its senses or meanings is meant in a given sentence .",system description,The tracking LSTM,0,78,25,6,0,system description : The tracking LSTM,0.3347639484978541,0.6097560975609756,0.5454545454545454,Most words have multiple senses or meanings and it is generally necessary to use the context in which a word occurs to determine which of its senses or meanings is meant in a given sentence ,36,"It is meant to maintain a low - resolution summary of the portion of the sentence that has been processed so far , which is used for two purposes : it supplies feature representations to the transition classifier , which allows the model to standalone as a parser , and it additionally supplies a secondary input e to the composition function - see Why a tree - sequence hybrid ? Lexical ambiguity is ubiquitous in natural language .","Even though TreeRNNs are more effective at composing meanings in principle , this ambiguity can give simpler sequence - based sentence - encoding models an advantage : when a sequence - based model first processes a word , it has direct access to a state vector that summarizes the left context of that word , which acts as a cue for dis ambiguation .",method
text-classification,5,We evaluate ULMFiT on different numbers of labeled examples in two settings : only labeled examples are used for LM fine - tuning ( 'supervised ' ) ; and all task data is available and can be used to fine - tune the LM ( ' semi-supervised ' ) .,training,Pretraining,0,198,4,4,0,training : Pretraining,0.7857142857142857,0.08888888888888889,0.4,We evaluate ULMFiT on different numbers of labeled examples in two settings only labeled examples are used for LM fine tuning supervised and all task data is available and can be used to fine tune the LM semi supervised ,40,task with a small number of labels .,We compare ULM - FiT to training from scratch - which is necessary for hypercolumn - based approaches .,experiment
natural_language_inference,63,We embed the sequence of words in q and d to get contextual representations .,model,Proposed Model,0,46,6,6,0,model : Proposed Model,0.25555555555555554,0.07894736842105263,0.75,We embed the sequence of words in q and d to get contextual representations ,15,"Answerspan a = {w d k } e k=s ( where , 1 ? s ? e ? n) should be returned , given q and d .",These contextual representations are used for calculating question - aware context representation which is controlled by external memory unit and used for predicting answer - span .,method
sentence_compression,1,We have not tuned these parameters nor the number of stacked layers .,baseline,Baseline,0,123,86,86,0,baseline : Baseline,0.5885167464114832,1.0,1.0,We have not tuned these parameters nor the number of stacked layers ,13,The number of nodes in each LSTM layer is always identical to the number of nodes in the input layer ., ,result
question-answering,3,"Then , we decompose s i linearly based on ?.",model,Decomposition Functions,0,130,77,60,0,model : Decomposition Functions,0.5118110236220472,0.77,0.8955223880597015,Then we decompose s i linearly based on ,9,"First , we calculate the cosine similarity ? between s i and ? i .",Eq. gives the corresponding definition :,method
sentiment_analysis,51,"Therefore , in this section , we briefly explain BERT and then describe our model architecture .",methodology,METHODOLOGY,0,79,5,5,0,methodology : METHODOLOGY,0.5266666666666666,0.16129032258064516,1.0,Therefore in this section we briefly explain BERT and then describe our model architecture ,15,We use pretrained BERT model to build a sentiment classifier ., ,method
text_summarization,5,"To make full use of supervisions from both sides , we combine the above two costs as the final loss function :",system description,Learning,0,102,43,9,0,system description : Learning,0.4047619047619048,0.9148936170212766,0.6923076923076923,To make full use of supervisions from both sides we combine the above two costs as the final loss function ,21,We adopt the common negative log-likelihood ( NLL ) as the loss function :,We use mini-batch Stochastic Gradient Descent ( SGD ) to tune model parameters .,method
natural_language_inference,77,This means that ends for the top -k starts are predicted and the span with the highest over all probability is predicted as final answer .,system description,Answer Layer,0,105,73,8,0,system description : Answer Layer,0.38461538461538464,1.0,1.0,This means that ends for the top k starts are predicted and the span with the highest over all probability is predicted as final answer ,26,"Beam- search During prediction time , we compute the answer span with the highest probability by employing beam - search using a beam - size of k .", ,method
relation_extraction,1,Simple BERT Models for Relation Extraction and Semantic Role Labeling,title,title,1,2,1,1,0,title : title,0.02247191011235955,1.0,1.0,Simple BERT Models for Relation Extraction and Semantic Role Labeling,10, , ,title
natural_language_inference,46,"Second , story summaries are abstractive and generally written by independent authors who know the work only as a reader .",system description,FRANK (to the baby),0,45,29,23,0,system description : FRANK (to the baby),0.15151515151515152,0.34523809523809523,0.2948717948717949,Second story summaries are abstractive and generally written by independent authors who know the work only as a reader ,20,"First , they are largely self - contained : beyond the basic fundamental vocabulary of English , all the information about salient entities and concepts required to understand the narrative is present in the document , with the expectation that a reasonably competent language user would be able to understand it .",We make the dataset available online .,method
natural_language_inference,51,"Except for the Copy task , which is too simple , other tasks observe convergence speed improvement of NUTM over that of NTM , thereby validating the benefit of using two programs across timesteps even for the single task setting .",result,NTM Single Tasks,1,148,15,14,0,result : NTM Single Tasks,0.5584905660377358,0.21428571428571427,0.6363636363636364,Except for the Copy task which is too simple other tasks observe convergence speed improvement of NUTM over that of NTM thereby validating the benefit of using two programs across timesteps even for the single task setting ,38,We run each experiments five times and report the mean with error bars of training losses for NTM tasks in .,NUTM requires fewer training samples to converge and it generalizes better to unseen sequences that are longer than training sequences .,result
paraphrase_generation,1,"Another variant How can I get the old Gmail account password ? tells us that accounts are related to the password , and recovering the account might mean recovering the password as well .",result,Results,0,178,12,12,0,result : Results,0.8054298642533937,0.24489795918367346,0.24489795918367346,Another variant How can I get the old Gmail account password tells us that accounts are related to the password and recovering the account might mean recovering the password as well ,32,It is further able to figure out that the input sentence is talking about recovering the account .,"Another variant How can I get the old Gmail account password ? tells us that accounts are related to the password , and recovering the account might mean recovering the password as well .",result
semantic_parsing,0,Complexity of Data base Schema,performance,performance,0,268,22,22,0,performance : performance,0.9640287769784172,0.7857142857142857,0.7857142857142857,Complexity of Data base Schema,5,"Overall , the result shows that our dataset presents a challenge for the model to generalize to new data bases .","In order to show how the complexity of the data base schema affects model performance , plots the exact matching accuracy as a function of the number of foreign keys in a data base .",result
machine-translation,2,"We trained the base models for a total of 100,000 steps or 12 hours .",training,Hardware and Schedule,1,162,12,4,0,training : Hardware and Schedule,0.7232142857142857,0.42857142857142855,0.6666666666666666,We trained the base models for a total of 100 000 steps or 12 hours ,16,"For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .","For our big models , ( described on the bottom line of table 3 ) , step time was 1.0 seconds .",experiment
natural_language_inference,65,"set the learning rate for epoch t , ? t , using the equation :",system description,Neural Networks Setup,0,177,15,11,0,system description : Neural Networks Setup,0.7695652173913043,0.9375,0.9166666666666666,set the learning rate for epoch t t using the equation ,12,The data is obtained from 5 https://code.google.com/p/word2vec/,"In our experiments , the four NN architectures QA - CNN , AP - CNN , QA - biLSTM and AP - biLSTM are implemented using Theano .",method
relation_extraction,0,"For each entity , both entity mentions and it s head phrase are annotated .",experiment,Data,0,156,5,4,0,experiment : Data,0.609375,0.4545454545454545,0.4,For each entity both entity mentions and it s head phrase are annotated ,14,"There are 7 main entity types namely Person ( PER ) , Organization ( ORG ) , Geographical Entities ( GPE ) , Location ( LOC ) , Facility ( FAC ) , Weapon ( WEA ) and Vehicle ( VEH ) .","For the scope of this paper , we only use the entity head phrase similar to and .",experiment
natural_language_inference,94,We conducted manual analysis on a 50 sample subset of the Narrative QA test set to check the effectiveness of our commonsense selection algorithm .,analysis,analysis,0,237,4,4,0,analysis : analysis,0.618798955613577,0.2857142857142857,0.2857142857142857,We conducted manual analysis on a 50 sample subset of the Narrative QA test set to check the effectiveness of our commonsense selection algorithm ,25,"We also conduct human evaluation analysis on both the quality of the selected commonsense relations , as well as the performance of our final model .","Specifically , given a context - query pair , as well as the commonsense selected by our algorithm , we conduct two independent evaluations : ( 1 ) was any external commonsense knowledge necessary for answering the question ? ; ( 2 ) were the commonsense relations provided by our algorithm relevant to the question ? The result for these two evaluations as well as how they overlap with each other are shown in , where we see that 50 % of the cases required external commonsense knowledge , and on a majority ( 34 % ) of those cases our algorithm was able to select the correct / relevant commonsense information to fill in gaps of inference .",result
text_generation,3,We find that the model performs badly for the inputs with unseen words .,analysis,Error Analysis,0,133,3,3,0,analysis : Error Analysis,0.9432624113475178,0.42857142857142855,0.42857142857142855,We find that the model performs badly for the inputs with unseen words ,14,"Although our model achieves the best performance , there are still several failure cases .","For instance , given "" Bonjour "" as the input , it generates "" Stay out of here "" as the output .",result
named-entity-recognition,6,We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia .,abstract,abstract,0,7,5,5,0,abstract : abstract,0.0330188679245283,0.5,0.5,We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia ,26,"In this work , we show that this is unfair : lexical features are actually quite useful .","From this , we compute - offline - a feature vector representing each word .",abstract
natural_language_inference,37,Question : Which British general was killed at Khartoum in 1885 ? Answer : Gordon Context : In February,method,Handling Noisy Labels,0,51,13,2,0,method : Handling Noisy Labels,0.19844357976653693,0.43333333333333335,0.10526315789473684,Question Which British general was killed at Khartoum in 1885 Answer Gordon Context In February,15, ,Question : Which British general was killed at Khartoum in 1885 ? Answer : Gordon Context : In February,method
machine-translation,7,The capacity of a neural network to absorb information is limited by its number of parameters .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.010723860589812331,0.04878048780487805,0.04878048780487805,The capacity of a neural network to absorb information is limited by its number of parameters ,17, ,"Conditional computation , where parts of the network are active on a per-example basis , has been proposed in theory as away of dramatically increasing model capacity without a proportional increase in computation .",abstract
sentiment_analysis,25,"Such models include Attention - based LSTM with Aspect Embedding ( ATAE - LSTM ) for ACSA ; Target - Dependent Sentiment Classification ( TD - LSTM ) , Gated Neural Networks and Recurrent Attention Memory Network ( RAM ) for ATSA .",introduction,introduction,0,24,12,12,0,introduction : introduction,0.10810810810810813,0.4,0.4,Such models include Attention based LSTM with Aspect Embedding ATAE LSTM for ACSA Target Dependent Sentiment Classification TD LSTM Gated Neural Networks and Recurrent Attention Memory Network RAM for ATSA ,31,"Many existing models use LSTM layers to distill sentiment information from embedding vectors , and apply attention mechanisms to enforce models to focus on the text spans related to the given aspect / entity .",Attention mechanisms has been successfully used in many NLP tasks .,introduction
relation_extraction,4,presents our results .,evaluation,evaluation,0,162,28,28,0,evaluation : evaluation,0.7826086956521741,0.9655172413793104,0.9655172413793104,presents our results ,4,"We then plugin the corresponding relation extractor trained on TACRED , keeping all other modules unchanged .","We find that : ( 1 ) by only training our logistic regression model on TACRED ( in contrast to on the 2 million bootstrapped examples used in the 2015 Stanford system ) and combining it with patterns , we obtain a higher hop - 0 F 1 score than the 2015 Stanford sys -",result
sentiment_analysis,47,"In summary , when employing deep neural networks for aspect - based sentiment classification , the following two problems remain to be further studied :",introduction,introduction,0,35,21,21,0,introduction : introduction,0.16279069767441862,0.5384615384615384,0.5384615384615384,In summary when employing deep neural networks for aspect based sentiment classification the following two problems remain to be further studied ,22,"We can see that the representations of contexts are related to targets , meanwhile it is natural that targets are influenced by their contexts .",Problem 1 : how to more efficiently represent the target especially when the target contains multiple words ? Problem 2 : how to utilize the interaction between targets and contexts to capture the most important words in the representation of targets and contexts ?,introduction
sentiment_analysis,8,"Overall , we can conclude that our simple ML methods are very robust to have achieved comparable performance even though they are modeled to predict six - classes as opposed to four in previous works .",result,RESULTS,1,219,23,23,0,result : RESULTS,0.9358974358974358,0.7666666666666667,1.0,Overall we can conclude that our simple ML methods are very robust to have achieved comparable performance even though they are modeled to predict six classes as opposed to four in previous works ,34,"We can say that the textual features helped incorrect classification of "" angry "" and "" happy "" classes whereas the audio features enabled the model to detect "" sad "" better .", ,result
natural_language_inference,24,Experiments results on MS MARCO,result,SAN:,0,200,68,58,0,result : SAN:,0.8547008547008547,0.85,0.8285714285714286,Experiments results on MS MARCO,5,"The only exception is the Why questions , but there is too little data to derive strong conclusions .","MS MARCO is a large scale real - word RC dataset which contains 100,100 ( 100K ) queries collected from anonymized user logs from the Bing search engine .",result
natural_language_inference,40,"Adding the same information as one - hot features fails to improve the performance , which indicates that the inductive bias we employ on MAGE is useful .",performance,Performance Comparison,1,184,15,15,0,performance : Performance Comparison,0.6642599277978339,0.5,0.5,Adding the same information as one hot features fails to improve the performance which indicates that the inductive bias we employ on MAGE is useful ,26,"Moreover , we observe that the proposed MAGE architecture can substantially improve the performance for both bi - GRUs and GAs .","The DAG - RNN baseline from and the shared version of MAGE ( where edge representations are tied ) also perform worse , showing that our proposed architecture is superior .",result
question_generation,0,"NQG + Based on NQG , we incorporate copy mechanism to deal with rare words problem .",experiment,Experiments and Results,1,78,16,16,0,experiment : Experiments and Results,0.42857142857142855,0.7619047619047619,0.7619047619047619,NQG Based on NQG we incorporate copy mechanism to deal with rare words problem ,15,We extend the s 2s+ att with our feature - rich encoder to build the NQG system .,"NQG + Pretrain Based on NQG + , we initialize the word embedding matrix with pre-trained GloVe vectors .",experiment
text-to-speech_synthesis,1,"Furthermore , FastSpeech achieves 270x speedup on mel-spectrogram generation and 38x speedup on final speech synthesis compared with the autoregressive Transformer TTS model , almost eliminates the problem of word skipping and repeating , and can adjust voice speed smoothly .",introduction,introduction,0,39,26,26,0,introduction : introduction,0.1780821917808219,0.9629629629629628,0.9629629629629628,Furthermore FastSpeech achieves 270x speedup on mel spectrogram generation and 38x speedup on final speech synthesis compared with the autoregressive Transformer TTS model almost eliminates the problem of word skipping and repeating and can adjust voice speed smoothly ,39,"The results show that in terms of speech quality , FastSpeech nearly matches the autoregressive Transformer model .",We attach some audio files generated by our method in the supplementary materials .,introduction
sarcasm_detection,1,"In such scenarios , potential solutions would be to create user networks and derive information from similar users within the network .",analysis,F1,0,328,31,3,0,analysis : F1,0.9820359281437124,0.96875,0.75,In such scenarios potential solutions would be to create user networks and derive information from similar users within the network ,21,Acc. misclassifications were common for users with lesser historical posts .,These are some of the issues which we plan to address in future work .,result
natural_language_inference,53,"Since it is easier to linearly separate in high dimension , especially with logistic regression , it is not surprising that increased embedding sizes lead to increased performance for almost all models .",model,Embedding size,1,153,12,2,0,model : Embedding size,0.7355769230769231,0.19672131147540986,0.2,Since it is easier to linearly separate in high dimension especially with logistic regression it is not surprising that increased embedding sizes lead to increased performance for almost all models ,31, ,"However , this is particularly true for some mod - :",method
natural_language_inference,58,shows the graph reachability statistics on the two datasets .,system description,Graph Reachability Task,0,279,46,22,0,system description : Graph Reachability Task,0.8328358208955224,0.4646464646464646,0.6470588235294118,shows the graph reachability statistics on the two datasets ,10,Duplicated edges are removed .,"In , we show examples of a small graph and a large graph in the synthetic dataset .",method
sentiment_analysis,5,"To the best of our knowledge , this is the first work to integrate the electrodes ' discrepancy relation on two hemispheres into deep learning models to improve EEG emotion recognition .",introduction,introduction,0,56,47,47,0,introduction : introduction,0.21132075471698114,0.903846153846154,0.903846153846154,To the best of our knowledge this is the first work to integrate the electrodes discrepancy relation on two hemispheres into deep learning models to improve EEG emotion recognition ,30,"Considering the tremendous data distribution shift of EEG emotional signal , especially in the case of subject - independent task where the source ( training ) and target ( testing ) data come from different subjects , we leverage a domain discriminator that works cooperatively with the classifier to encourage the emotion - related but domain - invariant data representation appeared .",The experimental results verify the discrimination and effectiveness of this differential information between the left and right hemispheres for EEG emotion recognition .,introduction
named-entity-recognition,9,"We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER , RE and QA .",method,Fine-tuning BioBERT,0,77,31,3,0,method : Fine-tuning BioBERT,0.3869346733668342,0.62,0.13636363636363635,We fine tune BioBERT on the following three representative biomedical text mining tasks NER RE and QA ,18,"With minimal architectural modification , BioBERT can be applied to various downstream text mining tasks .","Named entity recognition is one of the most fundamental biomedical text mining tasks , which involves recognizing numerous domain - specific proper nouns in a biomedical corpus .",method
sentiment_analysis,17,"task , or it can learn to preserve the representation of sentiment - rich children for sentiment classification .",system description,Tree-Structured LSTMs,0,83,44,12,0,system description : Tree-Structured LSTMs,0.3688888888888889,0.5641025641025641,0.75,task or it can learn to preserve the representation of sentiment rich children for sentiment classification ,17,"Labeled edges correspond to gating by the indicated gating vector , with dependencies omitted for compactness .","As with the standard LSTM , each Tree - LSTM unit takes an input vector x j .",method
sentiment_analysis,1,DE extends the idea of Shannon entropy and measures the complexity of a continuous random variable .,dataset,Datasets,0,288,9,9,0,dataset : Datasets,0.7272727272727273,0.5,0.5,DE extends the idea of Shannon entropy and measures the complexity of a continuous random variable ,17,"To make a fair comparison with existing studies , we directly use the pre-computed differential entropy ( DE ) features smoothed by linear dynamic systems ( LDS ) , in SEED .","For a fixed length EEG segment , DE features are computed as the logarithm energy spectrum in a certain frequency band .",experiment
sentiment_analysis,48,The architecture is depicted in .,method,Method Description,0,70,8,8,0,method : Method Description,0.2966101694915254,0.12307692307692307,0.4705882352941176,The architecture is depicted in ,6,the goal is to utilize S u to improve the classification performance over the supervised model using S l only .,"The method consists of three main components , i.e. , the classifier , the encoder , and the decoder .",method
sentiment_analysis,45,Sentiment analysis ( SA ) is an important task in natural language processing .,introduction,introduction,0,8,2,2,0,introduction : introduction,0.05555555555555555,0.07142857142857142,0.07142857142857142,Sentiment analysis SA is an important task in natural language processing ,12, ,"It solves the computational processing of opinions , emotions , and subjectivity - sentiment is collected , analyzed and summarized .",introduction
sentiment_analysis,41,shows the representation of how attention focuses on words with the influence of a given aspect .,analysis,Qualitative Analysis,0,195,4,4,0,analysis : Qualitative Analysis,0.8744394618834079,0.16,0.2857142857142857,shows the representation of how attention focuses on words with the influence of a given aspect ,17,We can obtain the attention weight ? in Equation 8 and visualize the attention weights accordingly .,We use a visualization tool Heml ( Deng,result
text_generation,3,"Automatic dialogue generation task is of great importance to many applications , ranging from open - domain chatbots to goal - oriented technical support agents .",introduction,introduction,1,12,2,2,0,introduction : introduction,0.0851063829787234,0.09090909090909093,0.09090909090909093,Automatic dialogue generation task is of great importance to many applications ranging from open domain chatbots to goal oriented technical support agents ,23, ,"Recently there is an increasing amount of studies about purely datadriven dialogue models , which learn from large corpora of human conversations without handcrafted rules or templates .",introduction
paraphrase_generation,1,"In addition , unlike the standard VAE , note that our VAE decoder model p ? ( x ( p ) | z , x ( o ) ) is also conditioned on the vector representation x ( o ) of the original sentence .",model,Model Architecture,0,75,7,7,0,model : Model Architecture,0.3393665158371041,0.2692307692307692,0.2692307692307692,In addition unlike the standard VAE note that our VAE decoder model p x p z x o is also conditioned on the vector representation x o of the original sentence ,32,"shows a macro view ( without the LSTM ) of our proposed model architecture , which is essentially a VAE based generative model for each paraphrase 's vector representation x ( p ) , which in turn is generated by a latent code z and the original sentence x o .","In particular , as shows , the VAE encoder as well as decoder are conditioned on the original sentence .",method
sentence_compression,2,"In the experiments below , we learn models to predict the first pass duration of word fixations and the total duration of regressions to a word .",system description,Our contributions,0,30,11,11,0,system description : Our contributions,0.297029702970297,0.22448979591836726,0.44,In the experiments below we learn models to predict the first pass duration of word fixations and the total duration of regressions to a word ,26,"Being able to identify constituent borders is important for sentence compression , and reading fixation data may help our model learn a representation of our data that makes it easy to identify constituent boundaries .",These two measures constitute a perfect separation of the total reading time of each word split between the first pass and subsequent passes .,method
natural_language_inference,75,The main results are given in .,model,WikiMovies,0,168,119,8,0,model : WikiMovies,0.8275862068965517,0.7727272727272727,0.3076923076923077,The main results are given in ,7,"As MemNNs do not support key - value pairs , we concatenate key and value together when they differ instead .","The QA system of outperforms Supervised Embeddings and Memory Networks for KB and IE - based KB representations , but is designed to work with a KB , not with documents ( hence the N / A in that column ) .",method
part-of-speech_tagging,6,identify two types of data - driven methodologies for dependency parsing : graph - based approaches and transition - based approaches .,introduction,introduction,0,10,3,3,0,introduction : introduction,0.06711409395973154,0.21428571428571427,0.21428571428571427,identify two types of data driven methodologies for dependency parsing graph based approaches and transition based approaches ,18,"Dependency parsing has become a key research topic in NLP in the last decade , boosted by the success of the shared tasks on multilingual dependency parsing .",Most traditional graph - or transition - based parsing approaches manually define a set of core and combined features associated with one - hot representations .,introduction
named-entity-recognition,1,"The model is parameterized by defining a probability distribution over actions at each time step , given the current contents of the stack , buffer , and output , as well as the history of actions taken .",training,Chunking Algorithm,0,106,38,8,0,training : Chunking Algorithm,0.5120772946859904,0.4470588235294118,0.5333333333333333,The model is parameterized by defining a probability distribution over actions at each time step given the current contents of the stack buffer and output as well as the history of actions taken ,34,"The algorithm is depicted in , which shows the sequence of operations required to process the sentence Mark Watney visited Mars .","Following , we use stack LSTMs to compute a fixed dimensional embedding of each of these , and take a concatenation of these to obtain the full algorithm state .",experiment
sentiment_analysis,31,"After that , a pooling operation is applied over the feature map to get one single general sentiment feature ? gin each map .",system description,Convolutional Neural Networks,0,64,20,14,0,system description : Convolutional Neural Networks,0.4025157232704403,0.35714285714285715,0.7368421052631579,After that a pooling operation is applied over the feature map to get one single general sentiment feature gin each map ,22,"Sliding the filter window from the beginning of the word matrix till the end , we get a feature map","After that , a pooling operation is applied over the feature map to get one single general sentiment feature ? gin each map .",method
sentiment_analysis,24,"Following existing works for AE , we use F1 to measure the performance of aspect term extraction and opinion term extraction , which are denoted as F1 - a and F1 -o respectively .",method,Learning,0,183,128,55,0,method : Learning,0.5884244372990354,0.9552238805970148,0.9016393442622952,Following existing works for AE we use F1 to measure the performance of aspect term extraction and opinion term extraction which are denoted as F1 a and F1 o respectively ,31,"We employ five metrics for evaluation , where two measure the AE performance , two measure the AS performance , and one measures the over all performance .","Following existing works for AS , we adopt accuracy and macro - F1 to measure the performance of AS .",method
sentiment_analysis,46,"All the weight matrices are initialized as random orthogonal matrices , and we set all the bias vectors as zero vectors .",implementation,Implementation Details,1,101,4,4,1,implementation : Implementation Details,0.8278688524590164,0.4,0.4,All the weight matrices are initialized as random orthogonal matrices and we set all the bias vectors as zero vectors ,21,"Kernel sizes of multi-gram convolution for Char - CNN are set to 2 , 3 , respectively .","We optimize the proposed model with RMSprop algorithm , using mini-batch training .",experiment
text_summarization,10,"Following , we also report ME - TEOR using the MS - COCO evaluation script .",model,Fast Multi-Task Training,0,132,87,12,0,model : Fast Multi-Task Training,0.5019011406844106,0.935483870967742,0.6666666666666666,Following we also report ME TEOR using the MS COCO evaluation script ,13,"Following previous work , we use ROUGE full - length F 1 variant for all our results .",Human Evaluation Criteria :,method
prosody_prediction,0,"As the proposed dataset has been automatically generated as described in Section 3 , we also tested the best two models , BERT and BiLSTM , with a manually annotated test set from The Boston University radio news corpus . :",result,Results,1,127,23,23,0,result : Results,0.6614583333333334,0.696969696969697,0.7666666666666667,As the proposed dataset has been automatically generated as described in Section 3 we also tested the best two models BERT and BiLSTM with a manually annotated test set from The Boston University radio news corpus ,37,"However , between 5 % and 100 % the BiLSTM model improvement is almost linear .",Experimental results ( % ) for the 2 and 3 - way classification tasks .,result
natural_language_inference,39,"Neural networks and distributed representations can alleviate such sparsity , thus neural network - based models are widely used by recent systems for the STS problem .",introduction,introduction,0,11,4,4,0,introduction : introduction,0.05339805825242718,0.2352941176470588,0.2352941176470588,Neural networks and distributed representations can alleviate such sparsity thus neural network based models are widely used by recent systems for the STS problem ,25,"Traditional NLP approaches , e.g. , developing hand - crafted features , suffer from sparsity because of language ambiguity and the limited amount of annotated data available .","However , most previous neural network approaches are based on sentence modeling , which first maps each input sentence into a fixed - length vector and then performs comparisons on these representations .",introduction
semantic_role_labeling,1,We refer the reader to Dozat and Manning ( 2017 ) for more details .,model,Syntactically-informed self-attention,0,97,64,17,1,model : Syntactically-informed self-attention,0.44495412844036697,0.7272727272727273,0.8095238095238095,We refer the reader to Dozat and Manning 2017 for more details ,13,"We also predict dependency labels using perclass bi-affine operations between parent and dependent representations Q parse and K parse to produce per-label scores , with locally normalized probabilities over dependency labels y dep t given by the softmax function .","This attention head now becomes an oracle for syntax , denoted P , providing a dependency parse to downstream layers .",method
question-answering,3,"To cope with the third challenge , we assume that each semantic unit ( word ) can be partially matched , and can be decomposed into a similar component and a dissimilar component based on its semantic matching vector .",model,Model Overview,0,58,5,5,0,model : Model Overview,0.2283464566929134,0.05,0.5,To cope with the third challenge we assume that each semantic unit word can be partially matched and can be decomposed into a similar component and a dissimilar component based on its semantic matching vector ,36,"To tackle the second challenge , we assume that each word can be semantically matched by several words in the other sentence , and we calculate a semantic matching vector for each word vector based on all the word vectors in the other side .",shows an overview of our sentence similarity model .,method
natural_language_inference,45,The gated attention ( GA ) reader defines an element - wise product asp i g i where g i is agate computed by attention mechanism on the token pi and the entire query .,system description,DOCUMENT-QUERY FINE-GRAINED GATING,0,121,53,8,0,system description : DOCUMENT-QUERY FINE-GRAINED GATING,0.6080402010050251,0.828125,0.42105263157894735,The gated attention GA reader defines an element wise product asp i g i where g i is agate computed by attention mechanism on the token pi and the entire query ,32,"AoA imposes pair - wise interactions between the document and the query , but using a dot product is potentially not expressive enough and hard to generalize to multi - layer networks .",The intuition for the gate g i is to attend to important information in the document .,method
natural_language_inference,20,SciTail : SciTail ) is an NLI dataset created from multiple - choice science exams consisting of 27 k sentence pairs .,evaluation,SNLI:,0,105,23,15,0,evaluation : SNLI:,0.4320987654320987,0.2446808510638297,0.5,SciTail SciTail is an NLI dataset created from multiple choice science exams consisting of 27 k sentence pairs ,19,For our experiment with the annotation dataset we use the annotations for the MultiNLI mismatched dataset .,Each question and the correct answer choice have been converted into an assertive statement to form the hypothesis .,result
natural_language_inference,95,"However , as is discussed in Section 1 , there could be multiple answer candidates from different passages and some of them may mislead the MRC model to make an incorrect prediction .",model,Cross-Passage Answer Verification,0,108,41,3,0,model : Cross-Passage Answer Verification,0.4615384615384616,0.82,0.25,However as is discussed in Section 1 there could be multiple answer candidates from different passages and some of them may mislead the MRC model to make an incorrect prediction ,31,"The boundary model and the content model focus on extracting and modeling the answer within a single passage respectively , with little consideration of the cross - passage information .",It 's necessary to aggregate the information from different passages and choose the best one from those candidates .,method
natural_language_inference,99,The Gated Recurrent Unit ) which is variant from LSTM ( Hochreiter and Schmidhuber 1997 ) is employed throughout our model .,system description,Implemental Details,0,192,4,4,1,system description : Implemental Details,0.7619047619047619,0.4444444444444444,0.4444444444444444,The Gated Recurrent Unit which is variant from LSTM Hochreiter and Schmidhuber 1997 is employed throughout our model ,19,The size of char - level embedding is also set as 100 - dimensional and is obtained by CNN filters under the instruction of ( Kim 2014 ) .,We adopt the AdaDelta ( Zeiler 2012 ) optimizer for training with an initial learning rate of 0.0005 .,method
natural_language_inference,84,"Representations of rare words trained directly on end tasks are usually poor , requiring us to pre-train embeddings on external data , or treat all rare words as out - of - vocabulary words with a unique representation .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.026086956521739132,0.5714285714285714,0.5714285714285714,Representations of rare words trained directly on end tasks are usually poor requiring us to pre train embeddings on external data or treat all rare words as out of vocabulary words with a unique representation ,36,"Learning representations for words in the "" long tail "" of this distribution requires enormous amounts of data .",We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained end - to - end for the downstream task .,abstract
natural_language_inference,69,"In fact , if we were not to address this issue , a model designed to exploit these regularities could achieve 74.6 % accuracy ( detailed in Section 6 ) .",system description,Document-Answer Correlations,0,128,3,3,0,system description : Document-Answer Correlations,0.3710144927536232,0.1111111111111111,0.2727272727272727,In fact if we were not to address this issue a model designed to exploit these regularities could achieve 74 6 accuracy detailed in Section 6 ,27,problem unique to our multi-document setting is the possibility of spurious correlations between candidates and documents induced by the graph traversal method .,"Concretely , we observed that certain documents frequently co-occur with the correct answer , independently of the query .",method
temporal_information_extraction,0,"The TempEval - 3 - platinum corpus , containing 20 news articles , was annotated / reviewed by the TempEval - 3 organizers .",dataset,dataset,0,133,4,4,0,dataset : dataset,0.7037037037037037,0.25,0.25,The TempEval 3 platinum corpus containing 20 news articles was annotated reviewed by the TempEval 3 organizers ,18,"The TimeBank 1.2 corpus contains 183 documents coming from a variety of news report , specifically from the ACE program and PropBank , while the AQUAINT corpus contains 73 news report documents and often referred to as the Opinion corpus .",The TimeBank - Dense corpus is created to address the sparsity issue in the existing Tim eML corpora .,experiment
natural_language_inference,11,but uh these guys tion about candidate answers in DQA suggest that our knowledge incorporating strategy is exploiting heterogeneous knowledge sources in semantically sensible ways .,analysis,Qualitative Analysis,0,182,21,21,0,analysis : Qualitative Analysis,0.6594202898550725,1.0,1.0,but uh these guys tion about candidate answers in DQA suggest that our knowledge incorporating strategy is exploiting heterogeneous knowledge sources in semantically sensible ways ,26,His off - the - cuff style seems amateurish the net cost of operations ., ,result
sentiment_analysis,1,"where S = D ? 1 2 AD 1 2 , and W = W L?1 W L?2 ... W 0 .",system description,Simple Graph Convolution Network (SGC),0,151,79,30,0,system description : Simple Graph Convolution Network (SGC),0.3813131313131313,0.38164251207729466,0.967741935483871,where S D 1 2 AD 1 2 and W W L 1 W L 2 W 0 ,19,l across all layers into one linear transformation W as follows :,"Essentially , SGC computes a topology - aware linear trans - formation X = S L X , followed by one final linear transformation Z = XW .",method
named-entity-recognition,6,"We establish a new state - of - the - art F1 score of 87.95 on ONTONOTES 5.0 , while matching state - of - the - art performance with a F 1 score of 91.73 on the over - studied CONLL - 2003 dataset .",abstract,abstract,0,10,8,8,0,abstract : abstract,0.04716981132075472,0.8,0.8,We establish a new state of the art F1 score of 87 95 on ONTONOTES 5 0 while matching state of the art performance with a F 1 score of 91 73 on the over studied CONLL 2003 dataset ,40,"When used with a vanilla recurrent neural network model , this representation yields substantial improvements .",This work is licensed under a Creative Commons Attribution 4.0 International License .,abstract
natural_language_inference,37,"The many document - dependent questions stem from the fact that questions are frequently about the subject of the document , so the article 's title is often sufficient to resolve coreferences or ambiguities that appear in the question .",result,Results,0,212,40,40,0,result : Results,0.8249027237354085,0.7407407407407407,0.7407407407407407,The many document dependent questions stem from the fact that questions are frequently about the subject of the document so the article s title is often sufficient to resolve coreferences or ambiguities that appear in the question ,38,"We find 67.4 % of the questions to be contextindependent , 22.6 % to be document - dependent , and the remaining 10 % to be paragraphdependent .","Since a reasonably high fraction of the questions can be understood given the document they are from , and to isolate our analysis from the retrieval mechanism used , we choose to evaluate on the document - level .",result
text_summarization,2,We consider six categories of structural labels in this work ; they are presented in .,approach,Structural info,0,95,44,3,0,approach : Structural info,0.3417266187050361,0.33587786259541985,0.1875,We consider six categories of structural labels in this work they are presented in ,15,"Example ( 1 ) depth in the dependency parse tree 0 ( 2 ) label of the incoming edge ' root ' ( 3 ) number of outgoing edges 3 ( 4 ) part - of - speech tag ' VBD ' ( 5 ) absolution position in the source text 9 ( 6 ) relative position in the source text ( 0.5 , 0.6 ] , we hypothesize that structural labels , such as the incoming dependency arc and the depth in a dependency parse tree , can be helpful to predict word importance .","Each structural label is mapped to a fixed - length , trainable structural embedding .",method
natural_language_inference,85,We used the same split as in and other previous work .,experiment,Experimental Setup,0,160,5,5,0,experiment : Experimental Setup,0.6808510638297872,0.7142857142857143,0.7142857142857143,We used the same split as in and other previous work ,12,"As in the related work , we remove this category .",The parse trees used in this paper are produced by the Stanford PCFG Parser 3.5.3 and they are delivered as part of the SNLI corpus .,experiment
natural_language_inference,65,"In this task , given a question q and an candidate answer pool P = {a 1 , a 2 , , a p } for this question , the goal is to search for and select the candidate answer a ? P that correctly answers q .",introduction,introduction,0,25,17,17,0,introduction : introduction,0.10869565217391304,0.5151515151515151,0.5151515151515151,In this task given a question q and an candidate answer pool P a 1 a 2 a p for this question the goal is to search for and select the candidate answer a P that correctly answers q ,40,"In this work , we perform an extensive number of experiments on applying attentive pooling CNNs ( AP - CNN ) and biLSTMs ( AP - biLSTM ) for the answer selection task .","In this task , given a question q and an candidate answer pool P = {a 1 , a 2 , , a p } for this question , the goal is to search for and select the candidate answer a ? P that correctly answers q .",introduction
text_summarization,13,an image of twitter founder jack dorsey in < unk > was posted alongside a rant in arabic </ s > ... GOLD : diatribe in arabic posted anonymously yesterday and shared online FIRST : isis supporters have vowed to murder twitter staff because they believe the site 's policy of shutting down their extremist pages is a ' virtual war ' .,result,result,0,228,26,26,0,result : result,0.8539325842696629,0.8666666666666667,0.8666666666666667,an image of twitter founder jack dorsey in unk was posted alongside a rant in arabic s GOLD diatribe in arabic posted anonymously yesterday and shared online FIRST isis supporters have vowed to murder twitter staff because they believe the site s policy of shutting down their extremist pages is a virtual war ,54,"mocked - up image of the site 's founder jack dorsey in < unk > was posted yesterday alongside a diatribe written in arabic , which claimed twitter employees ' necks are ' a target for the soldiers of the caliphate ' . </ s > addressing mr dorsey personally , it claimed twitter was taking sides in a ' media war ' which allowed ' slaughter ' , adding : ' your virtual war onus will cause are al war on you . </ s >",ILP : ISIS supporters have vowed to murder Twitter staff because they believe the site 's policy of shutting down their extremist pages is a ' virtual war ' .,result
natural_language_inference,43,"Overall , we compute a total of 892 features over the dataset .",model,Candidate Ranking,0,100,23,16,0,model : Candidate Ranking,0.6451612903225806,1.0,1.0,Overall we compute a total of 892 features over the dataset ,12,"Last , our context features are defined in a 6 - word window around the candidate answer mention , where the feature value decays exponentially as the distance from the candidate answer mention grows .","contains 1,300 training examples and 800 test examples .",method
relation_extraction,2,Entity Attention Bi-LSTM 85.2 R- BERT 89.25,model,CR,0,115,10,4,0,model : CR,0.8518518518518519,0.4,0.21052631578947367,Entity Attention Bi LSTM 85 2 R BERT 89 25,10,Att-Pooling-CNN 88.0,entities in the sentence and discard the hidden vector output of the two entities from concatenating with the hidden vector output of the sentence .,method
text_summarization,6,Our model DRGD achieves the best summarization performance on all the ROUGE metrics .,experiment,Experimental Settings,0,223,16,16,0,experiment : Experimental Settings,0.8511450381679391,0.8,0.8,Our model DRGD achieves the best summarization performance on all the ROUGE metrics ,14,"Actually , the performance of the standard and respectively .","Although ASC + FSC 1 also uses a generative method to model the latent summary variables , the representation ability is limited and it can not bring in noticeable improvements .",experiment
sentiment_analysis,44,This Neural net also shares the same structure as the one in .,model,Neural Net Models,0,144,39,16,1,model : Neural Net Models,0.6545454545454545,0.4814814814814815,1.0,This Neural net also shares the same structure as the one in ,13,Net takes as input the compressed vector representation for two Dis - course Unit thatare determined to be connected in the Discourse Tree and learns the sentiment label for the parent node ., ,method
text_summarization,12,The sentence encoder and decoder are built with recurrent neural networks .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.02631578947368421,0.5,0.5,The sentence encoder and decoder are built with recurrent neural networks ,12,"It consists of a sentence encoder , a selective gate network , and an attention equipped decoder .",The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder .,abstract
text-classification,0,Long - short term memory .,method,Deep Learning Methods,0,131,29,11,0,method : Deep Learning Methods,0.5770925110132159,0.24369747899159666,0.6470588235294118,Long short term memory ,5,LSTM LSTM LSTM ... : long - short term memory,"We also offer a comparison with a recurrent neural network model , namely long - short term memory ( LSTM ) .",method
machine-translation,5,"As a result , only 0.17 million sentence pairs from the ParaCrawl corpus were used for training of the constrained systems .",system,Data Filtering,0,61,38,13,0,system : Data Filtering,0.42657342657342656,0.5135135135135135,0.8666666666666667,As a result only 0 17 million sentence pairs from the ParaCrawl corpus were used for training of the constrained systems ,22,"To reduce the high level of noise , this corpus was filtered using stricter content overlap ( a threshold of 0.3 instead of 0.1 ) and language adherence filters ( both the language detection and the valid alphabet filters had to validate a sentence pair instead of just one of the filters ) than all other corpora .","Due to the quality concerns , the corpus was not used for training of the unconstrained systems .",method
natural_language_inference,33,and demonstrate that neural ma -chine translation ( NMT ) systems appear to capture morphology and some syntactic properties .,introduction,introduction,0,21,9,9,0,introduction : introduction,0.08045977011494253,0.4090909090909091,0.4090909090909091,and demonstrate that neural ma chine translation NMT systems appear to capture morphology and some syntactic properties ,18,Understanding the inductive biases of distinct neural models is important for guiding progress in representation learning .,also present evidence that sequence - to - sequence parsers more strongly encode source language syntax .,introduction
text-classification,7,"In addition , the good results on Reuters - Full also indicate that the capsule network has robust superiority over competitors on single - label documents .",ablation,Single-Label to Multi-Label Text Classification,0,179,29,24,0,ablation : Single-Label to Multi-Label Text Classification,0.7366255144032922,0.5178571428571429,1.0,In addition the good results on Reuters Full also indicate that the capsule network has robust superiority over competitors on single label documents ,24,The capsule network has much stronger transferring capability than the conventional deep neural networks ., ,result
text_summarization,2,"This is because the ( source , summary ) pairs in the Gigaword test set are not pruned ( see 4.1 ) .",result,Results,0,230,18,18,0,result : Results,0.8273381294964028,0.5454545454545454,0.5454545454545454,This is because the source summary pairs in the Gigaword test set are not pruned see 4 1 ,19,Notice that the scores on the valid - 2000 dataset are generally higher than those of test - 1951 .,"In some cases , none ( or very few ) of the summary words appear in the source .",result
machine-translation,7,"The model with 4 always - active experts performed ( unsurprisingly ) similarly to the computationally - matched baseline models , while the largest of the models ( 4096 experts ) achieved an impressive 24 % lower perplexity on the test set .",performance,BALANCING EXPERT UTILIZATION,0,163,62,20,0,performance : BALANCING EXPERT UTILIZATION,0.43699731903485256,0.5344827586206896,0.5128205128205128,The model with 4 always active experts performed unsurprisingly similarly to the computationally matched baseline models while the largest of the models 4096 experts achieved an impressive 24 lower perplexity on the test set ,35,The results of these models are shown in - left .,"Varied Computation , High Capacity :",result
relation_extraction,0,"Our model significantly outperforms the feature - based structured perceptron model of , showing improvements on both entity and relation extraction on the ACE05 dataset .",introduction,introduction,0,30,22,22,0,introduction : introduction,0.1171875,0.8461538461538461,0.8461538461538461,Our model significantly outperforms the feature based structured perceptron model of showing improvements on both entity and relation extraction on the ACE05 dataset ,24,We also add an additional layer to our network to encode the output sequence from right - to - left and find significant improvement on the performance of relation identification using bi-directional encoding .,"In comparison to the dependency treebased LSTM model of , our model performs within 1 % on entities and 2 % on relations on ACE05 dataset .",introduction
sentiment_analysis,23,Dependency trees are another representation of sentence structures .,system description,d-TBCNN,0,116,80,2,0,system description : d-TBCNN,0.3972602739726027,0.7619047619047619,0.2,Dependency trees are another representation of sentence structures ,9, ,The nature of dependency representation leads to d-TBCNN 's major difference from traditional convolution : there exist nodes with different numbers of child nodes .,method
natural_language_inference,4,"In addition to the mixed objective , we improve dynamic coattention networks ( DCN ) with a deep residual coattention encoder that is inspired by recent work in deep self - attention and residual networks .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03535353535353535,0.7142857142857143,0.7142857142857143,In addition to the mixed objective we improve dynamic coattention networks DCN with a deep residual coattention encoder that is inspired by recent work in deep self attention and residual networks ,32,The objective uses rewards derived from word overlap to solve the mis alignment between evaluation metric and optimization objective .,"Our proposals improve model performance across question types and input lengths , especially for long questions that requires the ability to capture long - term dependencies .",abstract
natural_language_inference,3,"In this section , we present an end - to - end deep architecture for matching two sentences , as shown in .",model,Comparison of LC-LSTMs and word-by-word Attention LSTMs,0,95,72,5,0,model : Comparison of LC-LSTMs and word-by-word Attention LSTMs,0.4567307692307692,0.6371681415929203,1.0,In this section we present an end to end deep architecture for matching two sentences as shown in ,19,End - to - End Architecture for Sentence Matching, ,method
negation_scope_resolution,0,The SVM classifier delivered the best results .,approach,Machine Learning Approaches,0,108,31,31,0,approach : Machine Learning Approaches,0.4695652173913044,0.3406593406593407,0.9393939393939394,The SVM classifier delivered the best results ,8,"looked at negation cue detection and experimented with 3 methods : lexicon - based , syntaxbased ( both rule - based ) and an SVM classifier .",They collected their own dataset which had data from the biomedical domain .,method
natural_language_inference,19,"In practice , however , additive attention outperforms dot product attention for large values of d k .",system description,Dot-product Attention,0,73,12,6,0,system description : Dot-product Attention,0.28627450980392155,0.9230769230769232,0.8571428571428571,In practice however additive attention outperforms dot product attention for large values of d k ,16,"On implementation , dot-product attention is much faster and more space - efficient than additive attention due to optimized matrix multiplication .",So used scaled dot-product attention instead of normal dot -product attention to prevent performance loss in large dimension as following equation 5 .,method
natural_language_inference,91,An illustration of two standard designs for sentence encoders .,introduction,introduction,0,13,5,5,0,introduction : introduction,0.055793991416309016,0.2380952380952381,0.2380952380952381,An illustration of two standard designs for sentence encoders ,10,Common sentence encoders include sequence - based recurrent neural network * The first two authors contributed equally . :,"The TreeRNN , unlike the sequence - based RNN , requires a substantially different connection structure for each sentence , making batched computation impractical .",introduction
natural_language_inference,84,"When we use both , we combine the information coming from the embeddings e ( w ) and the definition embeddings ed ( w ) by computing e c ( w ) = e (w ) + W ed ( w ) , where Wis a trainable matrix , or just by simply summing the two , e c ( w ) = e (w ) + ed ( w ) .",system description,ON THE FLY EMBEDDINGS,0,90,23,23,0,system description : ON THE FLY EMBEDDINGS,0.39130434782608703,0.2072072072072072,0.7419354838709677,When we use both we combine the information coming from the embeddings e w and the definition embeddings ed w by computing e c w e w W ed w where Wis a trainable matrix or just by simply summing the two e c w e w ed w ,50,"While the primary purpose of definition embeddings ed ( w ) is to inform the network about the rare words , they might also contain useful information for the words in V train .",Alternatively it is possible to just use e c ( w ) = e ( w ) for w from V train and e c ( w ) = ed ( w ) otherwise .,method
temporal_information_extraction,1,"For example , relations such as immediately before or immediately after barely exist in a corpus compared to before and after .",system description,Temporal Relation Types,0,61,7,7,0,system description : Temporal Relation Types,0.2373540856031129,0.3333333333333333,0.7,For example relations such as immediately before or immediately after barely exist in a corpus compared to before and after ,21,The non-uniform distribution of all the relation types makes it difficult to separate lowfrequency ones from the others ( see in ) .,"Due to the ambiguity in natural language , determining relations like before and immediately before can be a difficult task itself .",method
sentence_compression,1,Joint PoS tags of selected tokens .,baseline,Baseline,0,58,21,21,0,baseline : Baseline,0.27751196172248804,0.2441860465116279,0.2441860465116279,Joint PoS tags of selected tokens ,7,"Ultimately , our implementation of McDonald 's model contained 463,614 individual features , summarized in three categories :","Unigram , bigram and trigram PoS context of selected and dropped tokens .",result
natural_language_inference,20,"In particular , our results are close to the current state of the art on SNLI in this category and strong on both , the matched and mismatched test sets of MultiNLI .",evaluation,Model Performance on the NLI task,1,125,43,5,0,evaluation : Model Performance on the NLI task,0.51440329218107,0.4574468085106383,0.625,In particular our results are close to the current state of the art on SNLI in this category and strong on both the matched and mismatched test sets of MultiNLI ,31,It clearly outperforms the similar but non-hierarchical BiLSTM models reported in the literature and fares well in comparison to other state of the art architectures in the sentence encoding category .,"Finally , on SciTail , we achieve the new state of the art with an accuracy of 86.0 % .",result
natural_language_inference,3,"Recently , there is rising interest in modelling the interactions of two sentences with deep neural networks .",abstract,abstract,1,4,2,2,0,abstract : abstract,0.019230769230769232,0.2857142857142857,0.2857142857142857,Recently there is rising interest in modelling the interactions of two sentences with deep neural networks ,17, ,"However , most of the existing methods encode two sequences with separate encoders , in which a sentence is encoded with little or no information from the other sentence .",abstract
sentiment_analysis,47,We first make use of an average representation of the target to obtain better representations of left and right contexts .,model,Rotatory Attention Mechanism,0,90,20,6,0,model : Rotatory Attention Mechanism,0.4186046511627907,0.4444444444444444,0.5,We first make use of an average representation of the target to obtain better representations of left and right contexts ,21,1 ) Target2 Context Attention,An average pooling operation is used to obtain the simple representation of target phrase :,method
text_summarization,1,"During inference , m is sampled from SELECTOR and fed to the generator .",training,Training,0,124,6,6,0,training : Training,0.5166666666666667,0.2857142857142857,0.2857142857142857,During inference m is sampled from SELECTOR and fed to the generator ,13,"During training , m guide acts as a target for SELECTOR and is a given input for generator ( teacher forcing ) .","In question generation , we set m guide t to 1 if there is a target question token which shares the same word stem with passage token x t .",experiment
natural_language_inference,59,Note that we tried multiple orders of character n-.,hyperparameters,hyperparameters,0,101,4,4,0,hyperparameters : hyperparameters,0.7481481481481481,0.8,0.8,Note that we tried multiple orders of character n ,10,"We examined whether self - attention helps or not for all model variants , and found that it does for our best model .","grams with n ? { 3 , 4 , 5 } both individually and separately but 5 - grams alone worked better than these alternatives .",experiment
text_summarization,5,"Our model is encouraged to generate according to human - written soft templates , which relatively diminishes copying from the source sentences .",evaluation,Linguistic Quality Evaluation,0,198,37,17,0,evaluation : Linguistic Quality Evaluation,0.7857142857142857,0.5873015873015873,0.8095238095238095,Our model is encouraged to generate according to human written soft templates which relatively diminishes copying from the source sentences ,21,"Therefore , the copy mechanism is severely overweighted in OpenNMT .","Look at the last row "" NEW NE "" .",result
machine-translation,9,"We quantize the filtered Glo Ve embeddings with the codes provided by the authors , and train the models based on the quantized embeddings .",analysis,SENTIMENT ANALYSIS,0,204,27,27,0,analysis : SENTIMENT ANALYSIS,0.710801393728223,0.3068181818181818,0.84375,We quantize the filtered Glo Ve embeddings with the codes provided by the authors and train the models based on the quantized embeddings ,24,We also show the results using normalized product quantization ( NPQ ) .,"To make the results comparable , we report the codebook size in numpy format .",result
question_generation,1,"For generalization , we have considered 5 tokens from each category of the Tags .",ablation,Tag net,0,275,25,13,0,ablation : Tag net,0.7015306122448979,0.17605633802816895,0.52,For generalization we have considered 5 tokens from each category of the Tags ,14,Each tag token is represented as a one - hot vector of the dimension of vocabulary size .,The embedding network consists of word embedding followed by temporal convolutions neural network followed by maxpooling network .,result
natural_language_inference,79,"It can be argued that this is because movie reviews are often short and compositionality plays an important role in deciding whether the review is positive or negative , as well as similarity between words does given the rather tiny size of the training set .",experiment,Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0,161,26,19,0,experiment : Sentiment Analysis with the Stanford Sentiment Treebank Dataset,0.6007462686567164,0.52,1.0,It can be argued that this is because movie reviews are often short and compositionality plays an important role in deciding whether the review is positive or negative as well as similarity between words does given the rather tiny size of the training set ,45,Socher et al . apply several methods to this dataset and find that their Recursive Neural Tensor Network works much better than bag - of - words model ., ,experiment
natural_language_inference,38,shows a simple example from the popular dataset SQuAD .,introduction,introduction,0,14,4,4,0,introduction : introduction,0.07407407407407407,0.12903225806451613,0.12903225806451613,shows a simple example from the popular dataset SQuAD ,10,It s task is to teach machine to understand the content of a given passage and then answer the question related to it .,"Many significant works are based on this task , and most of them focus on the improvement of a sequence model that is augmented with an attention mechanism .",introduction
natural_language_inference,96,Example : Someone is shown sitting on a fence and talking to the camera while pointing out horses .,system description,Stylistic models for adversarial filtering,0,165,107,25,0,system description : Stylistic models for adversarial filtering,0.4230769230769231,0.8916666666666667,0.9615384615384616,Example Someone is shown sitting on a fence and talking to the camera while pointing out horses ,18,"likely , if it completes the caption in a reasonable way ; unlikely , if it sounds ridiculous or impossible ; gibberish if it has such serious errors that it does n't feel like a valid English sentence .", ,method
natural_language_inference,23,"Then , two - layer BiLSTM with shortcut connection is used to encode the input words for both premise P and hypothesis H , i.e. , .",DETAILED CONFIGURATIONS IN THE ABLATION STUDY,APPLICATION TO NATURAL LANGUAGE INFERENCE,0,367,68,19,0,DETAILED CONFIGURATIONS IN THE ABLATION STUDY : APPLICATION TO NATURAL LANGUAGE INFERENCE,0.7153996101364523,0.7234042553191491,0.4222222222222222,Then two layer BiLSTM with shortcut connection is used to encode the input words for both premise P and hypothesis H i e ,24,The input vectors for both P and H are the same as the input vectors for context C described in Section 3 .,"Then , two - layer BiLSTM with shortcut connection is used to encode the input words for both premise P and hypothesis H , i.e. , .",result
topic_models,0,This section presents the topic ID results in terms of classification accuracy ( in % ) and cross-entropy ( CE ) on the test sets .,result,result,0,359,2,2,0,result : result,0.8713592233009708,0.07407407407407407,0.07407407407407407,This section presents the topic ID results in terms of classification accuracy in and cross entropy CE on the test sets ,22, ,Cross - entropy gives a notion of how confident the classifier is about its prediction .,result
natural_language_inference,71,"The input sequence contains 10 randomly located data steps and the rest are filled by "" noise "" .",experiment,Denoise Task,0,126,27,6,0,experiment : Denoise Task,0.586046511627907,0.45,0.4,The input sequence contains 10 randomly located data steps and the rest are filled by noise ,17,"Similarly to the labels of copying memory task above , an alphabet consists of symbols {a i } , i ? { 0 , 1 , , n ? 1 , n , n + 1 } , the first n of which represent data , and the remaining two represent "" noise "" and "" marker "" , respectively .","The RNN model is supposed to output those 10 data in a sequence after it sees the "" marker "" .",experiment
natural_language_inference,99,"After confirming , we scan the passage and refine the right answer we thought .",model,Interaction Modeling Layer,0,83,6,6,0,model : Interaction Modeling Layer,0.32936507936507936,0.0625,0.6666666666666666,After confirming we scan the passage and refine the right answer we thought ,14,"Sometimes we may not directly ensure the answer 's boundary , we go back and confirm the question .",We also check the answer for insurance .,method
natural_language_inference,40,As a baseline we also compare to the setting where coreference information is added as extra features at the input of the GRU .,experiment,Text Comprehension with Coreference,0,163,21,20,0,experiment : Text Comprehension with Coreference,0.5884476534296029,0.7777777777777778,0.7692307692307693,As a baseline we also compare to the setting where coreference information is added as extra features at the input of the GRU ,24,"To test our contribution , we replace the pair of bi - GRUs with the single MAGE - GRU model described for multiple sequences for computing the document and query representations , and compare the final performance .","Let M be the number of coreference chains for the ( d , q ) pair : we append a one - hot vector o t ? { 0 , 1 } M to the input of the GRU x t indicating which coreference chain , if any , that token is apart of .",experiment
text_summarization,3,"indicates the KL divergence between y * and x d l , and ? is a constant parameter that is tuned via adaption to the testing set .",system description,Distant Supervision (DS) for Model Adaption,0,142,87,14,0,system description : Distant Supervision (DS) for Model Adaption,0.5867768595041323,0.6641221374045801,0.2413793103448276,indicates the KL divergence between y and x d l and is a constant parameter that is tuned via adaption to the testing set ,25,The model adaption with the distant labels is defined as :,"indicates the KL divergence between y * and x d l , and ? is a constant parameter that is tuned via adaption to the testing set .",method
sentiment_analysis,32,"After learning ? , we test the instance by feeding the target with its contexts into the IAN model , and the label with the highest probability stands for the predicted sentiment polarity of the target .",training,Model Training,0,112,16,16,0,training : Model Training,0.4869565217391305,1.0,1.0,After learning we test the instance by feeding the target with its contexts into the IAN model and the label with the highest probability stands for the predicted sentiment polarity of the target ,34,"In order to avoid overfitting , we use dropout strategy to randomly omit half of the feature detectors on each training case .", ,experiment
sentiment_analysis,2,"The forward L - STM handles the sentence from left to right , and the backward LSTM processes it in the reverse order .",model,Word Representation,0,68,19,3,0,model : Word Representation,0.2995594713656388,0.3220338983050847,0.06976744186046513,The forward L STM handles the sentence from left to right and the backward LSTM processes it in the reverse order ,22,"Bidirectional LSTMs have been successfully applied to various NLP tasks , and it models the context dependency with the forward LSTM and the backward LSTM .","Therefore , we can obtain two hidden representation , and then concatenate the forward hidden state and backward hidden state of each word .",method
sentiment_analysis,25,"IAN has better performance than TD - LSTM and ATAE - LSTM , because two attention layers guides the representation learning of the context and the entity interactively .",analysis,ATSA,1,197,31,6,0,analysis : ATSA,0.8873873873873874,0.8611111111111112,0.5454545454545454,IAN has better performance than TD LSTM and ATAE LSTM because two attention layers guides the representation learning of the context and the entity interactively ,26,The models other than GCAE is based on LSTM and attention mechanisms .,"RAM also achieves good accuracy by combining multiple attentions with a recurrent neural network , but it needs more training time as shown in the following section .",result
machine-translation,0,Related Approaches : Neural Networks in Machine Translation,system description,Related Approaches: Neural Networks in Machine Translation,0,96,61,1,0,system description : Related Approaches: Neural Networks in Machine Translation,0.4383561643835616,0.7439024390243902,0.045454545454545456,Related Approaches Neural Networks in Machine Translation,7, , ,method
question_answering,4,"With augmentation , this problem is aggravated .",system description,Bidirectional Attention Connectors (BAC),0,64,21,21,0,system description : Bidirectional Attention Connectors (BAC),0.2490272373540856,0.20192307692307693,0.5384615384615384,With augmentation this problem is aggravated ,7,This is not only computationally undesirable but also require a large network at the end to reduce this vector .,"Hence , standard birectional attention is not suitable here .",method
sentiment_analysis,18,"Given a sentence and an opinion target ( also called aspect expression ) occurring in the sentence , the task aims to determine the sentiment polarity of the sentence towards the opinion target .",introduction,introduction,0,17,3,3,0,introduction : introduction,0.07112970711297073,0.07894736842105263,0.07894736842105263,Given a sentence and an opinion target also called aspect expression occurring in the sentence the task aims to determine the sentiment polarity of the sentence towards the opinion target ,31,Aspect - level sentiment classification is an important task in fine - grained sentiment analysis .,An opinion target or target for short refers to a word or a phrase ( a sequence of words ) describing an aspect of an entity .,introduction
machine-translation,2,Most competitive neural sequence transduction models have an encoder - decoder structure .,model,Model Architecture,0,43,2,2,0,model : Model Architecture,0.19196428571428573,0.01834862385321101,0.3333333333333333,Most competitive neural sequence transduction models have an encoder decoder structure ,12, ,"Here , the encoder maps an input sequence of symbol representations ( x 1 , ... , x n ) to a sequence of continuous representations z = ( z 1 , ... , z n ) .",method
natural_language_inference,96,"broad definition of annotation artifacts might include aspects besides lexical / stylistic features : for instance , certain events are less likely semantically regardless of the context ( e.g. riding a horse using a hose ) .",system description,Stylistic models for adversarial filtering,0,157,99,17,0,system description : Stylistic models for adversarial filtering,0.4025641025641026,0.825,0.6538461538461539,broad definition of annotation artifacts might include aspects besides lexical stylistic features for instance certain events are less likely semantically regardless of the context e g riding a horse using a hose ,33,"Only once the other stylistic models are added does the ensemble accuracy drop substantially , suggesting that our approach is effective at reducing stylistic artifacts .","For this work , we erred more conservatively and only filtered based on style .",method
named-entity-recognition,4,"small ? is preferred in most cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ? ( not shown ) .",analysis,Alternate layer weighting schemes,0,136,16,10,0,analysis : Alternate layer weighting schemes,0.5,0.2352941176470588,0.16129032258064516,small is preferred in most cases with ELMo although for NER a task with a smaller training set the results are insensitive to not shown ,26,"Averaging all biLM layers instead of using just the last layer improves F 1 another 0.3 % ( comparing "" Last Only "" to ?= 1 columns ) , and allowing the task model to learn individual layer weights improves F 1 another 0.2 % ( ?= 1 vs. ?= 0.001 ) .","small ? is preferred in most cases with ELMo , although for NER , a task with a smaller training set , the results are insensitive to ? ( not shown ) .",result
named-entity-recognition,8,"For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al. ; Seo et al. ( 2017 ) .",model,Fine-tuning BERT,0,143,65,3,1,model : Fine-tuning BERT,0.3695090439276486,0.8783783783783784,0.25,For applications involving text pairs a common pattern is to independently encode text pairs before applying bidirectional cross attention such as Parikh et al Seo et al 2017 ,29,Fine- tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks whether they involve single text or text pairs - by swapping out the appropriate inputs and outputs .,"BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences .",method
question_answering,0,"For an entity e ? V or a relation type r ? R we retrieve the label and tokenize it : l = {l 1 , l 2 . . . l n }.",system description,Gated Graph Neural Networks,0,116,67,10,0,system description : Gated Graph Neural Networks,0.3932203389830509,0.7444444444444445,0.7142857142857143,For an entity e V or a relation type r R we retrieve the label and tokenize it l l 1 l 2 l n ,26,This enables us to directly incorporate the information on millions of entities and hundreds of relation types from the KB .,"For an entity e ? V or a relation type r ? R we retrieve the label and tokenize it : l = {l 1 , l 2 . . . l n }.",method
sentiment_analysis,11,The dyadic conversations are present in the form of videos .,approach,Multimodal Feature Extraction,0,100,5,3,0,approach : Multimodal Feature Extraction,0.2906976744186047,0.1020408163265306,0.5,The dyadic conversations are present in the form of videos ,11,The first phase of CMN is to extract multimodal features of all utterances in the conversations .,Each utterance of a particular conversation is thus a small segment of the full video .,method
sentiment_analysis,8,Recent introduction of deep neural networks to the domain has also significantly improved the state - of - the - art performance .,system description,II. LITERATURE REVIEW,0,32,6,6,0,system description : II. LITERATURE REVIEW,0.13675213675213674,0.5454545454545454,0.6,Recent introduction of deep neural networks to the domain has also significantly improved the state of the art performance ,20,majority of the early approaches ( [ 6 ] ) used Hidden Markov Models ( HMMs ) for identifying emotion from speech .,"For instance , and use recurrent autoencoders to solve the task .",method
question_answering,3,Sequence encoders are crucial components in many neural architectures for learning to read and comprehend .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.014285714285714284,0.25,0.25,Sequence encoders are crucial components in many neural architectures for learning to read and comprehend ,16, ,This paper presents a new compositional encoder for reading comprehension ( RC ) .,abstract
natural_language_inference,21,"We further add two bias terms to the parts in and out activation ? ( ) , i.e. , We then compute a categorical distribution p ( z k | x , q ) over all then tokens for each feature k ? [ d e ] .",system description,Multi-dimensional Attention,0,97,50,8,0,system description : Multi-dimensional Attention,0.3344827586206897,0.3937007874015748,0.6666666666666666,We further add two bias terms to the parts in and out activation i e We then compute a categorical distribution p z k x q over all then tokens for each feature k d e ,37,"Instead of computing a single scalar score f ( x i , q ) for each token x i as shown in Eq. ( 5 ) , multi-dimensional attention computes a feature - wise score vector for xi by replacing weight vector win Eq. ( 5 ) with a matrix W , i.e. , where f ( x i , q ) ? R de is a vector with the same length as x i , and all the weight matrices W , W ( 1 ) , W ( 2 ) ? R dede .","We further add two bias terms to the parts in and out activation ? ( ) , i.e. , We then compute a categorical distribution p ( z k | x , q ) over all then tokens for each feature k ? [ d e ] .",method
sentence_compression,0,2 ) These neural network models trained on data from one domain may notwork well on out - of - domain data .,introduction,introduction,0,21,14,14,0,introduction : introduction,0.07526881720430108,0.3888888888888889,0.3888888888888889,2 These neural network models trained on data from one domain may notwork well on out of domain data ,20,"Although it maybe easy to automatically obtain such training data in some domains ( e.g. , the news domain ) , for many other domains , it is not possible to obtain such a large amount of training data .","For example , when we trained a standard neural sequence - to - sequence model 1 on 3.8 million title - article pairs from the Gigaword corpus and applied it to both in - domain data and out - of - domain data , we found that the performance on in - domain data was good but the performance on out - of - domain data could be very poor .",introduction
semantic_parsing,0,"For a question with multiple possible SQL translations , the reviewers double check whether the SQL label is correctly chosen under our protocol .",system description,SQL Review,0,138,95,3,0,system description : SQL Review,0.4964028776978418,0.9047619047619048,0.75,For a question with multiple possible SQL translations the reviewers double check whether the SQL label is correctly chosen under our protocol ,23,"Once the data base is labeled with question - query pairs , we ask a different annotator to check if the questions are clear and contain enough information to answer the query .","Finally , the reviewers check if all the SQL labels in the current data base cover all the common SQL clauses .",method
natural_language_inference,76,"Furthermore , a generic sequence - to - sequence solution allows to extend the concept of capturing entailment across any sequential data , not only natural language .",introduction,introduction,0,22,9,9,0,introduction : introduction,0.1506849315068493,0.6,0.6,Furthermore a generic sequence to sequence solution allows to extend the concept of capturing entailment across any sequential data not only natural language ,24,"In particular , there is no need for language features like part - of - speech tags or dependency parses .",and size of SNLI compared to the two orders of magnitude smaller and partly synthetic datasets so far used to evaluate RTE systems .,introduction
natural_language_inference,11,Such models are general reading architectures .,model,model,0,98,4,4,0,model : model,0.355072463768116,0.10526315789473684,0.4,Such models are general reading architectures ,7,"We use single - layer bidirectional LSTMs ( BiLSTMs ) as encoders of the inputs represented by the refined or unrefined embeddings with a task - specific , feed - forward network for the final prediction .","To demonstrate that our reading module can be integrated into arbitrary task architectures , we also add our refinement module to a reimplementation of a state of the art architecture for RTE called ESIM .",method
question-answering,1,"We use the SENNA - type sentence model for sentence representation ; SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",method,Competitor Methods,1,145,5,5,0,method : Competitor Methods,0.7512953367875648,0.16129032258064516,0.7142857142857143,We use the SENNA type sentence model for sentence representation SENMLP We take the whole sentence as input with word embedding aligned sequentially and use an MLP to obtain the score of coherence ,34,"We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :","We use the SENNA - type sentence model for sentence representation ; SENMLP : We take the whole sentence as input ( with word embedding aligned sequentially ) , and use an MLP to obtain the score of coherence .",method
phrase_grounding,0,"In this work , we use VGG as a baseline for fair comparison with other works , and the state of the art CNN , PNASNet - 5 , to study the ability of our model to exploit this more powerful visual model .",method,Feature Extraction and Common Space,0,88,15,9,0,method : Feature Extraction and Common Space,0.3911111111111111,0.2,0.45,In this work we use VGG as a baseline for fair comparison with other works and the state of the art CNN PNASNet 5 to study the ability of our model to exploit this more powerful visual model ,39,An overview of the feature extraction and common space mapping for image can be seen in the left part of .,Textual Feature Extraction : State - of - the - art works in grounding use a variety of approaches for textual feature extraction .,method
sentiment_analysis,16,This is the first approach that utilizes a non-linear projection to capture the interplay between an aspect and its context .,approach,approach,0,127,5,5,0,approach : approach,0.3993710691823901,0.06097560975609756,0.06097560975609756,This is the first approach that utilizes a non linear projection to capture the interplay between an aspect and its context ,22,Non- linear Projection ( NP ) :,"Instead of directly following the common linear combination as shown in Eq. 3 , we use a non-linear projection ( tanh ) as the replacement to calculate the aspect - specific sentiment score .",method
natural_language_inference,52,"Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications , where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.02298850574712644,0.5714285714285714,0.5714285714285714,Here we propose an approach that uses answer ranking as distant supervision for learning how to select informative justifications where justifications serve as inferential connections between the question and the correct answer while often containing little lexical overlap with either ,41,"However , the lack of labeled data for answer justifications makes learning this difficult and expensive .",We propose a neural network architecture for QA that reranks answer justifications as an intermediate ( and human - interpretable ) step in answer selection .,abstract
text_summarization,9,Tackling this task is an important step towards natural language understanding .,introduction,introduction,0,11,3,3,0,introduction : introduction,0.0718954248366013,0.15,0.15,Tackling this task is an important step towards natural language understanding ,12,Generating a condensed version of a passage while preserving its meaning is known as text summarization .,Summarization systems can be broadly classified into two categories .,introduction
natural_language_inference,85,"To closely examine local inference , we explore both the sequential and syntactic tree models that have been discussed above .",model,Local Inference Modeling,0,84,39,3,0,model : Local Inference Modeling,0.3574468085106383,0.375,0.08571428571428573,To closely examine local inference we explore both the sequential and syntactic tree models that have been discussed above ,20,Modeling local subsentential inference between a premise and hypothesis is the basic component for determining the over all inference between these two statements .,"The former helps collect local inference for words and their context , and the tree LSTM helps collect local information between ( linguistic ) phrases and clauses .",method
text_generation,4,GANs output differentiable values and hence the task of discrete text generation has to use vectors as differentiable inputs .,introduction,introduction,0,16,7,7,0,introduction : introduction,0.14678899082568808,0.875,0.875,GANs output differentiable values and hence the task of discrete text generation has to use vectors as differentiable inputs ,20,GANs are a class of neural networks that explicitly train a generator to produce high - quality samples by pitting against an adversarial discriminative model .,"This is achieved by training the GAN with sentence embedding vectors produced by Skip - Thought , a neural network model for learning fixed length representations of sentences .",introduction
relation-classification,6,Relation classification differs from sentence classification in that information about entities is given along with sentences .,model,Bidirectional LSTM Network,0,93,47,10,0,model : Bidirectional LSTM Network,0.5138121546961326,0.6103896103896104,0.8333333333333334,Relation classification differs from sentence classification in that information about entities is given along with sentences ,17,"However , for the relation classification task , these models lack of prior knowledge forgiven entity pairs , which could be powerful hints for solving the task .",We propose a novel entity - aware attention mechanism for fully utilizing informative factors in given entity pairs .,method
sentence_compression,3,"are positive integers ; e.g. a = 2 , b = 2 could lead the compression rate to 0.5 ) is also used to attain a compressed sentence of similar length .",methodology,Syntax-based Evaluator,0,56,31,13,0,methodology : Syntax-based Evaluator,0.4516129032258064,0.9117647058823528,0.8125,are positive integers e g a 2 b 2 could lead the compression rate to 0 5 is also used to attain a compressed sentence of similar length ,29,"Further , it is noteworthy that the performance comparison should be based on a similar compression rate 1 ( CR ) , and a smooth reward function",The total reward is R = R SLM + R CR .,method
text_generation,2,The curves clearly show that Leak GAN yields larger performance gain over the baselines when the generated sentences are longer .,system description,Performance Robustness in Long Text Generation,0,221,7,7,0,system description : Performance Robustness in Long Text Generation,0.6314285714285715,0.2692307692307692,0.875,The curves clearly show that Leak GAN yields larger performance gain over the baselines when the generated sentences are longer ,21,News data are shown in .,This fact supports our claim that LeakGAN is a robust framework for long text .,method
natural_language_inference,44,We use the hidden size ( h ) of 200 .,training,training,0,222,7,7,0,training : training,0.7762237762237763,0.1,0.2,We use the hidden size h of 200 ,9,"Hence , the dimension of the embedding ( d h ) is 600 .",We apply dropout with 0.2 drop rate to encodings and LSTMs for regularization .,experiment
natural_language_inference,83,"We judge the model 's understanding of real - world stories by inquiring if , like humans , it can develop an expectation of what will happen next in a given story .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.0196078431372549,0.5,0.5,We judge the model s understanding of real world stories by inquiring if like humans it can develop an expectation of what will happen next in a given story ,30,"In this paper , we present a story comprehension model that explores three distinct semantic aspects : ( i ) the sequence of events described in the story , ( ii ) its emotional trajectory , and ( iii ) its plot consistency .","Specifically , we use it to predict the correct ending of a given short story from possible alternatives .",abstract
text_generation,5,We can see SCNN - VAE - Unsup + init performs better than other baselines .,result,Unsupervised clustering results,0,251,67,15,0,result : Unsupervised clustering results,0.8508474576271187,0.7362637362637363,0.38461538461538464,We can see SCNN VAE Unsup init performs better than other baselines ,13,The detailed results are in .,"LSTM+ GMM performs very bad probably because the feature dimension is 1024 and is too high for GMM , even though we already used PCA to reduce the dimension .",result
sentiment_analysis,27,Aspect - level sentiment classification is a fine - grained task in sentiment analysis .,system description,Aspect-level sentiment classification,0,58,3,3,0,system description : Aspect-level sentiment classification,0.21014492753623187,0.13636363636363635,0.1875,Aspect level sentiment classification is a fine grained task in sentiment analysis ,13,"Sentiment analysis , also known as opinion mining , is an important research topic in Natural Language Processing ( NLP ) .","In aspect - level sentiment classification , early works mainly focus on extracting a set of features like bag - of - words features and sentiment lexicons features to train a sentiment classifier .",method
natural_language_inference,20,"In this section , we focus on transfer learning experiments that apply sentence embeddings trained on NLI to other downstream tasks .",system description,Transfer Learning,0,208,28,2,0,system description : Transfer Learning,0.8559670781893004,0.6511627906976745,0.1176470588235294,In this section we focus on transfer learning experiments that apply sentence embeddings trained on NLI to other downstream tasks ,21, ,"In order to better understand how well the sentence encoding model generalizes to different tasks , we conducted various tests implemented in the SentEval sentence embedding evaluation library ) and compared our results to the results published for In - fer Sent and SkipThought .",method
natural_language_inference,34,"Instead , we use the Stanford corenlp toolkit to recognize named entities from the context C .",system description,Constructing Entity Graph,0,119,16,3,0,system description : Constructing Entity Graph,0.4033898305084746,0.15841584158415842,0.3333333333333333,Instead we use the Stanford corenlp toolkit to recognize named entities from the context C ,16,We do not assume a global knowledge base .,The number of extracted entities is denoted as N .,method
question-answering,1,Experiment II : Matching A Response to A Tweet,method,Experiment II: Matching A Response to A Tweet,0,164,24,1,0,method : Experiment II: Matching A Response to A Tweet,0.8497409326424871,0.7741935483870968,1.0,Experiment II Matching A Response to A Tweet,8, , ,method
named-entity-recognition,1,The model is trained to maximize the conditional probability of sequences of reference actions ( extracted from a labeled training corpus ) given the input sentences .,training,Chunking Algorithm,0,109,41,11,0,training : Chunking Algorithm,0.5265700483091788,0.4823529411764706,0.7333333333333333,The model is trained to maximize the conditional probability of sequences of reference actions extracted from a labeled training corpus given the input sentences ,25,This representation is used to define a distribution over the possible actions that can betaken at each time step .,"To label a new input sequence at test time , the maximum probability action is chosen greedily until the algorithm reaches a termination state .",experiment
natural_language_inference,72,"For each entity , a concept unique identifier ( CUI ) is also available , which links it to the UMLS R Metathesaurus R.",system description,Dataset design,0,84,8,8,0,system description : Dataset design,0.2692307692307692,0.18181818181818185,0.7272727272727273,For each entity a concept unique identifier CUI is also available which links it to the UMLS R Metathesaurus R ,21,Clamp assigns entities following the i 2b2-2010 shared task specifications .,"To check the quality of the recognized entities , we carried out a small manual analysis on 250 entities .",method
relation_extraction,4,"In this model , each shortest dependency path is divided into two separate sub-paths from the subject entity and the object entity to the lowest common ancestor node .",model,model,0,115,20,20,0,model : model,0.5555555555555556,0.8695652173913043,0.8695652173913043,In this model each shortest dependency path is divided into two separate sub paths from the subject entity and the object entity to the lowest common ancestor node ,29,We follow the SDP - LSTM model proposed by .,"Each sub- path is fed into an LSTM network , and the resulting hidden units at each word position are passed into a max - over - time pooling layer to form the output of this sub-path .",method
text-classification,9,"We also compare two different kinds of additional context : topics ( TopCNN ) and translations ( CNN + B1 , CNN + B2 , CNN + MCFA ) .",result,Results and Discussion,0,188,22,22,0,result : Results and Discussion,0.7460317460317459,0.88,0.88,We also compare two different kinds of additional context topics TopCNN and translations CNN B1 CNN B2 CNN MCFA ,20,"This is resolved by CNN + B2 by applying L2 regularization , however the increase in performance is marginal .","Overall , we conclude that translations are better additional contexts than topics .",result
named-entity-recognition,7,"Generally , each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions .",introduction,introduction,1,18,8,8,0,introduction : introduction,0.1139240506329114,0.4444444444444444,0.4444444444444444,Generally each sentence with nested mentions is mapped to a forest where each outermost mention forms a tree consisting of its inner mentions ,24,"To achieve a scalable and effective solution for recognizing nested mentions , we design a transition - based system which is inspired by the recent success of employing transition - based methods for constituent parsing ) and named entity recognition , especially when they are paired with neural networks .",Then our transition - based system learns to construct this forest through a sequence of shift - reduce actions .,introduction
question_answering,4,The BAC module can bethought of as a connector component that connects two sequences / layers .,system description,Bidirectional Attention Connectors (BAC),0,46,3,3,0,system description : Bidirectional Attention Connectors (BAC),0.17898832684824906,0.02884615384615385,0.07692307692307693,The BAC module can bethought of as a connector component that connects two sequences layers ,16,This section introduces the Bidirectional Attention Connectors ( BAC ) module which is central to our over all architecture .,"The key goals of this module are to ( 1 ) connect any two layers of P/Q in the network , returning a residual feature that can be propagated 1 to deeper layers , ( 2 ) model cross-hierarchical interactions between P/Q and ( 3 ) minimize any costs incurred to other network components such that this component maybe executed multiple times across all layers .",method
natural_language_inference,17,What high maintenance part did Tesla 's AC motor not require ? Answer : mechanical brushes Context :,analysis,analysis,0,246,17,17,0,analysis : analysis,0.946153846153846,0.7391304347826086,0.7391304347826086,What high maintenance part did Tesla s AC motor not require Answer mechanical brushes Context ,16,"This innovative electric motor , patented in May 1888 , was a simple self - starting design that did not need a commutator , thus avoiding sparking and the high maintenance of constantly servicing and replacing mechanical brushes .",What high maintenance part did Tesla 's AC motor not require ? Answer : mechanical brushes Context :,result
sentiment_analysis,41,"Attention has become an effective mechanism to obtain superior results , as demonstrated in image recognition , machine translation , reasoning about entailment and sentence summarization .",introduction,introduction,0,22,11,11,0,introduction : introduction,0.09865470852017937,0.44,0.44,Attention has become an effective mechanism to obtain superior results as demonstrated in image recognition machine translation reasoning about entailment and sentence summarization ,24,"However , those models can only take into consideration the target but not aspect information which is proved to be crucial for aspect - level classification .","Even more , neural attention can improve the ability to read comprehension .",introduction
natural_language_inference,40,Linguistic Knowledge as Memory for Recurrent Neural Networks,title,title,1,2,1,1,0,title : title,0.007220216606498195,1.0,1.0,Linguistic Knowledge as Memory for Recurrent Neural Networks,8, , ,title
natural_language_inference,96,while convergence not reached do,system description,solution to annotation artifacts,0,102,44,8,0,system description : solution to annotation artifacts,0.26153846153846155,0.3666666666666665,0.8,while convergence not reached do,5,"During our experiments , we set N easy = 2 for refining a population of N ? = 1023 negative examples to k = 9 , and used a 80 % / 20 % train / test split .",Split the dataset D randomly up into training and testing portions D tr and D te .,method
sentiment_analysis,11,"is ordered temporally , where s i ? is the i th utterance by P ? and l ? is the total number of utterances spoken by person P ? , ? ? {a , b}.",system description,Task Definition,0,82,6,6,0,system description : Task Definition,0.23837209302325585,0.3157894736842105,0.3157894736842105,is ordered temporally where s i is the i th utterance by P and l is the total number of utterances spoken by person P a b ,28,"Here , U ? = ( s 1 ? , s 2 ? , ... , s l ? ? )","is ordered temporally , where s i ? is the i th utterance by P ? and l ? is the total number of utterances spoken by person P ? , ? ? {a , b}.",method
relation-classification,5,"The NER component feeds the sequence of vectors v 1:n with an additional context position index i into another BiLSTM ( BiLSTM NER ) to learn a "" latent "" feature vector representing the i th word token .",model,Our proposed model,0,40,6,6,0,model : Our proposed model,0.3539823008849557,0.2608695652173913,0.2608695652173913,The NER component feeds the sequence of vectors v 1 n with an additional context position index i into another BiLSTM BiLSTM NER to learn a latent feature vector representing the i th word token ,36,Named entity recognition ( NER ) :,Then the NER component performs linear transformation of each latent feature vector by using a single - layer feed - forward network ( FFNN NER ) :,method
relation-classification,9,SciERC annotates entities and relations from computer science ab - 1 https://github.com/google/sentencepiece,dataset,Datasets,0,54,4,4,0,dataset : Datasets,0.3673469387755102,0.5714285714285714,0.5714285714285714,SciERC annotates entities and relations from computer science ab 1 https github com google sentencepiece,15,EBM - NLP annotates PICO spans in clinical trial abstracts .,"https://github.com/allenai/SciSpaCy stracts . ACL - ARC and Sci -Cite assign intent labels ( e.g. Comparison , Extension , etc. ) to sentences from scientific papers that cite other papers .",experiment
natural_language_inference,18,"We validate this framework on two very different text modelling applications , generative document modelling and supervised question answering .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.02564102564102564,0.625,0.625,We validate this framework on two very different text modelling applications generative document modelling and supervised question answering ,19,"While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables , here we construct an inference network conditioned on the discrete text input to provide the variational distribution .",Our neural variational document model combines a continuous stochastic document representation with a bagof - words generative model and achieves the lowest reported perplexities on two standard test corpora .,abstract
natural_language_inference,26,"At last , the word representations and semantic embedding are concatenated to form the joint representation for downstream tasks .",model,Explicit Contextual Semantics,0,65,31,19,0,model : Explicit Contextual Semantics,0.30660377358490565,0.5166666666666667,1.0,At last the word representations and semantic embedding are concatenated to form the joint representation for downstream tasks ,19,"In parallel , the input sequence is segmented to subwords ( if any ) by BERT word - piece tokenizer , then the subword representation is transformed back to word level via a convolutional layer to obtain the contextual word representations .", ,method
natural_language_inference,96,Predictions were combined using a majority vote .,performance,performance,0,233,3,3,0,performance : performance,0.5974358974358974,1.0,1.0,Predictions were combined using a majority vote ,8,"To benchmark human performance , five Mechanical Turk workers were asked to answer 100 dataset questions , as did an ' expert ' annotator ( the first author of this paper ) .", ,result
sentiment_analysis,21,"sensible value of ? should be around 6 , since looking at the graph of the sigmoid function , for input values greater than 6 and less than - 6 , the sigmoid function does n't change much and has values of close to 1 and 0 , respectively .",hyperparameters,Optimal Hyperparameters,0,98,6,6,0,hyperparameters : Optimal Hyperparameters,0.7,0.4615384615384616,0.4615384615384616,sensible value of should be around 6 since looking at the graph of the sigmoid function for input values greater than 6 and less than 6 the sigmoid function does n t change much and has values of close to 1 and 0 respectively ,45,"We did however tune the number of iterations from , learning rate from [ 0.25 , 0.025 , 0.0025 , 0.001 ] and ? from .","sensible value of ? should be around 6 , since looking at the graph of the sigmoid function , for input values greater than 6 and less than - 6 , the sigmoid function does n't change much and has values of close to 1 and 0 , respectively .",experiment
sentiment_analysis,12,"Finally , we augment the standard training objective with a regularizer , which enforces attention distributions of these mined context words to be consistent with their expected distributions .",introduction,introduction,0,38,27,27,0,introduction : introduction,0.16964285714285715,0.8181818181818182,0.8181818181818182,Finally we augment the standard training objective with a regularizer which enforces attention distributions of these mined context words to be consistent with their expected distributions ,27,"Then , we mask all extracted context words of each training instance so far and then refollow the above process to discover more supervision information for attention mechanisms .",Our main contributions are three - fold :,introduction
part-of-speech_tagging,6,"Another reason is due to a large number of non-projective trees in Dutch test set ( 106/386 ? 27.5 % ) , while we use the Eisner 's decoding algorithm , producing only projective trees .",result,result,0,93,5,5,0,result : result,0.6241610738255033,0.1111111111111111,0.5,Another reason is due to a large number of non projective trees in Dutch test set 106 386 27 5 while we use the Eisner s decoding algorithm producing only projective trees ,33,possible reason is that the hyper - parameters we selected on English are not optimal for Dutch .,"Without taking "" nl "" into account , our averaged LAS score over all remaining languages is 1.1 % absolute higher than Stack - propagation 's .",result
question-answering,3,"presents the performances of the state - of - the - art systems and our model , where the performances were evaluated with the standard trec eval - 8.1 script",model,Comparing with State-of-the-art Models,0,203,33,4,0,model : Comparing with State-of-the-art Models,0.7992125984251969,0.4714285714285714,0.0975609756097561,presents the performances of the state of the art systems and our model where the performances were evaluated with the standard trec eval 8 1 script,26,"In this subsection , we evaluated our model on the test sets of QASent , WikiQA and MSRP .","Given a pair of sentences , Severyn and Moschitti ( 2015 ) employed a CNN model to compose each sentence into a vector separately , and joined the two sentence vectors to compute the sentence similarity .",method
natural_language_inference,99,The final result shows the superiority of our proposed method .,ablation,Ablation Results,0,228,16,16,0,ablation : Ablation Results,0.9047619047619048,0.6153846153846154,1.0,The final result shows the superiority of our proposed method ,11,"Finally , we compare the last module of our proposed self - alignment checking with original pointer network .", ,result
natural_language_inference,68,"This resulted in 1.5 % improvement in F1 on the development data and that outperformed the DCR model , which also introduced some language features such as POS and NE into their model .",result,RESULTS,0,197,12,12,0,result : RESULTS,0.7911646586345381,0.3,0.7058823529411765,This resulted in 1 5 improvement in F1 on the development data and that outperformed the DCR model which also introduced some language features such as POS and NE into their model ,33,"Observing that most of the answers are the spans with relatively small sizes , we simply limit the largest predicted span to have no more than 15 tokens and conducted experiment with span searching","Besides , we tried to increase the memory dimension l in the model or add bi-directional pre-processing LSTM or add bi-directional Ans - Ptr .",result
natural_language_inference,11,Analysis shows that our model learns to exploit knowledge in a semantically appropriate way .,abstract,abstract,0,8,6,6,0,abstract : abstract,0.028985507246376805,0.6666666666666666,0.6666666666666666,Analysis shows that our model learns to exploit knowledge in a semantically appropriate way ,15,Experiments on document question answering ( DQA ) and recognizing textual entailment ( RTE ) demonstrate the effectiveness and flexibility of the approach .,"Sungjin Ahn , Heeyoul Choi , Tanel Prnamaa , and Yoshua Bengio. 2016 .",abstract
natural_language_inference,87,"Although a variety of attentive models have been proposed to imitate human learning , most of them , especially global attention methods equally tackle each word and attend to all words in a sentence without explicit pruning and prior focus , which would result in inaccurate concentration on some dispensable words .",introduction,introduction,0,16,6,6,0,introduction : introduction,0.09090909090909093,0.2727272727272727,0.2727272727272727,Although a variety of attentive models have been proposed to imitate human learning most of them especially global attention methods equally tackle each word and attend to all words in a sentence without explicit pruning and prior focus which would result in inaccurate concentration on some dispensable words ,49,"person reads most words superficially and pays more attention to the key ones during reading and understanding sentences ( Wang , .",We observe that the accuracy of MRC models decreases when answering long questions ( shown in Section 5.1 ) .,introduction
natural_language_inference,60,"After all , one of the main motivations behind work on lexical entailment is that it allows for better downstream textual entailment .",model,Specialization,0,168,11,4,0,model : Specialization,0.8704663212435233,0.3055555555555556,0.3333333333333333,After all one of the main motivations behind work on lexical entailment is that it allows for better downstream textual entailment ,22,"Given the recent interest in the community in specializing , retro-fitting and counter - fitting word embeddings forgiven tasks , we examine whether the lexical - level benefits of specialization extend to sentencelevel downstream tasks .","Hence , we take the LEAR embeddings by , which do very well on the HyperLex lexical entailment evaluation dataset .",method
prosody_prediction,0,"In general for 0 labels BERT seems to have higher precision and BiLSTM better recall , whereas for label 2 BERT has clearly higher recall and precision .",analysis,Analysis,0,149,12,12,0,analysis : Analysis,0.7760416666666666,0.7058823529411765,0.7058823529411765,In general for 0 labels BERT seems to have higher precision and BiLSTM better recall whereas for label 2 BERT has clearly higher recall and precision ,27,BiLSTM makes more predictions with 0 ( non prominent ) compared to BERT .,Both models have low precision and recall for the less distinctive prominence ( label 1 ) .,result
natural_language_inference,100,"During training stage , we will scan all the triples in training data .",training,Model Training,0,194,9,9,0,training : Model Training,0.5300546448087432,0.16981132075471694,0.45,During training stage we will scan all the triples in training data ,13,"where S (q , a ) denote the predicted matching score for QA pair ( q , a ) .","Given a triple ( q , a + , a ? ) , we will compute ?S = 1 ? S ( q , a + ) + S (q , a ? ) . If ?S ? 0 , we will skip this triple .",experiment
temporal_information_extraction,1,"In difference from existing methods , we also discuss how to effectively use unlabeled data and how to handle the overwhelming fraction of missing relations in a principled way .",introduction,introduction,0,49,40,40,0,introduction : introduction,0.19066147859922167,0.9090909090909092,0.9090909090909092,In difference from existing methods we also discuss how to effectively use unlabeled data and how to handle the overwhelming fraction of missing relations in a principled way ,29,"More importantly , they compared structured learning to local baselines , while we find that the comparison between structured learning and L+ I is more interesting and important for understanding the effect of global considerations in the learning phase .","Our solution targets on these issues and , as we show , achieves significant improvements on two commonly used evaluation sets .",introduction
natural_language_inference,8,Answer sentence selection is the task of identifying sentences that contain the answer to a given question .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.018957345971563986,0.2857142857142857,0.2857142857142857,Answer sentence selection is the task of identifying sentences that contain the answer to a given question ,18, ,This is an important problem in its own right as well as in the larger context of open domain question answering .,abstract
natural_language_inference,90,"We leverage this intuition to build a simpler and more lightweight approach to NLI within a neural framework ; with considerably fewer parameters , our model outperforms more complex existing neural architectures .",introduction,introduction,0,23,16,16,0,introduction : introduction,0.15333333333333332,0.6666666666666666,0.6666666666666666,We leverage this intuition to build a simpler and more lightweight approach to NLI within a neural framework with considerably fewer parameters our model outperforms more complex existing neural architectures ,31,"Similarly , one can conclude that It is sunny outside contradicts the first sentence , by aligning thunder and lightning with sunny and recognizing that these are most likely incompatible .","In contrast to existing approaches , our approach only relies on alignment and is fully computationally decomposable with respect to the input text .",introduction
machine-translation,2,"Separable convolutions , however , decrease the complexity considerably , to O ( k n d + n d 2 ) .",model,Why Self-Attention,0,146,105,20,0,model : Why Self-Attention,0.6517857142857143,0.963302752293578,0.8333333333333334,Separable convolutions however decrease the complexity considerably to O k n d n d 2 ,16,"Convolutional layers are generally more expensive than recurrent layers , by a factor of k.","Even with k = n , however , the complexity of a separable convolution is equal to the combination of a self - attention layer and a point - wise feed - forward layer , the approach we take in our model .",method
natural_language_inference,72,"These datasets were collected from Wikipedia , web search queries , news articles , books and English exams .",dataset,Related datasets,0,59,3,3,0,dataset : Related datasets,0.1891025641025641,0.15,0.15,These datasets were collected from Wikipedia web search queries news articles books and English exams ,16,Numerous general - domain datasets have been recently created to allow machine comprehension using data - intensive methods .,"In , we compare our dataset to several domain - specific datasets for machine comprehension .",experiment
part-of-speech_tagging,2,"Since different domains are "" sub-languages "" that have domain - specific regularities , sequence taggers trained on one domain might not have optimal performance on another domain .",architecture,CROSS-DOMAIN TRANSFER,0,67,6,2,0,architecture : CROSS-DOMAIN TRANSFER,0.3764044943820225,0.2,0.16666666666666666,Since different domains are sub languages that have domain specific regularities sequence taggers trained on one domain might not have optimal performance on another domain ,26, ,The goal of cross - domain transfer is to learn a sequence tagger that transfers knowledge from a source domain to a target domain .,method
sarcasm_detection,1,They embed topical information that selectively incur bias towards degree of sarcasm in the comments of a discussion .,method,Discourse Features,0,199,130,3,0,method : Discourse Features,0.5958083832335329,0.8609271523178808,0.3,They embed topical information that selectively incur bias towards degree of sarcasm in the comments of a discussion ,19,"Similar to how a user influences the degree of sarcasm in a comment , we assume that the discourse of comments belonging to a certain discussion forum contain contextual information relevant to the sarcasm classification .","For example , comments on political leaders or sports matches are generally more susceptible to sarcasm than natural dis asters .",method
natural_language_inference,46,"For short answers of one or two wordstypically main characters in a story - the candidate , i.e. the closest span to the reference answer , is easily found due to being mentioned throughout the text .",experiment,experiment,0,212,25,25,0,experiment : experiment,0.7138047138047138,0.5813953488372093,0.5813953488372093,For short answers of one or two wordstypically main characters in a story the candidate i e the closest span to the reference answer is easily found due to being mentioned throughout the text ,35,"As previously , we considered all spans of a given length across the entire story for this model .","For longer answers it becomes much less likely , compared to the summaries , that a high - scoring span can be found in the story .",experiment
text_summarization,3,Unique to our concept pointer is a set of concept candidates particular for a word that is drawn from a huge knowledge base .,introduction,introduction,0,30,18,18,0,introduction : introduction,0.12396694214876032,0.75,0.75,Unique to our concept pointer is a set of concept candidates particular for a word that is drawn from a huge knowledge base ,24,"Finally , the output is also consistent with language model by the seq2seq generator .","The set of candidates adheres to a concept distribution , where the probability of each concept being generated is linked to how strongly the candidate represents each word .",introduction
natural_language_inference,56,"At each step t , each node sends a message to each of its neighboring nodes .",system description,Recurrent Relational Networks,0,56,15,15,0,system description : Recurrent Relational Networks,0.16666666666666666,0.3947368421052632,0.3947368421052632,At each step t each node sends a message to each of its neighboring nodes ,16,"At each step teach node has a hidden state vector ht i , which is initialized to the features , such that h 0 i = xi .","We define the message mt ij from node i to node j at step t by where f , the message function , is a multi - layer perceptron .",method
machine-translation,0,"Furthermore , many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing ( NLP ) .",introduction,introduction,0,11,3,3,0,introduction : introduction,0.0502283105022831,0.1111111111111111,0.1111111111111111,Furthermore many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing NLP ,23,"Deep neural networks have shown great success in various applications such as objection recognition ( see , e.g. , ) and speech recognition ( see , e.g. , ) .","These include , but are not limited to , language modeling , paraphrase detection and word embedding extraction .",introduction
named-entity-recognition,7,Note that the composition function is distinct for each label l .,model,Representation of Parser States,0,104,61,13,0,model : Representation of Parser States,0.6582278481012658,0.8970588235294118,0.65,Note that the composition function is distinct for each label l ,12,"where W u , l and W b , l denote the weight matrices for unary ( u ) and binary ( b ) composition with parent node being label ( l ) .",Recall that the leaf nodes of each tree element are raw words .,method
natural_language_inference,23,"Context : According to PolitiFact the top 400 richest Americans "" have more wealth than half of all Americans combined . """,model,FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0,430,37,14,0,model : FUSIONNET ANSWERS CORRECTLY WHILE BIDAF IS INCORRECT,0.8382066276803118,0.3083333333333333,0.16470588235294115,Context According to PolitiFact the top 400 richest Americans have more wealth than half of all Americans combined ,19,Prediction : Confusion ID : 5727e8424b864d1900163fc1-high-conf-turk1,"According to the New York Times on July 22 , 2014 , the "" richest 1 percent in the United States now own more wealth than the bottom 90 percent "" .",method
relation_extraction,6,"Therefore , in addition to the target entity pair , we take other entities from the same sentence that were extracted at the data generation step .",baseline,baseline,0,88,8,8,0,baseline : baseline,0.676923076923077,0.5,0.5,Therefore in addition to the target entity pair we take other entities from the same sentence that were extracted at the data generation step ,25,"Some relation types may tend to co-occur , such as DI - RECTED BY and PRODUCED BY , whereas others maybe restrictive ( e. g. one can only have a single PLACE OF BIRTH ) .",We construct a set of context relations by taking each possible pair of entities .,result
sentence_compression,0,This is an abstractive sequence - to - sequence model trained on 3.8 million Gigaword title - article pairs as described in Section 1 .,dataset,Datasets and Experiment Settings,1,184,39,39,0,dataset : Datasets and Experiment Settings,0.6594982078853047,1.0,1.0,This is an abstractive sequence to sequence model trained on 3 8 million Gigaword title article pairs as described in Section 1 ,23,This method does not use neural network models and is an unsupervised method that relies heavily on the syntactic structures of the input sentences, ,experiment
natural_language_inference,59,"Moreover , identifying questions with the same semantic content could help Web-scale question answering systems thatare increasingly concentrating on retrieving focused answers to users ' queries .",introduction,introduction,0,11,5,5,0,introduction : introduction,0.08148148148148149,0.38461538461538464,0.38461538461538464,Moreover identifying questions with the same semantic content could help Web scale question answering systems thatare increasingly concentrating on retrieving focused answers to users queries ,26,Identifying these duplicates and consolidating their answers increases the efficiency of such QA forums .,"Here , we focus on a recent dataset published by the QA website Quora.com containing over 400 K annotated question pairs containing binary paraphrase labels .",introduction
topic_models,0,"This approximation to the true posterior is referred as variational distribution q ( w ) , and is obtained by minimizing the Kullback - Leibler ( KL ) divergence , D KL ( q || p ) between the approximate and true posterior .",model,model,0,76,21,21,0,model : model,0.18446601941747573,0.40384615384615385,0.4565217391304348,This approximation to the true posterior is referred as variational distribution q w and is obtained by minimizing the Kullback Leibler KL divergence D KL q p between the approximate and true posterior ,34,"In such cases , one can resort to variational inference and find an approximation to the posterior distribution p ( w|x ) .",We can express log marginal ( evidence ) of the data as :,method
sentiment_analysis,35,"Given a sentence - aspect pair ( x , a ) from the source or target domain , we assume that the sentence consists of n words , i.e. , x = {w 1 , w 2 , ... , w n } , and the aspect contains m words , i.e. , a = {w a 1 , w a 2 , ... , w am }.",system description,Bi-directional LSTM layer,0,83,24,3,0,system description : Bi-directional LSTM layer,0.3346774193548387,0.24489795918367346,0.25,Given a sentence aspect pair x a from the source or target domain we assume that the sentence consists of n words i e x w 1 w 2 w n and the aspect contains m words i e a w a 1 w a 2 w am ,49,"In the following sections , we introduce each component of MGAN in details .",Then we map them into it s embedding vectors e = {e i } n i =1 ? R ndw and ea = {e a j } m j=1 ? R mdw respectively .,method
natural_language_inference,65,"In , we illustrate the application of AP over the output of the convolution or the biLSTM to construct the representations r q and r a .",system description,Attentive Pooling Networks for Answer Selection,0,102,7,7,0,system description : Attentive Pooling Networks for Answer Selection,0.4434782608695652,0.21212121212121213,0.35,In we illustrate the application of AP over the output of the convolution or the biLSTM to construct the representations r q and r a ,26,We use a similarity measure that has a bilinear form but followed by a non-linearity .,"Consider the input pair ( q , a ) where the question has size M and the answer has size L 1 .",method
named-entity-recognition,9,"With almost the same architecture across tasks , BioBERT largely outperforms BERT and previous state - of - the - art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora .",abstract,abstract,0,11,9,9,0,abstract : abstract,0.055276381909547735,0.6923076923076923,0.6923076923076923,With almost the same architecture across tasks BioBERT largely outperforms BERT and previous state of the art models in a variety of biomedical text mining tasks when pre trained on biomedical corpora ,33,"We introduce BioBERT ( Bidirectional Encoder Representations from Transformers for Biomedical Text Mining ) , which is a domain - specific language representation model pre-trained on large - scale biomedical corpora .","While BERT obtains performance comparable to that of previous state - of - the - art models , BioBERT significantly outperforms them on the following three representative biomedical text mining tasks : biomedical named entity recognition ( 0.62 % F1 score improvement ) , biomedical relation extraction ( 2.80 % F1 score improvement ) and biomedical question answering ( 12.24 % MRR improvement ) .",abstract
text_summarization,6,where ? ? R kz is an auxiliary noise variable .,abstract,Abstractive Summary Generation,0,148,39,39,0,abstract : Abstractive Summary Generation,0.5648854961832062,0.8297872340425532,0.8297872340425532,where R kz is an auxiliary noise variable ,9,The latent structure variable z t ? R kz can be calculated using the reparameterization trick :,where ? ? R kz is an auxiliary noise variable .,abstract
natural_language_inference,37,"We note that for Trivia QA web we do not subsample as was done by , instead training on the full 530 k question - document training pairs .",system description,Preprocessing,0,144,2,2,0,system description : Preprocessing,0.5603112840466926,0.1,0.25,We note that for Trivia QA web we do not subsample as was done by instead training on the full 530 k question document training pairs ,27, ,"We also observed that the metrics for Trivia QA are computed after applying a small amount of text normalization ( stripping punctuation , removing articles , ect. ) to both the ground truth text and the predicted text .",method
part-of-speech_tagging,3,"Google 's Word2 Vec 300 dimensional embeddings obtain similar performance with Senna on POS tagging , still slightly behind Glo Ve .",system description,Word Embeddings,0,158,23,13,0,system description : Word Embeddings,0.7783251231527094,0.71875,0.6842105263157895,Google s Word2 Vec 300 dimensional embeddings obtain similar performance with Senna on POS tagging still slightly behind Glo Ve ,21,"This is different from the results reported by , where Senna achieved slightly better performance on NER than other embeddings .","But for NER , the performance on Word2 Vec is far behind GloVe and Senna .",method
question-answering,8,MACHINE COMPREHENSION USING MATCH - LSTM AND ANSWER POINTER,title,title,1,2,1,1,0,title : title,0.008032128514056224,1.0,1.0,MACHINE COMPREHENSION USING MATCH LSTM AND ANSWER POINTER,8, , ,title
natural_language_inference,36,"In QA - SRL , the focus is commonly simple and short factoid questions thatare less related to the context , let alone making inference .",introduction,introduction,0,24,15,15,1,introduction : introduction,0.11428571428571427,0.5769230769230769,0.5769230769230769,In QA SRL the focus is commonly simple and short factoid questions thatare less related to the context let alone making inference ,23,"However , our task is quite different .","Actually , text comprehension and inference are quite challenging tasks in NLP , requiring to dig the deep semantics between the document and comprehensive question which are usually raised or re-written by humans , instead of shallow argument alignment around the same predicate in QA - SRL .",introduction
named-entity-recognition,6,Neural network approaches to Named - Entity Recognition reduce the need for carefully handcrafted features .,abstract,abstract,1,4,2,2,0,abstract : abstract,0.018867924528301886,0.2,0.2,Neural network approaches to Named Entity Recognition reduce the need for carefully handcrafted features ,15, ,"While some features do remain in state - of - the - art systems , lexical features have been mostly discarded , with the exception of gazetteers .",abstract
natural_language_inference,46,Annotators were encouraged to use their own words and we prevented them from copying .,method,method,0,116,16,16,0,method : method,0.39057239057239057,0.32,0.32,Annotators were encouraged to use their own words and we prevented them from copying ,15,"We required questions thatare specific enough , given the length and complexity of the narratives , and to provide a diverse set of questions about characters , events , why this happened , and soon .","We asked for answers thatare grammatical , complete sentences , and explicitly allowed short answers ( one word , or a few - word phrase , or a short sentence ) as we think that answering with a full sentence is frequently perceived as artificial when asking about factual information .",method
natural_language_inference,83,The model outperforms the stateof - the - art approaches and achieves best results on a publicly available dataset .,abstract,abstract,0,10,8,8,0,abstract : abstract,0.032679738562091505,1.0,1.0,The model outperforms the stateof the art approaches and achieves best results on a publicly available dataset ,18,"Our experiments demonstrate the potential of our approach to characterize these semantic aspects , and the strength of the hidden variable based approach .", ,abstract
text_generation,2,The MANAGER produces the 16 - dimensional goal embedding feature vector wt using the feature map extracted by CNN .,training,Training Settings,1,171,13,13,0,training : Training Settings,0.4885714285714286,0.6842105263157895,0.6842105263157895,The MANAGER produces the 16 dimensional goal embedding feature vector wt using the feature map extracted by CNN ,19,"For the generator , we adopt LSTM as the architectures of MANAGER and WORKER to capture the sequence context information .",The goal duration time c is a hyperparameter set as 4 after some preliminary experiments .,experiment
natural_language_inference,57,This is possible only if the mapping satisfies the following crucial property :,system description,LEARNING ORDER-EMBEDDINGS,0,42,8,8,0,system description : LEARNING ORDER-EMBEDDINGS,0.2441860465116279,0.14035087719298245,0.8,This is possible only if the mapping satisfies the following crucial property ,13,The idea is to predict the ordering of an unseen pair in X based on its ordering in the embedding space .,"This definition implies that each combination of embedding space Y , order Y , and orderembedding f determines a unique completion of our data as a partial order X .",method
sentiment_analysis,14,"The model is divided into two parts : a shared embedding layer ( i.e. Emo2 Vec ) , and task - specific classifiers .",methodology,Methodology,0,30,3,3,0,methodology : Methodology,0.1973684210526316,0.2,0.2,The model is divided into two parts a shared embedding layer i e Emo2 Vec and task specific classifiers ,20,We train Emo2 Vec using an end - to - end multi-task learning framework with one larger dataset and several small task - specific datasets .,"All datasets share the same word - level representations ( i.e. Emo2 Vec ) , thus forcing the model to encode shared knowledge into a single matrix .",method
natural_language_inference,68,The machine is then expected to answer one or multiple questions related to the text .,introduction,introduction,0,15,5,5,0,introduction : introduction,0.060240963855421686,0.125,0.125,The machine is then expected to answer one or multiple questions related to the text ,16,"In this setup , typically the machine is first presented with apiece of text such as a news article or a story .","In most of the benchmark datasets , a question can be treated as a multiple choice question , whose correct answer is to be chosen from a set of provided candidate answers .",introduction
sentiment_analysis,26,Our loss function is the cross -entropy function,approach,Dual-Module Memory based CNNs,0,101,80,42,0,approach : Dual-Module Memory based CNNs,0.4122448979591837,0.9523809523809524,0.9130434782608696,Our loss function is the cross entropy function,8,Loss Function and Training .,"where n is the number of training examples , C is the set of ( two ) classes , y i , c are ground truth labels for a given training example and class c , and ? i , c are corresponding label probabilities predicted by the model , as emitted by the softmax layer .",method
text_generation,3,An overview is presented in Section 2.1 .,approach,Approach,0,35,3,3,0,approach : Approach,0.24822695035460995,0.08571428571428573,0.6,An overview is presented in Section 2 1 ,9,"In this section , we introduce our proposed model .","The details of the modules are shown in Sections 2.2 , 2.3 and 2.4 .",method
sentiment_analysis,36,"We also remove a few examples having the "" conflict label "" as done in ; TWITTER is built by , containing twitter posts .",experiment,Experimental Setup,0,145,4,3,0,experiment : Experimental Setup,0.58,0.11428571428571427,0.08823529411764706,We also remove a few examples having the conflict label as done in TWITTER is built by containing twitter posts ,21,"As shown in , we evaluate the proposed TNet on three benchmark datasets : LAPTOP and REST are from SemEval ABSA challenge , containing user reviews in laptop domain and restaurant domain respectively .","All tokens are lowercased without removal of stop words , symbols or digits , and sentences are zero - padded to the length of the longest sentence in the dataset .",experiment
text_summarization,10,"Further , we also present novel multi-task learning architectures based on multi-layered encoder and decoder models , where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks , while keeping the lower - level ( lexico- syntactic ) layers unshared .",introduction,introduction,1,22,14,14,0,introduction : introduction,0.08365019011406842,0.7,0.7,Further we also present novel multi task learning architectures based on multi layered encoder and decoder models where we empirically show that it is substantially better to share the higherlevel semantic layers between the three aforementioned tasks while keeping the lower level lexico syntactic layers unshared ,47,"Our entailment generation task is based on the recent SNLI classification dataset and task , converted to a generation task .",We also explore different ways to optimize the shared parameters and show that ' soft ' parameter sharing achieves higher performance than hard sharing .,introduction
natural_language_inference,70,Longest common substring measure determines the length of the longest contiguous sequence of characters shared by two text segments .,system description,Intra-pair similarities,0,57,28,11,0,system description : Intra-pair similarities,0.3257142857142857,0.3010752688172043,0.5789473684210527,Longest common substring measure determines the length of the longest contiguous sequence of characters shared by two text segments ,20,It performs a deep syntactic comparison .,"Longest common subsequence measure ( Allison and Dix , 1986 ) drops the contiguity requirement of the previous measure and allows to detect similarity in case of word insertions / deletions .",method
natural_language_inference,43,Semantic parsing - based QA datasets contain question - answer pairs alongside a background KB .,system description,Compositionality,0,50,8,8,0,system description : Compositionality,0.3225806451612903,0.2285714285714285,0.4444444444444444,Semantic parsing based QA datasets contain question answer pairs alongside a background KB ,14,An example for a training example is provided in .,"To convert such datasets to our setup , we run the question q against Google 's search engine and scrape the top - K web snippets .",method
natural_language_inference,81,"Sang Keun Lee Deunsol Yoon , Dongbok Lee. Dynamic self - attention :",abstract,abstract,0,31,29,29,0,abstract : abstract,0.07363420427553444,0.7631578947368421,0.7631578947368421,Sang Keun Lee Deunsol Yoon Dongbok Lee Dynamic self attention ,11,"arXiv preprint ar Xiv : 1808.09920 , 2018 .",Computing attention over words dynamically for sentence embedding .,abstract
natural_language_inference,79,The hyperparameters of our paragraph vector model are selected in the same manner as in the previous task .,experiment,Experimental protocols:,0,168,33,7,0,experiment : Experimental protocols:,0.6268656716417911,0.66,0.4666666666666667,The hyperparameters of our paragraph vector model are selected in the same manner as in the previous task ,19,"Once the vectors are learned , we feed them through the neural network to predict the sentiment of the reviews .","In particular , we cross validate the window size , and the optimal window size is 10 words .",experiment
sentiment_analysis,47,"On this basis , the normalized importance weight ? i are computed as follows :",model,Representation Concatenation,0,105,35,9,0,model : Representation Concatenation,0.4883720930232558,0.7777777777777778,0.9,On this basis the normalized importance weight i are computed as follows ,13,The score f is used as a weight that denotes the importance of a word in the target phrase influenced by left context .,"On this basis , the normalized importance weight ? i are computed as follows :",method
smile_recognition,0,The Facial Action Coding System ( FACS ) 1 is a system to taxonomize any facial expression of a human being by their appearance on the face .,introduction,introduction,0,15,8,8,0,introduction : introduction,0.16666666666666666,0.6153846153846154,0.6153846153846154,The Facial Action Coding System FACS 1 is a system to taxonomize any facial expression of a human being by their appearance on the face ,26,"Nonetheless , deep neural networks are not a magic bullet and successful training is still heavily based on experimentation .","Action units describe muscles or muscle groups in the face , are set or un - set and the activation maybe on different intensity levels .",introduction
part-of-speech_tagging,6,Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural network - based classifiers .,introduction,introduction,0,13,6,6,0,introduction : introduction,0.08724832214765099,0.42857142857142855,0.42857142857142855,Several authors represent the core features with dense vector embeddings and then feed them as inputs to neural network based classifiers ,22,Recent work shows that using deep learning in dependency parsing has obtained state - of - the - art performances .,"In addition , others propose novel neural architectures for parsing to handle feature - engineering .",introduction
natural_language_inference,44,"In the last example , the model fails to select the oracle sentence .",training,SQuAD-Adversarial,0,277,62,24,0,training : SQuAD-Adversarial,0.9685314685314684,0.8857142857142857,0.75,In the last example the model fails to select the oracle sentence ,13,"In the second example , the model predicts the answer from the wrong sentence , although it selects the oracle sentence .","two examples , our sentence selector choose the oracle sentence , but the QA model fails to answer correctly , either predicting the wrong answer from the oracle sentence , or predicting the answer from the wrong sentence .",experiment
question-answering,7,This is possibly due to the encoding memory of the document level NSE that preserves the long dependency in documents with a large number of sentences .,analysis,Document Sentiment Analysis,0,220,21,21,0,analysis : Document Sentiment Analysis,0.8,0.328125,1.0,This is possibly due to the encoding memory of the document level NSE that preserves the long dependency in documents with a large number of sentences ,27,The stacked NSEs ( NSE - NSE ) performed slightly better than the NSE - LSTM on the IMDB dataset ., ,result
topic_models,0,"In the original model proposed by Blei , the parameters ? were obtained using maximum likelihood approach .",model,model,0,208,18,18,0,model : model,0.5048543689320388,0.4864864864864865,0.4864864864864865,In the original model proposed by Blei the parameters were obtained using maximum likelihood approach ,16,"Further , meanfield approximation was made to make the inference tractable , i.e. , q (? , z ) = q (? ) i q ( z i ) .","In the original model proposed by Blei , the parameters ? were obtained using maximum likelihood approach .",method
sentiment_analysis,9,"Surprisingly , for the Laptop and Restaurant datasets , guests occasionally have a unified "" global "" view in a specific review .",analysis,Overall Performance Analysis,0,239,5,5,0,analysis : Overall Performance Analysis,0.8628158844765343,0.14285714285714285,0.3333333333333333,Surprisingly for the Laptop and Restaurant datasets guests occasionally have a unified global view in a specific review ,19,"The CDM layer works better on twitter dataset because there are a lot of non-standard grammar usage and language abbreviations within it , and the local context focus techniques can promote to infer the polarity of terms .","That is , if the customer is not satisfied with one aspect , it is likely to criticize the other .",result
natural_language_inference,80,Our evaluation shows that DR - BiLSTM obtains the best single model and ensemble model results achieving the new state - of - the - art scores on the Stanford NLI dataset .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.03103448275862069,1.0,1.0,Our evaluation shows that DR BiLSTM obtains the best single model and ensemble model results achieving the new state of the art scores on the Stanford NLI dataset ,29,"Finally , we demonstrate how the results can be improved further with an additional preprocessing step .", ,abstract
sentiment_analysis,34,"In our implementation , we used the following steps :",methodology,Data Preprocessing,0,74,6,4,0,methodology : Data Preprocessing,0.5,0.3157894736842105,0.5,In our implementation we used the following steps ,9,The steps are mainly based on the work in .,Letter normalis ation : unifying the letters that appear in different forms .,method
natural_language_inference,34,"In the following , we introduce two metrics to evaluate the quality of multi-hop reasoning through entity - level supporting ( ESP ) scores . :",result,Evaluation on Graph Construction and Reasoning Chains,0,247,27,14,0,result : Evaluation on Graph Construction and Reasoning Chains,0.8372881355932204,0.36,0.7368421052631579,In the following we introduce two metrics to evaluate the quality of multi hop reasoning through entity level supporting ESP scores ,22,"For each supporting sentence , we use the k paths to calculate how many supporting sentences are hit .",Evaluation of reasoning chains by ESP scores on two versions of the entity graphs in the development set . ? 40 and ? 80 indicate to the maximum number of nodes in entity graphs .,result
text_summarization,0,"First , for the decoder , we need to know which aspect in document has been focused by our summary generator in the past decoding steps .",model,Supervisor,0,155,18,3,0,model : Supervisor,0.5149501661129569,0.3333333333333333,0.0967741935483871,First for the decoder we need to know which aspect in document has been focused by our summary generator in the past decoding steps ,25,"To model the semantic gap between the generated summary and the reader focused aspects , we design a supervisor module .","We sum up the latest k attention distributions {? t , ? t?1 , . . . , ? t?k+1 } and result in ? t ? R T d as the focus distribution of generated summary over T d document words , shown in Equation 17 .",method
question_answering,4,Both models are reinforcement learning based extensions of existing strong baselines such as BiDAF and Match - LSTM .,result,Quasar-T,0,204,14,5,0,result : Quasar-T,0.7937743190661478,0.6666666666666666,0.4166666666666667,Both models are reinforcement learning based extensions of existing strong baselines such as BiDAF and Match LSTM ,18,"On the over all setting , our model outperforms both AQA ( + 18.1 % EM / + 18 % F1 ) and Reinforced Reader Ranker ( + 7.8 % EM / + 8.3 % F1 ) .",Narrative QA reports the results on Narrative QA .,result
natural_language_inference,56,"These architectures take the entire Sudoku as an input and output the entire solution in a single forward pass , ignoring the inductive bias that objects exists in the world , and that they affect each other in a consistent manner .",introduction,introduction,0,21,8,8,0,introduction : introduction,0.0625,0.2857142857142857,0.2857142857142857,These architectures take the entire Sudoku as an input and output the entire solution in a single forward pass ignoring the inductive bias that objects exists in the world and that they affect each other in a consistent manner ,40,"Contrast this with the canonical deep learning approach to solving problems , the multilayer perceptron ( MLP ) , or multilayer convolutional neural net ( CNN ) .",Not surprisingly these models fall short when faced with problems that require even basic relational reasoning .,introduction
relation-classification,6,"For sequentially encoding the output of self attention layer , we use a BLSTM that consists of two sub LSTM networks : a forward LSTM network which encodes the context of a input sentence and a backward LSTM network which encodes that one of the reverse sentence .",model,Bidirectional LSTM Network,0,86,40,3,0,model : Bidirectional LSTM Network,0.4751381215469613,0.5194805194805194,0.25,For sequentially encoding the output of self attention layer we use a BLSTM that consists of two sub LSTM networks a forward LSTM network which encodes the context of a input sentence and a backward LSTM network which encodes that one of the reverse sentence ,46,"As a result , each type has a set of words with similar characteristics , which can prove that LET works effectively .","More formally , BLSTM works as follows :",method
natural_language_inference,78,This outperforms the state - of - theart ESIM and DIIN models with only a fraction of the parameter cost .,experiment,Experimental Results,1,216,14,14,0,experiment : Experimental Results,0.7826086956521741,0.35897435897435903,0.35897435897435903,This outperforms the state of theart ESIM and DIIN models with only a fraction of the parameter cost ,19,"Due to resource constraints , we did not train CAFE + ELMo ensembles but a single run ( and single model ) of CAFE + ELMo already achieves 89.0 score on SNLI .","At 88.1 % , our model has about three times less parameters than ESIM / DIIN ( i.e. , 1.4 M versus 4.3M / 4.4M ) .",experiment
machine-translation,1,depicts the two networks and their combination .,model,Desiderata,0,77,27,16,0,model : Desiderata,0.38308457711442784,0.4426229508196721,0.32,depicts the two networks and their combination ,8,The encoder processes the source string into a representation and is formed of one - dimensional convolutional layers that use dilation but are not masked .,notable feature of the proposed family of architectures is the way the encoder and the decoder are connected .,method
natural_language_inference,76,"More precisely , an LSTM with attention for RTE does not need to capture the whole semantics of the premise in its cell state .",method,ATTENTION,0,57,29,4,0,method : ATTENTION,0.3904109589041096,0.5087719298245614,0.21052631578947367,More precisely an LSTM with attention for RTE does not need to capture the whole semantics of the premise in its cell state ,24,"The idea is to allow the model to attend over past output vectors ( see ) , thereby mitigating the LSTM 's cell state bottleneck .","Instead , it is sufficient to output vectors while reading the premise and accumulating a representation in the cell state that informs the second LSTM which of the output vectors of the premise it needs to attend over to determine the RTE class .",method
text_summarization,6,"extended the seq2seq framework and proposed a generative model to capture the latent summary information , but they did not consider the recurrent dependencies in their generative model leading to limited representation ability .",introduction,introduction,0,25,16,16,0,introduction : introduction,0.09541984732824428,0.5714285714285714,0.5714285714285714,extended the seq2seq framework and proposed a generative model to capture the latent summary information but they did not consider the recurrent dependencies in their generative model leading to limited representation ability ,33,The deterministic transformations in these discriminative models lead to limitations on the representation ability of the latent structure information .,"To tackle the above mentioned problems , we design a new framework based on sequence to - sequence oriented encoder - decoder model equipped with a latent structure modeling component .",introduction
sentiment_analysis,13,"The output is a span across the positions ind ( after the [ SEP ] token of the input ) , indicated by two pointers ( indexes ) sand e computed from l 1 and l 2 : s = arg max Idx [ SEP ]<s<|x| ( l 1 ) and e = arg max s?e<| x | ( l 2 ) , where Idx is the position of token [ SEP ] ( so the pointers will never point to tokens from the question ) .",system description,Review Reading Comprehension (RRC),0,99,23,9,0,system description : Review Reading Comprehension (RRC),0.3561151079136691,0.23,0.6,The output is a span across the positions ind after the SEP token of the input indicated by two pointers indexes sand e computed from l 1 and l 2 s arg max Idx SEP s x l 1 and e arg max s e x l 2 where Idx is the position of token SEP so the pointers will never point to tokens from the question ,68,The softmax is applied along the dimension of the sequence .,"As such , the final answer will always be a valid text span from the review as a = ( d s , . . . , d e ) .",method
text_summarization,2,Glove embedding before it is fed to the system ; the vector is denoted by xi ( i ? [ S ] ; ' S ' for source ) .,approach,Our Approach,0,55,4,4,0,approach : Our Approach,0.19784172661870505,0.03053435114503817,0.5714285714285714,Glove embedding before it is fed to the system the vector is denoted by xi i S S for source ,21,source word is replaced by it s,"Similarly , a summary word is denoted by y t ( t ? [ T ] ; ' T ' for target ) .",method
prosody_prediction,0,"Statistical features like unigrams , bigrams , and TF - IDF have also been frequently used .",system description,Predicting Prosodic Prominence from Text,0,53,23,7,0,system description : Predicting Prosodic Prominence from Text,0.2760416666666667,0.8518518518518519,0.6363636363636364,Statistical features like unigrams bigrams and TF IDF have also been frequently used ,14,"Before the deep learning paradigm shift in NLP , several linguistic features were examined for prominence prediction , including function - content word distinction , part - of - speech class , and information status .","Later , the accent ratio , or simply the average accent status of a word type in the given corpus , was found to be a stronger predictor than linguistic features in the accent prediction task , suggesting that lexical information maybe more relevant than linguistic structure for the prominence prediction task .",method
natural_language_inference,73,"Based on this idea , we develop a novel self - attention termed ReSA .",model,Reinforced Self-Attention (ReSA),0,107,25,3,0,model : Reinforced Self-Attention (ReSA),0.4083969465648855,0.4166666666666667,0.09375,Based on this idea we develop a novel self attention termed ReSA ,13,The fundamental idea behind this paper is that the hard and soft attention mechanisms can mutually benefit each other to overcome their inherent dis advantages via interaction within an integrated model .,"On the one hand , the proposed RSS provides a sparse mask to a self - attention module that only needs to model the dependencies for the selected token pairs .",method
sentiment_analysis,29,The initial learning rate is 0.01 for the Adam optimizer .,hyperparameters,Hyperparameters Setting,1,126,8,8,0,hyperparameters : Hyperparameters Setting,0.7241379310344828,0.8,0.8,The initial learning rate is 0 01 for the Adam optimizer ,12,The dimension of LSTM hidden states is set to 150 .,"If the training loss does not drop after every three epochs , we decrease the learning rate by half .",experiment
natural_language_inference,7,"In contrast , RASOR can efficiently and explicitly model the quadratic number of possible answers , which leads to a 14 % error reduction over the best performing Match - LSTM model .",result,COMPARISONS TO OTHER WORK,1,125,12,9,0,result : COMPARISONS TO OTHER WORK,0.702247191011236,1.0,1.0,In contrast RASOR can efficiently and explicitly model the quadratic number of possible answers which leads to a 14 error reduction over the best performing Match LSTM model ,29,"However , both training and evaluation are greedy , making their system susceptible to search errors when decoding .", ,result
text_generation,0,"In our work , we use 88 numbers to represent 88 pitches , which correspond to the 88 keys on the piano .",Real-world Scenarios,Music Generation,0,246,24,4,0,Real-world Scenarios : Music Generation,0.7592592592592593,0.8571428571428571,0.5,In our work we use 88 numbers to represent 88 pitches which correspond to the 88 keys on the piano ,21,We study the solo track of each music .,"With the pitch sampling for every 0.4s 7 , we transform the midi files into sequences of numbers from 1 to 88 with the length 32 .",others
sentiment_analysis,16,"In these datasets , aspect sentiment polarities are labeled .",experiment,Experiments,0,226,3,3,0,experiment : Experiments,0.710691823899371,0.1875,0.25,In these datasets aspect sentiment polarities are labeled ,9,"We perform experiments on the datasets of Se -m Eval Task 2014 , which contain online reviews from domain Laptop and Restaurant .",The training and test sets have also been provided .,experiment
natural_language_inference,69,This ensures that precisely one of the end points corresponds to a correct answer to q .,dataset,Dataset Assembly,0,63,13,13,0,dataset : Dataset Assembly,0.1826086956521739,0.5652173913043478,0.5652173913043478,This ensures that precisely one of the end points corresponds to a correct answer to q ,17,"Note that whenever there is another fact ( s , r , o ) in the KB , i.e. a fact producing the same q but with a different a * , we will not include o into the set of end points for this sample .","When traversing the graph starting at s , several end points will be visited , though generally not all ; those visited define the candidate set C q .",experiment
sentiment_analysis,33,We use a CNN architecture that is similar to the CNN- static model described in for sentence classification .,architecture,CNN architectures,0,56,7,7,0,architecture : CNN architectures,0.44799999999999995,0.4666666666666667,0.7777777777777778,We use a CNN architecture that is similar to the CNN static model described in for sentence classification ,19,Then a max - over - time pooling operation is used to produce a fixed size sentence representation : v = max{C}.,"After obtaining the sentence representation v , a fully connected softmax layer is used to map v to an output probability distribution .",method
natural_language_inference,50,Hyperbolic space is an embedding space with a constant negative curvature in which the distance towards the border is increasing exponentially .,abstract,abstract,1,26,24,24,0,abstract : abstract,0.08201892744479496,0.5217391304347826,0.5217391304347826,Hyperbolic space is an embedding space with a constant negative curvature in which the distance towards the border is increasing exponentially ,22,Our neural ranking models the relationships between QA pairs in Hyperbolic space instead of Euclidean space .,"Intuitively , this makes it suitable for learning embeddings that reflect a natural hierarchy ( e.g. , networks , text , etc. ) which we believe might benefit neural ranking models for QA .",abstract
relation_extraction,13,"In this paper , we build on extensions of Harris ' distributional hypothesis to relations , as well as recent advances in learning text representations ( specifically , BERT ) , to build task agnostic relation representations solely from entity - linked text .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03286384976525822,0.7142857142857143,0.7142857142857143,In this paper we build on extensions of Harris distributional hypothesis to relations as well as recent advances in learning text representations specifically BERT to build task agnostic relation representations solely from entity linked text ,36,"However , both of these approaches are limited in their ability to generalize .",We show that these representations significantly outperform previous work on exemplar based relation extraction ( FewRel ) even without using any of that task 's training data .,abstract
natural_language_inference,70,Results : shows the results on Subtask B .,result,Subtask C Model:,0,154,32,11,0,result : Subtask C Model:,0.88,0.6037735849056604,0.34375,Results shows the results on Subtask B ,8,"We made two additional submissions in which the model has minor variations : in the Contrastive 1 ( KC1 ) , we substituted All SPTK with SPTK + whereas in the contrastive 2 ( KC2 ) we do not include the features derived from the Subtask A scores .","On the official test set , our primary submission achieved the third position w.r.t. MAP among 11 systems .",result
natural_language_inference,39,"The training objective is to minimize the following loss , summed over examples x , y gold :",experiment,Experimental Setup,0,151,21,21,0,experiment : Experimental Setup,0.7330097087378641,0.525,0.525,The training objective is to minimize the following loss summed over examples x y gold ,16,We used a hinge loss for the answer selection task on WikiQA and TrecQA data .,"where y gold is the ground truth label , input x is the pair of sentences x = { S 1 , S 2 } , ? is the model weight vector , and the function f ? ( x , y ) is the output of our model .",experiment
sentiment_analysis,13,"For AE , we found that great performance boost comes mostly from domain knowledge posttraining , which indicates that contextualized representations of domain knowledge are very important for AE .",analysis,analysis,1,259,12,12,0,analysis : analysis,0.9316546762589928,0.4615384615384616,0.4615384615384616,For AE we found that great performance boost comes mostly from domain knowledge posttraining which indicates that contextualized representations of domain knowledge are very important for AE ,28,We further investigated the examples improved by BERT - MRC and found that the boundaries of spans ( especially short spans ) were greatly improved .,"BERT - MRC has almost no improvement on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant .",result
relation_extraction,6,We use HeidelTime to extract dates .,system description,Data generation with Wikidata,0,57,12,12,0,system description : Data generation with Wikidata,0.4384615384615385,0.35294117647058826,0.6666666666666666,We use HeidelTime to extract dates ,7,We retrieve IDs for those entities by searching through entity labels in Wikidata .,"For each pair of entities , we query Wikidata for relation types that connect them .",method
sentiment_analysis,12,"Therefore , for a training sentence , if the prediction of ASC model is correct , we believe that it is reasonable to continue focusing on this context word .",approach,Basic Intuition,0,73,8,4,0,approach : Basic Intuition,0.32589285714285715,0.1111111111111111,0.4,Therefore for a training sentence if the prediction of ASC model is correct we believe that it is reasonable to continue focusing on this context word ,27,"Thus , the context word with the maximum attention weight has the most important impact on the sentiment prediction of the input sentence .","Conversely , the attention weight of this context word should be decreased .",method
sentiment_analysis,31,We introduce a novel parameterized convolutional neural network for aspect level sentiment classification .,abstract,abstract,0,4,2,2,0,abstract : abstract,0.025157232704402517,0.5,0.5,We introduce a novel parameterized convolutional neural network for aspect level sentiment classification ,14, ,"Using parameterized filters and parameterized gates , we incorporate aspect information into convolutional neural networks ( CNN ) .",abstract
sentiment_analysis,49,Accuracy score is used as the evaluation metric .,experiment,Experiments,0,148,4,4,0,experiment : Experiments,0.5849802371541502,0.19047619047619047,0.19047619047619047,Accuracy score is used as the evaluation metric ,9,We evaluate our proposed approach for CMU - MOSI ( test data ) & CMU - MOSEI ( dev data ),"We use Bi-directional GRUs having 300 neurons , each followed by a dense layer consisting of 100 neurons .",experiment
natural_language_inference,14,This produces an accuracy of 75.2 % for BERT ( using the cross-validation grid search ) and 63.6 % for GPT . :,result,result,1,124,5,5,0,result : result,0.7251461988304093,0.7142857142857143,0.7142857142857143,This produces an accuracy of 75 2 for BERT using the cross validation grid search and 63 6 for GPT ,21,"To adjust for the difference in size between our dataset and SWAG , we also train the models on a sample of 2,241 SWAG questions ( the size of the training set in each of CODAH 's crossvalidation folds ) and evaluate them on the full SWAG validation set .",Accuracy of BERT and GPT on different training settings when tested on CODAH .,result
machine-translation,7,Each expert has a larger hidden layer of size 8192 .,APPENDICES A LOAD-BALANCING LOSS,MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0,336,114,17,0,APPENDICES A LOAD-BALANCING LOSS : MACHINE TRANSLATION -EXPERIMENTAL DETAILS,0.900804289544236,0.7549668874172185,0.53125,Each expert has a larger hidden layer of size 8192 ,11,"We used noisy - top - k gating as described in Section 2.1 , not the scheme from Appendix F. The MoE layers in the encoder and decoder are non-hierarchical MoEs with n = 512 experts , and k = 2 .","This doubles the amount of computation in the MoE layers , raising the computational budget of the entire model from 85 M to 102M ops / timestep .",others
natural_language_inference,75,The key addressing equation is transformed accordingly to use the updated query :,model,Model Description,0,69,20,20,0,model : Model Description,0.3399014778325123,0.12987012987012986,0.6896551724137931,The key addressing equation is transformed accordingly to use the updated query ,13,"The memory access is then repeated ( specifically , only the addressing and reading steps , but not the hashing ) , using a different matrix R j on each hop , j.",The motivation for this is that new evidence can be combined into the query to focus on and retrieve more pertinent information in subsequent accesses .,method
text_summarization,8,"We instead stick to the theme of modifying inference , and modify the scoring function to include a length penalty lp and a coverage penalty cp , and is defined as s ( x , y ) = log p ( y |x ) / lp ( x ) + cp ( x ; y ) .",system description,Inference,0,149,79,4,0,system description : Inference,0.5209790209790209,0.8681318681318682,0.25,We instead stick to the theme of modifying inference and modify the scoring function to include a length penalty lp and a coverage penalty cp and is defined as s x y log p y x lp x cp x y ,42,Proposed solutions include modifying models with extensions such as a coverage mechanism or intra-sentence attention .,"To encourage the generation of longer sequences , we apply length normalizations during beam search .",method
natural_language_inference,48,"Given a query gate g q , producing r q , and a document gate g d , producing rd , the inputs of the inference GRU are given by the reset version of the query and document glimpses , i.e. , .",result,Gating Search Results,0,109,11,11,0,result : Gating Search Results,0.4932126696832579,0.6111111111111112,0.8461538461538461,Given a query gate g q producing r q and a document gate g d producing rd the inputs of the inference GRU are given by the reset version of the query and document glimpses i e ,38,"The fourth argument of the gate takes into account multiplicative interactions between query and document glimpses , making it easier to determine the degree of matching between them .","Given a query gate g q , producing r q , and a document gate g d , producing rd , the inputs of the inference GRU are given by the reset version of the query and document glimpses , i.e. , .",result
natural_language_inference,4,"In particular , we note that without mixing in the cross entropy loss , it is extremely difficult to learn the policy .",experiment,EXPERIMENTS,0,147,25,25,0,experiment : EXPERIMENTS,0.7424242424242424,0.6410256410256411,0.6410256410256411,In particular we note that without mixing in the cross entropy loss it is extremely difficult to learn the policy ,21,The training curves for DCN + with reinforcement learning and DCN + without reinforcement learning are shown in to illustrate the effectiveness of our proposed mixed objective .,"When we combine the cross entropy loss with the reinforcement learning objective , we find that the model initially performs worse early on as it begins policy learning from scratch ( shown in ) .",experiment
natural_language_inference,5,"The first direction ( i.e. , the fine - grained approach ) finds an exact answer to a question within a given passage .",introduction,introduction,0,12,4,4,0,introduction : introduction,0.08163265306122447,0.17391304347826084,0.17391304347826084,The first direction i e the fine grained approach finds an exact answer to a question within a given passage ,21,"Recently , research on this task has taken two major directions based on the answer span considered by the model .","The second direction ( i.e. , the coarse - level approach ) is an information retrieval ( IR ) - based approach that provides the most relevant sentence from a given document in response to a question .",introduction
semantic_parsing,0,"The output can be in many formats , e.g. , logic forms .",system description,Related Work and Existing Datasets,0,46,3,3,0,system description : Related Work and Existing Datasets,0.16546762589928055,0.028571428571428567,0.08823529411764706,The output can be in many formats e g logic forms ,12,Several semantic parsing datasets with different queries have been created .,"These datasets include ATIS , , and JOBS ( Tang and Mooney , 2001 a ) .",method
part-of-speech_tagging,2,"As shown in and 2 ( e ) , our transfer learning approach can improve the performance on Twitter POS tagging and NER for all labeling rates , and the improvements with 0.1 labels are more than 8 % for both datasets .",performance,TRANSFER LEARNING PERFORMANCE,1,146,15,15,0,performance : TRANSFER LEARNING PERFORMANCE,0.8202247191011236,0.36585365853658536,0.6,As shown in and 2 e our transfer learning approach can improve the performance on Twitter POS tagging and NER for all labeling rates and the improvements with 0 1 labels are more than 8 for both datasets ,39,"For example , as shown in ( a ) , we can obtain an tagging accuracy of 83 %+ with zero labels and 92 % with only 0.001 labels when transferring from PTB to Genia .",Cross - application transfer also leads to substantial improvement under low - resource conditions .,result
natural_language_inference,91,"This is a simple sequence - based LSTM RNN that operates in tandem with the model , taking inputs from the buffer and stack at each step .",system description,The tracking LSTM,0,75,22,3,0,system description : The tracking LSTM,0.3218884120171674,0.5365853658536586,0.2727272727272727,This is a simple sequence based LSTM RNN that operates in tandem with the model taking inputs from the buffer and stack at each step ,26,"In addition to the stack , buffer , and composition function , our full model includes an additional component : the tracking LSTM .","It is meant to maintain a low - resolution summary of the portion of the sentence that has been processed so far , which is used for two purposes : it supplies feature representations to the transition classifier , which allows the model to standalone as a parser , and it additionally supplies a secondary input e to the composition function - see Why a tree - sequence hybrid ? Lexical ambiguity is ubiquitous in natural language .",method
natural_language_inference,65,"Specifically , AP enables the pooling layer to be aware of the current input pair , in a way that information from the two input items can directly influence the computation of each other 's representations .",introduction,introduction,1,16,8,8,0,introduction : introduction,0.06956521739130435,0.2424242424242425,0.2424242424242425,Specifically AP enables the pooling layer to be aware of the current input pair in a way that information from the two input items can directly influence the computation of each other s representations ,35,"The key contribution of this work is that we propose Attentive Pooling ( AP ) , a two - way attention mechanism , that significantly improves such discriminative models ' performance on pair - wise ranking or classification , by enabling a joint learning of the representations of both inputs as well as their similarity measurement .","The main idea in AP consists of learning a similarity measure over projected segments ( e.g. trigrams ) of the two items in the input pair , and using the similarity scores between the segments to compute attention vectors in both directions .",introduction
relation_extraction,9,"Given a sentence S with a labeled pair of entity mentions e 1 and e 2 ( as in our example from Section 1 ) , relation classification is the task of identifying the semantic relation holding between e 1 and e 2 among a set of candidate relation types .",model,The Proposed Model,0,49,2,2,0,model : The Proposed Model,0.23444976076555024,0.022727272727272728,0.16666666666666666,Given a sentence S with a labeled pair of entity mentions e 1 and e 2 as in our example from Section 1 relation classification is the task of identifying the semantic relation holding between e 1 and e 2 among a set of candidate relation types ,48, ,"Since the only input is a raw sentence with two marked mentions , it is non-trivial to obtain all the lexical , semantic and syntactic cues necessary to make an accurate prediction .",method
question_answering,2,The maximum function maxi is used reduce the first dimension of the high - dimensional tensor .,architecture,Cross Sequence Interaction,0,168,89,19,0,architecture : Cross Sequence Interaction,0.6064981949458483,0.8165137614678899,0.4871794871794872,The maximum function maxi is used reduce the first dimension of the high dimensional tensor ,16,"We obtain the visual - text sequence attention matrix A ? R T 2 by A = sof tmax ( max M i=1 ( S i : : )) and the visual - text attention vector B ? R 2 by B = sof tmax ( max T i = 1 max M j=1 ( S ji : ) ) , where the softmax operation is applied to the first dimension .",Then the attended context vector is given by :,method
natural_language_inference,4,"In addition , we concatenate these embeddings with context vectors ( CoVe ) trained on .",experiment,EXPERIMENTS,1,128,6,6,0,experiment : EXPERIMENTS,0.6464646464646465,0.15384615384615385,0.15384615384615385,In addition we concatenate these embeddings with context vectors CoVe trained on ,13,"For word embeddings , we use GloVe embeddings pretrained on the 840B Common Crawl corpus as well as character ngram embeddings by .","For out of vocabulary words , we set the embeddings and context vectors to zero .",experiment
natural_language_inference,34,"To mimic human reasoning behavior , we develop five components in our proposed QA system (: a paragraph selection subnetwork , a module for entity graph construction , an encoding layer , a fusion block for multi-hop reasoning , and a final prediction layer .",system description,Dynamically Fused Graph Network,0,108,5,5,0,system description : Dynamically Fused Graph Network,0.36610169491525424,0.04950495049504952,1.0,To mimic human reasoning behavior we develop five components in our proposed QA system a paragraph selection subnetwork a module for entity graph construction an encoding layer a fusion block for multi hop reasoning and a final prediction layer ,40,"One starts from an entity of interest in the query , focuses on the words surrounding the start entities , connects to some related entity either found in the neighborhood or linked by the same surface mention , repeats the step to form a reasoning chain , and lands on some entity or snippets likely to be the answer .", ,method
text_generation,5,"Following , we also use drop word for the LSTM decoder , the drop word ratio is selected from [ 0 , 0.3 , 0.5 , 0.7 ] .",system description,Model configurations and Training details,1,177,25,25,0,system description : Model configurations and Training details,0.6,0.78125,0.78125,Following we also use drop word for the LSTM decoder the drop word ratio is selected from 0 0 3 0 5 0 7 ,25,"We select dropout ratio of LSTMs ( both encoder and decoder ) from [ 0.3 , 0.5 ] .","For the CNN decoder , we use a dropout ratio of 0.1 at each layer .",method
part-of-speech_tagging,1,We conduct detailed studies on investigating the gap between learning word representations and sentence representations .,introduction,introduction,0,47,35,35,0,introduction : introduction,0.188,0.813953488372093,0.813953488372093,We conduct detailed studies on investigating the gap between learning word representations and sentence representations ,16,The main contributions of this paper are the following :,We provide in - depth experiments and empirical comparisons of different convolutional models and explore the advantages and dis advantages of their components for learning character - to - word representations .,introduction
natural_language_inference,23,It is illustrated as arrow in .,architecture,END-TO-END ARCHITECTURE,0,159,87,15,0,architecture : END-TO-END ARCHITECTURE,0.30994152046783624,0.6126760563380281,0.7142857142857143,It is illustrated as arrow in ,7,about what kind of words are in Q .,"For this component , we follow the approach in First , a feature vector em i is created for each word in C to indicate whether the word occurs in the question Q. Second , attention - based fusion on Glo Ve embedding g i is used",method
machine-translation,3,Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path .,system description,Neural Machine Translation,0,63,21,21,0,system description : Neural Machine Translation,0.2012779552715655,0.175,0.65625,Different kinds of shortcut connections are proposed to decrease the length of the gradient propagation path ,17,"In computer vision , models with more than ten convolution layers outperform shallow ones on a series of image tasks in recent years .","Training networks based on LSTM layers , which are widely used in language problems , is a much more challenging task .",method
sentence_classification,2,"We posit that a can be used to fix b when a is used as a context of b , and vice versa",introduction,introduction,0,39,29,29,0,introduction : introduction,0.15476190476190474,0.6904761904761905,0.6904761904761905,We posit that a can be used to fix b when a is used as a context of b and vice versa,22,Suppose there are two translated sentences a and b with slight errors .,"Revisiting the example above , to fix the vector of the English sentence "" The movie is terribly amazing "" , we use the Korean translation to move the vector towards the location where the vector "" The movie is greatly magnificent "" is .",introduction
text_summarization,4,This may introduce errors and irrelevance to the summary .,system description,Possible issues,0,65,26,10,0,system description : Possible issues,0.25193798449612403,0.8387096774193549,0.6666666666666666,This may introduce errors and irrelevance to the summary ,10,"Second , the linked entities may also be too common to be considered an entity .","In the example , "" Wednesday "" is erroneous because it is wrongly linked to the entity "" Wednesday Night Baseball "" .",method
natural_language_inference,3,ARC - II and MV - LSTM .,model,model,0,26,3,3,0,model : model,0.125,0.02654867256637168,0.15789473684210525,ARC II and MV LSTM ,6,These models directly build an interaction space between two sentences and model the interaction at different positions .,These models enable the model to easily capture the difference between semantic capacity of two sentences .,method
natural_language_inference,69,Answer does not follow .,analysis,36%,0,170,17,7,0,analysis : 36%,0.4927536231884058,0.3148148148148148,0.35,Answer does not follow ,5,11 % Only single document required .,12 % WIKIDATA / WIKIPEDIA discrepancy .,result
sentence_compression,0,"For example , the method above as well as the original method by can not impose any length constraint on the compressed sentences .",model,Global Inference through ILP,0,106,49,4,0,model : Global Inference through ILP,0.3799283154121864,0.5632183908045977,0.5714285714285714,For example the method above as well as the original method by can not impose any length constraint on the compressed sentences ,23,We hypothesize that there are also hard constraints we can impose on the compressed sentences .,"This is because the labels y 1 , y 2 , . . . , y n are not jointly predicted .",method
relation_extraction,0,"Also , there are relation types namely Physical ( PHYS ) , Person - Social ( PER - SOC ) , Organization - Affiliation ( ORG - AFF ) , Agent - Artifact ( ART ) , GPE - Affiliation ( GPE - AFF ) .",experiment,Data,0,158,7,6,0,experiment : Data,0.6171875,0.6363636363636364,0.6,Also there are relation types namely Physical PHYS Person Social PER SOC Organization Affiliation ORG AFF Agent Artifact ART GPE Affiliation GPE AFF ,24,"For the scope of this paper , we only use the entity head phrase similar to and .",ACE05 has a total of 6 relation types including PART - WHOLE .,experiment
sentiment_analysis,20,We use bidirectional LSTM ( BiLSTM ) in order to get word annotations that summarize the information from both directions .,model,MSA Model (message-level),0,87,11,9,0,model : MSA Model (message-level),0.4677419354838709,0.15942028985507245,0.375,We use bidirectional LSTM BiLSTM in order to get word annotations that summarize the information from both directions ,19,"An LSTM takes as input the words of a tweet and produces the word annotations H = ( h 1 , h 2 , ... , h T ) , where hi is the hidden state of the LSTM at time - step i , summarizing all the information of the sentence up to xi .",bidirectional LSTM consists of a forward LSTM ? ? f that reads the sentence from x 1 to x T and a backward LSTM ? ? f that reads the sentence from x T to x 1 .,method
sentiment_analysis,13,"To post-train on domain knowledge , we leverage the two novel pre-training objectives from BERT : masked language model ( MLM ) and next sentence 7 prediction ( NSP ) .",system description,Post-training,0,141,65,6,0,system description : Post-training,0.5071942446043165,0.65,0.14634146341463414,To post train on domain knowledge we leverage the two novel pre training objectives from BERT masked language model MLM and next sentence 7 prediction NSP ,27,"Eventually , we aim to have a general - purpose post - training strategy that can exploit the above two kinds of knowledge for end tasks .",The former predicts randomly masked words and the latter detects whether two sides of the input are from the same document or not .,method
text_summarization,3,"The network is then optimized end - to - end using reinforcement learning , with the distant - supervision strategy as a complement to further improve the summary .",introduction,introduction,1,35,23,23,0,introduction : introduction,0.1446280991735537,0.9583333333333334,0.9583333333333334,The network is then optimized end to end using reinforcement learning with the distant supervision strategy as a complement to further improve the summary ,25,The optimization function is adaptive so as to cater for different datasets with distantly - supervised training .,"Overall , the contributions of this paper are : 1 ) a novel concept pointer generator network that leverages context - aware conceptualization and a concept pointer , both of which are jointly integrated into the generator to deliver informative and abstract - oriented summaries ; 2 ) a novel distant supervision training strategy that favors model adaptation and generalization , which results in performance that outperforms the wellaccepted evaluation - based reinforcement learning optimization on a test - only dataset in terms of ROUGE metrics ; 3 ) a statistical analysis of quantitative results and human evaluations from comparative experiments with several state - of - the - art models that shows the proposed method provides promising performance .",introduction
natural_language_inference,30,The scoring function becomes :,architecture,Fine-tuning the Similarity between Embeddings,0,173,54,12,0,architecture : Fine-tuning the Similarity between Embeddings,0.6705426356589147,0.8709677419354839,0.6,The scoring function becomes ,5,We propose to learn a matrix M ? R kk parameterizing the similarity between words and triples embeddings .,has only k 2 parameters and can be efficiently determined by solving the following convex problem ( fixing the embedding matrices W and V ) :,method
natural_language_inference,0,"Specifically , for i = 1 , . . . , | D|:",model,Gated-Attention Module,0,81,20,4,0,model : Gated-Attention Module,0.41116751269035534,0.4166666666666667,0.5714285714285714,Specifically for i 1 D ,6,"For each token d i in D , the GA module forms a token - specific representation of the queryq i using soft attention , and then multiplies the query representation element - wise with the document token representation .",In equation we use the multiplication operator to model the interactions between d i andq i .,method
sentiment_analysis,37,"After H hops , q ( H ) becomes the target - aspectaware sentence representation vector for the final classification :",model,Multiple Hops,0,129,69,9,0,model : Multiple Hops,0.5098814229249012,0.9324324324324323,0.6428571428571429,After H hops q H becomes the target aspectaware sentence representation vector for the final classification ,17,"Output memory vectors of hop ? , Q ?(? ) , is updated as the input memory vectors of hop ? +","where W smax ? R DoC , b smax ? RC , and ? is the estimated sentiment polarity ( 0 for negative , 1 for positive , and 2 for neutral ) .",method
natural_language_inference,0,"Best performance on standard training sets is in bold , and on larger training sets in italics .",analysis,GA Reader Analysis,0,168,17,17,0,analysis : GA Reader Analysis,0.8527918781725888,0.7083333333333334,0.7083333333333334,Best performance on standard training sets is in bold and on larger training sets in italics ,17,"Results marked with "" "" were obtained by training on a larger training set .",At the bottom of we show the effect of varying the number of hops K of the GA Reader on the final performance .,result
natural_language_inference,83,"Using these definitions and assumptions , the probability of an answer given the context and the ending - options can be modeled as :",model,Hidden Coherence Model,0,121,11,11,0,model : Hidden Coherence Model,0.3954248366013072,0.24444444444444444,0.24444444444444444,Using these definitions and assumptions the probability of an answer given the context and the ending options can be modeled as ,22,"The model achieves this by assuming that each instance belongs to a latent category , z ? { 1 , 2 , 3 . . . K} , which advises the model on the importance of these aspects for the given instance .","We parameterize P ( z |c , o 1 , o 2 ) as :",method
named-entity-recognition,9,The Species - 800 dataset was preprocessed and split based on the dataset of Pyysalo ( https://github. com/spyysalo/s800 ) .,dataset,Datasets,0,103,6,6,0,dataset : Datasets,0.5175879396984925,0.3,0.3,The Species 800 dataset was preprocessed and split based on the dataset of Pyysalo https github com spyysalo s800 ,20,We used the CoNLL format ( https :// github.com/spyysalo/standoff2conll ) for pre-processing the 2010 i 2b2 / VA and JNLPBA datasets .,"We did not use alternate annotations for the BC2 GM dataset , and all NER evaluations are based on entity - level exact matches .",experiment
sentiment_analysis,13,"that are intended to provide additional features for a particular architecture that bears human 's understanding of the end task , BERT adopts a fine - tuning approach that requires almost no specific architecture for each end task .",system description,BERT,0,82,6,4,0,system description : BERT,0.2949640287769784,0.06,0.3333333333333333,that are intended to provide additional features for a particular architecture that bears human s understanding of the end task BERT adopts a fine tuning approach that requires almost no specific architecture for each end task ,37,"The idea behind the progress is that even though the word embedding layer ( in a typical neural network for NLP ) is trained from large - scale corpora , training a wide variety of neural architectures that encode contextual representations only from the limited supervised data on end tasks is insufficient .",This is desired as an intelligent agent should minimize the use of prior human knowledge in the model design .,method
part-of-speech_tagging,0,"Finally , we study the effects of AT on word representation learning ( 5.3 ) , and the applicability of AT to different sequential tasks ( 5.4 ) .",analysis,Analysis,0,158,7,7,0,analysis : Analysis,0.6422764227642277,0.2692307692307692,1.0,Finally we study the effects of AT on word representation learning 5 3 and the applicability of AT to different sequential tasks 5 4 ,25,"We illustrate these findings using two major languages , English ( WSJ ) and French ( UD ) , which have substantially large training and testing data to discuss vocabulary statistics and sentence - level performance .", ,result
machine-translation,9,Word embeddings play an important role in neural - based natural language processing ( NLP ) models .,introduction,introduction,0,15,2,2,0,introduction : introduction,0.05226480836236934,0.03773584905660377,0.03773584905660377,Word embeddings play an important role in neural based natural language processing NLP models ,15, ,Neural word embeddings encapsulate the linguistic information of words in continuous vectors .,introduction
natural_language_inference,23,The ablation study in Section 4.4 demonstrates that this lightweight enhancement offers a decent improvement in performance .,architecture,FULLY-AWARE FUSION NETWORK,0,127,55,3,0,architecture : FULLY-AWARE FUSION NETWORK,0.2475633528265107,0.3873239436619718,0.15,The ablation study in Section 4 4 demonstrates that this lightweight enhancement offers a decent improvement in performance ,19,This allows us to be fully aware of the complete understanding of each word .,"To fully utilize history - of - word in attention , we need a suitable attention scoring function S (x , y ) .",method
sentiment_analysis,24,"However , since no opinion is expressed in the given sentence , "" Pizza "" should not be considered as an aspect term .",analysis,Results and Analysis,0,264,51,51,0,analysis : Results and Analysis,0.8488745980707395,0.8793103448275862,0.8793103448275862,However since no opinion is expressed in the given sentence Pizza should not be considered as an aspect term ,20,"As observed in example 2 , both PIPELINE and INABSA extract "" Pizza "" .",IMN avoids extracting this kind of terms by aggregating knowledge from opinion prediction and sentiment prediction .,result
sentiment_analysis,41,"Classical models including Recursive Neural Network , Recursive Neural Tensor Network , Recurrent Neural Network , LSTM and Tree - LSTMs were applied into sentiment analysis currently .",system description,Sentiment Classification with Neural Networks,0,50,12,3,0,system description : Sentiment Classification with Neural Networks,0.2242152466367713,0.15584415584415584,0.16666666666666666,Classical models including Recursive Neural Network Recursive Neural Tensor Network Recurrent Neural Network LSTM and Tree LSTMs were applied into sentiment analysis currently ,24,"Since a simple and effective approach to learn distributed representations was proposed , neural networks advance sentiment analysis substantially .","By utilizing syntax structures of sentences , tree - based LSTMs have been proved to be quite effective for many NLP tasks .",method
named-entity-recognition,9,Predicted named entities for NER and predicted answers for QA are in bold .,BioBERT,Note:,0,198,17,2,0,BioBERT : Note:,0.9949748743718592,0.9444444444444444,1.0,Predicted named entities for NER and predicted answers for QA are in bold ,14, , ,others
natural_language_inference,95,1 ] A culture is a society 's total way of living and a society is a group . . . 1.0 10 ?2 1.0 10 ?1 1.1 10 ?1,result,Results on DuReader,0,170,10,10,0,result : Results on DuReader,0.7264957264957265,0.625,0.625,1 A culture is a society s total way of living and a society is a group 1 0 10 2 1 0 10 1 1 1 10 1,29,What is the difference between a mixed and pure culture Scores Answer Candidates :,2 ] The mixed economy is a balance between socialism and capitalism .,result
natural_language_inference,3,"The word embeddings for all of the models are initialized with the 100d GloVe vectors ( 840B token version , ) and fine - tuned during training to improve the performance .",training,Hyperparameters and Training,1,151,2,2,0,training : Hyperparameters and Training,0.7259615384615384,0.5,0.5,The word embeddings for all of the models are initialized with the 100d GloVe vectors 840B token version and fine tuned during training to improve the performance ,28, ,"The other parameters are initialized by randomly sampling from uniform distribution in [ ? 0.1 , 0.1 ] .",experiment
natural_language_inference,20,We build on top of one such model and propose a hierarchy of BiLSTM and max pooling layers that implements an iterative refinement strategy and yields state of the art results on the SciTail dataset as well as strong results for SNLI and MultiNLI .,abstract,abstract,0,6,4,4,0,abstract : abstract,0.02469135802469136,0.6666666666666666,0.6666666666666666,We build on top of one such model and propose a hierarchy of BiLSTM and max pooling layers that implements an iterative refinement strategy and yields state of the art results on the SciTail dataset as well as strong results for SNLI and MultiNLI ,45,Recurrent neural networks have proven to be very effective in learning distributed representations and can be trained efficiently on natural language inference tasks .,"We can show that the sentence embeddings learned in this way can be utilized in a wide variety of transfer learning tasks , outperforming InferSent on 7 out of 10 and SkipThought on 8 out of 9 SentEval sentence embedding evaluation tasks .",abstract
text_summarization,12,"If | y | ? | x | , which means all words in summary y must appear in given input , we denote this as extractive sentence summarization .",system description,Problem Formulation,0,72,6,6,0,system description : Problem Formulation,0.3157894736842105,0.4615384615384616,0.6666666666666666,If y x which means all words in summary y must appear in given input we denote this as extractive sentence summarization ,23,"For sentence summarization , given an input sentence x = ( x 1 , x 2 , . . . , x n ) , where n is the sentence length , x i ? V sand V sis the source vocabulary , the system summarizes x by producing y = ( y 1 , y 2 , . . . , y l ) , where l ? n is the summary length , y i ? Vt and Vt is the target vocabulary .","If | y | | x | , which means not all words in summary come from input sentence , we denote this as abstractive sentence summarization .",method
relation-classification,9,Pretraining BERT for long sentences can be slow .,system description,SCIBERT,0,69,12,6,0,system description : SCIBERT,0.4693877551020408,0.2553191489361702,0.35294117647058826,Pretraining BERT for long sentences can be slow ,9,The other two models that use the new SCIVOCAB are trained from scratch .,"Following the original BERT code , we set a maximum sentence length of 128 tokens , and train the model until the training loss stops decreasing .",method
text-classification,7,"common approach to the question is to treat the texts as sequences and focus on their spatial patterns , whose representatives include convolutional neural networks ( CNNs ) and long shortterm memory networks ( LSTMs ) .",introduction,introduction,0,19,9,9,0,introduction : introduction,0.07818930041152264,0.32142857142857145,0.32142857142857145,common approach to the question is to treat the texts as sequences and focus on their spatial patterns whose representatives include convolutional neural networks CNNs and long shortterm memory networks LSTMs ,32,"In order to model higher level concepts and facts in texts , an NLP researcher has to think cautiously the so - called "" what "" question : what is actually modeled beyond word meanings .","Another common approach is to completely ignore the order of words but focus on their compositions as a collection , whose representatives include probabilistic topic modeling and Earth Mover 's Distance based modeling .",introduction
natural_language_inference,20,It has been argued that the lexical similarity of the sentences in SciTail sentence pairs make it a particularly difficult dataset ) .,evaluation,SciTail,0,143,61,7,0,evaluation : SciTail,0.588477366255144,0.6489361702127661,0.875,It has been argued that the lexical similarity of the sentences in SciTail sentence pairs make it a particularly difficult dataset ,22,The results achieved by our proposed model are significantly higher than the previously published results .,"If this is the case , we hypothesize that our model is indeed better at identifying entailment relations beyond focusing on the lexical similarity of the sentences .",result
text_summarization,2,Our approach surpasses state - of - the - art published systems on the benchmark dataset .,introduction,introduction,0,34,25,25,0,introduction : introduction,0.1223021582733813,0.9615384615384616,0.9615384615384616,Our approach surpasses state of the art published systems on the benchmark dataset ,14,"To the best of our knowledge , this is the first attempt at comparing various neural architectures for this purpose ; we study the effectiveness of several important components , including the vocabulary size , a coveragebased regularizer , and a beam search with reference mechanism ; through extensive experiments we demonstrate that incorporating syntactic information in neural sentence summarization is effective .", ,introduction
relation-classification,2,We employ a BiLSTM which is able to encode information from left to right ( past to future ) and right to left ( future to past ) .,model,Bidirectional LSTM encoding layer,0,134,29,4,0,model : Bidirectional LSTM encoding layer,0.4542372881355932,0.4461538461538462,0.6666666666666666,We employ a BiLSTM which is able to encode information from left to right past to future and right to left future to past ,25,"In this work , we use multi - layer LSTMs , a specific kind of RNNs which are able to capture long term dependencies well .","This way , we can combine bidirectional information for each word by concatenating the forward ( hi ) and the backward ( hi ) output at timestep i .",method
sentence_classification,0,"We are only interested in the output y ( 1 ) and the rest of outputs ( y ( 2 ) , ... , y ( n ) ) are regarding the scaffold tasks and only used in training to inform the model of knowledge in the structure of the scientific documents .",model,Structural scaffolds,0,75,48,29,0,model : Structural scaffolds,0.2808988764044944,0.96,0.935483870967742,We are only interested in the output y 1 and the rest of outputs y 2 y n are regarding the scaffold tasks and only used in training to inform the model of knowledge in the structure of the scientific documents ,42,"In particular , given the vector z we pass it ton MLPs and obtain n output vectors y ( i ) :","For each task , we output the class with the highest probability in y .",method
natural_language_inference,69,"simple majority class baseline would thus prove successful , but would tell us little about multi-hop reasoning .",dataset,Mitigating Dataset Biases,0,124,7,7,0,dataset : Mitigating Dataset Biases,0.3594202898550725,0.875,0.875,simple majority class baseline would thus prove successful but would tell us little about multi hop reasoning ,18,"For example , in the majority of the samples the property country has the United States of America as the answer .","To combat this issue , we subsampled the dataset to ensure that samples of anyone particular answer candidate makeup no more than 0.1 % of the dataset , and omitted articles about the United States .",experiment
part-of-speech_tagging,0,This result again corroborates the data augmentation power of AT under small training examples .,analysis,Word-level Analysis,0,167,16,9,0,analysis : Word-level Analysis,0.6788617886178862,0.6153846153846154,0.5,This result again corroborates the data augmentation power of AT under small training examples ,15,"In both languages , the AT model achieves large improvements over the baseline on rare words ( e.g. , frequency 1 - 10 in training ) , as opposed to more frequent words .","On the other hand , we did not observe meaningful improvements on unseen words ( frequency 0 in training ) .",result
relation_extraction,11,We briefly present the components of RESIDE below .,system description,RESIDE Overview,0,100,33,5,0,system description : RESIDE Overview,0.4032258064516129,0.2578125,0.25,We briefly present the components of RESIDE below ,9,"RESIDE consists of three components for learning a representation of a given bag , which is fed to a softmax classifier .",Each component will be described in detail in the subsequent sections .,method
machine-translation,5,The models were trained using two sets of back - translated data in a 1 - to - 1 proportion to the clean parallel data - one set was backtranslated using a system trained on parallelonly data and the other set -using an NMT system trained on parallel data and the first set of back - translated data .,system,System Overview,1,33,10,10,0,system : System Overview,0.2307692307692308,0.13513513513513514,1.0,The models were trained using two sets of back translated data in a 1 to 1 proportion to the clean parallel data one set was backtranslated using a system trained on parallelonly data and the other set using an NMT system trained on parallel data and the first set of back translated data ,54,Constrained English - Estonian and Estonian - English NMT systems ( tilde - c - nmt - 2 bt ) averaged from multiple best NMT models ., ,method
question_answering,4,"The key idea is that self - attention models each word in the query - dependent passsage representation against all other words , enabling each word to benefit from a wider global view of the context .",system description,Gated Attention,0,127,84,9,0,system description : Gated Attention,0.4941634241245136,0.8076923076923077,0.4736842105263158,The key idea is that self attention models each word in the query dependent passsage representation against all other words enabling each word to benefit from a wider global view of the context ,34,"Gated Self - Attention Next , we employ a self - attention layer , applying Equation yet again on P , matching P against itself to form B , the output representation of the core layer .","At this point , we note that there are two intermediate representations of P , i.e. , one after the gated bi-attention layer and one after the gated self - attention layer .",method
question_generation,0,"To capture more context information , we use bidirectional GRU ( BiGRU ) to read the inputs in both forward and backward orders .",approach,Feature-Rich Encoder,0,31,6,3,0,approach : Feature-Rich Encoder,0.17032967032967036,0.16216216216216214,0.16666666666666666,To capture more context information we use bidirectional GRU BiGRU to read the inputs in both forward and backward orders ,21,"In the NQG framework , we use Gated Recurrent Unit ( GRU ) to build the encoder .","Inspired by ; , the BiGRU encoder not only reads the sentence words , but also handcrafted features , to produce a sequence of word - and - feature vectors .",method
relation_extraction,7,"In the experiment , we utilize word2vec 2 to train word embeddings on NYT corpus .",experiment,Experimental Settings,1,191,2,2,0,experiment : Experimental Settings,0.7374517374517374,0.04878048780487805,0.2,In the experiment we utilize word2vec 2 to train word embeddings on NYT corpus ,15, ,We use the cross-validation to tune our model and grid search to determine model parameters .,experiment
natural_language_inference,100,shows the comparison between a NMM with previous methods using feature engineering on TRAIN - ALL without combining additional features .,result,Learning without Combining Additional Features,0,312,51,16,0,result : Learning without Combining Additional Features,0.8524590163934426,0.6219512195121951,0.8,shows the comparison between a NMM with previous methods using feature engineering on TRAIN ALL without combining additional features ,20,We leave the study of impact of the number of hidden layers in a NMM to future work .,We find that both a NMM - 1 and a NMM - 2 achieve better performance comparing with other methods using feature engineering .,result
natural_language_inference,79,Both approaches have weaknesses .,introduction,introduction,0,38,26,26,0,introduction : introduction,0.1417910447761194,0.7647058823529411,0.7647058823529411,Both approaches have weaknesses ,5,"more sophisticated approach is combining the word vectors in an order given by a parse tree of a sentence , using matrix - vector operations .","The first approach , weighted averaging of word vectors , loses the word order in the same way as the standard bag - of - words models do .",introduction
named-entity-recognition,2,Speed Bi-LSTM-CRF 1 Bi-LSTM 4.60 ID- CNN 7.96 85.76 0.13 21.19 ID-CNN 85.27 0.24 13.21 ID - CNN ( 1 block ) 84.28 0.10 26.01 : F1 score of sentence and document models on OntoNotes .,model,Model,0,198,2,2,0,model : Model,0.9295774647887324,0.15384615384615385,0.15384615384615385,Speed Bi LSTM CRF 1 Bi LSTM 4 60 ID CNN 7 96 85 76 0 13 21 19 ID CNN 85 27 0 24 13 21 ID CNN 1 block 84 28 0 10 26 01 F1 score of sentence and document models on OntoNotes ,47, ,"icalized greedy model of , and our ID - CNN out - performs the Bi - LSTM as well as the more complex model of which leverages the parallel coreference annotation available in the OntoNotes corpus to predict named entities jointly with entity linking and co-reference .",method
natural_language_inference,42,In this section we analyze the performance of FABIR and BiDAF in the different types of question in SQuAD .,evaluation,FABIR and BiDAF Statistics,1,255,30,2,0,evaluation : FABIR and BiDAF Statistics,0.8823529411764706,0.5660377358490566,0.08,In this section we analyze the performance of FABIR and BiDAF in the different types of question in SQuAD ,20, ,"shows that shorter answers are easier for both models : while they reach more than 75 % F1 for answers that are shorter than four words , for answers longer than ten words these scores drop to 60.4 % and 67.3 % for FABIR and BiDAF , respectively .",result
sentiment_analysis,35,"Obviously , the i - th context word closer to a possible aspect term with a large value in ? f will have a larger position relevance p s i .",system description,Context2Aspect (C2A) Attention,0,142,83,50,0,system description : Context2Aspect (C2A) Attention,0.5725806451612904,0.8469387755102041,0.7692307692307693,Obviously the i th context word closer to a possible aspect term with a large value in f will have a larger position relevance p s i ,28,Then we calculate the source position relevance for i - th context word with the aid of C2F attention weights by,"Obviously , the i - th context word closer to a possible aspect term with a large value in ? f will have a larger position relevance p s i .",method
natural_language_inference,10,"Gumbel - Softmax distribution is motivated by Gumbel - Max trick , an algorithm for sampling from a categorical distribution .",model,Gumbel-Softmax,0,74,18,6,0,model : Gumbel-Softmax,0.31223628691983124,0.2307692307692308,0.2222222222222222,Gumbel Softmax distribution is motivated by Gumbel Max trick an algorithm for sampling from a categorical distribution ,18,Gumbel - Softmax is known to have an advantage over score - functionbased gradient estimators such as REINFORCE which suffer from high variance and slow convergence .,: Visualization of forward and backward computation path of ST Gumbel - Softmax .,method
prosody_prediction,0,The initial word embeddings are fine - tuned during training .,experiment,Experimental Setup,0,93,13,11,0,experiment : Experimental Setup,0.484375,0.5416666666666666,0.5,The initial word embeddings are fine tuned during training ,10,For BiLSTM we use pre-trained 300D Glo Ve 840B word embeddings .,"As with BERT , we add one fullyconnected classifier layer on top of the BiLSTM , mapping the representation of each word to the labels .",experiment
text_generation,2,That ' s why we ' re the most important people for the African American community and we ' ve made a good response .,SeqGAN,SeqGAN,0,325,7,7,0,SeqGAN : SeqGAN,0.9285714285714286,0.21875,0.21875,That s why we re the most important people for the African American community and we ve made a good response ,22,"am proud to be able to move forward because we do n't have to look at about , "" he said .","But the move will be only in a fight against them , as well as likely to prevent an agreement to remain in the EU .",others
natural_language_inference,84,By normalizing L with row - wise and column - wise softmaxes we construct context - to - question and question - to - context attention maps AC and A Q .,system description,QUESTION ANSWERING,0,108,41,10,0,system description : QUESTION ANSWERING,0.4695652173913044,0.36936936936936937,0.17857142857142858,By normalizing L with row wise and column wise softmaxes we construct context to question and question to context attention maps AC and A Q ,26,"Next , we compute the affinity scores L = CQ T ? R n , m .","These are used to construct a joint question - document representation U 0 as a concatenation along the feature axis of the matrices C , AC Q and AC A T QC .",method
relation-classification,3,"Further , as seen in , the improvement for CoNLL04 is particularly small on the evaluation set .",result,Results,0,132,6,6,0,result : Results,0.9635036496350364,0.8571428571428571,0.8571428571428571,Further as seen in the improvement for CoNLL04 is particularly small on the evaluation set ,16,"However , the relation extraction performance increases by ? 1 % F 1 scoring points , except for the ACE04 dataset .","This may indicate a correlation between the dataset size and the benefit of adversarial training in the context of joint models , but this needs further investigation in future work .",result
sentiment_analysis,8,We feed the feature vectors as input to the network and finally pass the output of the LSTM network through a softmax layer to get probability scores for each of the six emotion classes .,model,model,0,154,19,19,0,model : model,0.6581196581196581,0.95,0.95,We feed the feature vectors as input to the network and finally pass the output of the LSTM network through a softmax layer to get probability scores for each of the six emotion classes ,35,shows the network implemented in this work .,"Since we are using feature vectors as input , we do not need another decoder network to transform it back from hidden to output space thereby reducing network size .",method
relation_extraction,12,"Moreover , the improvement achieved by C - AGGCN with pruned trees decays when the sentence length increases .",analysis,Analysis and Discussion,1,254,21,21,0,analysis : Analysis and Discussion,0.7720364741641338,0.6774193548387096,0.6774193548387096,Moreover the improvement achieved by C AGGCN with pruned trees decays when the sentence length increases ,17,We also notice that C - AGGCN with pruned trees performs better than C - GCN in most cases .,"Such a performance degradation can be avoided by using full trees , which provide more information of the underlying graph structures .",result
natural_language_inference,67,Other simpler variants of the aforementioned RC task were explored in the past .,system description,Remark: Categories of RC Tasks,0,46,6,2,0,system description : Remark: Categories of RC Tasks,0.2254901960784313,0.06382978723404255,0.2857142857142857,Other simpler variants of the aforementioned RC task were explored in the past ,14, ,"For example , quiz - style datasets ( e.g. , MCTest ( Richardson , Burges , and Renshaw 2013 ) , Movie QA ) have multiple - choice questions with answer options .",method
natural_language_inference,21,This yields a temporal order encoded and context - aware vector representation for each element / token .,system description,Directional Self-Attention Network,0,155,108,29,0,system description : Directional Self-Attention Network,0.5344827586206896,0.8503937007874016,0.6041666666666666,This yields a temporal order encoded and context aware vector representation for each element token ,16,"Given input sequence x and a mask M , we compute f ( x i , x j ) according to Eq. ( 15 ) , and follow the standard procedure of multi-dimensional token 2 token self - attention to The final output u ? Rd h n of DiSA is obtained by combining the output sand the input h of the masked multidimensional token 2 token self - attention block .","The combination is accomplished by a dimension - wise fusion gate , i.e. , where W ( f 1 ) , W ( f 2 ) ? Rd h d hand b ( f ) ? Rd hare the learnable parameters of the fusion gate .",method
natural_language_inference,49,Layer : We adopt DenseNet as convolutional feature extractor in DIIN .,model,DENSELY INTERACTIVE INFERENCE NETWORK,0,103,57,33,0,model : DENSELY INTERACTIVE INFERENCE NETWORK,0.4055118110236221,0.8382352941176471,0.75,Layer We adopt DenseNet as convolutional feature extractor in DIIN ,11,"Though there are many implementations of interaction , we find ? ( a , b ) = a b very useful .","Though our experiments show ResNet works well in the architecture , we choose Dense Net because it is effective in saving parameters .",method
relation-classification,6,The LET constructs the type representations by weighting K latent type vectors based on attention mechanisms .,model,Entity Features with Latent Type,0,118,72,10,0,model : Entity Features with Latent Type,0.6519337016574586,0.935064935064935,0.6666666666666666,The LET constructs the type representations by weighting K latent type vectors based on attention mechanisms ,17,"Because the annotated types are not given , we use the latent type representations by applying the LET inspired by latent topic clustering , a method for predicting latent topic of texts in question answering task .",The mathematical formulation is the follows :,method
question-answering,3,"Without pretraining , they got a much worse performance ( the second row of ) .",model,Comparing with State-of-the-art Models,0,229,59,30,0,model : Comparing with State-of-the-art Models,0.9015748031496064,0.8428571428571429,0.7317073170731707,Without pretraining they got a much worse performance the second row of ,13,"However , their model heavily depends on the pretraining strategy .",proposed a similar model to .,method
text_summarization,2,ROUGE results on test set .,result,Results,0,224,12,12,0,result : Results,0.8057553956834532,0.3636363636363637,0.3636363636363637,ROUGE results on test set ,6,"Struct + 2 Way + Word "" also demonstrates strong performance , achieving 43.21 % , 21. 84 % , and 40.86 % F 1 scores , for R - 1 , R - 2 , and R - L respectively .",We compare our proposed approach with a range of state - of - the - art neural summarization systems .,result
natural_language_inference,81,Iceland is volcanically and geologically active .,APPENDIX,Answer town,0,340,132,52,0,APPENDIX : Answer town,0.8076009501187649,0.6197183098591549,0.3909774436090225,Iceland is volcanically and geologically active ,7,Reykjavk and the surrounding are as in the southwest of the country are home to over two - thirds of the population .,"The interior consists of a plateau characterised by sand and lava fields , mountains and glaciers , while many glacial rivers flow to the sea through the lowlands .",others
text-to-speech_synthesis,0,"At last , we report the results of our method and conduct some comparisons and analyses .",experiment,Experiments and Results,0,96,4,4,0,experiment : Experiments and Results,0.5925925925925926,0.8,1.0,At last we report the results of our method and conduct some comparisons and analyses ,16,"We first introduce the datasets used , and then describe the implementation details .", ,experiment
paraphrase_generation,1,"The block diagram of our VAE - LSTM architecture for paraphrase generation similar vein but the key difference lies in the design of a novel VAE - LSTM architecture , specifically customized for the paraphrase generation task , where the training examples are given inform of pairs of sentences ( original sentence and its paraphrased version ) , and both encoder and decoder of the VAE - LSTM are conditioned on the original sentence .",methodology,Variational Autoencoder (VAE),0,67,26,21,0,methodology : Variational Autoencoder (VAE),0.3031674208144796,0.9629629629629628,0.9545454545454546,The block diagram of our VAE LSTM architecture for paraphrase generation similar vein but the key difference lies in the design of a novel VAE LSTM architecture specifically customized for the paraphrase generation task where the training examples are given inform of pairs of sentences original sentence and its paraphrased version and both encoder and decoder of the VAE LSTM are conditioned on the original sentence ,67,Our work is in a :,We describe our VAE - LSTM architecture in more detail in the next section .,method
natural_language_inference,71,Thus a unitary / orthogonal RNN can capture long term dependencies more effectively in sequential data than a conventional RNN or LSTM .,introduction,introduction,0,20,11,11,0,introduction : introduction,0.09302325581395347,0.4230769230769231,0.4230769230769231,Thus a unitary orthogonal RNN can capture long term dependencies more effectively in sequential data than a conventional RNN or LSTM ,22,This trend was following the demonstration that these matrices can be effective in solving tasks involving long - term dependencies and gradients vanishing / exploding problem .,"As a result , this type of model has been shown to perform well on tasks that would require rote memorization and simple reasoning , such as the copy task ( Hochreiter and Schmidhuber 1997 ) and the sequential MNIST .",introduction
natural_language_inference,79,"On average , our implementation takes 30 minutes to compute the paragraph vectors of the IMDB test set , using a 16 core machine ( 25,000 documents , each document on average has 230 words ) .",model,Some further observations,0,248,63,15,0,model : Some further observations,0.9253731343283582,1.0,1.0,On average our implementation takes 30 minutes to compute the paragraph vectors of the IMDB test set using a 16 core machine 25 000 documents each document on average has 230 words ,33,"Paragraph Vector can be expensive , but it can be done in parallel at test time .", ,method
sentiment_analysis,22,"However , such kind of feature engineering work often relies on human ingenuity , which is a timeconsuming process and lacks generalization .",introduction,introduction,0,15,5,5,0,introduction : introduction,0.06550218340611354,0.15151515151515152,0.15151515151515152,However such kind of feature engineering work often relies on human ingenuity which is a timeconsuming process and lacks generalization ,21,"Traditional methods for aspect - level sentiment analysis mainly focus on designing a set of features ( such as bag - of - words , sentiment lexicons , and linguistic features ) to train a classifier for sentiment classification .","In recent years , more and more neural network based models have been proposed and obtained the stateof - the - art results .",introduction
natural_language_inference,4,"To address this problem , we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning .",introduction,introduction,1,18,9,9,0,introduction : introduction,0.09090909090909093,0.5625,0.5625,To address this problem we propose a mixed objective that combines traditional cross entropy loss over positions with a measure of word overlap trained with reinforcement learning ,28,"The span "" Warriors "" is also a correct answer , but from the perspective of traditional cross entropy based training it is no better than the span "" history "" .",We obtain the latter objective using self - critical policy learning in which the reward is based on word overlap between the proposed answer and the ground truth answer .,introduction
natural_language_inference,95,"For the ablation of the content model , we analyze that it will not only affect the content score itself , but also violate the verification model since the content probabilities are necessary for the answer representation , which will be further analyzed in Section 4.3 .",ablation,Ablation Study,1,187,10,10,0,ablation : Ablation Study,0.7991452991452992,0.3125,0.7692307692307693,For the ablation of the content model we analyze that it will not only affect the content score itself but also violate the verification model since the content probabilities are necessary for the answer representation which will be further analyzed in Section 4 3 ,45,"From , we can see that the answer verification makes a great contribution to the over all improvement , which confirms our hypothesis that cross - passage answer verification is useful for the multi-passage MRC .","Another discovery is that jointly training the three models can provide great benefits , which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers .",result
natural_language_inference,11,"In short : building suitably large corpora to capture all relevant information , and keeping the corpus and derived models up to date with changes to the world would be impractical .",introduction,introduction,0,20,9,9,0,introduction : introduction,0.07246376811594203,0.39130434782608703,0.39130434782608703,In short building suitably large corpora to capture all relevant information and keeping the corpus and derived models up to date with changes to the world would be impractical ,30,"Second , as the world changes , the facts that may influence how a text is understood will likewise change .","In this paper , we develop a new architecture for dynamically incorporating external background knowledge in NLU models .",introduction
natural_language_inference,40,"Henceforth , we refer to this baseline as "" one - hot "" .",experiment,Text Comprehension with Coreference,0,167,25,24,0,experiment : Text Comprehension with Coreference,0.6028880866425993,0.925925925925926,0.9230769230769232,Henceforth we refer to this baseline as one hot ,10,Such additional features were shown to be useful by .,presented a multi - layer architecture called GA Reader for text comprehension which achieves state of the art performance over several datasets .,experiment
natural_language_inference,45,gap exists because our architecture described in Section 3.1 does not specifically model the answer span structure that is unique to SQuAD .,performance,SQUAD,0,182,30,10,0,performance : SQUAD,0.9145728643216079,0.967741935483871,0.9090909090909092,gap exists because our architecture described in Section 3 1 does not specifically model the answer span structure that is unique to SQuAD ,24,"Note that at the time of submission , the best score on the leaderboard is 0.716 in exact match and 0.804 in F1 without published papers .","In this work , we focus on this general architecture to study the effectiveness of fine - grained gating mechanisms .",result
natural_language_inference,29,"Without using pre-trained embeddings , we randomly initialize the network parameters at the beginning of our experiments .",implementation,implementation,1,284,2,2,0,implementation : implementation,0.7780821917808219,0.057142857142857134,0.057142857142857134,Without using pre trained embeddings we randomly initialize the network parameters at the beginning of our experiments ,18, ,All the RNN networks have 512 hidden units and the dimension of word embedding is 256 .,experiment
relation_extraction,2,We apply dropout before each fully connected layer during training .,model,Model Architecture,0,81,23,23,0,model : Model Architecture,0.6,0.9583333333333334,0.9583333333333334,We apply dropout before each fully connected layer during training ,11,We use cross entropy as the loss function .,We call our approach as R - BERT .,method
text_generation,2,"Recently , by combining with policy gradient , Generative Adversarial Nets ( GAN ) that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation .",abstract,abstract,0,5,3,3,0,abstract : abstract,0.014285714285714284,0.3,0.3,Recently by combining with policy gradient Generative Adversarial Nets GAN that use a discriminative model to guide the training of the generative model as a reinforcement learning policy has shown promising results in text generation ,36,"Automatically generating coherent and semantically meaningful text has many applications in machine translation , dialogue systems , image captioning , etc .","However , the scalar guiding signal is only available after the entire text has been generated and lacks intermediate information about text structure during the generative process .",abstract
natural_language_inference,68,"For each match - LSTM in a particular direction , h q i , which is defined as H q ? i , is computed using the ? in the corresponding direction , as described in either Eqn. ( 2 ) or Eqn. ( 5 ) .",method,MATCH-LSTM,0,64,14,10,0,method : MATCH-LSTM,0.2570281124497992,0.12280701754385966,0.625,For each match LSTM in a particular direction h q i which is defined as H q i is computed using the in the corresponding direction as described in either Eqn 2 or Eqn 5 ,36,"Both models consist of an LSTM preprocessing layer , a match - LSTM layer and an Answer Pointer layer .","For each match - LSTM in a particular direction , h q i , which is defined as H q ? i , is computed using the ? in the corresponding direction , as described in either Eqn. ( 2 ) or Eqn. ( 5 ) .",method
text_summarization,5,"Intuitively , the appearance of new named entities in the summary is likely to bring unfaithfulness .",evaluation,Evaluation Metrics,0,136,23,23,0,evaluation : Evaluation Metrics,0.5396825396825397,0.9583333333333334,0.9583333333333334,Intuitively the appearance of new named entities in the summary is likely to bring unfaithfulness ,16,The number of the named entities that do not appear in the source sentence or actual summary .,We use Stanford Co - re NLP to recognize named entities .,result
semantic_role_labeling,1,Our model is designed for the more realistic setting in which gold predicates are not provided at test - time .,model,Model,0,42,9,9,0,model : Model,0.1926605504587156,0.10227272727272728,0.45,Our model is designed for the more realistic setting in which gold predicates are not provided at test time ,20,We introduce this syntactically - informed self - attention ( ) in more detail in 2.2 .,Our model predicts predicates and integrates part - of - speech ( POS ) information into earlier layers by re-purposing representations closer to the input to predict predicate and POS tags us - MatMul :,method
part-of-speech_tagging,2,"Since different applications share the same alphabet , the case is similar to cross - domain transfer with disparate label sets .",architecture,CROSS-APPLICATION TRANSFER,0,83,22,6,0,architecture : CROSS-APPLICATION TRANSFER,0.4662921348314607,0.7333333333333333,0.8571428571428571,Since different applications share the same alphabet the case is similar to cross domain transfer with disparate label sets ,20,"In the cross - application setting , we assume that multiple applications are in the same language .",We adopt the architecture of model T - B for cross-application transfer learning where only the CRF layers are disjoint for different applications .,method
semantic_parsing,2,The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure .,introduction,introduction,0,12,4,4,0,introduction : introduction,0.041237113402061855,0.13793103448275862,0.13793103448275862,The fact that meaning representations are typically structured objects has prompted efforts to develop neural architectures which explicitly account for their structure ,23,The successful application of recurrent neural networks to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence - to - sequence problem .,"Examples include tree decoders , decoders constrained by a grammar model , or modular decoders which use syntax to dynamically compose various submodels .",introduction
sentiment_analysis,4,We specifically focus on dyadic conversations where two entities participate in a dialogue .,introduction,introduction,0,19,10,10,0,introduction : introduction,0.05828220858895705,0.2272727272727273,0.2272727272727273,We specifically focus on dyadic conversations where two entities participate in a dialogue ,14,"In this paper , we address the problem of emotion recognition in conversational videos .","We propose Interactive COnversational memory Network ( ICON ) , a multimodal network for identifying emotions in utterance - videos .",introduction
natural_language_inference,95,"To verify the effectiveness of our model on multipassage machine reading comprehension , we conduct experiments on the MS - MARCO and DuReader datasets .",experiment,Experiments,0,133,2,2,0,experiment : Experiments,0.5683760683760684,0.6666666666666666,0.6666666666666666,To verify the effectiveness of our model on multipassage machine reading comprehension we conduct experiments on the MS MARCO and DuReader datasets ,23, ,Our method achieves the state - of - the - art performance on both datasets .,experiment
sentiment_analysis,26,The multi-domain nature of these distinguish them from the kinds of generic polarity scores captured in sentiment polarity lexicons .,approach,Sentiment Embedding Computation,0,31,10,9,0,approach : Sentiment Embedding Computation,0.12653061224489795,0.11904761904761905,0.2432432432432433,The multi domain nature of these distinguish them from the kinds of generic polarity scores captured in sentiment polarity lexicons ,21,"To this end , our goal is to induce sentiment embeddings that capture sentiment polarity signals in multiple domains and hence maybe useful across a range of different sentiment analysis tasks .","We achieve this via transfer learning from trained models , benefiting from supervision on a series of sentiment polarity tasks from different domains .",method
sentiment_analysis,41,We experiment on the SemEval 2014 dataset and results show that our model achieves state - of the - art performance on aspect - level sentiment classification .,abstract,abstract,0,11,9,9,0,abstract : abstract,0.04932735426008968,1.0,1.0,We experiment on the SemEval 2014 dataset and results show that our model achieves state of the art performance on aspect level sentiment classification ,25,The attention mechanism can concentrate on different parts of a sentence when different aspects are taken as input ., ,abstract
sentiment_analysis,10,This provides context from the relevant ( based on attention score ) future and preceding utterances .,training,DialogueRNN Variants,0,134,15,11,0,training : DialogueRNN Variants,0.5214007782101168,0.7142857142857143,0.6470588235294118,This provides context from the relevant based on attention score future and preceding utterances ,15,"For each emotion representation e t , attention is applied over all surrounding emotion representations in the dialogue by matching them withe t ( Eqs. and ) .",Emotional attention ( BiDi - alogueRNN + Att ) :,experiment
text_generation,0,Finally we apply a max - over - time pooling operation over the feature mapc = max {c } and pass all pooled features from different kernels to a fully connected softmax layer to get the probability that a given sequence is real .,implementation,Model Implementations,0,303,34,34,0,implementation : Model Implementations,0.9351851851851852,0.6938775510204082,0.9189189189189192,Finally we apply a max over time pooling operation over the feature mapc max c and pass all pooled features from different kernels to a fully connected softmax layer to get the probability that a given sequence is real ,40,"Specifically , a kernel w with window size l applied to the concatenated embeddings of input sequence will produce a feature map",We perform an empirical experiment to choose the kernel window sizes and numbers as shown in .,experiment
sarcasm_detection,1,"Recent findings suggest that such modeling of the user and their preferences , is highly effective for the given task .",introduction,introduction,0,28,19,19,0,introduction : introduction,0.08383233532934131,0.6129032258064516,0.6129032258064516,Recent findings suggest that such modeling of the user and their preferences is highly effective for the given task ,20,"First , it performs user profiling to create user embeddings that capture indicative behavioral traits for sarcasm .","It makes use of users ' historical posts to model their writing style ( stylometry ) and personality indicators , which are then fused into comprehensive user embeddings using a multi-view fusion approach , Canonical Correlation Analysis ( CCA ) .",introduction
sentiment_analysis,6,"Overall , audio modality has performed better than visual on all the datasets .",baseline,baseline,0,259,13,13,0,baseline : baseline,0.8961937716262975,0.65,0.65,Overall audio modality has performed better than visual on all the datasets ,13,"As expected , in all kinds of experiments , bimodal and trimodal models have outperformed unimodal models .","On MOSI and IEMOCAP datasets , the textual classifier achieves the best performance over other unimodal classifiers .",result
sentence_classification,1,"For another instance , the sentence "" A post hoc analysis was conducted with the use of data from the evaluation study of congestive heart failure and pulmonary artery catheterization effectiveness ( escape ) . "" belongs to the Result label according to the gold standard , but it makes more sense that it should be classified as a Method label .",result,Results and Discussion,0,146,23,23,0,result : Results and Discussion,0.8439306358381503,0.5476190476190477,0.5609756097560976,For another instance the sentence A post hoc analysis was conducted with the use of data from the evaluation study of congestive heart failure and pulmonary artery catheterization effectiveness escape belongs to the Result label according to the gold standard but it makes more sense that it should be classified as a Method label ,55,"For example , the sentence "" Depressive disorders are one of the leading components of the global burden of disease with a prevalence of up to 14 % in the general population . "" is indeed introducing the background of the problem ( depressive disorders ) on which this article is going to focus ; however , the gold label classifies it into the Objective category .","presents an example of the transition matrix after the HSLN - RNN model has been trained on the PubMed 20 k dataset , which encodes the transition probability between two subsequent labels .",result
natural_language_inference,12,"In later experiments , we found that a residual connection can achieve similar accuracies with fewer number of parameters , compared to a shortcut connection .",Addendum: Shortcut vs. Residual,Addendum: Shortcut vs. Residual,0,86,2,2,0,Addendum: Shortcut vs. Residual : Addendum: Shortcut vs. Residual,0.9772727272727272,0.5,0.5,In later experiments we found that a residual connection can achieve similar accuracies with fewer number of parameters compared to a shortcut connection ,24, ,"Therefore , in order to reduce the model size and to also follow the SNLI leaderboard settings ( e.g. , 300D and 600D embeddings ) , we performed some additional SNLI experiments with the shortcut connections replaced with residual connections , where the input to each next biL - STM layer is the concatenation of the word embedding and the summation of outputs of all previous layers ( related to ResNet in computer vision ) .",others
sentiment_analysis,13,We adopt BERT BASE ( uncased ) as the basis for all experiments,experiment,Experiments,1,210,34,34,0,experiment : Experiments,0.7553956834532374,0.85,0.85,We adopt BERT BASE uncased as the basis for all experiments,11,"For MRC task - awareness post - training , we leverage SQuAD 1.1 ) that come with 87,599 training examples from 442 Wikipedia articles .","10 . Since post - training may take a large footprint on GPU memory ( as BERT pretraining ) , we leverage FP16 computation 11 to reduce the size of both the model and hidden representations of data .",experiment
natural_language_inference,10,"Similar to 100D experiments , we initialize the word embedding matrix with GloVe 300D pretrained vectors 4 , however we do not update the word representations during training .",experiment,Natural Language Inference,1,150,16,12,0,experiment : Natural Language Inference,0.6329113924050633,0.5161290322580645,0.4444444444444444,Similar to 100D experiments we initialize the word embedding matrix with GloVe 300D pretrained vectors 4 however we do not update the word representations during training ,27,For We also apply dropout on the word vectors with probability 0.1 .,"Since our model converges relatively fast , it is possible to train a model of larger size in a reasonable time .",experiment
sentiment_analysis,33,"In an NLP system , a convolution operation is typically a sliding window function that applies a convolution filter to every possible window of words in a sentence .",introduction,introduction,0,12,3,3,0,introduction : introduction,0.096,0.1875,0.1875,In an NLP system a convolution operation is typically a sliding window function that applies a convolution filter to every possible window of words in a sentence ,28,"Convolutional neural networks ( CNNs ) have been shown to achieve state - of - the - art results on various natural language processing ( NLP ) tasks , such as sentence classification , question answering , and machine translation .","Hence , the key components of CNNs are a set of convolution filters that compose low - level word features into higher - level representations .",introduction
sentiment_analysis,13,"We use all default hyper - parameter settings for this baseline except the number of epochs , which is set as 60 for better convergence .",method,method,0,222,6,6,0,method : method,0.7985611510791367,0.1935483870967742,0.1935483870967742,We use all default hyper parameter settings for this baseline except the number of epochs which is set as 60 for better convergence ,24,We run the document reader with random initialization and train it directly on Review RC .,DrQA + MRC is derived from the above baseline with official pre-trained weights on SQ u AD .,method
natural_language_inference,15,The model is the basic 5 - layer RNN with attention and is the one without attention .,ablation,ablation,0,168,17,17,0,ablation : ablation,0.7433628318584071,0.4146341463414634,0.4146341463414634,The model is the basic 5 layer RNN with attention and is the one without attention ,17,"The results of ( 8 - 9 ) demonstrate that the dense connection using concatenation operation over deeper layers , has more powerful capability retaining collective knowledge to learn textual semantics .",The result of ( 10 ) shows that the connections among the layers are important to help gradient flow .,result
natural_language_inference,46,"While this could be used as a QA task , the MCTest corpus is in fact intended as an answer selection corpus .",system description,FRANK (to the baby),0,57,41,35,0,system description : FRANK (to the baby),0.1919191919191919,0.4880952380952381,0.4487179487179487,While this could be used as a QA task the MCTest corpus is in fact intended as an answer selection corpus ,22,"Each such question has set of possible answers , one of which is labelled as correct .","The data is human generated , and the answers can be phrases or sentences .",method
named-entity-recognition,8,"Compared to pre-training , fine - tuning is relatively inexpensive .",model,Fine-tuning BERT,0,149,71,9,0,model : Fine-tuning BERT,0.3850129198966408,0.9594594594594594,0.75,Compared to pre training fine tuning is relatively inexpensive ,10,"At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis .","All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU , or a few hours on a GPU , starting from the exact same pre-trained model .",method
semantic_role_labeling,3,We list the detailed performance on frequent labels in .,model,Detailed Scores,0,219,26,2,0,model : Detailed Scores,0.8295454545454546,0.3880597014925373,0.1176470588235294,We list the detailed performance on frequent labels in ,10, ,The results of the previous stateof - the - art ) are also shown for comparison .,method
natural_language_inference,73,"On the other hand , ReSA uses the output of the soft self - attention module for prediction , whose correctness ( as compared to the ground truth ) is used as reward signal to train the RSS .",model,Reinforced Self-Attention (ReSA),0,110,28,6,0,model : Reinforced Self-Attention (ReSA),0.4198473282442748,0.4666666666666667,0.1875,On the other hand ReSA uses the output of the soft self attention module for prediction whose correctness as compared to the ground truth is used as reward signal to train the RSS ,34,"Hence , heavy memory loads and computations associated with soft self - attention can be effectively relieved .",This alleviates the difficulty of training hard attention module .,method
machine-translation,5,The alignments are used to generate a table of all possible word translations relative to each position in the primary hypothesis .,system,System Combination,0,121,24,7,0,system : System Combination,0.8461538461538461,0.8275862068965517,0.5833333333333334,The alignments are used to generate a table of all possible word translations relative to each position in the primary hypothesis ,22,The majority voting scheme assumes a single base translation hypothesis ( primary hypothesis ) which is aligned at the word level to each of the other hypotheses ( secondary hypotheses ) .,The table is then used to count the number of occurrences of different translations .,method
semantic_role_labeling,1,"So , the role label scores sf t for the token at index t with respect to the predicate at index f ( i.e. token t and frame f ) are given by :",model,Predicting semantic roles,0,118,85,6,0,model : Predicting semantic roles,0.5412844036697247,0.9659090909090908,0.6666666666666666,So the role label scores sf t for the token at index t with respect to the predicate at index f i e token t and frame f are given by ,32,We then provide these representations to a bilinear transformation U for scoring .,which can be computed in parallel across all semantic frames in an entire minibatch .,method
sentiment_analysis,9,"ATAE - LSTM is a classical LSTM - based network for the APC task , which applies the attention mechanism to focus on the important words in the context .",method,Compared Methods,1,192,4,4,0,method : Compared Methods,0.6931407942238267,0.16,0.16,ATAE LSTM is a classical LSTM based network for the APC task which applies the attention mechanism to focus on the important words in the context ,27,Experimental results show that the proposed model achieves state - of - the - art performance both in the ATE and APC tasks .,"Besides , ATAE - LSTM appends aspect embedding and the learned features to make full use of the aspect features .",method
natural_language_inference,74,"The number of epochs is set to be 10 , and the feedforward dropout rate is 0.2 .",implementation,Implementation Details,1,163,12,12,0,implementation : Implementation Details,0.7276785714285714,0.9230769230769232,0.9230769230769232,The number of epochs is set to be 10 and the feedforward dropout rate is 0 2 ,18,"For DMP task , we use stochastic gradient descent with initial learning rate as 0.1 , and we anneal by half each time the validation accuracy is lower than the previous epoch .",The learned encoder in subsequent NLI task is trainable .,experiment
relation-classification,3,"Specifically , we follow the 5 - fold crossvalidation defined by for the ACE04 dataset .",experiment,Experimental setup,0,83,4,4,0,experiment : Experimental setup,0.6058394160583942,0.0851063829787234,0.0851063829787234,Specifically we follow the 5 fold crossvalidation defined by for the ACE04 dataset ,14,"We evaluate our models on four datasets , using the code as available from our github codebase .","For the CoNLL04 ) EC task ( assuming boundaries are given ) , we use the same splits as in ; .",experiment
natural_language_inference,50,"There are two versions , namely clean and raw , as noted by which we evaluate our models on .",dataset,Datasets,0,183,14,14,0,dataset : Datasets,0.5772870662460567,0.9333333333333332,0.9333333333333332,There are two versions namely clean and raw as noted by which we evaluate our models on ,18,"This dataset was collected from TREC QA tracks 8 - 13 and is comprised of factoid based questions which mainly answer the ' who' , ' what ' , ' where ' , ' when ' and ' why ' types of questions .",Statistics pertaining to each dataset is given in .,experiment
sentiment_analysis,21,The results are most apparent in the case of unigrams + bigrams .,result,Results,0,110,5,5,0,result : Results,0.7857142857142857,0.4545454545454545,0.4545454545454545,The results are most apparent in the case of unigrams bigrams ,12,From here we see that using cosine similarity instead of dot product improves accuracy across the board .,However it is not to suggest that switching from dot product to cosine similarity alone improves accuracy as other minor ad - justments and hyperparameter tuning as explained was done .,result
text_summarization,8,We additionally set a minimum length based on the training data .,system description,Inference,0,155,85,10,0,system description : Inference,0.5419580419580421,0.934065934065934,0.625,We additionally set a minimum length based on the training data ,12,"We use the length penalty by , which is formulated as lp ( y ) = ( 5 + | y | ) ? ( 5 + 1 ) ? , with a tunable parameter ? , where increasing ? leads to longer summaries .","Copy models often repeatedly attend to the same source tokens , generating the same phrase multiple times .",method
natural_language_inference,78,"The main competitor on this dataset is the ESIM model , a powerful state - of - the - art SNLI baseline .",experiment,MultiNLI,0,182,14,3,0,experiment : MultiNLI,0.6594202898550725,0.6363636363636364,0.2727272727272727,The main competitor on this dataset is the ESIM model a powerful state of the art SNLI baseline ,19,We compare on two test sets ( matched and mismatched ) which represent indomain and out - domain performance .,We also compare with ESIM + Read .,experiment
natural_language_inference,46,As a span- prediction model we consider a simplified version of the Bi- Directional Attention Flow network .,baseline,baseline,0,171,17,17,0,baseline : baseline,0.5757575757575758,0.5151515151515151,0.5151515151515151,As a span prediction model we consider a simplified version of the Bi Directional Attention Flow network ,18,"We adapt the model for sequence prediction by using an LSTM sequence decoder and choosing a token from the input at each step of the 8 Note that we do not consider the span 's context when computing the MRR for IR baselines , as the candidate spans ( i.e. all answers to questions on the story ) are given and simply ranked by their similarity to the query .",We omit the character embedding layer and learn a mapping from words to a vector space rather than making use of pre-trained embeddings ; and we use a single layer bi-directional LSTM to model interactions among context words conditioned on the query ( modelling layer ) .,result
natural_language_inference,29,"For human evaluation , we ask annotators to rate each generated answer according to two aspects : consistency and fluency .",implementation,implementation,0,296,14,14,0,implementation : implementation,0.8109589041095892,0.4,0.4,For human evaluation we ask annotators to rate each generated answer according to two aspects consistency and fluency ,19,"In , we see that our PAAG outperforms all the baseline significantly in semantic distance with respect to the ground truth .","The rating score ranges from 1 to 3 , and 3 is the best .",experiment
sentiment_analysis,8,New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall .,methodology,Evaluation Metrics:,0,126,80,6,0,methodology : Evaluation Metrics:,0.5384615384615384,0.8988764044943821,0.4,New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall ,26,"It represents each training example as a point in space , mapped such that the examples of the separate categories are divided by a clear gap that is as wide as possible ( this is usually achieved by minimizing the hinge loss ) .","SVMs were originally introduced to perform linear classification ; however , they can efficiently perform a non-linear classification using the kernel trick , implicitly mapping their inputs into high - dimensional feature spaces .",method
sentiment_analysis,35,The challenges in fulfillment of this setting are two - fold : ( 1 ) task discrepancy : the two tasks concern with the aspects with different granularity .,introduction,introduction,0,33,19,19,0,introduction : introduction,0.13306451612903225,0.5428571428571428,0.5428571428571428,The challenges in fulfillment of this setting are two fold 1 task discrepancy the two tasks concern with the aspects with different granularity ,24,"Motivated by this observation , we propose a new problem named coarse - to - fine task transfer across both domain and granularity , with the aim of borrowing knowledge from an abundant source domain of the coarse - grained AC task to a small - scale target domain of the fine - grained AT task .","Source aspects are coarse - grained aspect categories , which lack a priori position information in the context .",introduction
natural_language_inference,97,It maybe that linearization of the dependency graph removes too much of its information .,analysis,analysis,0,269,13,13,0,analysis : analysis,0.9212328767123288,0.4642857142857143,0.4642857142857143,It maybe that linearization of the dependency graph removes too much of its information ,15,We found this surprising .,"Finally , the exogenous word weights make a significant contribution of almost 5 % .",result
machine-translation,4,"We evaluate the proposed approach on English - German , English - French and Chinese - to - English translation tasks",experiment,Experiments and Results,0,143,2,2,0,experiment : Experiments and Results,0.5983263598326359,0.5,0.5,We evaluate the proposed approach on English German English French and Chinese to English translation tasks,16, ,"We firstly describe the datasets , pre-processing and model hyper - parameters we used , then we introduce the baseline systems , and finally we present our experimental results .",experiment
natural_language_inference,33,They directly model the conditional distribution of outputs given inputs P ( y|x ) .,system description,SEQUENCE-TO-SEQUENCE LEARNING,0,61,4,4,0,system description : SEQUENCE-TO-SEQUENCE LEARNING,0.23371647509578544,0.05970149253731344,0.21052631578947367,They directly model the conditional distribution of outputs given inputs P y x ,14,"Briefly , sequence - to - sequence models are a specific case of encoder - decoder models where the inputs and outputs are sequential .","The input x and output y are sequences x 1 , x 2 , . . . , x m and y 1 , y 2 . . . , y n .",method
natural_language_inference,17,"Therefore , we define the computation of reattention as follows .",architecture,Alignment Architecture for MRC,0,82,39,39,0,architecture : Alignment Architecture for MRC,0.3153846153846154,0.2846715328467153,0.3,Therefore we define the computation of reattention as follows ,10,"In this case , the similarity of word pair ( team , Broncos ) is higher than ( team , Panthers ) .",Let E t?1 and B t?1 denote the past similarity matrices thatare temporally memorized .,method
text_generation,2,"Nonetheless , the reported results are limited to the cases that the generated text samples are short ( say , fewer than 20 words ) while more challenging long text generation is hardly studied , which is necessary for practical tasks such as auto-generation of news articles or product descriptions .",introduction,introduction,0,25,13,13,0,introduction : introduction,0.07142857142857142,0.34210526315789475,0.34210526315789475,Nonetheless the reported results are limited to the cases that the generated text samples are short say fewer than 20 words while more challenging long text generation is hardly studied which is necessary for practical tasks such as auto generation of news articles or product descriptions ,47,"Since then , various methods have been proposed in text generation via GAN .",main drawback of existing methods to long text generation is that the binary guiding signal from Dis sparse as it is only available when the whole text sample is generated .,introduction
natural_language_inference,23,"Now , we compare adding normal and fully - aware self - boosted fusion into the architecture .",result,EFFECTIVENESS OF HISTORY-OF-WORD,0,288,60,27,0,result : EFFECTIVENESS OF HISTORY-OF-WORD,0.5614035087719298,0.9523809523809524,0.9,Now we compare adding normal and fully aware self boosted fusion into the architecture ,15,We have achieved decent performance without self - boosted fusion .,"Comparing None and Normal in , we can see that the use of normal self - boosted fusion is not very effective under our improved C , Q Fusion .",result
part-of-speech_tagging,3,"Comparing the two tasks , NER relies",system description,Word Embeddings,0,150,15,5,0,system description : Word Embeddings,0.7389162561576355,0.46875,0.2631578947368421,Comparing the two tasks NER relies,6,"According to the results in , models using pretrained word embeddings obtain a significant improvement as opposed to the ones using random embeddings .",We run experiments using the same setting and get 91.37 % F1 score .,method
sentiment_analysis,25,"Moreover , the attention scores generated by the similarity scoring function are for the entire context vector .",analysis,ACSA,0,176,10,9,0,analysis : ACSA,0.7927927927927928,0.2777777777777778,0.375,Moreover the attention scores generated by the similarity scoring function are for the entire context vector ,17,The context vectors generated by LSTM have to convey the two kinds of information at the same time .,GCAE improves the performance by 1.1 % to 2.5 % compared with ATAE - LSTM .,result
part-of-speech_tagging,3,"Finally , we construct our neural network model by feeding the output vectors of BLSTM into a CRF layer .",architecture,BLSTM-CNNs-CRF,0,75,44,2,0,architecture : BLSTM-CNNs-CRF,0.3694581280788177,0.8979591836734694,0.2857142857142857,Finally we construct our neural network model by feeding the output vectors of BLSTM into a CRF layer ,19, ,illustrates the architecture of our network in detail .,method
natural_language_inference,61,"We could conclude that our a ESIM model had the higher weight than ESIM model on each keyword pair , especially in.b , where the similarity of ' happy ' and ' grin ' in aESIM model is much higher than that in ESIM model .",model,Models,0,149,7,7,0,model : Models,0.9551282051282052,0.875,0.875,We could conclude that our a ESIM model had the higher weight than ESIM model on each keyword pair especially in b where the similarity of happy and grin in aESIM model is much higher than that in ESIM model ,41,"In each the brighter the color , the higher the weight is .","erefore , our a ESIM model was able to capture the most important word pair in each pair of sentences .",method
natural_language_inference,9,The learning rate is controlled by AdaGrad with the initial learning rate of 0.5 ( 0.1 for QA 10 k ) .,model,MODEL DETAILS,1,213,33,33,0,model : MODEL DETAILS,0.6454545454545455,0.4177215189873418,0.9705882352941176,The learning rate is controlled by AdaGrad with the initial learning rate of 0 5 0 1 for QA 10 k ,22,"The loss is minimized by stochastic gradient descent for maximally 500 epochs , but training is early stopped if the loss on the development data does not decrease for 50 epochs .","Since the model is sensitive to the weight initialization , we repeat each training procedure 10 times ( 50 times for 10 k ) with the new random initialization of the weights and report the result on the test data with the lowest loss on the development data .",method
text-to-speech_synthesis,0,1 ) We propose token - level ensemble distillation for grapheme - to - phoneme conversion .,introduction,introduction,0,31,20,20,0,introduction : introduction,0.191358024691358,0.9090909090909092,0.9090909090909092,1 We propose token level ensemble distillation for grapheme to phoneme conversion ,13,Our contributions are listed as follows :,"We are the first to use unlabeled words to boost the accuracy of grapheme - to - phoneme conversion , and also the first to introduce Transformer into this task and achieve better performance .",introduction
sentiment_analysis,16,"For instance , TMN generates a sentiment score vector of the context "" over "" for target aspect price : { 0.1373 , 0.0066 , - 0.1433 } ( negative ) and for target aspect dinner : { 0.0496 , 0.0591 , - 0.1128 } ( neutral ) accurately .",analysis,Result Analysis,0,307,50,50,0,analysis : Result Analysis,0.9654088050314464,0.9615384615384616,0.9615384615384616,For instance TMN generates a sentiment score vector of the context over for target aspect price 0 1373 0 0066 0 1433 negative and for target aspect dinner 0 0496 0 0591 0 1128 neutral accurately ,37,Notice that TMNs can also improve the neutral sentiment ( see ) .,"But MN produces both negative scores { 0.0069 , 0.0025 , - 0.0090 } ( negative ) and { 0.0078 , 0.0028 , - 0.0102 } ( negative ) for the two different targets .",result
natural_language_inference,18,"Based on the joint representation , an MLP spends LK 2 to generate a sample , where L is the number of layers and K represents the average dimension .",t-SNE Visualis ation of Document Representations,Computational Complexity,0,266,15,8,0,t-SNE Visualis ation of Document Representations : Computational Complexity,0.9743589743589745,0.6818181818181818,0.5333333333333333,Based on the joint representation an MLP spends LK 2 to generate a sample where L is the number of layers and K represents the average dimension ,28,"It takes 2 SW + 2 K to produce the joint representation for a question - answer pair and it s label , where Wis the total number of parameters of an LSTM and S is the average length of the sentences .",The generative model requires C ? = 2SW + LK 2 + SK 2 +5K 2 + 2K 2 = O ( ( L+S ) K 2 + SW ) .,others
text-classification,0,"The full dataset contains 600,000 training samples and 130,000 testing samples in each class , whereas the polarity dataset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment .",method,Large-scale Datasets and Results,0,183,81,41,0,method : Large-scale Datasets and Results,0.8061674008810573,0.6806722689075629,0.9111111111111112,The full dataset contains 600 000 training samples and 130 000 testing samples in each class whereas the polarity dataset contains 1 800 000 training samples and 200 000 testing samples in each polarity sentiment ,36,"Similarly to the Yelp review dataset , we also constructed 2 datasets - one full score prediction and another polarity prediction .",The fields used are review title and review content .,method
text_generation,2,"am proud to be able to move forward because we do n't have to look at about , "" he said .",SeqGAN,SeqGAN,0,324,6,6,0,SeqGAN : SeqGAN,0.9257142857142856,0.1875,0.1875,am proud to be able to move forward because we do n t have to look at about he said ,21,"Do n't ask me , but I know , if I ' ll be able to be out of Hillary Clinton , I think it 's being made for the Congress . """,That ' s why we ' re the most important people for the African American community and we ' ve made a good response .,others
natural_language_inference,96,The model must then select the most appropriate verb phrase v ? V .,dataset,dataset,0,52,7,7,0,dataset : dataset,0.13333333333333333,0.5384615384615384,0.5384615384615384,The model must then select the most appropriate verb phrase v V ,13,"See for an example triple ( s , n , vi ) .",Short for Situations With Adversarial Generations .,experiment
sentiment_analysis,0,"This architecture analyzes speech data from the signal level to the language level , and it thus utilizes the information within the data more comprehensively than models that focus on audio features .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.03932584269662921,0.7142857142857143,0.7142857142857143,This architecture analyzes speech data from the signal level to the language level and it thus utilizes the information within the data more comprehensively than models that focus on audio features ,32,"As emotional dialogue is composed of sound and spoken content , our model encodes the information from audio and text sequences using dual recurrent neural networks ( RNNs ) and then combines the information from these sources to predict the emotion class .",Extensive experiments are conducted to investigate the efficacy and properties of the proposed model .,abstract
machine-translation,1,We may consider two variants of the ByteNet that use recurrent networks for one or both of the networks ( see ) .,model,model,0,118,7,7,0,model : model,0.5870646766169154,0.21212121212121213,0.21212121212121213,We may consider two variants of the ByteNet that use recurrent networks for one or both of the networks see ,21,This way of combining the networks is not tied to the networks being strictly convolutional .,The first variant replaces the convolutional decoder with a recurrent one that is similarly stacked and dynamically unfolded .,method
natural_language_inference,24,": Illustration of "" stochastic prediction dropout "" in the answer module during training .",introduction,introduction,0,23,17,17,0,introduction : introduction,0.09829059829059827,0.8095238095238095,0.8095238095238095, Illustration of stochastic prediction dropout in the answer module during training ,13,"Intuitively this works because while the model successively refines its prediction over multiple steps , each step is still trained to generate the same answer ; we are performing a kind of stochastic ensemble over the model 's successive predic -s t - 1 st s t+1","At each reasoning step t , the model combines memory ( bottom row ) with hidden states s t?1 to generate a prediction ( multinomial distribution ) .",introduction
text_summarization,11,"where the representations , are computed through the attention mechanism with itself and packed into a matrix .",system description,Convolutional Gated Unit,0,64,35,26,0,system description : Convolutional Gated Unit,0.4444444444444444,0.8536585365853658,0.8125,where the representations are computed through the attention mechanism with itself and packed into a matrix ,17,"pointed out that self - attention encourages the model to learn long - term dependencies and does not create much computational complexity , so we implement its scaled dot -product attention for the connection between the annotation at each time step and the global information :","To be specific , we refer Q and V to the representation matrix generated by the CNN module , while K = Watt V where Watt is a learnable matrix .",method
natural_language_inference,94,"For example , if we were at node mother , this allows us to select the child node daughter and married over the child node book .",method,method,0,188,120,120,0,method : method,0.4908616187989556,0.9448818897637796,0.9448818897637796,For example if we were at node mother this allows us to select the child node daughter and married over the child node book ,25,"Starting at the root , we recursively take two of its children with the highest cumulative scores until we reach a leaf , selecting up to 2 4 = 16 paths .","These selected paths , as well as their partial sub-paths , are what we add as external information to the QA model , i.e. , we add the complete path lady , AtLocation , church , Relat - ed To , house , Related To , child , Related To , their , but also truncated versions of the path , including lady , AtLocation , church , Related To , house , Re-lated To , child .",method
text_summarization,6,ABS and ABS + are both the neural network based models with local attention modeling for abstractive sentence summarization .,method,Comparative Methods,1,196,9,9,0,method : Comparative Methods,0.7480916030534351,0.45,0.45,ABS and ABS are both the neural network based models with local attention modeling for abstractive sentence summarization ,19,"It also augments the phrase table with "" deletion "" rulesto improve the baseline performance , and MERT is also used to improve the quality of generated summaries .","ABS + is trained on the Gigaword corpus , but combined with an additional log - linear extractive summarization model with handcrafted features .",method
sentiment_analysis,36,"The TNet variants can still outperform LSTM - FC - CNN - LF / AS with significant gaps , e.g. , on LAPTOP and REST , the accuracy gaps between TNet - LF and LSTM - FC - CNN - LF are 0.42 % ( p < 0.03 ) and 0.38 % ( p < 0.04 ) respectively .",performance,CPT versus Alternatives,0,210,21,12,0,performance : CPT versus Alternatives,0.84,0.4117647058823529,1.0,The TNet variants can still outperform LSTM FC CNN LF AS with significant gaps e g on LAPTOP and REST the accuracy gaps between TNet LF and LSTM FC CNN LF are 0 42 p 0 03 and 0 38 p 0 04 respectively ,45,They obtain competitive results on all datasets : comparable with or better than the state - of - the - art methods ., ,result
sentiment_analysis,28,"In this paper , we set the prior label distribution to be uniform u ( k ) = 1/C. LSR is equivalent to the KL divergence between the prior label distribution u ( k ) and the network 's predicted distribution p ? .",training,training,0,114,8,8,0,training : training,0.6333333333333333,0.5714285714285714,0.5714285714285714,In this paper we set the prior label distribution to be uniform u k 1 C LSR is equivalent to the KL divergence between the prior label distribution u k and the network s predicted distribution p ,38,"where u ( k ) is the prior distribution over labels , and is the smoothing parameter .","Formally , LSR term is defined as :",experiment
sentiment_analysis,15,We investigate two types of negation .,experiment,Model Analysis: High Level Negation,0,239,43,2,0,experiment : Model Analysis: High Level Negation,0.8851851851851852,0.6231884057971014,0.08695652173913042,We investigate two types of negation ,7, ,"For each type , we use a separate dataset for evaluation .",experiment
machine-translation,0,Each set has more than 70 thousand words and a single reference translation .,baseline,Data and Baseline System,0,131,12,12,0,baseline : Data and Baseline System,0.5981735159817352,0.7058823529411765,0.7058823529411765,Each set has more than 70 thousand words and a single reference translation ,14,"We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT , and newstest2014 as our test set .","For training the neural networks , including the proposed RNN Encoder - Decoder , we limited the source and target vocabulary to the most frequent 15,000 words for both English and French .",result
sentiment_analysis,34,The extracted features are fed into an LSTM which is followed by a softmax layer that would give a probability distribution over the output classes .,model,Model Architecture,0,94,7,7,0,model : Model Architecture,0.6351351351351351,0.7777777777777778,0.875,The extracted features are fed into an LSTM which is followed by a softmax layer that would give a probability distribution over the output classes ,26,"shows the architecture , the embeddings are fed into the CNN , after that they are fed to a max - pooling layer , the reason behind using max pooling is to have the most important features which conforms with the fact that sentiment is usually expressed in specific words .",The hyper - parameters used in our architecture is shown in .,method
natural_language_inference,37,paragraph separator token with a learned embedding is added before each paragraph .,method,Merge,0,111,28,3,0,method : Merge,0.4319066147859921,0.5185185185185185,0.75,paragraph separator token with a learned embedding is added before each paragraph ,13,"As an alternative to the previous method , we experiment with concatenating all paragraphs sampled from the same context together during training .",Our motive is to test whether simply exposing the model to more text will teach the model to be more adept at ignoring irrelevant text .,method
natural_language_inference,40,"The nodes X = {x i } T i =1 correspond to the elements of the original sequence , and edges E 0 = { ( x , x , e ) } are tuples consisting of the source , target and type of the link .",method,From Sequences to DAGs,0,88,11,10,0,method : From Sequences to DAGs,0.3176895306859206,0.16923076923076924,0.43478260869565216,The nodes X x i T i 1 correspond to the elements of the original sequence and edges E 0 x x e are tuples consisting of the source target and type of the link ,36,"Let G = ( X , E ) denote the resulting directed graph which includes the sequential edges between consecutive elements , as well as the extra typed edges .",The graph G results from augmenting the edges in E 0 with inverse edges .,method
sentiment_analysis,18,") Compared with all other neural baselines , our full model achieves statistically significant improvements ( p < 0.05 ) on both accuracies and macro - F1 scores for D1 , D3 , D4 .",model,Model Comparisons,1,180,18,18,0,model : Model Comparisons,0.7531380753138075,0.72,0.72, Compared with all other neural baselines our full model achieves statistically significant improvements p 0 05 on both accuracies and macro F1 scores for D1 D3 D4 ,29,") Feature - based SVM is still a strong baseline , our best model achieves competitive results on D1 and D2 without relying on so many manually - designed features and external resources .",") Compared with LSTM + ATT , all three settings of our model are able to achieve statistically significant improvements ( p < 0.05 ) on all datasets .",method
sentiment_analysis,17,- ary Tree- LSTMs,system description,-ary Tree-LSTMs,0,97,58,1,0,system description : -ary Tree-LSTMs,0.4311111111111111,0.7435897435897436,0.047619047619047616, ary Tree LSTMs,4, , ,method
natural_language_inference,3,"An interesting thing is there are two words describing color in the sentence "" A person in a red shirt and black pants hunched over . "" .",result,Understanding Behaviors of Neurons in C-LSTMs,0,187,20,13,0,result : Understanding Behaviors of Neurons in C-LSTMs,0.8990384615384616,0.8,0.7222222222222222,An interesting thing is there are two words describing color in the sentence A person in a red shirt and black pants hunched over ,25,"This is informative pattern for the relation prediction of these two sentences , whose ground truth is contradiction .","Our model ignores the useless word "" black "" , which indicates that this neuron selectively captures pattern by contextual understanding , not just word level interaction .",result
sentiment_analysis,11,The model does n't accommodate inter-speaker dependencies .,baseline,bc-LSTM:,0,265,11,6,0,baseline : bc-LSTM:,0.7703488372093024,0.34375,0.2222222222222222,The model does n t accommodate inter speaker dependencies ,10,"For fair comparison in an end - toend learning paradigm , we remove the penultimate SVM of this model .","Memn2n : The original memory network as proposed by Contrasting to CMN , the model generates the memory representations for each historical utterance using an embedding matrix B as used in equation 7 , without sequential modeling .",result
relation_extraction,13,} is the corresponding relation type .,system description,Relation Classification and Extraction Tasks,0,61,31,13,0,system description : Relation Classification and Extraction Tasks,0.2863849765258216,0.8857142857142857,0.8125, is the corresponding relation type ,7,"Specifically , we are given K sets of N labeled relation statements S k = { ( r 0 , t 0 ) . . . ( r N , t N ) } where ti ? { 1 . . .",The goal is to predict the t q ? { 1 . . .,method
natural_language_inference,22,The input to this layer is s tiled Q times ( S Q ? R 2 dQ ) .,model,Summarization Layer,0,94,49,9,0,model : Summarization Layer,0.4272727272727273,0.7424242424242424,0.4090909090909091,The input to this layer is s tiled Q times S Q R 2 dQ ,16,"The query ruminate layer fuses the summarization vector representation with the query encoding Q , helping reformulate the query representation in order to maximize the chance of retrieving the correct answer .",gating function then fuses this with the existing query encoding :,method
text_summarization,0,shows a document and its corresponding summaries generated by different methods .,evaluation,evaluation,0,282,9,9,0,evaluation : evaluation,0.9368770764119602,0.9,0.9,shows a document and its corresponding summaries generated by different methods ,12,"To prove the significance of the above results , we also do the paired student t- test between our model and CGU model ( row with shaded background ) , the p-value are 0.0017 and 0.0012 for fluency and consistency respectively .",We can observe that : Cosine distance between current content and reader content .,result
natural_language_inference,80,"We show a sentence pair , where the premise is "" Male in a bluejacket decides to lay the grass . "" , and the hypothesis is "" The guy in yellow is rolling on the grass . "" , and its logical relationship is contradiction .",analysis,analysis,0,232,26,26,0,analysis : analysis,0.8,0.9285714285714286,0.9285714285714286,We show a sentence pair where the premise is Male in a bluejacket decides to lay the grass and the hypothesis is The guy in yellow is rolling on the grass and its logical relationship is contradiction ,38,"Finally , we show a visualization of the normalized attention weights ( energy function , Equation 3 ) of our model in .","indicates the model 's ability in attending to critical pairs of words like < Male , guy > , < decides , rolling > , and < lay , rolling > .",result
semantic_role_labeling,0,"By retaining ? p = 0.4 predicates per word , we are able to keep over 99.7 % argument - bearing predicates .",analysis,Analysis,0,82,6,6,0,analysis : Analysis,0.7809523809523811,0.2608695652173913,0.75,By retaining p 0 4 predicates per word we are able to keep over 99 7 argument bearing predicates ,20,shows the recall of predicate words on the CoNLL 2012 development set .,"Compared to having a part - of - speech tagger ( POS : X in ) , our joint beam pruning allowing the model to have a soft trade - off between efficiency and recall .",result
sentence_compression,1,"Regarding the learning procedure , the original model uses a large - margin learning framework , namely MIRA , but with some minor changes as presented by .",baseline,Baseline,0,45,8,8,0,baseline : Baseline,0.21531100478468893,0.09302325581395347,0.09302325581395347,Regarding the learning procedure the original model uses a large margin learning framework namely MIRA but with some minor changes as presented by ,24,The first change was related to the learning procedure and the second one to the family of features used .,"In this set - up , online learning is performed , and at each step an optimization procedure is made where K constraints are included , which correspond to the top - K solutions for a given training observation .",result
natural_language_inference,85,"In many problems , syntax and semantics interact closely , including in semantic composition , among others .",introduction,introduction,0,27,17,17,0,introduction : introduction,0.1148936170212766,0.85,0.85,In many problems syntax and semantics interact closely including in semantic composition among others ,15,Exploring syntax for NLI is very attractive to us .,"Complicated tasks such as natural language inference could well involve both , which has been discussed in the context of recognizing textual entailment ( RTE ) .",introduction
sentiment_analysis,1,"Specifically , we convert each training label Y i ? { 0 , 1 , ... , C ? 1 } into a prior probability distribution of all classes ? i ? RC , where ? ic denotes the probability of class c in ? i .",system description,Emotion-aware Distribution Learning,0,244,172,5,0,system description : Emotion-aware Distribution Learning,0.6161616161616161,0.8309178743961353,0.21739130434782608,Specifically we convert each training label Y i 0 1 C 1 into a prior probability distribution of all classes i RC where ic denotes the probability of class c in i ,33,"To address the problem of noisy emotion labels in the datasets , we propose an emotion - aware distribution learning method ( EmotionDL ) to learn a distribution of classes instead of one single class for each training sample .","Specifically , we convert each training label Y i ? { 0 , 1 , ... , C ? 1 } into a prior probability distribution of all classes ? i ? RC , where ? ic denotes the probability of class c in ? i .",method
sentiment_analysis,11,This ignites an emotional shift for A who then replies angrily .,result,Case Study:,0,337,51,14,0,result : Case Study:,0.9796511627906976,0.9622641509433962,0.875,This ignites an emotional shift for A who then replies angrily ,12,"But when he expresses his inhibitions , his wife B reacts in an angry and sarcastic manner ( utterance 7 ) .","In this example , CMN is able to focus on utterance 7 spoken by B to anticipate A 's test utterance to bean angry statement , thus showing its ability to model inter-speaker influences .",result
natural_language_inference,23,FA All - Level .,DETAILED CONFIGURATIONS IN THE ABLATION STUDY,DETAILED CONFIGURATIONS IN THE ABLATION STUDY,0,314,15,15,0,DETAILED CONFIGURATIONS IN THE ABLATION STUDY : DETAILED CONFIGURATIONS IN THE ABLATION STUDY,0.6120857699805068,0.1595744680851064,0.4545454545454545,FA All Level ,4,Note that our proposed symmetric form with nonlinearity should be used to guarantee the boost .,"First , we use the same procedure as High - Level to obtain Next we make use of the fully - aware attention similar to FA High - Level , but take back the entire history - of - word .",result
semantic_role_labeling,3,RNNs treat each sentence as a sequence of words and recursively compose each word with its previous hidden state .,introduction,introduction,0,21,11,11,0,introduction : introduction,0.07954545454545454,0.3928571428571429,0.3928571428571429,RNNs treat each sentence as a sequence of words and recursively compose each word with its previous hidden state ,20,"Despite recent successes , these RNN - based models have limitations .","The recurrent connections make RNNs applicable for sequential prediction tasks with arbitrary length , however , there still remain several challenges in practice .",introduction
sentiment_analysis,29,"Aspect Level Sentiment Classification Aspect level sentiment classification is a branch of sentiment classification , the goal of which is to identify the sentiment polarity of one specific aspect in a sentence .",introduction,introduction,0,45,37,37,0,introduction : introduction,0.25862068965517243,0.6981132075471698,0.6981132075471698,Aspect Level Sentiment Classification Aspect level sentiment classification is a branch of sentiment classification the goal of which is to identify the sentiment polarity of one specific aspect in a sentence ,32,These approaches have achieved promising results on sentiment analysis .,"Some early works designed several rule based models for aspect level sentiment classification , such as .",introduction
natural_language_inference,55,"When there are multiple correct answers , we average these representations , see Section 3.4 . )",system description,Representing Candidate Answers,0,84,59,3,0,system description : Representing Candidate Answers,0.5714285714285714,0.8939393939393939,0.3,When there are multiple correct answers we average these representations see Section 3 4 ,15,We now describe possible feature representations for a single candidate answer .,"We consider three different types of representation , corresponding to different subgraphs of Freebase around it .",method
natural_language_inference,47,"In order to ensure that our labeling scheme assigns a single correct label to every pair , we must select one of these approaches across the board , but both choices present problems .",system description,new corpus for NLI,0,47,12,12,0,system description : new corpus for NLI,0.2186046511627907,0.1518987341772152,0.5454545454545454,In order to ensure that our labeling scheme assigns a single correct label to every pair we must select one of these approaches across the board but both choices present problems ,32,"The pair could be labeled as a contradiction if one assumes that the two sentences refer to the same single event , but could also be reasonably labeled as neutral if that assumption is not made .","If we opt not to assume that events are coreferent , then we will only ever find contradictions between sentences that make broad universal assertions , but if we opt to assume coreference , new counterintuitive predictions emerge .",method
natural_language_inference,28,"For example , a practical way to implement associative memory is to set weight matrices as trainable structures that change according to input instances for training .",introduction,introduction,0,25,10,10,0,introduction : introduction,0.09225092250922508,0.4166666666666667,0.4166666666666667,For example a practical way to implement associative memory is to set weight matrices as trainable structures that change according to input instances for training ,26,Numerous works ; ) use associative memory to span a large memory space .,"Furthermore , the recent concept of unitary or orthogonal evolution matrices ; ) also provides a theoretical and empirical solution to the problem of memorizing long - term dependencies .",introduction
natural_language_inference,23,"In this section , we briefly introduce the task of machine comprehension as well as a conceptual architecture that summarizes recent advances in machine reading comprehension .",system description,MACHINE COMPREHENSION & FULLY-AWARE ATTENTION,0,52,2,2,0,system description : MACHINE COMPREHENSION & FULLY-AWARE ATTENTION,0.10136452241715402,0.09090909090909093,0.4,In this section we briefly introduce the task of machine comprehension as well as a conceptual architecture that summarizes recent advances in machine reading comprehension ,26, ,"Then , we introduce a novel concept called history - of - word .",method
natural_language_inference,45,The task is to predict the hashtags of each tweet .,experiment,EVALUATING WORD-CHARACTER GATING ON TWITTER,0,143,11,8,0,experiment : EVALUATING WORD-CHARACTER GATING ON TWITTER,0.7185929648241206,0.55,0.4705882352941176,The task is to predict the hashtags of each tweet ,11,"The dataset contains 2 million tweets for training , 10 K for validation and 50 K for testing , with a total of 2,039 distinct hashtags .",We compare several different methods as follows .,experiment
text_generation,2,"She had been fined almost 200 , 000 with couple of asylum seekers in Syria and Iraq .",SeqGAN,SeqGAN,0,347,29,29,0,SeqGAN : SeqGAN,0.9914285714285714,0.90625,0.90625,She had been fined almost 200 000 with couple of asylum seekers in Syria and Iraq ,17,"wanted to be made you decided to have a crisis that way up and get some sort of weapon , not much to give birth to for an American room .","Perhaps not , in looking for , housing officials would help the frustration of Government , with an FBI shortly before 2020 .",others
natural_language_inference,90,"The dominating trend in these models is to build complex , deep text representation models , for example , with convolutional networks or long short - term memory networks with the goal of deeper sentence comprehension .",introduction,introduction,0,13,6,6,0,introduction : introduction,0.08666666666666667,0.25,0.25,The dominating trend in these models is to build complex deep text representation models for example with convolutional networks or long short term memory networks with the goal of deeper sentence comprehension ,33,large body of work based on neural networks for text similarity tasks including NLI has been published in recent years .,"While these approaches have yielded impressive results , they are often computationally very expensive , and result in models having millions of parameters ( excluding embeddings ) .",introduction
sentiment_analysis,2,shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively .,dataset,Datasets,1,145,14,14,0,dataset : Datasets,0.6387665198237885,0.4827586206896552,0.4827586206896552,shows the performance of our model and other baseline models on datasets Restaurant and Laptop respectively ,17,The output of the last attention layer is fed to a softmax layer for predictions .,We can observe that our proposed PBAN model achieves the best performance among all methods .,experiment
relation_extraction,3,Entity - Aware Self - Attention based on Relative Distance,approach,Entity-Aware Self-Attention based on Relative Distance,0,44,17,1,0,approach : Entity-Aware Self-Attention based on Relative Distance,0.32116788321167883,0.4857142857142857,0.05263157894736842,Entity Aware Self Attention based on Relative Distance,8, , ,method
natural_language_inference,15,The proposed DRCN obtains an accuracy of 88.9 % which is a competitive score although we do not use any external knowledge like ESIM + ELMo and LM - Transformer .,experiment,SNLI and MultiNLI,1,133,7,6,0,experiment : SNLI and MultiNLI,0.5884955752212391,0.2916666666666667,0.4615384615384616,The proposed DRCN obtains an accuracy of 88 9 which is a competitive score although we do not use any external knowledge like ESIM ELMo and LM Transformer ,29,"However , they use additional contextualized word representations from language models as an externel knowledge .","The ensemble model achieves an accuracy of 90.1 % , which sets the new state - of the - art performance .",experiment
named-entity-recognition,8,Additional : Ablation over the pre-training tasks using the BERT BASE architecture .,ablation,ablation,0,226,3,3,0,ablation : ablation,0.5839793281653747,0.1153846153846154,0.1153846153846154,Additional Ablation over the pre training tasks using the BERT BASE architecture ,13,"In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance .","No NSP "" is trained without the next sentence prediction task .",result
natural_language_inference,88,"Finally , we examine whether the LSTMN can recognize the semantic relationship between two sentences by applying it to a natural language inference task .",experiment,Experiments,0,143,5,5,0,experiment : Experiments,0.5789473684210527,0.7142857142857143,0.7142857142857143,Finally we examine whether the LSTMN can recognize the semantic relationship between two sentences by applying it to a natural language inference task ,24,We then assess the model 's ability to extract meaning representations for generic sentence classification tasks such as sentiment analysis .,Our code is available at https://github.com/cheng6076/,experiment
natural_language_inference,93,Consider a question Q = {w Q t } m t =1 and a passage P = {w Pt } n t=1 .,system description,Question and Passage Encoder,0,58,20,9,0,system description : Question and Passage Encoder,0.2801932367149759,0.31746031746031744,0.6428571428571429,Consider a question Q w Q t m t 1 and a passage P w Pt n t 1 ,20,Question and Passage Encoder,We first convert the words to their respective word - level embeddings ( {e Q t } m t=1 and {e Pt } n t=1 ) and character - level embeddings ( {c Q t } m t=1 and {c Pt } n t=1 ) .,method
natural_language_inference,24,"First we compute an alignment matrix between the question and passage using an attention mechanism , and use this to derive a question - aware passage representation .",system description,Proposed model: SAN,0,65,38,38,0,system description : Proposed model: SAN,0.2777777777777778,0.4175824175824176,0.6031746031746031,First we compute an alignment matrix between the question and passage using an attention mechanism and use this to derive a question aware passage representation ,26,The third layer is the working memory :,"Then we concatenate this with the context representation of passage and the word embedding , and employ a self attention layer to re-arrange the information gathered .",method
natural_language_inference,11,The formal definition of this combination is given in Eq.,system description,Unrefined Word Embeddings (E 0 ),0,71,37,7,0,system description : Unrefined Word Embeddings (E 0 ),0.2572463768115942,0.6607142857142857,0.2692307692307692,The formal definition of this combination is given in Eq ,11,We compute e char w using a single - layer convolutional neural network with n convolutional filters of width 5 followed by a max - pooling operation overtime .,"In order to compute contextually refined word embeddings E given prior representations E ? 1 we assume a given set of texts X = {x 1 , x 2 , . . .} thatare to be read at refinement iteration .",method
question_generation,1,Exemplars aim to provide appropriate context .,approach,Approach,0,70,13,13,0,approach : Approach,0.17857142857142858,0.5652173913043478,0.5652173913043478,Exemplars aim to provide appropriate context ,7,"The contrasting exemplar image , on the other hand , has completely different probability scores .","To better understand the context , we experimented by analysing the questions generated through an exemplar .",method
natural_language_inference,27,"We also conduct similar studies on TriviaQA , another Q&A dataset , to show that the effectiveness and efficiency of our model are general .",experiment,EXPERIMENTS,0,181,4,4,0,experiment : EXPERIMENTS,0.5355029585798816,0.8,1.0,We also conduct similar studies on TriviaQA another Q A dataset to show that the effectiveness and efficiency of our model are general ,24,"We will primarily benchmark our model on the SQuAD dataset , considered to be one of the most competitive datasets in Q&A .", ,experiment
relation_extraction,11,Please see Section 5 for more details .,introduction,introduction,0,35,24,24,0,introduction : introduction,0.14112903225806453,0.7058823529411765,0.7058823529411765,Please see Section 5 for more details ,8,"Finally , bag representation with entity type information is fed to a softmax classifier .","Similarly , relation phrase "" was started by "" extracted using Open Information Extraction ( Open IE ) methods can be useful , given that the aliases of relation founder OfCompany , e.g. , founded , co -founded , etc. , are available .",introduction
text_summarization,4,This is done in a differentiable way as follows :,model,Pooling submodule,0,137,67,11,0,model : Pooling submodule,0.5310077519379846,0.7282608695652174,0.5789473684210527,This is done in a differentiable way as follows ,10,Our pooling submodule instead uses firm attention mechanism to consider only top k entities when constructing the topic vector .,"where the functions K = top k ( G ) gets the indices of the top k vectors in G and P = sparse vector ( K , 0 , ?? ) creates a sparse vector where the values of K is 0 and ?? otherwise 4 .",method
sentiment_analysis,6,The videos are segmented with each segments sentiment label scored between + 3 ( strong positive ) to - 3 ( strong negative ) by 5 annotators .,dataset,Multimodal Sentiment Analysis Datasets,0,205,4,4,0,dataset : Multimodal Sentiment Analysis Datasets,0.7093425605536332,0.2857142857142857,0.2857142857142857,The videos are segmented with each segments sentiment label scored between 3 strong positive to 3 strong negative by 5 annotators ,22,The MOSI dataset is a dataset rich in sentimental expressions where 93 people review topics in English .,"We took the average of these five annotations as the sentiment polarity and , hence , considered only two classes ( positive and negative ) .",experiment
natural_language_inference,3,LSTM maintains a memory cell that updates and exposes its content only when deemed necessary .,model,Sentence Modelling with LSTM,0,45,22,3,0,model : Sentence Modelling with LSTM,0.21634615384615385,0.1946902654867257,0.2307692307692308,LSTM maintains a memory cell that updates and exposes its content only when deemed necessary ,16,"Long short - term memory network ( LSTM ) ] is a type of recurrent neural network ( RNN ) , and specifically addresses the issue of learning long - term dependencies .","While there are numerous LSTM variants , here we use the LSTM architecture used by , which is similar to the architecture of but without peep - hole connections .",method
natural_language_inference,83,The model relies primarily on topic ( black ) and events ( light grey ) .,ablation,Ablation Study,0,213,19,19,0,ablation : Ablation Study,0.696078431372549,0.9047619047619048,0.9047619047619048,The model relies primarily on topic black and events light grey ,12,"The incorrect ending introduces a new entity / idea , apple pie , resulting in topical incoherence of this option with the rest of the story .",Reliance on events makes sense because it is likely for a person to enjoy what they fondly cook .,result
natural_language_inference,52,"Here we tackle this challenge by using the QA task performance as supervision for the justification reranking , allowing us to learn to choose both the correct answer and a compelling , human - readable justification for that answer .",approach,Approach,0,72,3,3,0,approach : Approach,0.27586206896551724,0.2307692307692308,0.2307692307692308,Here we tackle this challenge by using the QA task performance as supervision for the justification reranking allowing us to learn to choose both the correct answer and a compelling human readable justification for that answer ,37,"One of the primary difficulties with the explainable QA task addressed here is that , while we have supervision for the correct answer , we do not have annotated answer justifications .","Additionally , similar to the strategy applied to parsing , we combine representation - based features with explicit features that capture additional information that is difficult to model through embeddings , especially with limited training data .",method
sentence_classification,2,This results to moving the vectors in the same vector space .,model,Model,0,90,5,5,0,model : Model,0.35714285714285715,0.09433962264150944,0.8333333333333334,This results to moving the vectors in the same vector space ,12,"The sentence vectors are altered using other sentence vectors as context ( e.g. v t 1 is altered using v s , v t2 , ... , v tn ) .",The full architecture is shown in .,method
natural_language_inference,13,Search QA - the main competitor baseline is the AMANDA model proposed by .,method,Competitor Methods,1,157,7,7,0,method : Competitor Methods,0.6855895196506551,0.2916666666666667,0.5384615384615384,Search QA the main competitor baseline is the AMANDA model proposed by ,13,"It uses BiMPM 's matching functions for extensive matching between Q , P and A , multi-hop reasoning powered by ReasoNet and employs reinforcement learning techniques for dynamic strategy selection .","AMANDA uses a multi-factor self - attention module , along with a question focused span prediction .",method
natural_language_inference,17,"Concretely , given two sets of hidden vectors , Model",architecture,Alignment Architecture for MRC,0,50,7,7,0,architecture : Alignment Architecture for MRC,0.19230769230769232,0.051094890510948905,0.05384615384615385,Concretely given two sets of hidden vectors Model,8,"more popular approach is to compute attentions in parallel , resulting in a similarity matrix .","Concretely , given two sets of hidden vectors , Model",method
relation-classification,3,"The input is a sequence of tokens ( i.e. , sentence ) w = w 1 , ... , w n .",model,Joint learning as head selection,1,38,5,4,0,model : Joint learning as head selection,0.2773722627737226,0.10869565217391304,0.125,The input is a sequence of tokens i e sentence w w 1 w n ,16,It aims to detect ( i ) the type and the boundaries of the entities and ( ii ) the relations between them .,"We use character level embeddings to implicitly capture morphological features ( e.g. , prefixes and suffixes ) , representing each character by a vector ( embedding ) .",method
sentiment_analysis,29,The word embeddings are initialized with 300 - dimensional Glove vectors and are fixed during training .,hyperparameters,Hyperparameters Setting,1,123,5,5,0,hyperparameters : Hyperparameters Setting,0.7068965517241379,0.5,0.5,The word embeddings are initialized with 300 dimensional Glove vectors and are fixed during training ,16,The L 2 regularization coefficient is set to 10 ? 4 and the dropout keep rate is set to 0.2 .,"For the out of vocabulary words we initialize them randomly from uniform distribution U ( ? 0.01 , 0.01 ) .",experiment
natural_language_inference,51,One reason is NTM fails to change the memory access behavior ( perseveration ) .,result,NTM Sequencing Tasks,0,177,44,8,0,result : NTM Sequencing Tasks,0.6679245283018868,0.6285714285714286,0.8,One reason is NTM fails to change the memory access behavior perseveration ,13,"As shown in , some tasks such as Copy and Associative Recall , which are easy to solve if trained separately , become unsolvable by NTM when sequenced together .","For examples , NTM keeps following repeat copy reading strategy for all timesteps in C + RC task ) .",result
relation_extraction,2,"In this paper , we propose a model that both leverages the pretrained BERT language model and incorporates information from the target entities to tackle the relation classification task .",abstract,abstract,0,8,6,6,0,abstract : abstract,0.05925925925925926,0.75,0.75,In this paper we propose a model that both leverages the pretrained BERT language model and incorporates information from the target entities to tackle the relation classification task ,29,Relation classification differs from those tasks in that it relies on information of both the sentence and the two target entities .,We locate the target entities and transfer the information through the pre-trained architecture and incorporate the corresponding encoding of the two entities .,abstract
question-answering,5,"In order to stabilize the learning , we clip the gradients if their norm is greater than 5 and those marked with 2 are from .",training,Training Details,1,122,6,6,0,training : Training Details,0.5520361990950227,0.4615384615384616,0.4615384615384616,In order to stabilize the learning we clip the gradients if their norm is greater than 5 and those marked with 2 are from ,25,"Following , the GRU recurrent weights are initialized to be orthogonal and biases are initialized to zero .",the inputs to both the query and the document attention mechanisms .,experiment
text_summarization,8,We first test this hypothesis by posing summarization as a multi-task problem and training the tagger and summarization model with the same features .,system description,End-to-End Alternatives,0,138,68,12,0,system description : End-to-End Alternatives,0.4825174825174825,0.7472527472527473,0.631578947368421,We first test this hypothesis by posing summarization as a multi task problem and training the tagger and summarization model with the same features ,25,"Next , we investigate whether the content selector can be trained alongside the abstractive system .","For this setup , we use a shared encoder for both abstractive summarization and content selection .",method
text_summarization,13,"This seems unnecessary for a problem such as document summarization ; intuitively , we only need to attend to a few important chunks of text at a time .",model,model,0,92,23,23,0,model : model,0.3445692883895131,0.2323232323232323,0.2323232323232323,This seems unnecessary for a problem such as document summarization intuitively we only need to attend to a few important chunks of text at a time ,27,"The attention network of STANDARD is computationally expensive for long sequences - for each hidden state of the decoder , we need to compare it to every hidden state of the encoder in order to determine whereto attend to .","Therefore , we propose a hierarchical method of attending to the document - by segmenting the document into large top - level chunks of text , we first attend to these chunks , then to the words within the chunks .",method
natural_language_inference,52,"On our test data , TUPLEINF ( T+T ' ) achieves 46.17 % P@1 ( line 5 ) .",performance,QA Performance,0,181,11,11,0,performance : QA Performance,0.6934865900383141,0.22448979591836726,0.4230769230769231,On our test data TUPLEINF T T achieves 46 17 P 1 line 5 ,15,TUPLEINF ( T+T ' ) uses Integer Linear Programming to find support for questions via tuple representations of KB sentences 10 .,"As this model is independent of an IR component , we compare its performance against our full system without the IR - based features ( line 6 ) , whose performance is 48.66 % P@1 , an absolute improvement of 2.49 % P@1 ( 5.4 % relative ) despite our unstructured text inputs and the far smaller size of our knowledge base ( three orders of magnitude ) .",result
sentiment_analysis,1,"Zhang et al. and Zhang et al . incorporated spatial relations in EEG signals using convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) , respectively .",introduction,introduction,0,34,21,21,1,introduction : introduction,0.08585858585858586,0.3684210526315789,0.3684210526315789,Zhang et al and Zhang et al incorporated spatial relations in EEG signals using convolutional neural networks CNN and recurrent neural networks RNN respectively ,25,There have been several attempts to address the first challenge .,"Zhang et al. and Zhang et al . incorporated spatial relations in EEG signals using convolutional neural networks ( CNN ) and recurrent neural networks ( RNN ) , respectively .",introduction
natural_language_inference,19,"The ( i , j ) component of the distance mask is ?| i ? j| , representing the distance between ( i + 1 ) th word and ( j + 1 ) th word multiplied by ? 1 .",architecture,Masked Multi-Head Attention,0,108,33,17,0,architecture : Masked Multi-Head Attention,0.4235294117647059,0.4074074074074074,0.7391304347826086,The i j component of the distance mask is i j representing the distance between i 1 th word and j 1 th word multiplied by 1 ,28,"The diagonal component of M dir is also set to ?? so that each token does not consider itself to attention , and the information of each token is later transmitted through the fusion gate of section 4.2.3 M dis is shown in the .","By multiplying this value by ? and adding it to logit , the attention weight becomes smaller as distance increases .",method
sentiment_analysis,23,where W i ? R nenc is the weight parameter associated with node x i ; b ? R nc is the bias term .,system description,Tree-based Convolution,0,86,50,12,0,system description : Tree-based Convolution,0.2945205479452055,0.4761904761904762,0.4444444444444444,where W i R nenc is the weight parameter associated with node x i b R nc is the bias term ,22,"The output of the tree - based convolution window , evaluated at the current subtree , is given by the following generic equation .",where W i ? R nenc is the weight parameter associated with node x i ; b ? R nc is the bias term .,method
sentiment_analysis,40,"SVM : The traditional state - of - the - art method using SVMs on surface features , lexicon features and parsing features , which is the best team in SemEval 2014 .",method,Compared Methods,1,153,7,7,0,method : Compared Methods,0.6860986547085202,0.4117647058823529,0.4117647058823529,SVM The traditional state of the art method using SVMs on surface features lexicon features and parsing features which is the best team in SemEval 2014 ,27,"The second one , named AC , averages the word vectors of the full context .","Rec - NN : It firstly uses rules to transform the dependency tree and put the opinion target at the root , and then performs semantic composition with Recursive NNs for sentiment prediction .",method
relation_extraction,13,We train the BERT EM + MTB model by initializing the Transformer weights to the weights from BERT LARGE and use the following parameters :,experiment,Experimental Evaluation,0,182,4,4,0,experiment : Experimental Evaluation,0.8544600938967136,0.15384615384615385,0.8,We train the BERT EM MTB model by initializing the Transformer weights to the weights from BERT LARGE and use the following parameters ,24,"We start with the best BERT based model from Section 3.3 , which we call BERT EM , and we compare this to a variant that is trained with the matching the blanks task ( BERT EM + MTB ) .","We report results on all of the tasks from Section 3.1 , using the same task - specific training methodology for both BERT EM and BERT EM + MTB .",experiment
natural_language_inference,88,"As the input sequence gets compressed and blended into a single dense vector , suf - Color red represents the current word being fixated , blue represents memories .",introduction,introduction,0,34,15,15,0,introduction : introduction,0.13765182186234814,0.4545454545454545,0.4545454545454545,As the input sequence gets compressed and blended into a single dense vector suf Color red represents the current word being fixated blue represents memories ,26,The second issue relates to memory compression problems .,Shading indicates the degree of memory activation .,introduction
natural_language_inference,78,where ? i is the sub- phrase in P that is softly aligned to hi .,model,Inter-Attention Alignment Layer,0,103,28,6,0,model : Inter-Attention Alignment Layer,0.37318840579710144,0.3218390804597701,0.42857142857142855,where i is the sub phrase in P that is softly aligned to hi ,15,"where E ? R p h andp i ,h j are the i - th and j- th word in the premise and hypothesis respectively .",where ? i is the sub- phrase in P that is softly aligned to hi .,method
text_summarization,1,Low pairwise metric indicates high diversity between generated hypotheses .,baseline,Metrics: Accuracy and Diversity,0,191,38,21,0,baseline : Metrics: Accuracy and Diversity,0.7958333333333333,1.0,1.0,Low pairwise metric indicates high diversity between generated hypotheses ,10,This metric computes the average of sentencelevel metrics between all pairwise combinations of hypotheses {? 1 . . .? K } generated from each source sequence x ., ,result
natural_language_inference,10,"Obviously ST estimators are biased , however they perform well in practice , according to several previous works and our own result .",model,Gumbel-Softmax,0,91,35,23,0,model : Gumbel-Softmax,0.3839662447257384,0.4487179487179487,0.8518518518518519,Obviously ST estimators are biased however they perform well in practice according to several previous works and our own result ,21,"Similar to the STE , it maintains sparsity by taking different paths in the forward and backward propagation .","In the forward pass , it discretizes a continuous probability vector y sampled from the Gumbel - Softmax distribution into the one - hot vector",method
phrase_grounding,0,"In this section , we describe our method ( illustrated in ) for addressing the textual grounding task and elaborate on each part with details .",method,Method,0,75,2,2,0,method : Method,0.3333333333333333,0.02666666666666667,0.3333333333333333,In this section we describe our method illustrated in for addressing the textual grounding task and elaborate on each part with details ,23, ,"In Section 3.1 , we explain how we extract multi-level visual features from an image and word / sentence embeddings from the text , and then describe how we map them to a common space .",method
natural_language_inference,39,"Note that we used the same word embeddings , sparse distribution targets , and loss function as in and , thereby representing comparable experimental conditions . ) .",experiment,Experimental Setup,0,166,36,36,0,experiment : Experimental Setup,0.8058252427184466,0.9,0.9,Note that we used the same word embeddings sparse distribution targets and loss function as in and thereby representing comparable experimental conditions ,23,"In comparison , our model outperforms both by 1 % in Pearson 's r , over 1.1 % in Spearman 's ? , and 2 - 3 % in MSE .","Our model outperforms the work of , which already reports a Pearson 's r score of over 0.9 , STS2014 Results ) .",experiment
natural_language_inference,47,"If there was no such consensus , which occurred in about 2 % of cases , we assigned the placeholder label '- '.",system description,Data validation,0,96,61,9,0,system description : Data validation,0.4465116279069768,0.7721518987341772,0.42857142857142855,If there was no such consensus which occurred in about 2 of cases we assigned the placeholder label ,19,"If anyone of the three labels was chosen by at least three of the five annotators , it was 15 chosen as the gold label .","While these unlabeled examples are included in the corpus distribution , they are unlikely to be helpful for the standard NLI classification task , and we do not include them in either training or evaluation in the experiments that we discuss in this paper .",method
question-answering,6,It can be hypothesized that a larger hidden state is required for real data . Parallelization .,model,RESULTS.,0,236,56,22,0,model : RESULTS.,0.7151515151515152,0.7088607594936709,0.4888888888888889,It can be hypothesized that a larger hidden state is required for real data Parallelization ,16,"d ) Increasing the dimension of the hidden state to 100 in the dialog 's Task 6 ( DSTC2 ) helps , while there is not much improvement in the dialog 's Task 1 - 5 .",We implement QRN with and without parallelization in TensorFlow ) on a single Titan X GPU to qunaitify the computational gain of the parallelization .,method
natural_language_inference,46,The question becomes the query for retrieval .,baseline,baseline,0,181,27,27,0,baseline : baseline,0.6094276094276094,0.8181818181818182,0.8181818181818182,The question becomes the query for retrieval ,8,"We split the task into two steps : first , we retrieve a small number of relevant passages from the story using an IR system , and subsequently , apply one of the neural models above on the resulting document .","This IR problem is much harder that traditional document retrieval , as the documents , the passages here , are very similar , and the question is short and entities mentioned likely occur many times in the story .",result
sentiment_analysis,51,It shows the accuracy of various models on SST - 2 and SST - 5 .,result,Results,0,143,3,3,0,result : Results,0.9533333333333334,0.5,0.6,It shows the accuracy of various models on SST 2 and SST 5 ,14,The result and comparisons are shown in .,It includes results for all phrases as well as for just the root ( whole review ) .,result
natural_language_inference,7,"We couple the input and forget gates in our LSTMs , as described in , and we use a single dropout mask to apply dropout across all LSTM time - steps as proposed by .",experiment,EXPERIMENTAL SETUP,1,108,4,4,0,experiment : EXPERIMENTAL SETUP,0.6067415730337079,0.4444444444444444,0.4444444444444444,We couple the input and forget gates in our LSTMs as described in and we use a single dropout mask to apply dropout across all LSTM time steps as proposed by ,32,These embeddings cover 200 k words and all out of vocabulary ( OOV ) words are projected onto one of 1 m randomly initialized 300d embeddings .,Hidden layers in the feed forward neural networks use rectified linear units .,experiment
natural_language_inference,74,proposed to transfer the pre-trained encoder from the neural machine translation ( NMT ) to the NLI tasks .,system description,Natural Language Inference,0,216,18,8,0,system description : Natural Language Inference,0.9642857142857144,0.9,0.8,proposed to transfer the pre trained encoder from the neural machine translation NMT to the NLI tasks ,18,"incorpated Skipthought , which is an unsupervised sequence model that has been proven to generate useful sentence embedding .",Our method combines a pre-trained sentence encoder from the DMP task with an integrated NLI model to compose a novel framework .,method
natural_language_inference,64,"We demonstrate the benefits of our approach for answer sentence selection , which is a well - known inference task in Question Answering .",abstract,abstract,1,7,5,5,0,abstract : abstract,0.027888446215139442,0.4545454545454545,0.4545454545454545,We demonstrate the benefits of our approach for answer sentence selection which is a well known inference task in Question Answering ,22,We then perform a second fine - tuning step to adapt the transferred model to the target domain .,"We built a large scale dataset to enable the transfer step , exploiting the Natural Questions dataset .",abstract
natural_language_inference,99,"Sometimes we may not directly ensure the answer 's boundary , we go back and confirm the question .",model,Interaction Modeling Layer,0,82,5,5,0,model : Interaction Modeling Layer,0.3253968253968254,0.052083333333333336,0.5555555555555556,Sometimes we may not directly ensure the answer s boundary we go back and confirm the question ,18,Then we skim the passage to refine the answer .,"After confirming , we scan the passage and refine the right answer we thought .",method
text_generation,1,RankGAN learns the model from the relative ranking information between the machine - written and the human - written sentences in an adversarial framework .,introduction,introduction,1,29,19,19,0,introduction : introduction,0.10583941605839416,0.7037037037037037,0.7037037037037037,RankGAN learns the model from the relative ranking information between the machine written and the human written sentences in an adversarial framework ,23,"In this paper , we propose a novel adversarial learning framework , RankGAN , for generating highquality language descriptions .","In the proposed RankGAN , we relax the training of the discriminator to a learning - to - rank optimization problem .",introduction
sentiment_analysis,27,We also employ a graph to capture the sentiment dependencies between aspects .,system description,Graph convolutional network,0,74,19,3,0,system description : Graph convolutional network,0.26811594202898553,0.8636363636363636,0.5,We also employ a graph to capture the sentiment dependencies between aspects ,13,"GCN is widely used to deal with data which contains rich relationships and interdependency between objects , because GCN can effectively capture the dependence of graphs via message passing between the nodes of graphs .",The final output of each GCN node is designed to be the classifier of the corresponding aspect in our task .,method
sarcasm_detection,1,The comment The part where Obama signed it .,analysis,Case Studies,0,316,19,8,0,analysis : Case Studies,0.9461077844311376,0.59375,0.4705882352941176,The comment The part where Obama signed it ,9,So all of the US presidents are terrorists for the last 5 years .,does n't seem to be sarcastic until looked upon as a remark to its previous comment,result
sentiment_analysis,15,"The complete training and testing code , a live demo and the Stanford Sentiment Treebank dataset are available at http://nlp.stanford.edu/ sentiment .",introduction,introduction,0,34,22,22,0,introduction : introduction,0.1259259259259259,1.0,1.0,The complete training and testing code a live demo and the Stanford Sentiment Treebank dataset are available at http nlp stanford edu sentiment ,24,RNTNs also learn that sentiment of phrases following the contrastive conjunction ' but ' dominates ., ,introduction
natural_language_inference,44,"In the late 1920s , Tesla also befriended George Sylvester Viereck , a poet , writer , mystic , and later , a Nazi propagandist .",system description,SQuAD and NewsQA,0,137,44,34,0,system description : SQuAD and NewsQA,0.479020979020979,0.4835164835164835,0.41975308641975306,In the late 1920s Tesla also befriended George Sylvester Viereck a poet writer mystic and later a Nazi propagandist ,20,When did Germany found their first settlement ? 1883-84 1884 1884,"In middle age , Tesla became a close friend of Mark Twain ; they spent a lot of time together in his lab and elsewhere .",method
sentiment_analysis,9,"For pre-trained BERT , the fine - tuning learning process is indispensable .",model,BERT-Shared Layer,0,85,15,3,0,model : BERT-Shared Layer,0.3068592057761733,0.1875,0.6,For pre trained BERT the fine tuning learning process is indispensable ,12,"The pre-trained BERT model is designed to improve performance for most NLP tasks , and The LCF - ATEPC model deploys two independent BERT - Shared layers thatare aimed to extract local and global context features .","Both BERT - Shared layers are regarded as embed - ded layers , and the fine - tuning process is conducted independently according to the joint loss function of multi-task learning .",method
sentence_compression,0,This method does not use neural network models and is an unsupervised method that relies heavily on the syntactic structures of the input sentences,dataset,Datasets and Experiment Settings,0,181,36,36,0,dataset : Datasets and Experiment Settings,0.6487455197132617,0.9230769230769232,0.9230769230769232,This method does not use neural network models and is an unsupervised method that relies heavily on the syntactic structures of the input sentences,24,This is the ILP - based method proposed by .,This is an abstractive sequence - to - sequence model trained on 3.8 million Gigaword title - article pairs as described in Section 1 .,experiment
relation-classification,1,Our tagging methods are shown in part three ( row 7 to 9 ) .,hyperparameters,hyperparameters,0,166,15,15,0,hyperparameters : hyperparameters,0.6747967479674797,0.9375,0.9375,Our tagging methods are shown in part three row 7 to 9 ,13,Details of the data can be found in is the pipelined methods and the second part ( row 4 to 6 ) is the jointly extracting methods .,"In this part , we not only report the results of precision , recall and F1 , we also compute their standard deviation .",experiment
text-classification,7,"To visualize the connection strength between capsule layers clearly , we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly , where the primary capsules denote N-gram phrases in the form of capsules .",ablation,Connection Strength Visualization,0,181,31,2,0,ablation : Connection Strength Visualization,0.7448559670781894,0.5535714285714286,0.18181818181818185,To visualize the connection strength between capsule layers clearly we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer directly where the primary capsules denote N gram phrases in the form of capsules ,43, ,"The connection strength shows the importance of each primary capsule for text categories , acting like a parallel attention mechanism .",result
sentiment_analysis,32,"However , these studies always ignore the separate modeling of targets .",abstract,abstract,0,6,4,4,0,abstract : abstract,0.026086956521739132,0.5,0.5,However these studies always ignore the separate modeling of targets ,11,Previous approaches have realized the importance of targets in sentiment classification and developed various methods with the goal of precisely modeling their contexts via generating target - specific representations .,"In this paper , we argue that both targets and contexts deserve special treatment and need to be learned their own representations via interactive learning .",abstract
sentiment_analysis,30,"In terms of evaluation , we adopt the standard 70/10/20 train / validation / test split , and report the test performance corresponding to the model with the best validation score .",experiment,Experimental Setup,0,106,27,26,0,experiment : Experimental Setup,0.828125,0.8709677419354839,0.8666666666666667,In terms of evaluation we adopt the standard 70 10 20 train validation test split and report the test performance corresponding to the model with the best validation score ,30,We additionally implement a bi-directional EntNet with the same hyper - parameter settings and Glo Ve embeddings as our model .,"Following , we consider the top 4 aspects only ( GENERAL , PRICE , TRANSIT - LOCATION , and SAFETY ) and employ the following evaluation metrics : macro -average F 1 and AUC for aspect detection ignoring the none class , and accuracy and macro -average AUC for sentiment classification .",experiment
part-of-speech_tagging,2,"We study cross - domain , cross - application , and cross-lingual transfer , and present a parameter - sharing architecture for each case .",introduction,introduction,0,27,16,16,0,introduction : introduction,0.15168539325842698,0.7619047619047619,0.7619047619047619,We study cross domain cross application and cross lingual transfer and present a parameter sharing architecture for each case ,20,Our approach combines the objectives of the two tasks and uses gradient - based methods for efficient training .,Experimental results show that our approach can significantly improve the performance of the target task when the the target task has few labels and is more related to the source task .,introduction
natural_language_inference,53,"While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them , this is not yet clear with regard to representations that carry the meaning of a full sentence .",introduction,introduction,0,12,3,3,0,introduction : introduction,0.0576923076923077,0.2,0.2,While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them this is not yet clear with regard to representations that carry the meaning of a full sentence ,36,Distributed representations of words ( or word embeddings ) have shown to provide useful features for various tasks in natural language processing and computer vision .,"That is , how to capture the relationships among multiple words and phrases in a single vector remains an question to be solved .",introduction
natural_language_inference,94,We then obtain the updated context representation :,method,method,0,96,28,28,0,method : method,0.2506527415143603,0.2204724409448819,0.2204724409448819,We then obtain the updated context representation ,8,We then compute a query - to - context attention vector :,"where ; is concatenation , ct is the cell 's output .",method
text-to-speech_synthesis,0,"For the 3 CNN models , they share the same hidden size ( 256 ) but vary in the number of encoder - decoder layers ( 10 - 10 , 10 - 10 , 8 - 8 ) and convolutional kernel widths ( 3 , 2 , 2 ) respectively .",model,model,1,114,5,5,0,model : model,0.7037037037037037,0.8333333333333334,0.8333333333333334,For the 3 CNN models they share the same hidden size 256 but vary in the number of encoder decoder layers 10 10 10 10 8 8 and convolutional kernel widths 3 2 2 respectively ,36,The 4 Transformer models share the same hidden size ( 256 ) but vary in the number of the encoder - decoder layers .,"For the 3 Bi - LSTM models , they share the same number of encoder - decoder layers ( 1 - 1 ) , but with different hidden sizes ( 256 , 384 and 512 ) .",method
text_summarization,6,"For DUC - 2004 , the maximum length of summaries is 75 bytes .",experiment,Experimental Settings,1,212,5,5,0,experiment : Experimental Settings,0.8091603053435115,0.25,0.25,For DUC 2004 the maximum length of summaries is 75 bytes ,12,The batch size of mini-batch training is 256 .,"For the dataset of LCSTS , the dimension of word embeddings is 350 .",experiment
sentiment_analysis,8,The same split is used for all the experiments to ensure a fair comparison .,implementation,implementation,0,170,9,9,0,implementation : implementation,0.7264957264957265,0.2571428571428571,0.2571428571428571,The same split is used for all the experiments to ensure a fair comparison ,15,We randomly split our dataset into a train ( 80 % ) and test ( 20 % ) set .,The LSTM classifiers were trained on an NVIDIA Titan X GPU for faster processing .,experiment
natural_language_inference,68,"Recall that in the sequence model , the answer is represented by a sequence of integers a = ( a 1 , a 2 , . . . ) indicating the positions of the selected tokens in the original passage .",method,Answer Pointer Layer,0,142,92,40,0,method : Answer Pointer Layer,0.5702811244979921,0.8070175438596491,0.6451612903225806,Recall that in the sequence model the answer is represented by a sequence of integers a a 1 a 2 indicating the positions of the selected tokens in the original passage ,32,The Sequence Model :,The Ans - Ptr layer models the generation of these integers in a sequential manner .,method
relation_extraction,11,"Here , ? k luv ? R m andb k luv ? R are parameters which are trained and ?( ) is the sigmoid function .",system description,Integrating Edge Importance,0,91,24,6,0,system description : Integrating Edge Importance,0.36693548387096775,0.1875,0.6,Here k luv R m andb k luv R are parameters which are trained and is the sigmoid function ,20,"At k th layer , the importance of an edge ( u , v , l uv ) is computed as :","Here , ? k luv ? R m andb k luv ? R are parameters which are trained and ?( ) is the sigmoid function .",method
natural_language_inference,72,"Although all neural models use pre-trained word embeddings , for Ent and Anonym the multi-word entities do not have pre-trained embeddings since our embeddings are induced on the word level .",analysis,Results and analysis,0,220,15,15,0,analysis : Results and analysis,0.7051282051282052,0.6521739130434783,0.6521739130434783,Although all neural models use pre trained word embeddings for Ent and Anonym the multi word entities do not have pre trained embeddings since our embeddings are induced on the word level ,33,"For example , for the ground - truth entity "" chest CT "" , GA - NoEnt predicts "" interval CT scans of the chest "" .",This may partly explain the competitive performance of NoEnt compared to Ent .,result
natural_language_inference,50,This raises questions about the effectiveness of the 3D attentive pooling mechanism .,analysis,Results and Analysis,0,238,15,15,0,analysis : Results and Analysis,0.750788643533123,0.18987341772151892,0.5769230769230769,This raises questions about the effectiveness of the 3D attentive pooling mechanism ,13,"Evidently , its performance can be easily superseded in a much smaller training time and parameter cost .",reports the results on TrecQA ( raw ) .,result
named-entity-recognition,7,The model is trained using Adam and a gradient clipping of 3.0 .,experiment,Setup,1,126,10,5,0,experiment : Setup,0.7974683544303798,0.7142857142857143,0.5555555555555556,The model is trained using Adam and a gradient clipping of 3 0 ,14,The embeddings of POS tags are initialized randomly with dimension 32 .,Early stopping is used based on the performance of development sets .,experiment
machine-translation,0,"After training , the language model achieved a perplexity of 45.80 .",model,Neural Language Model,0,145,9,9,0,model : Neural Language Model,0.6621004566210046,0.6428571428571429,0.6428571428571429,After training the language model achieved a perplexity of 45 80 ,12,"All the weight parameters were initialized uniformly between ? 0.01 and 0.01 , and the model was trained until the validation perplexity did not improve for 10 epochs .",The validation set was a random selection of 0.1 % of the corpus .,method
natural_language_inference,94,The output of the self - attention layer is generated by another layer of bidirectional LSTM .,method,method,0,105,37,37,0,method : method,0.2741514360313316,0.2913385826771653,0.2913385826771653,The output of the self attention layer is generated by another layer of bidirectional LSTM ,16,"where W 4 , W 5 , and W 6 are trainable parameters .","At decoding step t , the decoder receives the input x t ( embedded representation of last timestep 's output ) , the last time step 's hidden state s t?1 and context vector a t?1 .",method
relation-classification,2,"On the other hand , our model performs within a reasonable margin ( ? 0.5 % for the NER task and ? 1 %",result,Results,0,234,14,13,0,result : Results,0.7932203389830509,0.35897435897435903,0.34210526315789475,On the other hand our model performs within a reasonable margin 0 5 for the NER task and 1 ,20,"Finally , we obtain more effective word representations by using character - level embeddings .","for the RE task ) compared to For the CoNLL04 dataset , there are two different evaluation settings , namely relaxed and strict .",result
negation_scope_resolution,0,looked at neural networks for scope detection .,approach,Deep Learning Approaches,0,142,65,6,0,approach : Deep Learning Approaches,0.6173913043478261,0.7142857142857143,0.1875,looked at neural networks for scope detection ,8,They used this system on the BioScope Corpus and outperformed all existing systems on the BioScope Abstracts .,They rightly point out that most systems were highly engineered and only tested on the same genre they were trained on .,method
relation_extraction,3,"Since our method uses a single - pass to compute all relations at once , it scales to larger datasets easily ; which makes it more usable in real - world applications .",abstract,abstract,0,7,5,5,0,abstract : abstract,0.051094890510948905,0.8333333333333334,0.8333333333333334,Since our method uses a single pass to compute all relations at once it scales to larger datasets easily which makes it more usable in real world applications ,29,Our solution is built on top of the pre-trained self - attentive models ( Transformer ) ., ,abstract
relation_extraction,7,"The transfer learning method pre-trains our model parameters in the entity type classification task , which in turn contributes to the relation extraction .",methodology,Parameter-Transfer Initializer,0,137,76,2,0,methodology : Parameter-Transfer Initializer,0.528957528957529,0.6666666666666666,1.0,The transfer learning method pre trains our model parameters in the entity type classification task which in turn contributes to the relation extraction ,24, , ,method
relation_extraction,0,We employ a pointer - like network on top of the sequence layer in order to find the relation tag for each token as shown in .,model,Model,0,64,10,10,0,model : Model,0.25,0.14084507042253522,0.9090909090909092,We employ a pointer like network on top of the sequence layer in order to find the relation tag for each token as shown in ,26,"Thus , we model the relations between "" ITV "" and "" Martin Geissler "" , and "" News "" and "" Martin Geissler "" separately .","At each time step , the network utilizes the information available about all output tags from the previous time steps in order to output the entity tag and relation tag jointly for the current token .",method
text-classification,7,"In addition , we also measure the Exact Match Ratio ( ER ) which considers partially correct prediction as incorrect and only counts fully correct samples .",ablation,Single-Label to Multi-Label Text Classification,0,173,23,18,0,ablation : Single-Label to Multi-Label Text Classification,0.7119341563786008,0.4107142857142857,0.75,In addition we also measure the Exact Match Ratio ER which considers partially correct prediction as incorrect and only counts fully correct samples ,24,"Any of these scores are firstly computed on individual class labels and then averaged over all classes , called label - based measures .",The experimental results are summarized in .,result
natural_language_inference,24,"Finally , we test SAN on two Adversarial SQuAD datasets , AddSent and Add OneSent , where the passages contain auto - generated adversarial distracting sentences to fool computer systems thatare developed to answer questions about the passages .",result,SAN:,0,173,41,31,0,result : SAN:,0.7393162393162394,0.5125,0.4428571428571429,Finally we test SAN on two Adversarial SQuAD datasets AddSent and Add OneSent where the passages contain auto generated adversarial distracting sentences to fool computer systems thatare developed to answer questions about the passages ,35,"In summary , we think it is useful to perform some approximate hyper - parameter tuning for the number of steps , but it is not necessary to find the exact optimal value .","For example , AddSent is constructed by adding sentences that look similar to the question , but do not actually contradict the correct answer .",result
question-answering,7,The encoder provides the semantic and syntactic information about the source sentences to the decoder and the decoder generates the target sentences by conditioning on this information and its partially produced translation .,analysis,Machine Translation,0,224,25,4,0,analysis : Machine Translation,0.8145454545454546,0.390625,0.16666666666666666,The encoder provides the semantic and syntactic information about the source sentences to the decoder and the decoder generates the target sentences by conditioning on this information and its partially produced translation ,33,The NMT problem is mostly defined within the encoder - decoder framework .,"For an efficient encoding , the attention - based NTM was introduced .",result
natural_language_inference,64,"RoBERTa - Large TANDA with ASNQ ? TREC - QA again establishes an impressive performance of 0.943 in MAP and 0.974 in MRR , outperforming the previous state of the art by .",result,TREC-QA,1,174,12,2,0,result : TREC-QA,0.6932270916334662,0.21428571428571427,0.18181818181818185,RoBERTa Large TANDA with ASNQ TREC QA again establishes an impressive performance of 0 943 in MAP and 0 974 in MRR outperforming the previous state of the art by ,31, ,"RoBERTa - Large TANDA with ASNQ ? TREC - QA again establishes an impressive performance of 0.943 in MAP and 0.974 in MRR , outperforming the previous state of the art by .",result
sentiment_analysis,42,"Given a list of sentences and a question , the task aims to find evidences from these sentences and generate an answer , e.g. a word .",system description,Deep Memory Network for Aspect Level Sentiment Classification,0,44,10,9,0,system description : Deep Memory Network for Aspect Level Sentiment Classification,0.1746031746031746,0.5555555555555556,0.5294117647058824,Given a list of sentences and a question the task aims to find evidences from these sentences and generate an answer e g a word ,26,Let us take question answering as an example to explain the workflow of memory network .,"During inference , I component reads one sentence s i at a time and encodes it into a vector representation .",method
part-of-speech_tagging,7,Our analysis suggests that bi - LSTMs are less sensitive to training data size and label corruptions ( at small noise levels ) than previously assumed .,abstract,abstract,0,9,7,7,0,abstract : abstract,0.07200000000000001,1.0,1.0,Our analysis suggests that bi LSTMs are less sensitive to training data size and label corruptions at small noise levels than previously assumed ,24,"The model obtains state - of - the - art performance across 22 languages , and works especially well for morphologically complex languages .", ,abstract
natural_language_inference,29,EXPERIMENTAL RESULT 6.1 Overall performance,implementation,implementation,0,290,8,8,0,implementation : implementation,0.7945205479452054,0.2285714285714285,0.2285714285714285,EXPERIMENTAL RESULT 6 1 Overall performance,6,We implement our model using TensorFlow framework and train our model and all baseline models on NVIDIA Tesla P40 GPU .,"For research question RQ1 , to demonstrate the effectiveness of PAAG , we examine the over all performance in term of BLEU , embedding metrics and human evaluation .",experiment
natural_language_inference,34,"The output representation are Q 0 ? R Ld 2 and C 0 ? R M d 2 , where d 2 is the output embedding size .",system description,Encoding Query and Context,0,132,29,7,0,system description : Encoding Query and Context,0.4474576271186441,0.2871287128712871,0.875,The output representation are Q 0 R Ld 2 and C 0 R M d 2 where d 2 is the output embedding size ,25,"In practice , we find adding the bi-attention layer achieves better performance than the BERT encoding only .","The output representation are Q 0 ? R Ld 2 and C 0 ? R M d 2 , where d 2 is the output embedding size .",method
natural_language_inference,68,The Stanford Question Answering Dataset ( SQuAD ) introduced recently by contains such more challenging questions whose correct answers can be any sequence of tokens from the given text .,introduction,introduction,0,18,8,8,0,introduction : introduction,0.07228915662650602,0.2,0.2,The Stanford Question Answering Dataset SQuAD introduced recently by contains such more challenging questions whose correct answers can be any sequence of tokens from the given text ,28,"Presumably , questions with more given candidate answers are more challenging .","Moreover , unlike some other datasets whose questions and answers were created automatically in Cloze style , the questions and answers in SQu AD were created by humans through crowdsourcing , which makes the dataset more realistic .",introduction
relation-classification,8,"Our full model , with the structured fine - tuning of attention layers , brings further improvement of about 5.5 % , in the MRE one - pass setting , and achieves a new state - of - the - art performance when compared to the methods with domain adaptation .",method,Results on ACE 2005,1,94,20,11,0,method : Results on ACE 2005,0.6861313868613139,0.6896551724137931,0.9166666666666666,Our full model with the structured fine tuning of attention layers brings further improvement of about 5 5 in the MRE one pass setting and achieves a new state of the art performance when compared to the methods with domain adaptation ,42,prior state - of - the - art level of the methods without domain adaptation .,It also beats the other two methods on BERT in Multi- Relation per Pass .,method
natural_language_inference,97,"Our model , significantly , can be trained end - to - end with backpropagation .",introduction,introduction,0,40,29,29,0,introduction : introduction,0.136986301369863,0.7837837837837838,0.7837837837837838,Our model significantly can be trained end to end with backpropagation ,12,"This is in contrast to most previous efforts on MCTest , whose numerous hand - engineered features can not be trained .","To facilitate learning with limited data , we also develop a unique training scheme .",introduction
part-of-speech_tagging,2,"For example , POS tags in the Genia biomedical corpus can be mapped to Penn Treebank tags , while some POS tags in Twitter ( e.g. , "" URL "" ) can not be mapped to Penn Treebank tags .",architecture,CROSS-DOMAIN TRANSFER,0,72,11,7,0,architecture : CROSS-DOMAIN TRANSFER,0.4044943820224719,0.3666666666666665,0.5833333333333334,For example POS tags in the Genia biomedical corpus can be mapped to Penn Treebank tags while some POS tags in Twitter e g URL can not be mapped to Penn Treebank tags ,34,"The two domains can have label sets that can be mapped to each other , or disparate label sets .","If the two domains have mappable label sets , we share all the model parameters and feature representation in the neural networks , including the word and character embedding , the word - level layer , the character - level layer , and the CRF layer .",method
natural_language_inference,6,"Czech , French , German and Spanish , so results between both models are directly comparable .",ablation,Multitask learning,0,175,14,6,0,ablation : Multitask learning,0.7056451612903226,0.875,0.75,Czech French German and Spanish so results between both models are directly comparable ,14,The effect in BUCC is negligible .,"As shown in , the full model equals or outperforms the one covering the evaluation languages only for all tasks but MLDoc .",result
natural_language_inference,55,"With N = NW + NS , the i - th column of Wis the embedding of the i - th element ( word , entity or relation type ) in the dictionary .",system description,Embedding Questions and Answers,0,72,47,9,0,system description : Embedding Questions and Answers,0.4897959183673469,0.7121212121212122,0.6428571428571429,With N NW NS the i th column of Wis the embedding of the i th element word entity or relation type in the dictionary ,26,Let NW denote the total number of words and NS the total number of entities and relation types .,"The function f ( . ) , which maps the questions into the embedding space R k is defined as f ( q ) = W ? ( q ) , where ?( q ) ? N N , is a sparse vector indicating the number of times each word appears in the question q ( usually 0 or 1 ) .",method
text_summarization,10,celtic captain scott brown - lrb - left - rrb-protests to referee steven mclean but the handball goes unpunished .,abstract,abstract,0,237,18,18,0,abstract : abstract,0.9011406844106464,0.4390243902439024,0.4390243902439024,celtic captain scott brown lrb left rrb protests to referee steven mclean but the handball goes unpunished ,18,"edward ofere then put caley thistle ahead , only for john guidetti to draw level for the bhoys . with the game seemingly heading for penalties , david raven scored the winner on 117 minutes , breaking thousands of celtic hearts .",griffiths shows off his acrobatic skills during celtic ? eventual surprise defeat by inverness . celtic pair aleksandar tonev - lrb - left - rrb - and john guidetti look dejected as their hopes of a domestic treble end .,abstract
sentence_classification,2,"By doing this on all possible windows of words we produce a feature map c = [ c 1 , c 2 , ... ] .",system description,Problem: Translated Sentences as Context,0,66,14,13,0,system description : Problem: Translated Sentences as Context,0.2619047619047619,0.42424242424242425,0.40625,By doing this on all possible windows of words we produce a feature map c c 1 c 2 ,20,bias vector and f ( . ) is a non-linear function .,We then apply a max - over - time pooling operation over the feature map and take the maximum value as the feature vector of the filter .,method
natural_language_inference,64,"The result of this transformation is an embedding , x , representing the text pair , which models the dependencies between words and segments of the two sentences .",system description,Transformers for AS2,0,85,14,7,0,system description : Transformers for AS2,0.3386454183266932,0.2153846153846154,0.25,The result of this transformation is an embedding x representing the text pair which models the dependencies between words and segments of the two sentences ,26,"These are fed as input to several blocks ( up to 24 ) containing layers for multi-head attention , normalization and feed forward processing .","For a downstream task , x is fed ( after applying anon linearity function ) to a fully connected layer having weights : W T and B T .",method
sentiment_analysis,49,"Unlike previous approaches that simply apply attentions over the contextual utterance for classification , we attend over the contextual utterances by computing correlations among the modalities of the target utterance and the context utterances .",introduction,introduction,1,38,28,28,0,introduction : introduction,0.15019762845849802,0.8484848484848485,0.8484848484848485,Unlike previous approaches that simply apply attentions over the contextual utterance for classification we attend over the contextual utterances by computing correlations among the modalities of the target utterance and the context utterances ,34,The attention mechanism is then used to attend to the important contextual utterances having higher relatedness or similarity ( computed using inter-modality correlations ) with the target utterance .,This explicitly helps us to distinguish which modalities of the relevant contextual utterances are more important for sentiment prediction of the target utterance .,introduction
named-entity-recognition,5,We find that the performances of both S - LSTM and BiLSTM decrease as the sentence length increases .,analysis,Analysis,0,196,3,3,0,analysis : Analysis,0.937799043062201,0.3,0.3,We find that the performances of both S LSTM and BiLSTM decrease as the sentence length increases ,18,"Figure 4 ( a ) and ( b ) show the accuracies against the sentence length on the movie review and CoNLL datasets , respectively , where test samples are binned in batches of 80 .","On the other hand , S - LSTM demonstrates relatively better robustness compared to BiLSTMs .",result
named-entity-recognition,5,"For classification , g is fed to a softmax classification layer :",baseline,Task settings,0,133,62,3,0,baseline : Task settings,0.6363636363636364,0.8378378378378378,0.2,For classification g is fed to a softmax classification layer ,11,"We consider two task settings , namely classification and sequence labelling .",where y is the probability distribution of output class labels and W c and b care model parameters .,result
natural_language_inference,57,"While our method is almost certainly worse than the state - of - the - art method of , which uses a word - by - word attention mechanism , it is also much simpler .",result,RESULTS,0,116,14,14,0,result : RESULTS,0.6744186046511628,0.3111111111111111,1.0,While our method is almost certainly worse than the state of the art method of which uses a word by word attention mechanism it is also much simpler ,29,We see that order- embeddings outperform the skipthought baseline despite not using external text corpora ., ,result
