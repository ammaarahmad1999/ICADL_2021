{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = ['title', 'abstract', 'introduction', 'related work', 'experiments', 'experiment', 'result', \n",
    "           'results', 'experimental-setup', 'experimental setup', 'hyperparameter', 'hyperparameters',\n",
    "           'ablation analysis', 'conclusion', 'background', 'future work', 'discussion', 'method']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_integers(filename):\n",
    "    with open(filename) as f:\n",
    "        return [int(x) for x in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s1, s2):\n",
    "    s1 = re.sub('[^A-Za-z0-9]+', ' ', s1).lower()\n",
    "    s2 = re.sub('[^A-Za-z0-9]+', ' ', s2).lower()\n",
    "    l1 = s1.split()\n",
    "    l2 = s2.split()\n",
    "    l2 = [token for token in l2 if not token.isnumeric()]\n",
    "    l2 = [token for token in l2 if not token == 'I']\n",
    "    return l1 == l2, l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../NCG_Dataset/training-data-master/paraphrase_generation/1\n",
      "../NCG_Dataset/training-data-master/text_summarization/0\n",
      "../NCG_Dataset/training-data-master/sentiment_analysis/51\n",
      "../NCG_Dataset/training-data-master/sentiment_analysis/8\n",
      "../NCG_Dataset/training-data-master/sentiment_analysis/5\n",
      "../NCG_Dataset/training-data-master/sentiment_analysis/19\n",
      "../NCG_Dataset/training-data-master/sentiment_analysis/35\n",
      "../NCG_Dataset/training-data-master/semantic_role_labeling/3\n",
      "../NCG_Dataset/training-data-master/topic_models/0\n",
      "../NCG_Dataset/training-data-master/negation_scope_resolution/0\n",
      "../NCG_Dataset/training-data-master/natural_language_inference/99\n",
      "../NCG_Dataset/training-data-master/natural_language_inference/42\n",
      "../NCG_Dataset/training-data-master/natural_language_inference/10\n",
      "../NCG_Dataset/training-data-master/natural_language_inference/38\n",
      "../NCG_Dataset/training-data-master/natural_language_inference/89\n",
      "../NCG_Dataset/training-data-master/natural_language_inference/15\n",
      "../NCG_Dataset/training-data-master/natural_language_inference/67\n",
      "../NCG_Dataset/training-data-master/text_generation/2\n",
      "../NCG_Dataset/training-data-master/text_generation/0\n"
     ]
    }
   ],
   "source": [
    "path = \"../NCG_Dataset/training-data-master/*/*\"\n",
    "directory = glob.glob(path)\n",
    "position = len(path)-3\n",
    "df = pd.DataFrame(columns = ['topic', 'paper_ID', 'text', 'main_heading', 'sub_heading', \n",
    "                             'label', 'pos1', 'pos2', 'pos3', 'citation'])\n",
    "ID_list, topic_list, text = [], [], []\n",
    "pos1, pos2, pos3, citations = [], [], [], []\n",
    "main_heading, sub_heading, labels = [], [], []\n",
    "length, count = -1, 0\n",
    "for dir_path in directory:\n",
    "    \n",
    "    # Directory of the paper\n",
    "    #print(dir_path)\n",
    "    \n",
    "    file = \"\".join((dir_path,\"/*-Stanza-out.txt\"))\n",
    "    stanza_file = glob.glob(file)[0]\n",
    "    grobid_file = stanza_file.replace(\"Stanza\", \"Grobid\")\n",
    "    sent_file = \"\".join((dir_path,\"/sentences.txt\"))\n",
    "    main_file = \"\".join((dir_path,\"/main_heading.txt\"))\n",
    "    sub_file = \"\".join((dir_path,\"/sub_heading.txt\"))\n",
    "    \n",
    "    # Reading sentences from stanza file\n",
    "    with open(stanza_file, encoding='ISO-8859-1') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    \n",
    "    # Extracting Topic and Paper_ID of the current paper\n",
    "    pos = dir_path.find('/', position)\n",
    "    topic = dir_path[position:pos]\n",
    "    ID = dir_path[pos+1:len(dir_path)]\n",
    "    \n",
    "    # Reading Integers (sentence number) from sentence.txt\n",
    "    sent = read_integers(sent_file)\n",
    "    \n",
    "    # Check if main heading file exists\n",
    "    if (os.path.isfile(main_file)):\n",
    "        # Reading Main Heading from main_heading.txt\n",
    "        with open(main_file, encoding='ISO-8859-1') as f:\n",
    "            headings = f.readlines()\n",
    "        # Reading Sub Heading from sub_heading.txt\n",
    "        with open(sub_file, encoding='ISO-8859-1') as f:\n",
    "            sub_headings = f.readlines()\n",
    "        # If heading file conain only title and abstract\n",
    "        if(len(headings) <= 4):\n",
    "            headings = sub_headings = default\n",
    "            print(dir_path)\n",
    "            count += 1\n",
    "    else:\n",
    "        headings = sub_headings = default\n",
    "        print(dir_path)\n",
    "        count += 1\n",
    "        \n",
    "    #Remove end of line from each line\n",
    "    headings = [head.strip() for head in headings]\n",
    "    sub_headings = [head.strip() for head in sub_headings]\n",
    "    \n",
    "    #Add paper_id and topic to list\n",
    "    ID_list.extend([ID]*len(lines))\n",
    "    topic_list.extend([topic]*len(lines))\n",
    "    \n",
    "    main_head, sub_head = \"\", \"\"\n",
    "    # Enumerating lines in stanza file\n",
    "    for i, line in enumerate(lines):\n",
    "        \n",
    "        # Check whether sentence contain a citation\n",
    "        citation = 1 if re.findall(r'\\(([^)]+)?(?:19|20)\\d{2}?([^)]+)?\\)', line) else 0\n",
    "        citation = 1 if re.findall(r'et al', line) else citation\n",
    "        \n",
    "        # Check whether sentence is a contribution sentence\n",
    "        label = 1 if (i+1 in sent) else 0\n",
    "        \n",
    "        head_flag, sub_flag = False, False\n",
    "        \n",
    "        # Check whether this sentence is a heading\n",
    "        for head in headings:\n",
    "            flag, temp_list = compare(head, line)\n",
    "            if(flag and len(temp_list)):\n",
    "                main_head = \" \".join(temp_list)\n",
    "                head_flag = True\n",
    "                break\n",
    "        \n",
    "        # Check whether this sentence is a sub heading\n",
    "        for head in sub_headings:\n",
    "            flag, temp_list = compare(head, line)\n",
    "            if(flag and len(temp_list)):\n",
    "                sub_head = \" \".join(temp_list)\n",
    "                sub_flag = True\n",
    "                break\n",
    "        \n",
    "        #print(i, head_flag, sub_flag)\n",
    "        #print(line)\n",
    "        \n",
    "        # Adding current sentence data to a list\n",
    "        text.append(line)\n",
    "        pos1.append(i+1)\n",
    "        labels.append(label)\n",
    "        citations.append(citation)\n",
    "        main_heading.append(main_head)\n",
    "        sub_heading.append(sub_head)\n",
    "        \n",
    "        # If sentence is a main_heading\n",
    "        if (head_flag == True):\n",
    "            pos2.append(1)\n",
    "            pos3.append(1)\n",
    "        else:\n",
    "            \n",
    "            pos2.append(pos2[length]+1)\n",
    "            # If sentence is a sub_heading\n",
    "            if (sub_flag == True):\n",
    "                pos3.append(1)\n",
    "            else:\n",
    "                pos3.append(pos3[length]+1)\n",
    "        length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55200\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(length)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"topic\": topic_list, \"paper_ID\" : ID_list, \"main_heading\" : main_heading, \n",
    "                   \"sub_heading\" : sub_heading, \"pos1\" : pos1, \"pos2\" : pos2, \"pos3\" : pos3, \n",
    "                   \"text\" : text, \"citation\" : citations, \"labels\" : labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrounding(df):\n",
    "    text = df['text'].tolist()\n",
    "    pos3 = df['pos3'].tolist()\n",
    "    prev_text = []\n",
    "    next_text = []\n",
    "    for i, x in enumerate(pos3):\n",
    "        if x > 2:\n",
    "            prev_text.append(text[i-1])\n",
    "        else:\n",
    "            prev_text.append(\" \")\n",
    "        if x < 3:\n",
    "            next_text.append(\" \")\n",
    "        else:\n",
    "            next_text.append(text[i])\n",
    "    next_text.pop(0)\n",
    "    next_text.append(\" \")\n",
    "    df['prev_text'] = prev_text\n",
    "    df['next_text'] = next_text\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = surrounding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = [\"citation\", \"labels\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(['topic', 'paper_ID', 'pos1', 'pos2', 'pos3'], ascending=[True, True, False, False, False], inplace=True)\n",
    "\n",
    "p1, p2, p3 = 1,1,1\n",
    "l1, l2, l3 = 1,1,1\n",
    "ofs1 = []\n",
    "ofs2 = []\n",
    "ofs3 = []\n",
    "for index, row in df.iterrows():\n",
    "    if(row['pos1']>=p1):\n",
    "        l1 = row['pos1']\n",
    "    p1 = row['pos1']\n",
    "    ofs1.append(p1/l1)\n",
    "    if(row['pos2']>=p2):\n",
    "        l2 = row['pos2']\n",
    "    p2 = row['pos2']\n",
    "    ofs2.append(p2/l2)\n",
    "    if(row['pos3']>=p3):\n",
    "        l3 = row['pos3']\n",
    "    p3 = row['pos3']\n",
    "    ofs3.append(p3/l3)\n",
    "\n",
    "df['ofs1'] = ofs1\n",
    "df['ofs2'] = ofs2\n",
    "df['ofs3'] = ofs3\n",
    "\n",
    "df.sort_values(['topic', 'paper_ID', 'pos1', 'pos2', 'pos3'], ascending=[True, True, True, True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Dataset_TaskA/1st_Jan/NCG_Dataset.csv\", index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
