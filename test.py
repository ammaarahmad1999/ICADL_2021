# -*- coding: utf-8 -*-
"""MultiTasking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pecbkB6vEsVxdGjrztZdmqLz120tZ0-2
"""

#from google.colab import drive
#drive.mount('/content/drive')

#!pip install transformers

import numpy as np
import pandas as pd
import torch
import os
import random
import math
import torch.nn as nn
import transformers

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from transformers import AutoTokenizer, AutoModel
from transformers import AdamW, get_polynomial_decay_schedule_with_warmup

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print(device)

seed_val = 0
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)
torch.backends.cudnn.benchmark = False
#torch.use_deterministic_algorithms(True)
torch.backends.cudnn.deterministic = True


label_dict = {'result': 0, 'background': 1, 'method': 2, 'introduction': 3, 'abstract': 4, 'title': 5}
print(label_dict)
length = len(label_dict)

"""# Import BERT Model and BERT Tokenizer"""

tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')
bert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')


"""#Define Model Architecture"""

class SciBERT_Classifier(nn.Module):

    def __init__(self, bert):
      
      super(SciBERT_Classifier, self).__init__()

      self.bert = bert
      self.drop = nn.Dropout(0.2)
      
      #Main Task
      self.fc1 = nn.Linear(768, 768)
      self.fc2 = nn.Linear(768, 2)
      
      #Section Identifcation
      self.fc4 = nn.Linear(768, 768)
      self.fc5 = nn.Linear(768, length)
      
      #Citation Worthiness
      self.fc6 = nn.Linear(768, 768)
      self.fc7 = nn.Linear(768, 2)
      
      #Activation Function
      self.act = nn.Tanh()

    #define the forward pass
    def forward(self, batch):

      #pass the inputs to the model  
      output1 = self.bert(batch[0], attention_mask=batch[1])
      pooled_output1 = output1[1]
      pooled_output1 = self.drop(pooled_output1)
      
      output1 = self.fc1(pooled_output1)
      output1 = self.act(output1)
      output1 = self.drop(output1)
      output1 = self.fc2(output1)
      
      return output1
# pass the pre-trained BERT to our define architecture
model = SciBERT_Classifier(bert)
# push the model to GPU
model = model.to(device)

def evaluate(val_dataloader):
  
  print("\nEvaluating...")
  
  # deactivate dropout layers
  model.eval()
  main_loss, section_loss, citation_loss, position_loss, total_accuracy = 0, 0, 0, 0, 0
  
  # empty list to save the model predictions
  predictions = []
  label = []
  
  # iterate over batches
  for step,batch in enumerate(val_dataloader):
    
    # Progress update every 50 batches.
    if step % 50 == 0 and not step == 0: 
      # Report progress.
      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))

    # push the batch to gpu
    batch = [r.to(device) for r in batch]
    labels = batch[2]
    
    # deactivate autograd
    with torch.no_grad():
      
      # model predictions
      output1 = model(batch)

      output1 = torch.argmax(output1, axis = 1)
      output1 = output1.detach().cpu().numpy()
      labels = labels.detach().cpu().numpy()
      predictions.extend(output1)
      label.extend(labels)
  
  #returns the loss and predictions
  return predictions, label

def custom_f1_score(predictions, labels):
  tn, fp, fn, tp = 0,0,0,0
  fn = 161
  for i in range(len(labels)):
    if (labels[i]==1 and predictions[i]==1):
      tp += 1
    elif (labels[i] == 1):
      fn += 1
    elif (predictions[i] == 1):
      fp += 1
    else:
      tn += 1
  precision = tp/(tp+fp)
  recall = tp/(tp+fn)
  f1_score = 2*precision*recall/(precision+recall)
  return precision, recall, f1_score

from sklearn.metrics import f1_score

#checkpoint = torch.load('model_citation.pt')
#model.load_state_dict(checkpoint['model_state_dict'])
model.load_state_dict(torch.load('model_256.pt'))

max_seq = 256

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
#define a batch size
batch_size = 16

df_test = pd.read_csv("test_data.csv")
df_test = df_test[(df_test['section']!='others')]

testing_text = df_test['text'] + "#" + df_test['sub_heading'] +"#" + df_test['prev_text'] + "#" + df_test['next_text']
testing_labels = df_test['label']

# tokenize and encode sequences in the test set
test_text = tokenizer.batch_encode_plus(testing_text.tolist(), padding='max_length', max_length = max_seq, truncation = True, return_token_type_ids=False)

# for test set
test_input = torch.tensor(test_text['input_ids'])
test_attention = torch.tensor(test_text['attention_mask'])
test_label = torch.tensor(testing_labels.tolist())

# wrap tensors
test_data = TensorDataset(test_input, test_attention, test_label)
# dataLoader for test set
test_dataloader = DataLoader(test_data, batch_size=batch_size)

predictions, label = evaluate(test_dataloader)
#print(confusion_matrix(label, predictions))

from sklearn.metrics import f1_score
test_pre, test_rec, test_f1 = custom_f1_score(predictions, label)
print((f'Precision: {test_pre}, Recall: {test_rec}, F1 Score: {test_f1}'))
print(confusion_matrix(label, predictions))
print(classification_report(label, predictions))

df_test['predict'] = predictions
df_test.to_csv("test_results.csv", index = None)
